<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 147]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.HC](#cs.HC) [Total: 21]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.RO](#cs.RO) [Total: 52]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 该研究构建了一个基于运动捕捉数据的交互模型，通过部分模仿和创造性增强输入的运动序列来生成人工舞伴。这是首个利用单人运动数据和高层特征实现此目标的模型，结合了扩散模型、运动修复和运动风格转换技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在创造性任务中支持用户，但缺乏具身交互特性。舞蹈作为人类表达的原始形式，能够补充这种体验，探索以舞蹈为例的创造性人机交互。

Method: 结合两种扩散模型、运动修复和运动风格转换的思想，利用单人运动数据和高层特征，生成既具有时间连贯性又能响应选定运动参考的运动表示。

Result: 通过定量评估生成样本特征分布与测试集的收敛性来证明模型成功，生成结果既多样化（显示与人类舞伴的各种偏差）又显得真实。

Conclusion: 该模型的生成结果是实现与AI创造性舞蹈共舞的第一步，为具身人机交互提供了新的可能性。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的珊瑚白化分类系统，使用包含健康和白化珊瑚的全球数据集，比较了ResNet、ViT和CNN三种模型，其中CNN模型取得了88%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁对海洋生态系统和海岸保护至关重要，但面临污染、海洋酸化和海水温度异常等威胁，需要高效的监测和保护方法。

Method: 使用包含不同环境条件下健康和白化珊瑚样本的全球数据集，对ResNet、Vision Transformer和CNN三种最先进的模型进行基准测试和比较，并进行全面的超参数调优。

Result: 经过超参数调优后，CNN模型达到了88%的最高准确率，优于现有的基准模型。

Conclusion: 研究结果为自主珊瑚监测提供了重要见解，并对最广泛使用的计算机视觉模型进行了全面分析。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 评估YOLOv8深度学习管道用于自动化肯尼亚和坦桑尼亚视频样带中鱼类科级识别的性能，为西印度洋珊瑚礁监测提供首个区域特定基准。


<details>
  <summary>Details</summary>
Motivation: 西印度洋珊瑚礁监测受到水下视觉普查劳动需求的限制，需要自动化解决方案来提高监测效率。

Method: 使用YOLOv8深度学习管道，在肯尼亚和坦桑尼亚收集的视频样带上测试24个鱼类科的识别，评估不同配置下的性能。

Result: 最佳模型达到mAP@0.5为0.52，对丰富鱼类科识别准确率高，但对稀有或复杂类群的检测较弱。

Conclusion: 深度学习有潜力作为传统监测方法的可扩展补充工具。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的数据增强方法，通过选择在自然扰动下具有高互信息的场景补丁作为正样本，用于对比学习中的表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的InfoNCE损失方法虽然减少了人工标注需求，但数据选择和增强仍依赖人工假设或工程，可能不是最优的。需要探索基于真实世界分布互信息的数据选择方法。

Method: 通过计算真实世界分布下的互信息，选择在自然扰动（如颜色变化和运动）下具有高互信息的场景补丁作为正样本，用于对比损失学习。

Result: 在多个基准测试和最先进的表示学习框架上评估了该方法，证明了其有效性。

Conclusion: 基于互信息的数据增强方法是一个有前景的研究方向，能够提升学习特征在开放环境中的泛化能力。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: 提出了Med-Banana-50K数据集，这是一个包含5万张图像的医疗图像编辑数据集，涵盖三种模态和23种疾病类型，通过Gemini-2.5-Flash-Image生成双向编辑，并采用医学质量控制和迭代优化确保质量。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医疗图像编辑方面取得显著进展，但研究社区缺乏大规模、高质量、开放可访问的专门针对医疗图像编辑的数据集，且需要满足严格的解剖学和临床约束。

Method: 利用Gemini-2.5-Flash-Image从真实医疗图像生成双向编辑（病灶添加和移除），采用LLM-as-Judge医学评估标准和历史感知迭代优化（最多5轮），确保编辑质量。

Result: 构建了包含5万张图像的数据集，涵盖胸部X光、脑部MRI和眼底摄影三种模态，23种疾病类型，还包括3.7万次失败尝试的完整对话记录。

Conclusion: Med-Banana-50K为训练和评估下一代医疗图像编辑模型奠定了基础，提供了大规模、医学验证且完全文档化的资源。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [6] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 本研究对三个主流联邦学习框架（NVIDIA FLARE、Flower和Owkin Substra）在医学影像应用中的适用性进行了基准测试，评估了模型性能、收敛效率、通信开销、可扩展性和开发者体验。


<details>
  <summary>Details</summary>
Motivation: 联邦学习已成为医学AI中的变革性范式，能够在机构间进行协作模型训练而无需直接共享数据。本研究旨在评估不同FL框架在真实医疗环境中的适用性。

Method: 使用PathMNIST数据集，对三个FL框架（NVIDIA FLARE、Flower和Owkin Substra）进行基准测试，评估多个关键指标。

Result: NVIDIA FLARE在生产可扩展性方面表现优异，Flower在原型设计和学术研究方面提供灵活性，Owkin Substra在隐私和合规性方面表现突出。

Conclusion: 每个框架都针对不同的使用场景优化了各自的优势，强调了它们在医疗环境实际部署中的相关性。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [7] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: SEPS框架通过整合密集和稀疏文本的统一语义来解决视觉-语言细粒度对齐中的补丁冗余和模糊性问题，采用两阶段机制识别显著视觉补丁，并通过相关性感知选择提升跨模态相似性评估。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理跨模态信息密度差异导致的补丁冗余和模糊性方面存在挑战，MLLMs的密集文本输出可能与原始稀疏描述产生冲突，准确量化视觉补丁与文本描述之间的语义相关性是核心难题。

Method: 提出语义增强补丁精简框架，采用两阶段机制整合密集和稀疏文本的统一语义来识别显著视觉补丁，利用相关性感知选择和均值计算突出关键补丁-单词对应关系。

Result: 在Flickr30K和MS-COCO数据集上的实验验证SEPS实现了优越性能，在不同模型架构下rSum指标超越现有方法23%-86%，在文本到图像检索场景中表现显著提升。

Conclusion: SEPS框架通过系统解决补丁冗余和模糊性问题，有效提升了视觉-语言细粒度对齐的性能，为多模态应用提供了更精确的局部对应关系。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [8] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 本文对水稻叶片图像进行了图像去噪与对比度增强方法的比较研究，结合CLAHE技术评估不同去噪方法的效果。


<details>
  <summary>Details</summary>
Motivation: 图像增强是图像处理链中的关键预处理阶段，对于水稻叶片分析尤为重要，有助于疾病检测、营养缺乏评估和生长分析。

Method: 采用知名的图像去噪方法与CLAHE（对比度受限自适应直方图均衡化）相结合，在水稻叶片图像数据集上进行实验，并使用多种指标全面测试增强方法。

Result: 通过实验验证了不同去噪方法结合CLAHE在水稻叶片图像处理中的效果，为评估数字图像处理方法提供了坚实基础。

Conclusion: 该方法为数字图像处理方法有效性评估提供了可靠基础，并为农业研究和其他领域的未来应用提供了有价值的见解。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [9] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: SurgVeo是首个用于手术视频生成模型评估的专家策划基准，结合手术合理性金字塔框架，揭示了Veo-3模型在手术AI中视觉模仿与因果理解之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 视频生成基础模型在模拟物理世界方面表现出色，但在需要深度专业因果知识的高风险领域如手术中的应用仍未被探索。

Method: 提出SurgVeo基准和手术合理性金字塔框架，使用Veo-3模型对腹腔镜和神经外科手术片段进行零样本预测任务，由四位认证外科医生按照SPP评估生成视频。

Result: 发现明显的"合理性差距"：Veo-3在视觉感知合理性方面表现优异，但在更高层次的器械操作合理性、环境反馈合理性和手术意图合理性方面严重失败。

Conclusion: 这项工作首次量化证明了手术AI中视觉可信模仿与因果理解之间的鸿沟，为开发能够应对专业医疗领域复杂性的未来模型奠定了关键基础。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [10] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 该论文研究了不同LiDAR扫描模式对路边感知系统性能的影响，通过CARLA仿真环境创建了InfraLiDARs' Benchmark数据集，发现非重复扫描LiDAR与128线重复扫描LiDAR在检测性能上相当，为非重复扫描LiDAR作为成本效益高的选择提供了依据。


<details>
  <summary>Details</summary>
Motivation: 虽然已有大量研究关注基础设施LiDAR的最优部署，但不同LiDAR扫描模式对感知性能的深远影响仍相对缺乏研究。传统重复扫描（机械/固态）与新兴非重复扫描（如棱镜式）系统在不同距离下产生不同的点云分布，这直接影响物体检测效果和环境理解能力。

Method: 在CARLA仿真环境中创建了InfraLiDARs' Benchmark数据集，使用同时运行的基础设施LiDAR采集数据，涵盖两种扫描模式。基于此基准，进行了全面的统计分析，评估了不同扫描模式对多种领先3D物体检测算法性能的影响。

Result: 研究发现非重复扫描LiDAR和128线重复扫描LiDAR在各种场景下表现出相当的检测性能。尽管非重复扫描LiDAR的感知范围有限，但考虑到其较低的价格，是一种成本效益高的选择。

Conclusion: 本研究为设置具有最优LiDAR扫描模式和兼容算法的路边感知系统提供了见解，并公开发布了InfraLiDARs' Benchmark数据集以促进进一步研究。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [11] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5是Cosmos世界基础模型的最新版本，基于流式架构统一了文本、图像和视频到世界的生成，并通过Cosmos-Reason1提供更丰富的文本基础和更精细的世界模拟控制。该模型在200M精选视频剪辑上训练，并通过强化学习后训练优化，在视频质量和指令对齐方面相比前代有显著提升，提供2B和14B两种规模。


<details>
  <summary>Details</summary>
Motivation: 开发更可靠的合成数据生成、策略评估以及机器人和自主系统的闭环仿真工具，推动物理AI和具身智能的发展。

Method: 采用基于流式的架构，统一文本、图像和视频到世界的生成；利用Cosmos-Reason1物理AI视觉语言模型提供文本基础和控制；在200M精选视频剪辑上进行训练，并通过强化学习进行后训练优化。

Result: 相比Cosmos-Predict1，在视频质量和指令对齐方面取得显著改进；Cosmos-Transfer2.5虽然比前代小3.5倍，但提供了更高的保真度和鲁棒的长时程视频生成能力。

Conclusion: Cosmos-Predict2.5和Cosmos-Transfer2.5成为扩展具身智能的多功能工具，通过开源代码、预训练检查点和精选基准测试，降低采用门槛并促进物理AI领域的创新。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [12] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 使用深度学习进行高山栖息地变化检测，比较了后分类变化检测和直接变化检测两种方法，发现Clay v1.0模型在复杂高山环境中表现最佳，准确率达到51%，LiDAR数据集成可将准确率从30%提升至50%。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统面临快速气候变化和其他干扰，需要频繁的栖息地监测，但人工测绘成本过高。本研究旨在填补地理空间基础模型在复杂自然环境应用中的空白，解决类别边界模糊和类别高度不平衡的问题。

Method: 在奥地利Gesaeuse国家公园使用长期高山栖息地数据，比较两种变化检测范式：后分类变化检测（评估Prithvi-EO-2.0、Clay v1.0和U-Net CNN）和直接变化检测（测试ChangeViT和U-Net基线）。使用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性），覆盖15.3平方公里内的4,480个记录变化。

Result: Clay v1.0在多类栖息地变化检测中达到51%的总体准确率，优于U-Net的41%；在二元变化检测中两者均达到67%。直接变化检测在二元检测中IoU更高（0.53 vs 0.35），但在多类检测中准确率仅为28%。跨时间评估显示GFM更具鲁棒性，Clay在2020年数据上保持33%准确率，而U-Net为23%。集成LiDAR可将语义分割准确率从30%提升至50%。

Conclusion: 虽然总体准确率低于更均质景观，但反映了复杂高山栖息地的真实性能。未来工作将集成基于对象的后处理和物理约束以增强适用性。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [13] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa是一个无需训练的高效扩散视频生成加速框架，通过词典最小最大路径优化策略限制最坏路径误差，在提升推理速度的同时改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略主要关注减少局部启发式误差，但忽略了全局误差累积，导致加速视频与原始视频之间存在明显内容退化。

Method: 将缓存调度建模为带误差权重边的有向图，引入词典最小最大路径优化策略明确限制最坏路径误差。

Result: 在多个文本到视频基准测试中，LeMiCa在推理速度和生成质量上实现双重提升：在Latte模型上达到2.9倍加速，在Open-Sora上LPIPS得分为0.05，优于现有缓存技术。

Conclusion: LeMiCa为加速扩散视频生成提供了一个稳健且可泛化的范式，在最小感知质量损失下实现显著性能提升，可作为未来高效可靠视频合成研究的基础。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [14] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD是一个三阶段即插即用框架，通过残差强化学习和分布感知数据收集来改进视觉语言动作模型，无需依赖昂贵的人工演示，在多个任务上实现了接近饱和的成功率。


<details>
  <summary>Details</summary>
Motivation: 监督微调依赖昂贵的人工演示，限制了视觉语言动作模型的可扩展性和泛化能力，需要一种更高效的改进方法。

Method: 提出三阶段框架：1)训练轻量级残差执行器探测VLA通用模型的失败区域；2)使用混合rollout方案收集与通用模型部署分布对齐的轨迹；3)通过标准SFT将筛选的轨迹蒸馏回通用模型。

Result: 在LIBERO上达到接近饱和的99%任务成功率，在SimplerEnv上提升超过50%，在真实世界Franka和YAM机械臂操作任务上实现100%成功率。

Conclusion: 残差探测和分布感知回放是收集部署对齐数据的关键，为自改进VLA模型提供了可扩展的路径。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [15] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM-R1系统，结合微调SAM和DeepSeek-R1，用于脊柱CT图像分割，通过解剖学引导注意力机制和自然语言交互提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT图像分割面临对比度低、椎体边界复杂等挑战，现有模型如SAM在脊柱CT领域表现受限，需要高标注成本和领域适应性差的问题。

Method: 提出SpinalSAM-R1多模态视觉语言交互系统，集成微调SAM和DeepSeek-R1，引入解剖学引导注意力机制和语义驱动交互协议，使用LoRA进行高效微调。

Result: 在脊柱解剖结构CT图像上验证，获得优越分割性能，开发基于PyQt5的交互软件，支持点、框和文本提示，实现94.3%解析准确率和低于800ms响应时间。

Conclusion: SpinalSAM-R1系统有效解决了脊柱CT图像分割的挑战，提供了高效准确的解决方案，并开发了实用的交互软件工具。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [16] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种无需训练、模态无关的方法，通过在推理过程中部分估计概念滑块公式，实现跨模态的细粒度概念控制生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像、音频和视频生成方面已成为最先进技术，但实现细粒度可控生成（即在不干扰无关内容的情况下持续控制特定概念）仍然具有挑战性。现有概念滑块方法需要每个概念的训练和特定架构微调，限制了新模态的可扩展性。

Method: 提出FreeSliders方法，完全无需训练且模态无关，通过在推理过程中部分估计概念滑块公式来实现。还引入两阶段程序自动检测饱和点并重新参数化遍历，实现感知均匀、语义有意义的编辑。

Result: 广泛的实验表明，该方法支持即插即用、无需训练的概念控制，改进现有基线，并为原则性可控生成建立了新工具。将概念滑块基准扩展到视频和音频，建立了首个多模态细粒度概念生成控制套件。

Conclusion: FreeSliders方法实现了跨模态的即插即用、无需训练的概念控制，改进了现有基线，并为原则性可控生成建立了新工具，同时提出了评估属性和新指标来提升评估质量。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [17] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI是一个新颖的分层框架，通过组合场景解析、时空注意力机制和渐进式视频细化，实现了高质量的文本到视频生成，在多个指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在保持时间一致性、组合理解和视觉叙事细粒度控制方面存在困难，需要更先进的框架来解决这些问题。

Method: 提出三个关键创新：组合场景解析器将文本描述分解为带时间标注的分层场景图；时空注意力机制确保帧间连贯运动动态同时保留空间细节；渐进式视频细化模块通过多尺度时间推理迭代提升视频质量。

Result: 在标准基准测试中，MOVAI实现了最先进性能，LPIPS指标提升15.3%，FVD指标提升12.7%，用户偏好研究提升18.9%。

Conclusion: MOVAI框架在生成复杂多对象场景方面表现出色，能够实现真实的时间动态和细粒度语义控制。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [18] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出了一种名为"Chain of Time"的认知启发方法，通过在模拟过程中生成一系列中间图像来改进和解释视觉语言模型中的物理模拟，无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 受机器学习中的上下文推理和人类心理模拟启发，旨在提升视觉语言模型在物理模拟方面的性能，并揭示其内部模拟动态。

Method: 在推理时生成模拟过程中的中间图像序列，应用于2D图形模拟和真实3D视频，测试速度、加速度、流体动力学和动量守恒等物理属性。

Result: Chain-of-Time方法显著提升了最先进图像生成模型的性能，分析揭示了模型能够模拟随时间展开的物理属性（如速度、重力和碰撞），但也发现模型在某些情况下难以从输入图像推断特定物理参数。

Conclusion: 该方法不仅提高了物理模拟性能，还提供了对图像生成模型内部物理推理过程的深入洞察，揭示了传统评估方法无法发现的模拟能力。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [19] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出了首个集成生成AI和深度强化学习的端到端框架，用于实现自主、可重复的心脏超声扫描，解决传统方法依赖操作者、缺乏可重复性等问题。


<details>
  <summary>Details</summary>
Motivation: 心脏超声诊断存在操作者依赖性强、时间限制、人为错误等问题，且偏远地区缺乏专业医生。现有AI方法缺乏可重复性、依赖专有数据、使用简化模型，需要开发自主、可重复的解决方案。

Method: 框架包含两个组件：(1) 条件生成模拟器，结合GAN和VAE生成逼真的动作条件图像；(2) 深度强化学习模块，利用模拟器学习自主、准确的扫描策略。

Result: VAE-GAN在性能评估中优于现有GAN变体，DRL扫描系统在不同配置下都表现出有效性，并发布了公开可用的真实心脏超声数据集确保可重复性。

Conclusion: 该框架通过专家验证的模型提供AI驱动指导，支持生成逼真超声图像，建立了可扩展到其他器官的可重复基础，为解决心脏超声扫描的挑战提供了有效方案。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [20] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D是一种新颖的双流架构，利用RGB-D输入的视觉和几何数据优势，通过集成自监督视觉变换器和PointNet++编码器，实现鲁棒且精确的6D物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中6D物体姿态精确计算的挑战，特别是当前方法在从合成数据泛化到真实世界场景时，面对光照变化、无纹理物体和严重遮挡时的脆弱性问题。

Method: 采用双流架构：使用自监督视觉变换器(DINOv2)处理RGB模态，利用其预训练的视觉理解能力；同时使用PointNet++编码器处理深度数据生成的3D点云。两种互补特征流有效融合后输入多任务预测头。

Result: 在具有挑战性的Occluded-LineMOD数据集上获得了新的SOTA性能，验证了其卓越的鲁棒性和准确性。

Conclusion: VLM6D通过集成视觉和几何数据的优势，成功解决了6D物体姿态估计在真实世界场景中的鲁棒性和精度问题。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [21] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 提出了一种结合ConvNeXt和Vision Transformers的混合架构用于面部年龄估计，在多个基准数据集上取得了优异的性能表现。


<details>
  <summary>Details</summary>
Motivation: 年龄估计是计算机视觉中的复杂挑战，需要结合CNN的局部特征提取能力和Transformer的全局注意力机制来提升性能。

Method: 使用ConvNeXt-ViT混合架构，利用预训练模型，通过线性层和高级正则化技术优化架构，并采用适应的注意力机制来关注年龄相关特征。

Result: 在MORPH II、CACD和AFAD数据集上取得了优越的MAE性能，超越了传统方法，并通过消融研究验证了各组件的重要性。

Conclusion: ConvNeXt-ViT混合架构不仅性能优异，还为年龄估计和相关视觉任务提供了稳健基础，展示了混合架构在计算机视觉中的变革潜力。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [22] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC是一个基于设施位置函数的高效视觉标记压缩框架，通过选择紧凑且具有代表性的视觉标记子集来解决长视频理解中的标记数量过多问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中的大型多模态模型面临视觉标记数量过多的可扩展性限制，需要高效的压缩方法来减少标记数量同时保持性能。

Method: 基于设施位置函数原理，使用懒惰贪心算法快速选择紧凑且多样化的视觉标记子集，在预定义标记预算内实现高效压缩。

Result: 在Video-MME、MLVU和LongVideoBench等大规模基准测试中，FLoC框架持续超越现有压缩技术，在有效性和处理速度方面表现优异。

Conclusion: FLoC提供了一个无需训练、模型无关且查询无关的通用解决方案，能够无缝集成到各种视频LLM和现有工作流程中，有效解决长视频理解的关键挑战。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [23] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 本文提出了一种增强图像保护方法对抗噪声逆转技术鲁棒性的简单方法，通过自适应区域高斯模糊调整噪声频率谱，提高现有方法在多种图像编辑场景下的保护性能。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型的发展，恶意图像编辑威胁增加。现有的保护性对抗噪声方法容易被简单技术（如JPEG压缩）逆转，因此需要开发既不可察觉又不可逆的保护方法。

Method: 提出一种简单方法，通过自适应区域高斯模糊处理噪声来调整整体频率谱，增强图像保护方法对抗噪声逆转技术的鲁棒性。

Result: 实验表明，该方法能一致提高现有方法在多种图像编辑场景下对抗各种逆转技术的保护性能，同时减少噪声引起的质量退化。

Conclusion: 通过自适应高斯模糊调整噪声频率谱，可以有效增强图像保护方法的鲁棒性，为对抗恶意图像编辑提供更实用的解决方案。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [24] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: 提出AI生成图像检测新框架AIFo，通过多智能体协作模拟人类取证调查，在6000张图像测试中达到97.05%准确率，显著优于传统分类器和先进视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速发展对信息完整性和媒体真实性构成挑战，现有检测方法存在可解释性差、泛化能力弱、仅限于单次分析和像素级推理等局限性。

Method: 开发训练免费的AIFo框架，采用多智能体协作方式，整合反向图像搜索、元数据提取、预训练分类器和VLM分析等取证工具，通过专门设计的LLM智能体收集、综合和推理跨源证据，并引入结构化多智能体辩论机制和记忆增强推理模块。

Result: 在包含现代生成平台和多样化在线来源的6000张图像综合评估中，AIFo达到97.05%的准确率，显著优于传统分类器和最先进的视觉语言模型。

Conclusion: 基于智能体的程序化推理为AI生成图像检测提供了更鲁棒、可解释和适应性更强的新范式。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [25] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 本文提出了一种基于能量的多提示学习方法（EMPL），通过从能量分布中生成多个提示嵌入，实现了视觉语言预训练模型的高效参数适应和泛化平衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单提示学习范式，很少探索多提示学习的技术潜力。本文旨在为视觉语言多提示学习提供一个原则性回顾，并证明多提示增强在视觉语言迁移中的优越性。

Method: 提出基于能量的多提示学习（EMPL），通过从视觉语言模型隐式定义的能量分布中抽取实例来生成多个提示嵌入，实现参数高效且严格平衡领域内外泛化。

Result: 综合实验验证了作者的主张和EMPL方法的优越性，证明了多提示学习在视觉语言迁移中的有效性。

Conclusion: EMPL方法不仅参数高效，而且能够严格平衡领域内和领域外的开放词汇泛化，为视觉语言多提示学习提供了新的技术路径。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [26] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出了一种高效的迁移学习方法，用于地面终端组件本地检测天气相关条件，包括雪、潮湿和其他恶劣天气状况，性能优于传统深度学习方法且具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星互联网在恶劣天气下性能会显著下降，需要细粒度的天气条件检测能力来协助故障诊断和缓解，但现有解决方案缺乏且泛化能力不足。

Method: 采用高效的迁移学习方法，使地面组件能够本地检测代表性的天气相关条件，包括雪、潮湿和其他恶劣天气状况。

Result: 该方法在检测雪、潮湿和其他天气条件方面表现出色，性能优于YOLOv7、YOLOv9、Faster R-CNN和R-YOLO等典型深度学习方法。

Conclusion: 所提出的迁移学习方法不仅性能优越，而且具有良好的泛化能力，能够适应各种实际部署场景，为卫星互联网的可靠运行提供了有效的天气条件检测解决方案。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [27] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出基于VQ-VAE的电子显微镜数据压缩框架，支持16x到1024x压缩比，提供按需解码功能，并引入ROI驱动的工作流进行选择性高分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 海量电子显微镜数据集对存储、传输和下游分析提出了挑战，需要高效的压缩解决方案。

Method: 使用向量量化变分自编码器(VQ-VAE)进行压缩，结合Transformer先验模型预测底层token，通过特征线性调制(FiLM)和连接恢复纹理，并采用ROI驱动的工作流。

Result: 实现了从16x到1024x的可扩展压缩比，支持仅顶层解码的极端压缩，以及选择性高分辨率重建能力。

Conclusion: 该框架为海量EM数据提供了高效的压缩和按需解码解决方案，显著降低了存储和传输需求。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [28] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD是一个基于视觉的放射学辅助框架，通过摄像头系统直接从显示器捕捉医学图像，绕过了与现有医院IT基础设施集成的障碍。该系统通过自动检测、恢复和分析屏幕上的医学图像，将摄像头捕捉的视觉数据转换为适合自动化分析和报告生成的诊断质量图像。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助诊断系统在临床广泛部署受到与现有医院IT基础设施集成挑战的阻碍。VisionCAD旨在通过摄像头系统直接捕捉显示器上的医学图像，避免对现有基础设施的修改，使AI辅助诊断在多样化临床环境中更易部署。

Method: VisionCAD采用模块化架构，通过自动化流水线检测、恢复和分析屏幕上的医学图像。该系统使用摄像头捕捉显示器上的医学图像，然后将这些视觉数据转换为诊断质量的图像，供先进的诊断模型进行特定任务分析。

Result: 在多样化医学影像数据集上的验证显示，VisionCAD的诊断性能与传统CAD系统在原始数字图像上的表现相当。分类任务的F1分数下降通常小于2%，自动报告的自然语言生成指标与原始图像相比保持在1%以内。

Conclusion: VisionCAD仅需摄像头设备和标准计算资源，为AI辅助诊断提供了一种可访问的方法，能够在多样化临床环境中部署诊断能力，无需修改现有基础设施。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [29] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: GUI-AIMA是一个基于注意力机制的无坐标GUI定位框架，通过监督微调激活MLLMs的固有定位能力，在3B参数模型上仅用85k截图训练就达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLMs的GUI定位方法将任务视为基于文本的坐标生成，但直接从视觉输入生成精确坐标具有挑战性且计算量大。直觉方法应是先选择相关视觉块再确定精确点击位置。

Method: 提出GUI-AIMA框架，将MLLMs的内在多模态注意力与基于块的定位信号对齐。通过多头聚合简化查询-视觉注意力矩阵来适应不同用户指令计算定位信号，无坐标方式便于集成即插即用的放大阶段。

Result: GUI-AIMA-3B仅用85k截图训练，在ScreenSpot-Pro上达到58.6%平均准确率，在OSWorld-G上达到62.2%平均准确率，在3B模型中表现最优。

Conclusion: 轻量训练可以触发MLLMs的固有定位能力，GUI-AIMA在数据效率和性能方面都表现出色，验证了基于注意力的无坐标GUI定位方法的有效性。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [30] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯表示和运动扩散先验的对象感知4D人体运动生成框架MSDI，通过运动扩散分数蒸馏采样和大型语言模型实现零样本的物理合理人体运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型生成的运动存在不现实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。

Method: 使用预生成的3D人体和对象，结合MSDS从预训练运动扩散模型蒸馏分数梯度，并利用LLMs的空间和语义信息进行空间感知运动优化。

Result: 实验表明该方法能生成自然且物理合理的人体运动，尊重3D空间上下文，无需在有限交互数据集上重新训练。

Conclusion: 该方法为逼真的4D生成提供了可扩展的解决方案，能泛化到分布外对象感知人体运动。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [31] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文介绍了L48数据集，这是一个细粒度的真实世界多标签数据集，用于单正多标签学习，并评估了现有方法在该数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的单正多标签方法在从完全标注数据集随机采样单正标签的合成数据集上开发和基准测试，但这不能反映真实世界场景，也无法捕捉可能导致困难错误分类的细粒度复杂性。

Method: 引入L48数据集，这是一个基于鸟类声音记录的细粒度真实世界多标签数据集，提供自然的单正多标签设置以及两个扩展设置，其中领域先验提供了额外的负标签。

Result: 在L48数据集上对现有单正多标签方法进行基准测试，观察到与合成数据集相比存在显著的性能差异，并分析了方法的弱点。

Conclusion: 需要更现实和困难的基准测试来推动单正多标签学习的发展。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [32] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 本文分析了自动语音驱动3D手势生成领域的人类评估实践，发现缺乏标准化和存在实验设计缺陷，导致无法比较不同方法的性能。作者提出了BEAT2数据集的人类评估协议，并对6个最新手势生成模型进行了大规模众包评估。


<details>
  <summary>Details</summary>
Motivation: 当前手势生成领域缺乏标准化的评估方法，实验设计存在缺陷，使得无法准确比较不同方法的性能，也无法确定技术发展水平。

Method: 引入详细的BEAT2数据集人类评估协议，通过大规模众包评估对6个最新手势生成模型进行排名，评估运动真实性和语音-手势对齐两个关键维度。

Result: 评估结果表明：1）新模型并不总是优于早期方法；2）已发表的高运动真实性或语音-手势对齐声明在严格评估下可能不成立；3）必须采用解耦的运动质量和多模态对齐评估才能准确基准测试。

Conclusion: 该领域需要采用标准化的评估协议，对运动质量和多模态对齐进行解耦评估，以推动技术进步。作者将发布合成运动数据、渲染视频刺激和人类偏好投票数据以促进标准化和新的评估研究。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [33] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 开发了一个3阶段自动化管道，用于处理大量甲虫图像，包括检测、裁剪和形态分割，旨在提高生物研究效率。


<details>
  <summary>Details</summary>
Motivation: 生物学研究中需要处理大量甲虫图像，传统手动方法效率低下，需要自动化处理大规模数据以加速研究。

Method: 使用基于transformer的开放词汇对象检测器和视觉语言模型进行迭代检测，然后对670张甲虫图像手动标注并微调基于transformer的分割模型进行精细分割。

Result: 构建了专门用于甲虫图像处理的集成深度学习管道，能够相对准确地完成甲虫检测和形态分割。

Conclusion: 该管道集成了多种深度学习方法，专门针对甲虫图像处理，可显著提高大规模甲虫数据处理效率，加速生物研究。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [34] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 提出一种结合冻结CLIP视觉变换器和轻量级变换器分类器的联邦学习框架，用于农业分类任务，在保护隐私的同时显著提升分类准确率和通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式训练中的隐私问题，以及标准联邦学习在非独立同分布数据和高通信成本方面的挑战。

Method: 使用预训练的CLIP ViT进行特征提取，仅对轻量级分类器进行联邦更新；共享1%的CLIP特征表示来缓解非独立同分布问题，同时确保隐私保护。

Result: 在农业分类任务上达到86.6%的准确率，比基线联邦学习方法高出4倍以上。

Conclusion: 结合视觉语言模型特征与联邦学习的框架在保护隐私和可扩展农业智能方面具有高效性和有效性。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [35] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 提出PersonalView方法，仅用100个训练样本即可为现有模型添加多视角生成能力，显著优于需要大量多视角数据训练的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成模型虽然能在不同场景下生成身份一致的图像，但无法控制生成图像的视角，也无法生成一致的多视角人物图像。

Method: 包含两个关键组件：1）利用预训练扩散变换器的上下文学习能力设计条件架构；2）通过语义对应对齐损失保持预训练模型的原始生成能力。

Result: 在100个训练样本下，PersonalView在多视角一致性、文本对齐、身份相似性和视觉质量方面显著优于需要大量多视角数据训练的基线方法。

Conclusion: PersonalView是一种轻量级适配方法，能够以极少的训练样本为现有模型赋予多视角生成能力，在保持原始生成能力的同时实现高质量的多视角一致性生成。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [36] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 本文介绍了LITHOS——一个用于自动化岩石学分析的大规模公开数据集和实验框架，包含211,604个高分辨率RGB偏振光图像块和105,802个专家标注的矿物颗粒，涵盖25种矿物类别。


<details>
  <summary>Details</summary>
Motivation: 岩石学分析是地质学中劳动密集型的任务，需要专家通过光学偏振显微镜进行详细视觉检查，这限制了可扩展性，因此需要开发自动化技术。

Method: 提出了双编码器transformer架构，整合两种偏振模态作为基线方法。评估了多种深度学习技术在LITHOS数据集上的矿物分类性能。

Result: 该方法在矿物分类任务中持续优于单偏振模型，证明了偏振协同在矿物分类中的价值。

Conclusion: LITHOS基准测试集（包括数据集、代码和预训练模型）已公开可用，以促进自动化岩石学分析的可重复性和进一步研究。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [37] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: 本研究系统评估了11种轻量级视觉模型在7个数据集上的跨域泛化能力，提出了Cross-Dataset Score (xScore)指标，发现ImageNet准确性不能可靠预测细粒度或医学数据集性能，并识别了促进泛化的关键架构组件。


<details>
  <summary>Details</summary>
Motivation: 当前轻量级视觉模型主要在ImageNet上评估，但缺乏对其跨域泛化能力的系统研究。需要量化模型在不同视觉领域的鲁棒性，并识别在资源受限条件下驱动泛化的架构要素。

Method: 在7个多样化数据集上，以固定100轮训练计划训练11种轻量级视觉模型(250万参数)，引入Cross-Dataset Score (xScore)作为统一指标来量化模型性能的一致性和鲁棒性。

Result: ImageNet准确性不能可靠预测细粒度或医学数据集性能；xScore仅需四个数据集即可估计移动模型性能；各向同性卷积配合更高空间分辨率和通道注意力促进泛化，而Transformer模块带来额外参数开销但增益有限。

Conclusion: 本研究提供了在ImageNet之外评估轻量级视觉模型的可复现框架，强调了移动友好架构的关键设计原则，为开发在多样化应用领域中鲁棒泛化的未来模型提供指导。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [38] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出了一种结合DeepONet和NTK的混合方法来解决复杂逆问题，包括Navier-Stokes方程控制的源定位和图像重建，克服了非线性、稀疏性和噪声数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决复杂逆问题中的非线性、稀疏性和噪声数据挑战，确保解具有物理一致性和准确性。

Method: 将Deep Operator Networks (DeepONet)与Neural Tangent Kernel (NTK)结合，在损失函数中融入物理约束和任务特定正则化。

Result: 在多种合成和真实数据集上的验证表明该方法具有鲁棒性、可扩展性和精确性。

Conclusion: 该方法在计算物理和成像科学领域具有广泛的应用潜力。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [39] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: FedDISC是一个联邦学习框架，用于解决多模态情感识别中模态缺失问题，通过扩散模型恢复缺失模态并保持语义一致性，在多种缺失模式下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据经常存在不可预测的模态缺失，传统方法依赖完整多模态数据进行训练，在极端数据分布下会出现语义失真问题。

Method: 提出FedDISC框架：1）联邦聚合客户端训练的模态特定扩散模型；2）DISC-Diffusion模块确保恢复模态与可用模态在上下文、说话人身份和语义上的一致性；3）交替冻结聚合策略循环冻结恢复和分类器模块以促进协作优化。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上的实验表明，FedDISC在多种缺失模态模式下实现了优越的情感分类性能，超越了现有方法。

Conclusion: FedDISC成功将联邦学习集成到缺失模态恢复中，克服了单客户端对模态完整性的依赖，同时保持了语义一致性，为现实世界多模态情感识别提供了有效解决方案。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [40] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型重建动态的AI生成图像检测方法，通过分析不同噪声强度下图像重建指标的变化来区分真实和合成图像。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测方法无法应对现代文本到图像系统（如Stable Diffusion和DALL-E）生成的无伪影逼真图像，需要新的检测技术。

Method: 利用多强度图像重建动态（扩散回弹）分析重建指标（LPIPS、SSIM、PSNR）随噪声强度的变化，提取可解释的流形特征来区分真实和合成图像。

Result: 在4000张图像的平衡数据集上，该方法在交叉验证下达到0.993 AUROC，对压缩和噪声等常见失真保持鲁棒性。

Conclusion: 该方法展示了强大的泛化能力和可解释性，为可扩展、模型无关的合成媒体取证提供了基础。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [41] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 该研究针对CubeSat热地球观测中的云分割任务，使用迁移学习和轻量级UNet架构，在NVIDIA Jetson Nano上实现5秒内全图像推理，显著提升云分割性能。


<details>
  <summary>Details</summary>
Motivation: CubeSat任务受限于硬件和光谱信息，通常只有单一热波段且缺乏标注数据，传统云掩码技术不可行，需要开发高效的热云分割方法。

Method: 使用UNet架构配合轻量级MobileNet编码器，在Landsat-7云覆盖评估数据集上预训练，然后通过联合训练方式在少量任务特定样本上微调，并将模型转换为TensorRT引擎。

Result: 与仅使用FOREST-2数据的基线相比，宏F1分数从0.850提升到0.877，在NVIDIA Jetson Nano上实现5秒内全图像推理。

Conclusion: 利用公共数据集和轻量级架构可以在轨道上实现准确高效的热云掩码，支持数据有限的地球观测任务中的实时决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [42] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D是一个免费的生成式AI框架，利用Google街景图像重建孟加拉国文化遗产的3D模型，解决了传统3D数字化方法成本高、技术要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产修复面临资源有限和技术专家稀缺的双重挑战，传统3D数字化方法成本高昂且需要专业设备和操作人员，导致许多建筑瑰宝无法得到数字化保护。

Method: 采用两阶段流程：使用Gemini 2.5 Flash Image进行多模态视觉推理实现结构-纹理合成，然后通过Hexagen进行神经图像到3D的几何恢复。

Result: 系统在几秒钟内生成逼真、度量一致的重建结果，相比传统运动结构恢复流程显著提速，且无需专业硬件或专家监督。在Ahsan Manzil、Choto Sona Mosque和Paharpur等标志性建筑上的实验证明其保持了视觉和结构保真度。

Conclusion: 通过将开放图像转化为数字遗产，这项工作将保护重新定义为社区驱动、AI辅助的文化延续行为，为资源有限国家大幅降低了经济和技术门槛。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [43] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习的视频时刻检索模型，通过多智能体系统和证据学习解决不同模型定位结果的冲突，并能识别无对应时刻的查询。


<details>
  <summary>Details</summary>
Motivation: 现有视频时刻检索方法未考虑不同模型定位结果间的冲突，导致模型无法有效整合。需要解决冲突问题并提升检索性能。

Method: 使用强化学习模型扫描整个视频定位时刻边界并生成位置证据，提出多智能体系统框架利用证据学习解决智能体定位输出的冲突。

Result: 在基准数据集上的广泛实验表明，所提方法相比最先进方法具有有效性，能够在不额外训练的情况下识别无对应时刻的查询。

Conclusion: 多智能体系统中的竞争和冲突建模是提高强化学习在时刻检索中性能的有效方法，证据学习在多智能体框架中发挥了新作用。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [44] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出了FERBench基准测试，系统评估了20个最先进的多模态大语言模型在面部表情识别任务上的表现，并开发了UniFER-7B模型，通过后训练策略显著提升了模型在表情推理和可解释性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域取得了成功，但在面部表情识别任务上的表现尚未得到系统评估。现有方法面临推理能力和可解释性不足的问题。

Method: 构建FERBench基准测试，涵盖4个广泛使用的FER数据集；开发后训练策略，包括使用UniFER-CoT-230K进行冷启动初始化和UniFER-RLVR-360K进行基于可验证奖励的强化学习；最终构建统一的UniFER-7B基础模型。

Result: MLLMs在分类性能上表现良好，但在推理和可解释性方面存在显著局限；UniFER-7B模型超越了多个开源和闭源通用MLLMs，包括Gemini-2.5-Pro和Qwen2.5-VL-72B。

Conclusion: 通过系统的基准测试和专门的后训练策略，可以显著提升多模态大语言模型在面部表情识别任务上的推理能力和可解释性，UniFER-7B模型在该领域表现出色。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [45] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: VinciCoder是一个统一的多模态代码生成模型，通过两阶段训练框架解决现有视觉语言模型在代码生成任务中的局限性，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在代码生成任务中依赖单任务训练，形成了狭隘的范式，阻碍了通用视觉代码智能的发展。

Method: 采用两阶段训练框架：首先构建包含160万图像-代码对的监督微调语料库，然后引入视觉强化学习策略，通过从粗到细的奖励机制计算局部和全局图像块的视觉相似度。

Result: 在各种多模态代码生成基准测试中，VinciCoder实现了最先进的性能，证明了从粗到细的视觉强化学习策略的有效性。

Conclusion: VinciCoder通过创新的两阶段训练框架和视觉强化学习策略，成功提升了多模态代码生成的性能，为通用视觉代码智能的发展提供了有效解决方案。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [46] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出了首个统一框架，通过将SOD、CoSOD和SIS三个异构显著性任务转化为视觉语言模型中的思维链推理过程，解决了任务异质性问题。采用两阶段训练范式（监督微调和强化学习），并提出置信度引导策略优化算法来提升推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为每个显著性任务设计专门模型，缺乏统一处理异构任务的框架。本文旨在通过思维链推理过程在视觉语言模型中统一处理三个操作上异构的显著性任务。

Method: 1. 将三个显著性任务转化为思维链推理过程；2. 采用两阶段训练：监督微调（SFT）和强化学习（RL）；3. 提出置信度引导策略优化（CGPO）算法，利用奖励与模型置信度差异作为优势信号；4. 引入"输出到推理"策略构建高质量SFT数据。

Result: 模型在所有任务上匹配或优于专门的SOTA方法和强闭源VLM，特别是在CoSOD任务上，CoCA的S-measure达到0.899，比之前最佳方法提升8.0个百分点，且使用更少的训练数据。

Conclusion: 提出的统一框架通过思维链推理有效处理异构显著性任务，CGPO算法解决了GRPO的关键限制，实现了优异的性能表现，证明了该方法的有效性和通用性。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [47] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: 提出LGCA框架，通过局部特征捕获和显著区域扩展来解决CLIP模型在图像裁剪时引入错误信息的问题，同时保持时间复杂度和提升零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP等预训练视觉语言模型在零样本图像分类中表现优异，但随机图像裁剪会引入错误信息和偏见，因为小尺度下许多图像具有相似特征。

Method: 提出局部-全局交叉对齐（LGCA）框架：先捕获图像局部特征，然后重复选择最显著区域并扩展，相似度评分结合原始图像和扩展图像，同时捕捉局部和全局特征。

Result: 广泛实验表明，该方法在多个数据集上显著提升零样本性能，优于最先进的基线方法。

Conclusion: LGCA框架有效解决了CLIP模型在图像裁剪中的错误信息问题，理论分析证明其时间复杂度与原模型相同，具有高效性和可扩展性。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [48] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像-文本对齐度的生成图像检测方法ITEM，通过利用预训练的CLIP模型检测真实图像与生成图像在图像-文本对齐方面的差异，解决了现有方法过度依赖视觉特征导致的泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，检测生成的虚假图像以防止恶意使用变得至关重要。现有方法仅关注视觉线索，导致训练出的检测器容易过拟合特定图像模式，无法泛化到未见过的生成模型。

Method: 提出ITEM检测器，利用预训练CLIP模型测量图像与标题的对齐度差异，然后训练MLP头部进行分类。采用分层对齐方案，先关注整张图像，再关注标题中描述的每个语义对象，探索全局和细粒度局部语义对齐差异。

Result: 大量实验证明该方法在多种最新生成模型上表现出优异的泛化性和鲁棒性，优于其他最先进的竞争对手。

Conclusion: 通过多模态视角利用图像-文本对齐差异作为判别线索，能够有效检测生成图像，并具有良好的泛化能力。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [49] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 提出了一种基于频率伪造线索（F^2C）的扩散模型生成图像检测方法，通过增强所有频段的频率差异特征来提高检测性能，在泛化性和鲁棒性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像质量很高，但可能被恶意使用。现有检测器难以捕捉不同模型和设置下的判别性线索，限制了其对未见扩散模型的泛化能力和对各种扰动的鲁棒性。

Method: 观察到扩散生成图像与自然真实图像在低频到高频波段存在渐进性差异，提出频率选择性函数作为加权滤波器来抑制判别性较弱的频段并增强信息量更大的频段。

Result: 在多个扩散生成图像数据集上的广泛实验表明，该方法在泛化性和鲁棒性方面优于最先进的检测器。

Conclusion: 基于频率伪造线索的方法能够有效检测来自未见扩散模型的图像，并对各种扰动具有强大的鲁棒性。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [50] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出一种弱监督深度学习框架，使用Grad-CAM解释进行肺炎分类和定位，无需像素级标注，仅使用图像级标签生成临床有意义的热力图。


<details>
  <summary>Details</summary>
Motivation: 解决传统肺炎诊断方法需要昂贵像素级标注的问题，开发能够提供可解释性结果的弱监督模型，增强AI辅助医学影像的临床信任度。

Method: 使用七种ImageNet预训练架构（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3和ViT-B16），在相同训练条件下采用焦点损失和患者级数据分割，利用Grad-CAM生成热力图。

Result: 在Kermany CXR数据集上，ResNet-18和EfficientNet-B0达到最佳测试准确率98%，ROC-AUC=0.997，F1=0.987；MobileNet-V2在准确性和计算成本间提供最佳平衡。

Conclusion: 弱监督可解释模型能够有效识别肺炎相关肺区域，增强肺炎筛查的透明度和临床信任，展示了可解释AI在放射诊断中的潜力。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [51] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: HumanCrafter是一个统一框架，能够从单张图像中联合建模外观和人体部位语义，通过集成人体几何先验和自监督语义先验，在3D人体重建和分割任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在3D人体重建方面取得了高保真度，但在特定任务（如人体3D分割）中的实用性仍然受限。现有方法缺乏对语义信息的有效建模，且标记的3D人体数据集稀缺。

Method: 提出HumanCrafter框架，在重建阶段集成人体几何先验，在分割阶段集成自监督语义先验。开发交互式标注程序生成高质量数据标签对，通过像素对齐聚合实现跨任务协同，多任务目标同时优化纹理建模保真度和语义一致性。

Result: 大量实验表明，HumanCrafter在单图像3D人体部位分割和3D人体重建任务上均超越了现有最先进方法。

Conclusion: HumanCrafter通过统一建模外观和语义，有效解决了3D人体重建和分割的协同问题，为相关应用提供了有力工具。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [52] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的迭代分割和质量优化框架，用于MRI中前庭神经鞘瘤的自动分割，通过多中心数据和专家共识构建标注数据集，显著提高分割精度并提升效率约37.4%。


<details>
  <summary>Details</summary>
Motivation: MRI中前庭神经鞘瘤的精确分割对患者管理至关重要，但传统手动标注耗时且依赖专家。现有深度学习方法在跨数据集和复杂临床病例中的鲁棒性仍面临挑战。

Method: 采用引导式深度学习框架进行迭代分割和质量优化，结合多中心数据并依赖专家共识确保标注可信度，实现人机协作模型训练。

Result: 在目标内部验证数据集上，Dice相似系数从0.9125显著提高到0.9670，在代表性外部数据集上保持稳定性能。专家评估143个扫描发现需要专家干预的复杂病例。

Conclusion: 该人机协作模型训练方法实现了高分割精度，展示了作为临床适应性强、可推广的自动VS分割策略的潜力，数据集已公开可用。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [53] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: FedMGP是一种新的个性化联邦提示学习方法，为每个客户端配备多组文本和视觉提示，通过多样性损失和动态提示聚合策略，在保持参数效率的同时实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习中客户端数据异构性问题，同时保持参数效率，需要开发一种能够捕获多样化细粒度语义和实例级线索的个性化联邦提示学习方法。

Method: 为每个客户端配备多组配对文本和视觉提示，引入多样性损失使各组专注于不同语义方面，采用基于相似性引导概率采样的动态提示聚合策略，通过软选择机制平衡共同知识和客户端特定特征。

Result: FedMGP在所有联邦提示学习方法中实现了最低通信参数的最优性能，在个性化和领域泛化方面均优于现有方法，理论分析表明其动态聚合策略能促进稳健的全局表示学习。

Conclusion: FedMGP通过多组提示和动态聚合策略，有效解决了联邦学习中的个性化需求，在保持参数效率的同时实现了卓越的性能表现。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [54] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat是一种前馈方法，可从单张图像合成可控的显式4D场景，结合视频扩散模型的生成先验与4D数据集学习到的几何和运动约束，直接预测可变形3D高斯场。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像生成高质量4D场景的挑战，避免测试时优化和后处理，提高合成效率。

Method: 使用视频潜在变换器增强视频扩散模型，联合捕获时空依赖性并预测时变3D高斯基元，通过外观保真度、几何精度和运动一致性目标进行训练。

Result: 在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取方面表现优异，匹配或超越基于优化的方法，且效率显著更高。

Conclusion: Diff4Splat提供了一种高效的单次前馈方法，用于从单张图像生成可控的4D场景，在保持质量的同时大幅提升效率。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [55] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个大规模可解释医学视觉问答数据集，包含17,597个问答对和4,394张胸片图像，带有放射科医生验证的边界框和临床推理解释，旨在推进可重现的医学VQA研究。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答系统存在幻觉问题，特别是在正常病例中，缺乏可靠的空间定位和临床解释能力。

Method: 构建包含六种诊断类型（位置、内容、存在性、数量、选择和是/非）的问题分类法，并平衡正负样本分布（41.7%阳性 vs 58.3%阴性），使用MedGemma-4B-it模型进行基准测试。

Result: 基准测试显示性能显著提升（F1=0.624，比基线提高11.8%），同时实现了病变定位功能。

Conclusion: VinDr-CXR-VQA数据集为医学视觉问答研究提供了可靠的基础，支持可重现和临床基础的研究，数据集和评估工具已公开可用。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [56] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: ID-Composer是一个新颖的视频生成框架，通过文本提示和参考图像实现多主体视频生成，解决了现有模型在可控性和适用性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型通常基于文本或单张图像进行条件生成，限制了可控性和应用范围。需要解决多主体视频生成中保持主体身份一致性、跨主体和模态语义整合以及时间一致性的挑战。

Method: 设计了分层身份保持注意力机制来聚合主体内和跨主体的特征；引入预训练视觉语言模型进行语义理解以提供细粒度指导；采用在线强化学习阶段来优化训练目标。

Result: 大量实验表明，该模型在身份保持、时间一致性和视频质量方面优于现有方法。

Conclusion: ID-Composer框架通过创新的注意力机制、语义理解技术和强化学习优化，成功解决了多主体视频生成的关键挑战，为视频生成领域提供了更强大的可控性。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [57] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 提出了一种无需额外训练或偏差标注的测试时去偏方法，通过预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别文本提示均匀相似，从而消除混杂视觉区域的意外偏差信号。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常需要训练数据和显式组标签进行微调或调整嵌入，限制了实际应用。测试时方法虽然避免这一约束，但许多仍依赖数据集特定偏差的先验知识，限制了在开放集设置中的泛化性。

Method: 使用预训练分割模型隔离目标视觉属性，调整非目标区域使其嵌入与所有类别特定文本提示均匀相似，在保留目标属性的同时消除混杂视觉区域的偏差信号。

Result: 在Waterbirds和CelebA数据集上的实验表明，该方法在组鲁棒性指标和Attention IoU方面优于现有测试时去偏方法。

Conclusion: 分割引导的干预措施在视觉语言模型中实现可扩展且无需标注的偏差缓解是有效的。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [58] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出T-VAD框架，基于大视觉语言模型实现文本引导的细粒度视频异常检测，通过异常热图解码器和区域感知异常编码器提升检测精度和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法多为半自动化，需要人工评估，且输出仅限于正常或异常两类，缺乏细粒度和交互性。

Method: 构建T-VAD框架，包含异常热图解码器进行像素级视觉-文本特征对齐生成热图，以及区域感知异常编码器将热图转换为可学习文本嵌入，指导大视觉语言模型准确定位异常。

Result: 在UBnormal数据集上达到94.8% AUC和67.8%/76.7%热图精度；在ShanghaiTech数据集上BLEU-4达62.67/88.84，Yes/No准确率97.67%；在UBnormal数据集上BLEU-4达50.32/78.10，Yes/No准确率89.73%。

Conclusion: T-VAD显著提升了异常检测的粒度和交互性，在多个数据集上实现最先进性能。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [59] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了Real-IAD Variety基准数据集，这是最大最全面的工业异常检测数据集，包含198,960张高分辨率图像，涵盖160个对象类别、28个行业、24种材料和22种颜色变化，旨在解决现有数据集类别多样性不足和规模限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工业异常检测算法受限于公开基准数据集，存在类别多样性不足、规模有限的问题，导致指标饱和和模型在真实场景中泛化能力差。

Method: 构建了Real-IAD Variety数据集，通过覆盖28个行业、24种材料类型和22种颜色变化来确保多样性，并设计了多类无监督、多视图和零/少样本设置的严格评估协议。

Result: 实验验证了该基准的挑战性：最先进的多类无监督异常检测方法在类别从30扩展到160时性能显著下降，而视觉语言模型表现出对类别扩展的显著鲁棒性，性能变化最小。

Conclusion: Real-IAD Variety的规模和复杂性使其成为训练和评估下一代异常检测基础模型的重要资源，有望加速超越领域特定约束的研究，开发可扩展的通用异常检测系统。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [60] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 该论文研究了自动驾驶模型的领域自适应方法，通过翻转美国右驾数据预训练再微调澳大利亚左驾数据，显著提升了PilotNet模型在左驾条件下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶模型需要良好的领域自适应能力来适应不同的道路条件，特别是左右驾驶习惯的差异。论文旨在探索如何有效适应PilotNet模型到左驾条件。

Method: 评估了四种训练方法：1) 美国右驾数据基线模型；2) 翻转美国数据训练；3) 美国数据预训练+澳大利亚高速微调；4) 翻转美国数据预训练+澳大利亚高速微调。使用显著性分析评估注意力变化。

Result: 仅使用翻转数据预训练会降低预测稳定性，但翻转预训练后微调显著改善了适应效果，降低了预测误差并增强了对左侧线索的关注。在ResNet上的验证也显示了相似趋势。

Conclusion: 翻转数据预训练结合微调是一种有效的领域自适应方法，能够以最小的再训练需求显著提升模型在不同驾驶条件下的适应能力。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [61] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出了一种从单张图像中精确学习和合成多实例语义的方法，通过基于惩罚的注意力优化来解耦相似语义，并在合成阶段引入注意力层的框控制来缓解语义泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决从单张图像中学习多实例语义的挑战，特别是在实例具有相似语义或外观时，现有方法面临训练数据有限和语义纠缠的问题。

Method: 采用基于惩罚的注意力优化来在训练阶段解耦相似语义，并在合成阶段引入和优化注意力层的框控制机制。

Result: 实验结果表明该方法实现了高质量的解耦语义学习和合成，在可编辑性和实例一致性之间取得了良好平衡，对语义或视觉相似实例以及罕见物体保持鲁棒性。

Conclusion: 该方法能够有效处理多实例语义学习和合成问题，特别是在面对相似语义或罕见物体时表现出色，代码已开源。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [62] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成水下环境中安装在AUV上的事件相机真实合成数据的流水线，用于训练视觉模型。该方法通过模拟恶劣水下条件（如能见度差和悬浮颗粒物）来验证有效性，特别以岩石检测任务为例。


<details>
  <summary>Details</summary>
Motivation: 传统视觉技术在水下恶劣条件（如光照不足、高动态范围场景）下表现不佳，而事件相机通过逐帧跟踪变化来缓解这些问题。需要合成数据来训练水下视觉模型。

Method: 开发了一个流水线来生成安装在AUV上的事件相机的真实合成数据，模拟水下环境中的恶劣条件，包括能见度差和悬浮颗粒物。

Result: 该流水线在岩石检测任务中展示了有效性，能够处理恶劣的水下视觉条件。

Conclusion: 该方法为训练水下视觉模型提供了有效的合成数据生成方案，并可推广到其他水下任务。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [63] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 4D-NVS结合体素表示与神经高斯泼溅，通过紧凑的神经体素和变形场建模动态场景，显著减少内存消耗并加速训练，同时保持高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在动态场景中因跨帧复制高斯函数导致内存开销大，需要更高效的动态场景建模方法。

Method: 使用紧凑的神经体素集合和学习的变形场来建模时间动态，避免为每个时间戳生成单独的高斯集合；引入视角细化阶段，通过针对性优化改进挑战性视角的渲染质量。

Result: 实验表明该方法在显著减少内存和加速训练的同时，优于现有最先进方法，实现实时渲染和优越的视觉保真度。

Conclusion: 4D-NVS通过体素表示和神经高斯泼溅的结合，为动态场景建模提供了内存高效且训练快速的解决方案，同时保持高质量的渲染效果。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [64] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 本文提出了一个频率引导的广义类别发现框架（FREE），用于解决领域偏移下的广义类别发现问题，通过频域信息增强模型在分布偏移下发现类别的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的广义类别发现方法在标准条件下表现良好，但在存在分布偏移时性能会显著下降。本文探索了一个更现实的任务：领域偏移广义类别发现（DS_GCD），其中未标记数据不仅包含未知类别，还包含来自未知领域的样本。

Method: 提出了FREE框架，包括：1）基于频率的领域分离策略，通过测量幅度差异将样本划分为已知和未知领域；2）跨领域和领域内频率扰动策略；3）扩展的自监督对比目标和语义聚类损失；4）聚类难度感知的重采样技术。

Result: 大量实验表明，该方法有效减轻了分布偏移的影响，在各种基准数据集上实现了优越的性能，能够更好地发现已知和未知类别。

Conclusion: FREE框架通过利用频域信息，显著提升了模型在分布偏移条件下发现类别的能力，为领域偏移广义类别发现问题提供了有效的解决方案。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [65] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 该论文提出了一种新的无监督深度和自运动联合学习框架DiMoDE，通过对运动分量进行区分处理，利用各自的刚性流几何规律来改进深度和自运动估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将自运动作为辅助任务，要么混合所有运动类型，要么排除与深度无关的旋转运动，这种设计限制了强几何约束的引入，降低了在不同条件下的可靠性和鲁棒性。

Method: 通过首先对齐源相机和目标相机的光轴和成像平面，将帧间光流通过这些对齐进行变换，并量化偏差以对每个自运动分量单独施加几何约束。这些对齐进一步将联合学习过程重新表述为同轴和共面形式，其中深度和每个平移分量可以通过闭式几何关系相互推导。

Result: DiMoDE在多个公共数据集和新收集的多样化真实世界数据集上实现了最先进的性能，特别是在具有挑战性的条件下表现突出。

Conclusion: 通过区分处理运动分量并引入互补的几何约束，DiMoDE框架显著提高了深度和自运动估计的鲁棒性和准确性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [66] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 本文提出了一种上下文感知的零样本异常检测方法，通过融合跨注意力时序融合和上下文记忆，在UCF-Crime和XD-Violence数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视频异常通常依赖于上下文信息和时序演化，现有异常检测器大多未能注意到这种上下文依赖性，限制了其在真实场景中的泛化能力。

Method: 采用记忆增强管道，通过跨注意力将时序信号与视觉嵌入相关联，并通过上下文相似性评分进行实时零样本异常分类。

Result: 在UCF-Crime上达到90.4% AUC，在XD-Violence上达到83.67% AP，实现了实时推理并具有高精度和可解释性。

Conclusion: 通过融合跨注意力时序融合和上下文记忆，实现了高保真度的异常检测，为零样本模型在真实世界监控和基础设施监测中的应用迈出了重要一步。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [67] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA是首个支持像素级推理和多模态提示的视觉-语言-动作模型，通过两阶段自动标注流程生成Pixel-160K数据集，在三个标准VLA基准测试中相比OpenVLA将操作成功率提升了10.1%-17.8%，同时仅需其1.5%的预训练成本。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型存在两个主要限制：(i)难以进行像素级场景理解，(ii)过度依赖文本提示，在真实世界环境中灵活性不足。

Method: 提出新的视觉运动指令调优框架，集成多尺度像素感知编码器和视觉提示编码器，采用两阶段自动标注流程生成Pixel-160K大规模像素级标注数据集。

Result: 在三个标准VLA基准测试和两个VLA模型变体上，PixelVLA相比OpenVLA将操作成功率提升了10.1%-17.8%，同时预训练成本仅为后者的1.5%。

Conclusion: PixelVLA可以集成到现有VLA中，在复杂环境中实现更准确、高效和通用的机器人控制。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [68] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: CueBench是首个专注于上下文感知视频异常理解的基准测试，通过事件中心化分层分类法定义了14种条件异常和18种绝对异常事件，涵盖174个场景和198个属性，统一了识别、时序定位、检测和预测等任务。


<details>
  <summary>Details</summary>
Motivation: 当前深度模型对真实世界视频异常的理解仍停留在表面，缺乏对复杂原理和微妙上下文的理解能力，无法区分如带安全装备与不带安全装备攀岩等细微差异。

Method: 提出CueBench基准测试框架，建立事件中心化分层分类法；开发Cue-R1模型，基于R1风格强化微调，采用可验证、任务对齐和层次细化的奖励机制进行统一生成式训练。

Result: 在CueBench上的广泛实验表明，现有视觉语言模型对真实世界异常理解仍不令人满意，而Cue-R1模型平均超越最先进方法超过24%。

Conclusion: 深度模型距离真实世界视频异常理解仍有很大差距，CueBench为评估上下文感知异常理解提供了严谨框架，Cue-R1在多个任务上显著优于现有方法。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [69] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个多平台、多模态的3D视觉定位基准数据集，包含车辆、无人机和四足机器人平台的RGB和LiDAR数据，提供超过128,000个对象和22,000个验证过的指代表达式，比现有数据集大10倍。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉定位基准局限于室内环境、单一平台约束和小规模，需要构建大规模、多平台的室外3D视觉定位基准来推动语言驱动的3D具身感知研究。

Method: 开发了可扩展的标注流程，结合视觉语言模型提示和人工验证来确保高质量的空间定位；提出了平台感知归一化和跨模态对齐技术来支持跨平台学习；建立了域内和跨平台评估的基准协议。

Result: 构建了目前最大规模的室外3D视觉定位数据集，发现存在显著的性能差距，揭示了可泛化3D视觉定位的挑战和机遇。

Conclusion: 3EED数据集和基准工具包的发布将推动语言驱动的3D具身感知的未来研究，为跨平台3D视觉定位提供了重要的基准和评估框架。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [70] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 提出了一种新的手术动作三元组分割任务，通过仪器实例分割实现空间定位，并开发了TargetFusionNet架构和CholecTriplet-Seg数据集来提升手术动作理解的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有手术动作三元组识别方法仅限于帧级分类，无法可靠地将动作与特定仪器实例关联，且空间定位主要依赖类激活图，缺乏精确性和鲁棒性。

Method: 提出三元组分割任务，创建CholecTriplet-Seg数据集（30,000+标注帧），开发TargetFusionNet架构，扩展Mask2Former并引入目标感知融合机制，将弱解剖学先验与仪器实例查询融合。

Result: TargetFusionNet在识别、检测和三元组分割指标上均优于现有基线，表明强实例监督与弱目标先验结合显著提升了手术动作理解的准确性和鲁棒性。

Conclusion: 三元组分割为手术动作三元组空间定位建立了统一框架，提出的基准和架构为更可解释的手术场景理解铺平了道路。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [71] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: 该研究介绍了首个大规模多光谱激光雷达基准数据集FGI-EMIT，用于单木分割，并系统比较了传统无监督算法和深度学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统单木分割方法缺乏大规模基准数据集，特别是多光谱激光雷达数据，而多光谱反射率已被证明能提高分割精度。

Method: 使用FGI-EMIT数据集，对4种传统无监督算法和4种深度学习模型进行基准测试，无监督方法采用贝叶斯优化超参数，深度学习模型从头训练。

Result: 无监督方法中Treeiso表现最佳，F1分数52.7%；深度学习方法整体表现更好，ForestFormer3D达到73.3%的F1分数，在下层树木分割中优势尤其明显。

Conclusion: 深度学习方法在单木分割中显著优于传统算法，但当前方法未能充分利用多光谱反射率信息，且即使在低点密度下也保持优势。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [72] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一个元数据引导的框架，通过将MRI体积图像与其DICOM采集参数对齐来学习MRI对比度表示，解决了MRI数据异质性和缺乏标准化对比度标签的问题。


<details>
  <summary>Details</summary>
Motivation: MRI存在严重的数据异质性和缺乏标准化对比度标签的问题，这限制了大规模自动化分析。需要一个统一的MRI对比度表示来支持自动序列识别、协调和质量控制等下游应用。

Method: 引入MR-CLIP框架，通过将体积图像与DICOM采集参数对齐来学习MRI对比度表示，利用常规可用的采集元数据作为监督信号。

Result: 生成的嵌入显示MRI序列的明显聚类，在数据稀缺情况下，在少样本序列分类中优于监督3D基线。此外，MR-CLIP通过图像-元数据嵌入距离识别损坏或不一致的元数据，实现无监督数据质量控制。

Conclusion: MR-CLIP通过将常规采集元数据转化为监督信号，为跨不同临床数据集的标签高效MRI分析提供了可扩展的基础。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [73] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER是一种基于新颖性搜索的方法，通过大型语言模型对自然语言提示进行语义演化，生成多样化的图像集合，解决了文本到图像扩散模型输出多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像，但输出多样性有限，限制了在探索性和构思任务中的应用。现有的提示优化技术主要关注美学适应性，不适合创意视觉领域。

Method: WANDER直接在自然语言提示上操作，使用大型语言模型进行语义演化，利用CLIP嵌入量化新颖性，并应用发射器引导搜索到不同的提示空间区域。

Result: 使用FLUX-DEV生成和GPT-4o-mini变异的实证评估表明，WANDER在多样性指标上显著优于现有的进化提示优化基线方法。消融研究证实了发射器的有效性。

Conclusion: WANDER通过新颖性搜索方法有效提升了文本到图像生成模型的输出多样性，发射器的应用进一步增强了生成图像的多样性。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [74] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 本文对低剂量CT图像增强中不同损失函数与图像质量指标的一致性进行了客观分析，发现损失函数与质量指标之间存在不一致性，强调在开发新损失函数时需要考虑图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT成像广泛用于减少辐射暴露，但常受噪声和伪影影响诊断准确性。虽然深度学习模型在PSNR和SSIM指标上表现优异，但这些指标在反映医学图像感知质量方面存在局限性。

Method: 重点关注深度学习架构中最关键的元素之一——损失函数，对低剂量CT图像质量增强中不同损失函数的相关性及其与图像质量指标的一致性进行客观分析。

Result: 研究发现损失函数与质量指标之间存在不一致性，揭示了当前损失函数在反映图像感知质量方面的局限性。

Conclusion: 在开发图像质量增强的新损失函数时，必须考虑图像质量指标，以确保损失函数能够更好地反映图像的感知质量。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [75] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: 深度学习模型在ADNI数据集上表现良好，但在拉丁美洲FLENI队列上性能显著下降，揭示显著的领域偏移。Transformer架构并未显示出明显优势，图像归一化和采样选择是泛化的关键因素。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型在阿尔茨海默病诊断中的泛化能力，特别是对代表性不足人群的适用性，因为现有研究主要基于北美队列。

Method: 在ADNI数据集上训练卷积和Transformer模型，然后在阿根廷FLENI临床队列上进行泛化测试，通过消融研究分析关键因素，并进行遮挡敏感性分析。

Result: 所有模型在ADNI上AUC高达0.96-0.97，但在FLENI上降至0.80-0.82，显示显著性能下降。不同架构表现相似，图像归一化和采样选择对泛化至关重要。

Conclusion: 需要基于人群的AI模型验证，未来工作应关注领域适应和队列多样化，以确保诊断模型在不同人群中的有效性。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [76] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 将地点识别重新定义为多类分类问题，通过为LiDAR扫描分配离散位置标签，使用编码器-解码器模型直接分类位置，在NuScenes数据集上获得与对比学习方法相当的性能，同时具有更高的训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有地点识别方法主要依赖对比学习，本文探索将其重新定义为多类分类问题的替代方法，以提升训练效率和稳定性。

Method: 为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接对每个扫描的位置进行分类。

Result: 在NuScenes数据集上的评估显示，该方法与基于对比学习的方法性能相当。

Conclusion: 多类分类方法在地点识别任务中具有竞争力，同时提供了更好的训练效率和稳定性优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [77] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 该研究探讨生成式AI模型如何编码'美'并消除'丑'，通过两个图像生成流程生成5984张图像，发现模型存在明显的肤色、年龄和性别偏见，86.5%图像描绘浅肤色人群，74%为年轻人群，非二元性别个体被过度性化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体加剧了西方审美标准的推广，导致负面自我形象和身体畸形恐惧。随着人工智能生成内容的增加，担忧这些审美标准被夸大，研究旨在探索生成式AI模型如何编码'美'概念及其社会影响。

Method: 创建两个图像生成流程：文本到图像模型和文本到语言模型到图像模型。开发结构化审美分类法，使用三个语言模型和两个文本到图像模型生成5984张图像。招募女性和非二元社交媒体用户通过李克特量表评估1200张图像。

Result: 86.5%生成图像描绘浅肤色人群；22%包含明确内容（尽管经过安全训练）；74%被评为年轻年龄段；非二元个体图像被评价为更年轻和过度性化；带有'负面'或'丑陋'特征的提示词产生更高NSFW评分。

Conclusion: 生成式AI模型存在与审美标准相关的普遍人口统计偏见，这些偏见通过模型开发者的行为（如负面提示）被积极延续，可能导致数据流污染和对不符合开发者审美刻板印象特征的积极消除。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [78] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 开发了一个结合YOLOv5和SSD算法的物联网动物检测系统，用于榴莲种植园，通过Telegram实时通知和声音威慑机制保护作物免受动物入侵。


<details>
  <summary>Details</summary>
Motivation: 传统农业实践因缺乏无人监控而无效，现有系统受限于单一检测算法、通知平台不可及和威慑机制有限的问题。

Method: 集成YOLOv5和SSD物体检测算法，结合物联网技术实现实时监控，通过Telegram发送自动通知，并触发声音威慑机制（如虎啸）。

Result: YOLO+SSD模型对大象、野猪和猴子的检测准确率分别为90%、85%和70%，白天准确率最高，夜间有所下降。

Conclusion: 该研究提供了一个结合检测、通知和威慑的全面实用框架，为自动化农业解决方案的未来创新铺平了道路。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [79] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 提出了一种粒度一致的自适应2D掩码跟踪方法，通过保持帧间时间一致性来消除冲突的3D伪标签，结合三阶段课程学习框架，从碎片化的单视图数据逐步训练到统一的多视图标注，最终实现全局一致的完整场景监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过将基础模型的2D掩码转移到3D来生成伪标签，但由于视频帧被独立处理，导致分割粒度不一致和冲突的3D伪标签，从而降低最终分割的准确性。

Method: 引入粒度一致的自适应2D掩码跟踪方法，保持跨帧的时间对应关系；结合三阶段课程学习框架，从单视图碎片化数据逐步训练到多视图统一标注，最终实现全局一致的完整场景监督。

Result: 实验结果表明，该方法能有效生成一致且准确的3D分割结果，在标准基准测试中达到了最先进的性能，并具备开放词汇能力。

Conclusion: 该方法能够从最初碎片化和矛盾的2D先验中稳健地提取出一致的3D表示，实现了高质量的3D实例分割。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [80] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench是一个用于隐私保护联邦学习的可重复基准测试平台，使用合成肿瘤CT扫描数据评估分割性能和隐私泄露，揭示了隐私与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护隐私敏感数据方面具有重要价值，但现有系统仍面临成员推理攻击和数据异构性等漏洞，需要建立标准化基准来评估隐私保护FL方法。

Method: 开发FedOnco-Bench基准平台，使用合成肿瘤CT扫描数据，评估FedAvg、FedProx、FedBN和FedAvg+DP-SGD等FL方法的分割性能和隐私泄露情况。

Result: FedAvg性能最佳（Dice约0.85）但隐私泄露最多（攻击AUC约0.72），DP-SGD隐私保护最强（AUC约0.25）但准确率下降（Dice约0.79），FedProx和FedBN在异构数据下表现均衡。

Conclusion: FedOnco-Bench为医学图像分割的隐私保护FL方法提供了标准化、开源的基准测试平台，有助于方法开发和比较。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [81] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: OMEGA是一个新颖的位置编码框架，通过模态特定位置编码和全局自适应编码步长缩放，解决了当前视觉语言模型中位置编码策略的局限性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型采用统一的1D或2D位置索引策略，没有考虑文本和视觉模态在结构特性和连续性方面的差异，限制了模型性能。

Method: 提出OMEGA框架，包含模态特定位置编码（MSPE）来分别处理文本和视觉的位置信息，以及全局自适应编码步长缩放（GAESS）来调整视觉token的位置编码步长。

Result: 在Qwen2.5-VL-3B模型上，OMEGA在视觉密集型任务上比基线位置编码策略提升了3.43%，在Qwen2.5-VL-7B和LLaVA-v1.5-7B等更大模型上也观察到一致的性能提升。

Conclusion: OMEGA通过模态特定的位置编码策略有效提升了视觉语言模型在各种架构和VQA基准测试中的性能表现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [82] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出LSSA攻击方法，通过局部图像块随机打乱和采样来增强多模态对抗样本的迁移性，在多种VLP模型和下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升多模态对抗样本迁移性时存在过拟合问题，主要原因是缺乏输入多样性，过度依赖单一模态的对抗样本信息。

Method: LSSA方法随机打乱局部图像块来扩展原始图像-文本对，生成对抗图像并在其周围采样，然后利用原始和采样图像生成对抗文本。

Result: 在多个模型和数据集上的实验表明，LSSA显著提升了多模态对抗样本在不同VLP模型和下游任务中的迁移性，并在大型视觉语言模型上优于其他先进攻击方法。

Conclusion: LSSA通过增加输入多样性有效解决了多模态对抗攻击中的过拟合问题，显著提升了攻击的迁移性能。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [83] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 本文提出视觉对比注意力（VCA）作为MHSA的替代方案，通过引入显式判别机制将理论复杂度从O(NNC)降至O(NnC)，在图像识别和生成任务中均显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformers的MHSA层对所有token对进行二次查询-键交互，将大量计算花费在视觉上弱或冗余的相关性上，需要更高效的注意力机制。

Method: VCA首先将每个头的密集查询场蒸馏为少量空间池化的视觉对比token，然后将其分为可学习的正负流，通过差分交互突出区域间的真正区别。

Result: VCA将DeiT-Tiny在ImageNet-1K上的top-1准确率从72.2%提升至75.6%（+3.4%），在三种强层次化ViT上提升达3.1%，在条件图像生成中将FID-50K降低2.1-5.2点。

Conclusion: VCA通过空间池化提供低方差全局线索，双位置嵌入对对比推理不可或缺，两者结合产生最强协同效应，为更快更锐利的Vision Transformers提供了简单路径。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [84] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个专门用于评估多模态大语言模型在脑成像分析中多模态理解能力的综合基准，包含15种脑成像模态、9,527个验证问答对和31,706张图像，涵盖15个多阶段临床任务。


<details>
  <summary>Details</summary>
Motivation: 当前面向脑部的视觉问答基准要么覆盖的成像模态有限，要么仅限于粗粒度的病理描述，阻碍了对MLLMs在整个临床连续体中能力的全面评估。

Method: 构建包含15种不同脑成像模态的综合多模态VQA基准，模拟临床工作流程，涵盖15个多阶段临床任务，并由专业放射科医生严格验证。

Result: 评估24个最先进模型显示：(1)专有MLLMs优于开源和医疗模型但落后于医生；(2)医疗MLLMs性能差异大；(3)开源MLLMs整体落后但在特定任务中表现出色；(4)MLLMs在复杂术前任务中表现明显不佳，揭示视觉到临床推理的差距。

Conclusion: OmniBrainBench为评估和推进MLLMs在脑成像分析中设定了新标准，突显了与专家临床推理相比的差距。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [85] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种遮挡感知扩散模型（ODM），用于在遮挡场景下预测行人过街意图，通过重建被遮挡的运动模式来指导未来意图预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在预测行人过街意图方面取得了显著成功，但很少考虑遮挡场景下的不完整观察问题。

Method: 采用遮挡感知扩散变换器架构，在去噪阶段估计与遮挡模式相关的噪声特征，并引入遮挡掩码引导的反向过程来有效利用观察信息。

Result: 在PIE和JAAD基准测试上的广泛实验结果表明，该方法在各种遮挡场景下比现有方法实现了更鲁棒的性能。

Conclusion: 所提出的遮挡感知扩散模型能够有效处理遮挡场景下的行人过街意图预测问题，提高了预测准确性。

Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [86] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: Fleming-VL是一个统一的端到端框架，用于解决医学多模态大语言模型在处理异构医学数据（包括2D图像、3D体积扫描和时序视频序列）时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异构性，包含2D图像、3D体积扫描和时序视频序列等多种模态，这些模态之间存在显著的领域差距和数据格式不一致问题，阻碍了统一医学MLLMs的发展。

Method: 从数据中心的视角出发，采用三个关键策略：(1) 整合自然和医学特定领域的长上下文数据进行预训练扩展；(2) 使用罕见医学数据（包括整体视频分析和代表性不足的2D模态）进行微调补充；(3) 扩展现有评估框架以纳入3D体积和视频理解基准。通过监督微调和组相对策略优化开发多个模型规模。

Result: Fleming-VL在多个基准测试中实现了最先进的性能，包括医学VQA、视频QA和3D医学图像理解。

Conclusion: Fleming-VL公开发布，旨在促进医学AI领域的透明、可复现和可审计的进展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [87] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一个端到端的虚拟试穿模型，仅需源图像和目标服装作为输入，无需复杂预处理，通过参考图像增强试穿准确性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法依赖复杂输入（如不可知人物图像、人体姿态等），导致劳动密集且不实用。EVTAR旨在简化输入要求，提高现实应用可行性。

Method: 采用两阶段训练策略，利用额外参考图像（不同人穿着相同服装）来保持服装纹理和细节，模拟人类选择服装时的参考行为。

Result: 在两个广泛使用的基准测试和多样化任务上评估，结果一致验证了方法的有效性。

Conclusion: EVTAR通过简化输入要求和利用参考图像，实现了更现实和高质量的虚拟试穿效果，具有更好的实用性。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [88] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出统一的零样本视频异常分析框架，通过链式推理连接时间检测、空间定位和文本解释任务，无需额外训练即可实现全面的异常分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究大多停留在帧级检测，缺乏空间和语义上下文，无法解释异常原因。现有方法虽然提高了可解释性，但仍依赖数据和特定任务。

Method: 基于链式测试时推理过程，通过任务内推理优化时间检测，任务间链式连接实现空间和语义理解，在完全零样本方式下提高可解释性和泛化能力。

Result: 在多个视频异常检测、定位和解释基准测试中实现最先进的零样本性能，无需额外数据或梯度更新。

Conclusion: 精心设计的提示与任务链式连接可以释放基础模型的推理能力，实现实用、可解释的视频异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [89] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: VesSAM是一个专门用于2D血管分割的高效框架，通过集成卷积适配器、多提示编码器和轻量级掩码解码器，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确的血管分割对于疾病诊断和手术规划至关重要，但由于血管结构细薄、分支复杂且纹理对比度低，现有基础模型如SAM在血管分割上表现不佳。

Method: VesSAM框架包含：(1)卷积适配器增强局部纹理特征；(2)多提示编码器通过分层交叉注意力融合解剖学提示（骨架、分叉点、段中点）；(3)轻量级掩码解码器减少锯齿伪影。还引入了自动化多提示注释生成流程。

Result: 实验结果显示，VesSAM在8个数据集、5种成像模态上，比最先进的PEFT-based SAM变体在Dice和IoU上分别提升超过10%和13%，与完全微调方法性能相当但参数更少，在分布外设置下也表现优异。

Conclusion: VesSAM是一个强大且高效的血管分割框架，通过专门设计的组件显著提升了血管分割性能，具有良好的泛化能力和参数效率。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [90] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 本文提出了一种用于戈雅画作认证的多模态机器学习框架，通过统一特征提取技术处理视觉和X射线图像，使用优化的一类支持向量机实现97.8%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 戈雅作品的艺术认证面临复杂计算挑战，包括其异质风格演变和广泛的历史伪造模式，需要开发更可靠的多模态认证方法。

Method: 采用统一特征提取管道，包括灰度共生矩阵描述符、局部二值模式、熵测量、能量计算和颜色分布分析，应用于视觉和X射线图像，通过优化的一类支持向量机进行分类。

Result: 在24幅认证戈雅画作数据集上，使用80/20训练测试配置和10折交叉验证，框架达到97.8%分类准确率和0.022假阳性率。案例研究显示对《巨人》的认证置信度达92.3%。

Conclusion: 多模态方法相比单模态方法性能显著提升，证明在视觉和放射影像上应用相同计算方法在艺术认证应用中的有效性。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [91] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net是一种混合CNN-Transformer架构，用于乳腺癌超声图像的同步分割和分类，具有内在可解释性，在BUSI数据集上表现优异，并通过交叉数据集研究验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决B型超声乳腺癌诊断面临的挑战：斑点噪声、操作者依赖性和边界模糊问题，以及现有深度学习方法存在的单任务学习、架构限制（CNN缺乏全局上下文、Transformer缺乏局部特征）和黑盒决策等问题。

Method: 提出HyFormer-Net混合架构，包含双分支编码器（集成EfficientNet-B3和Swin Transformer）、多尺度分层融合模块和注意力门控解码器，并引入双管道可解释性机制（内在注意力验证和Grad-CAM）。

Result: 在BUSI数据集上达到Dice分数0.761±0.072和准确率93.2%，恶性召回率92.1±2.2%；集成模型实现Dice 90.2%、准确率99.5%和100%恶性召回率；交叉数据集研究中，仅用10%目标域数据即可恢复92.5%性能。

Conclusion: HyFormer-Net在乳腺癌超声诊断中表现出色，通过多尺度融合和注意力机制显著提升性能，并证明了在有限目标域数据下实现有效泛化的能力，为临床采用提供了可行方案。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [92] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: 本文提出了T-MLA攻击框架，针对神经图像压缩系统进行针对性多尺度对数指数攻击，通过在小波域中制作对抗性扰动，显著降低重建图像质量同时保持视觉不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 现有对神经图像压缩系统的攻击方法多为像素空间方法的简单改编，忽视了压缩管道的独特结构化特性，需要开发更先进的漏洞利用方法。

Method: 提出T-MLA攻击框架，在小波域中制作对抗性扰动，直接针对攻击后和重建图像的质量，将扰动策略性地限制在特定小波子带中，最大化失真同时确保感知隐蔽性。

Result: 在多个最先进的神经图像压缩架构和标准图像压缩基准上的广泛评估显示，重建质量大幅下降，而扰动在视觉上仍然不可察觉。

Conclusion: 研究揭示了生成和内容传输管道核心存在关键安全漏洞，神经图像压缩系统面临严重的安全威胁。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [93] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 提出了一种基于分层序列预测的图像地理定位方法，使用S2网格单元从粗到细逐步精确定位，结合波束搜索和多样本推理策略，在Im2GPS3k和YFCC4k数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像地理定位中因视觉相似性和大搜索空间带来的挑战，模仿人类从大区域到具体地址的定位过程。

Method: 使用S2网格单元构建分层结构，通过自回归方式依次预测更精细的网格单元，结合波束搜索和多样本推理等语言模型采样策略。

Result: 在无MLLM设置下超越其他基线方法，准确率提升达13.9%；结合MLLM时在所有指标上均达到最先进水平。

Conclusion: 分层序列预测方法能有效处理图像地理定位问题，结合语言模型推理策略可显著提升性能。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [94] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 本文提出了一个包含1333个英语Rebus谜题的大型多样化基准测试，并开发了RebusDescProgICE框架，通过结合非结构化描述和基于代码的结构化推理，显著提升了视觉语言模型在Rebus谜题上的性能。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题需要图像识别、认知技能、常识推理、多步推理和基于图像的文字游戏等多种能力，这对当前的视觉语言模型来说是一个具有挑战性的任务。

Method: 提出了RebusDescProgICE框架，该框架使用非结构化描述和基于代码的结构化推理相结合的方法，并改进了基于推理的上下文示例选择策略。

Result: 相比思维链推理，该框架在闭源模型上提升了2.1-4.1%的性能，在开源模型上提升了20-30%的性能。

Conclusion: RebusDescProgICE框架有效提升了视觉语言模型在复杂Rebus谜题任务上的表现，证明了结合非结构化描述和结构化推理方法的有效性。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [95] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 该论文提出了Viewpoint Learning任务来评估和改进多模态大语言模型的空间推理能力，通过Viewpoint-100K数据集和两阶段微调策略显著提升了模型的空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在2D视觉理解方面取得了显著进展，但它们在复杂3D推理任务中的表现仍不明确，特别是能否有效捕捉详细的空间信息以确保跨视图一致性。

Method: 提出了Viewpoint Learning任务，构建了包含10万对以物体为中心的图像和问答对的Viewpoint-100K数据集。采用两阶段微调策略：首先通过监督微调注入基础知识，然后使用GRPO算法通过强化学习增强泛化能力，并引入了混合冷启动初始化方法。

Result: 实验结果表明，该方法显著激活了多模态大语言模型的空间推理能力，在领域内和领域外推理任务上的性能均有提升。

Conclusion: 研究强调了在多模态大语言模型中发展基础空间技能的重要性，为机器人、自主系统和3D场景理解的未来发展提供了支持。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [96] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 提出了一种结合Epanechnikov核密度估计和双峰逻辑回归分类器的新方法，用于基于医学图像诊断呼吸系统疾病，在COVID-19胸部X光数据集上取得中等性能。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够灵活建模医学图像数据分布而不需要假设特定形状的诊断方法，提高呼吸系统疾病诊断的准确性和可靠性。

Method: 使用Epanechnikov非参数核密度估计(EKDE)结合双峰逻辑回归分类器，构建基于统计模型的学习方案，利用EKDE对像素强度变化的适应性提取医学图像关键特征。

Result: 在13808张随机选择的胸部X光图像上测试，准确率70.14%，敏感性59.26%，特异性74.18%，在检测呼吸系统疾病方面表现出中等性能，但敏感性有待提高。

Conclusion: 虽然临床专业知识对于进一步改进模型仍然至关重要，但这项研究突出了基于EKDE的方法在提高医学影像诊断准确性和可靠性方面的潜力。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [97] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: ViACT是一个将解剖先验知识直接集成到transformer架构中的视频分析框架，通过表示变形解剖结构为点集并编码其空间几何和图像块，在预训练时仅重建解剖区域，使表示学习专注于解剖区域。


<details>
  <summary>Details</summary>
Motivation: 视频transformer在超声心动图分析中表现出潜力，但容易从非诊断区域（如图像背景）学习虚假相关性，需要克服这一限制。

Method: 提出ViACT框架，将解剖结构表示为点集，编码空间几何和图像块到transformer tokens，采用掩码自编码策略仅重建解剖区域，预训练后可用于特定任务的微调。

Result: ViACT在左心室射血分数回归和心脏淀粉样变性检测任务中表现出色，注意力图与已知病理区域对齐，且无需特定组件即可泛化到心肌点跟踪任务。

Conclusion: ViACT通过解剖约束将transformer注意力聚焦于心肌区域，产生可解释的注意力图，并在多个超声心动图分析任务中表现出良好的泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [98] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 该论文提出在边缘计算中使用嵌入式GPU设备来提升计算机视觉应用性能，通过实验验证GPU相比CPU能获得性能增益，从而改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的计算机视觉和增强现实应用对资源需求高，而边缘计算设备通常容量有限，会影响用户体验质量。

Method: 在边缘计算环境中使用带有图形处理单元（GPU）的嵌入式设备来卸载高强度的计算任务。

Result: 实验表明，与仅使用CPU相比，GPU能够获得性能增益。

Conclusion: 使用嵌入式GPU设备可以克服边缘计算设备的性能限制，为用户使用计算机视觉应用提供更好的体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [99] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种无需概念标注的弱监督概念预测框架PCP，利用类别级概念先验作为弱监督，在医学影像中实现可解释预测。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI需要可解释预测，但现有可解释设计框架需要昂贵的概念标注，而零射方法难以捕捉医学领域特征。

Method: PCP框架使用类别级概念先验作为弱监督，结合KL散度和熵正则化的精炼机制来对齐临床推理。

Result: 在PH2和WBCatt数据集上，PCP相比零射基线将概念级F1分数提升超过33%，在四个医学数据集上达到与全监督方法竞争的分类性能。

Conclusion: PCP提供了一种无需显式监督或语言模型的可行方案，在医学影像可解释预测方面表现出色。

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [100] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是一个针对统一多模态模型中跨模态推理能力评估的新基准，包含1312个任务和1876张图像，重点关注视觉与语言之间的相互推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法将多模态能力孤立评估，无法有效测试跨模态推理能力，而这是实现真正统一多模态智能的核心能力。

Method: 构建包含两个互补设置的人工标注基准：语言增强的视觉生成推理（用语言提示指导图像合成）和视觉增强的语言生成推理（生成中间可视化来加强推理过程）。

Result: 对17个统一模型的实验发现：跨模态推理决定视觉生成质量，交错模型显著优于非交错模型；模型在物理和符号推理之间存在分离，能解释感知概念但无法为符号任务构建视觉抽象。

Conclusion: 相互跨模态推理是实现真正全模态生成的关键前沿，现有模型在此方面仍有明显不足。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [101] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 提出了一个自动化流程从YouTube视频中提取动物中心剪辑，构建了包含30K视频（2M帧）的大规模数据集，并创建了Animal-in-Motion基准，用于4D四足动物重建任务。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模有限（仅2.4K 15帧剪辑），缺乏对动物中心3D/4D任务的关键处理，需要从野外视频进行大规模、无标记的4D动物重建。

Method: 开发自动化流程挖掘YouTube视频并处理成对象中心剪辑，附带姿态估计、跟踪和3D/4D重建等下游任务的有价值辅助注释。

Result: 收集了30K视频（2M帧），比先前工作多一个数量级；创建了包含230个手动筛选序列的Animal-in-Motion基准；发现基于模型的方法在2D指标上表现更好但3D形状不真实，而模型无关方法产生更自然的重建但得分较低。

Conclusion: 通过管道、基准和基线，旨在推进从野外视频进行大规模、无标记的4D动物重建及相关任务，并通过序列级优化建立了首个4D动物重建基线。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [102] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: 提出基于图像小波谱的扩散Transformer模型DTWSR，通过捕捉多尺度频率子带间的相互关系来解决图像超分辨率中的不一致性和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的超分辨率方法大多忽略了多尺度频率子带间的相互关系，导致重建图像存在不一致性和不自然伪影。

Method: 使用多级离散小波变换分解图像为小波谱，提出金字塔标记化方法将谱嵌入为token序列，设计双解码器分别处理低频和高频子带的差异并保持对齐。

Result: 在多个基准数据集上的广泛实验表明，该方法在感知质量和保真度方面均表现出高性能。

Conclusion: DTWSR结合了扩散模型和Transformer的优势，能够有效捕捉多尺度频率子带间的相互关系，生成更一致和真实的超分辨率图像。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [103] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa是一个用于文本驱动的3D人体运动生成的分层运动生成框架，通过多尺度标记保留策略和可扩展自回归建模，在保持高质量生成的同时显著减少推理步骤，实现了最先进的生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在生成3D人体运动时需要大量推理步骤，效率较低。MoSa旨在通过分层量化策略减少推理步骤，同时保持生成质量。

Method: 提出多尺度标记保留策略(MTPS)和分层残差向量量化变分自编码器(RQ-VAE)，结合可扩展自回归建模(SAR)和轻量级卷积-注意力混合VQ-VAE(CAQ-VAE)。

Result: 在Motion-X数据集上FID达到0.06，相比MoMask的0.20有显著提升，推理时间减少27%，在运动编辑等下游任务上表现良好。

Conclusion: MoSa通过分层运动生成框架实现了高质量的3D人体运动生成，在生成质量和效率方面均优于现有方法，具有良好的泛化能力。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [104] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: OmniVLA是一个多模态视觉-语言-动作模型，通过整合红外相机、毫米波雷达和麦克风阵列等新型传感模态，超越了仅依赖RGB相机的感知限制，在真实世界操作任务中取得了84%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖RGB相机，限制了感知能力和操作能力。需要整合更多传感模态来增强物理空间智能。

Method: 提出传感器掩码图像的统一表示方法，将空间基础和物理意义的掩码叠加到RGB图像上。基于RGB预训练的VLA骨干构建多感官架构，使用轻量级传感器投影器实现数据高效学习。

Result: 在需要传感器模态感知的真实世界操作任务中，OmniVLA达到84%的平均任务成功率，比仅RGB模型和原始传感器输入基线分别高出59%和28%，同时表现出更高的学习效率和更强的泛化能力。

Conclusion: OmniVLA通过多模态传感整合显著提升了VLA模型的感知和操作能力，证明了传感器掩码图像表示的有效性，为物理基础空间智能提供了新方向。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [105] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 该论文提出了一种针对印度食物的多步推理视觉问答方法，通过自动生成推理链来提升VQA系统的准确性，相比基线方法平均提升了10个百分点的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统主要面向西方食物，无法处理印度食物的文化多样性和复杂烹饪背景。现有印度食物VQA数据集采用两步法（先生成答案再解释），但作者认为需要多步推理过程才能准确理解印度食物的复杂关系。

Method: 1. 创建自动验证的推理链，最小化人工干预；2. 对较小的LLM和VLM进行微调；3. 使用强化学习在大规模数据上进一步训练；4. 通过推理链增强来提升模型性能。

Result: 通过推理链增强，在印度食物VQA任务上相比基线方法平均提升了10个百分点的准确率，并提供了推理链添加效果的详细分析。

Conclusion: 多步推理过程对于处理印度食物的复杂烹饪背景和食物关系至关重要，推理链增强能显著提升VQA系统的准确性，为文化多样性食物的视觉问答提供了有效解决方案。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [106] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Eyes on Target的深度感知和注视引导的物体检测框架，通过将注视特征注入Vision Transformer的注意力机制，偏向人类注视区域来增强物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 人类注视为理解复杂视觉环境中的视觉注意力提供了丰富的监督信号，但在传统物体检测器中所有区域被平等对待，无法有效利用注视信息来强调观察者优先关注的区域。

Method: 在Vision Transformer的注意力机制中注入注视衍生特征，使空间特征选择偏向人类注视区域，并引入了注视感知注意力头重要性指标来解释模型行为。

Result: 在自定义模拟器数据集和公共基准测试（Ego4D Ego-Motion和Ego-CH-Gaze数据集）上，相比无视注视的基线方法，检测精度持续提升。

Conclusion: 该方法有效利用人类注视信息来增强以自我为中心视频中的物体检测，在模拟场景中评估人类性能方面具有潜力，并通过注意力动态分析揭示了注视线索如何调节Transformer注意力。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [107] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于双阶信息的黑盒梯度可迁移攻击方法，通过引入对抗平坦度(AF)解决欺骗性平坦问题，并开发了对抗平坦度攻击(AFA)和蒙特卡洛对抗采样(MCAS)来提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击方法主要关注平坦损失，但仍陷入次优区域，特别是平坦但尖锐的区域（称为欺骗性平坦），这限制了对抗样本在未知受害者模型上的可迁移性。

Method: 1. 引入对抗平坦度(AF)解决欺骗性平坦问题；2. 提出对抗平坦度攻击(AFA)，通过目标函数的高效近似解决梯度符号改变问题；3. 设计蒙特卡洛对抗采样(MCAS)，提升内循环采样效率。

Result: 在ImageNet兼容数据集上的综合结果表明，该方法优于六个基线方法，生成的对抗样本位于更平坦区域，并提高了跨模型架构的可迁移性。在输入变换攻击和百度云API测试中均优于基线方法。

Conclusion: 所提出的基于双阶信息的可迁移攻击方法有效解决了欺骗性平坦问题，显著提升了对抗样本在不同模型间的可迁移性，在真实世界威胁场景中表现出优越性能。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [108] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出了一种轻量级几何感知适配器，通过方位对齐和水平循环填充来保护LiDAR边界连续性，使用局部KNN收集几何特征，在训练时驱动区域感知正则化以稳定结构脆弱区域的预测，在跨天气语义分割中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR语义分割在恶劣天气下性能下降，因为折射、散射和点丢失会破坏几何结构。现有方法忽视了边界、角落和稀疏区域的结构脆弱性。

Method: 设计轻量级几何感知适配器，包括方位对齐和水平循环填充来保持边界连续性，使用局部窗口KNN收集附近点并计算局部统计特征，压缩为紧凑的几何感知线索，在训练时驱动区域感知正则化。

Result: 在SemanticKITTI训练、SemanticSTF评估的跨天气设置中，相比数据增强基线提升mIoU 7.9个百分点，相比类别中心正则化基线提升0.6个百分点。

Conclusion: 几何驱动的正则化是全天气LiDAR分割的关键方向，该适配器即插即用，仅在训练时启用，推理成本可忽略。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [109] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: MotionStream是一个实时运动条件视频生成系统，通过将双向教师模型蒸馏为因果学生模型，实现亚秒级延迟和高达29 FPS的流式生成，支持无限长度视频的实时交互。


<details>
  <summary>Details</summary>
Motivation: 当前基于运动条件的视频生成方法存在延迟高（分钟级）和非因果处理的限制，无法实现实时交互。

Method: 通过自强制分布匹配蒸馏将双向教师模型转化为因果学生模型，采用滑动窗口因果注意力结合注意力汇技术，在训练中通过自展开和KV缓存滚动模拟推理时外推，实现固定上下文窗口的恒定速度生成。

Result: 在运动跟随和视频质量方面达到最先进水平，同时速度提升两个数量级，能够实现无限长度的流式生成。

Conclusion: MotionStream实现了真正交互式的实时视频生成体验，用户可以通过绘制轨迹、控制相机或传输运动来实时查看结果。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [110] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: PRevivor是一个基于先验引导的颜色转换器，通过学习明清时期绘画来恢复唐宋时期古画的色彩，通过亮度增强和色调校正两个子任务实现颜色恢复。


<details>
  <summary>Details</summary>
Motivation: 中国古代绘画是宝贵的文化遗产，但因不可逆的颜色退化而受损。由于复杂的化学机制和缺乏高质量数据集，颜色恢复非常困难，阻碍了端到端数字修复工具的开发。

Method: 将颜色恢复分解为亮度增强和色调校正两个顺序子任务。亮度增强使用两个变分U-Net和多尺度映射模块；色调校正设计双分支颜色查询模块，通过从褪色绘画中提取的局部色调先验进行引导。

Result: 与最先进的着色方法进行广泛实验，结果显示在定量和定性评估中均表现出优越性能。

Conclusion: PRevivor能够有效恢复古画的色彩，为文化遗产保护提供了有效的数字修复工具。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [111] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 这篇综述全面评估了基础模型在医学影像分析中的适应策略，包括监督微调、领域特定预训练、参数高效微调等方法，并探讨了持续学习、联邦学习等新兴方向，为开发适应性强、可信赖的临床集成基础模型提供了路线图。


<details>
  <summary>Details</summary>
Motivation: 基础模型在医学影像分析中具有变革潜力，但实际临床应用面临领域偏移、标注数据稀缺、计算需求大和隐私要求严格等挑战，需要系统评估适应策略。

Method: 评估了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态/跨模态框架等多种适应方法，并分析了每种方法的性能增益、临床适用性和局限性。

Result: 识别了现有方法的权衡和未解决挑战，同时突出了持续学习、联邦学习、混合自监督学习、数据为中心管道和系统基准测试等新兴方向，以解决当前差距。

Conclusion: 通过概述这些策略和相关研究空白，为开发能够满足真实世界医学影像需求的适应性、可信赖和临床集成的基础模型提供了路线图。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [112] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 提出了一种基于数据流形几何差异的图像生成检测框架，利用自然图像和生成图像在流形结构上的正交性差异，通过自监督模型损失变化进行检测，并使用归一化流放大可检测差异。


<details>
  <summary>Details</summary>
Motivation: 随着生成图像真实感的提升，其潜在滥用风险增加，需要强大的检测方法。现有方法主要依赖二元分类器，对生成图像的数量和质量依赖性强。

Method: 利用自然图像和生成图像数据流形的几何差异，设计一对函数使自然图像输出一致而生成图像输出发散，通过自监督模型在数据流形变换时的损失变化进行检测，并使用归一化流放大差异。

Result: 大量实验证明了该方法的有效性，代码已开源。

Conclusion: 该方法提供了一种简单有效的生成图像检测方案，能够应对先进生成模型中逐渐减小的流形差异问题。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [113] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出了UniREditBench基准测试，用于评估基于推理的图像编辑模型性能，涵盖真实世界和游戏世界场景，包含2700个样本和8个主要维度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成模型在需要隐式推理的复杂图像编辑任务上表现不佳，现有基准测试主要关注单对象属性变换，忽视了多对象交互和游戏世界场景，且仅依赖文本参考可能导致误判。

Method: 构建UniREditBench基准测试，包含2700个样本覆盖8个主要维度和18个子维度；引入多模态双参考评估（文本和真实图像参考）；设计自动化多场景数据合成流水线，创建UniREdit-Data-100K大规模合成数据集；在Bagel模型上微调开发UniREdit-Bagel。

Result: UniREdit-Bagel在领域内和领域外设置中均表现出显著改进；通过对开源和闭源图像编辑模型的全面基准测试，揭示了它们在不同方面的优势和弱点。

Conclusion: UniREditBench为基于推理的图像编辑提供了系统评估框架，多模态双参考评估提高了评估可靠性，大规模合成数据集支持模型训练和性能提升。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [114] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出一种名为REASON的两阶段概率图引导双分支融合框架，用于通过超声自动评估胃内容物，以改善全身麻醉诱导时的误吸风险分层。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手动追踪胃窦和使用经验公式，在效率和准确性方面存在显著局限性，需要更自动化和准确的胃内容物评估方法。

Method: 第一阶段使用分割模型生成抑制伪影并突出胃解剖结构的概率图；第二阶段使用双分支分类器融合右侧卧位和仰卧位两个标准视图的信息，以改善学习特征的判别能力。

Result: 在自收集数据集上的实验结果表明，该框架显著优于当前最先进的方法。

Conclusion: 该框架为自动化术前误吸风险评估提供了更稳健、高效和准确的解决方案，在临床实践中具有巨大潜力。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [115] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种用于全切片病理图像表示的三阶段解缠学习框架，通过潜在因子分组、聚类推理实例解缠和实例效应重加权来解决空间、语义和决策纠缠问题，在多个数据集上优于现有方法并实现病理学家对齐的可解释性。


<details>
  <summary>Details</summary>
Motivation: 多实例学习在病理图像表示中广泛使用，但实例间的空间、语义和决策纠缠限制了其表示能力和可解释性，需要解决这些挑战。

Method: 三阶段框架：1）正半定潜在因子分组映射实例到潜在子空间缓解空间纠缠；2）通过聚类推理实例解缠使用实例概率反事实推理和优化缓解语义纠缠；3）通过实例效应重加权的广义线性加权决策解决决策纠缠。

Result: 在多个多中心数据集上的广泛实验表明，该模型优于所有最先进模型，并通过解缠表示和透明决策过程实现了病理学家对齐的可解释性。

Conclusion: 提出的解缠学习框架有效解决了病理图像表示中的空间、语义和决策纠缠问题，在性能和可解释性方面均表现出色。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [116] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 本文提出了APDM框架，通过将保护目标从图像转移到扩散模型本身来防止特定主体的个性化，解决了现有对抗扰动方法在存在干净图像或简单图像变换时失效的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成特定主体（如身份或物体）方面取得了显著进展，但这种能力也带来了隐私风险，可能被恶意用户用于生成未经授权的内容。现有基于对抗扰动的方法存在不现实的假设，在少量干净图像或简单图像变换下就会失效。

Method: 提出了Anti-Personalized Diffusion Models (APDM)框架，包括：1）理论分析表明现有损失函数无法确保鲁棒反个性化收敛；2）引入Direct Protective Optimization (DPO)损失函数，在不影响生成质量的情况下有效干扰目标模型的主体个性化；3）提出双路径优化策略Learning to Protect (L2P)，通过交替进行个性化和保护路径来模拟未来个性化轨迹并自适应强化保护。

Result: 实验结果表明，APDM框架在防止未经授权个性化方面优于现有方法，达到了最先进的性能。

Conclusion: APDM通过将保护目标转移到扩散模型本身，并采用DPO损失函数和L2P优化策略，有效解决了扩散模型个性化带来的隐私风险问题，为内容创作提供了更安全的保障。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [117] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP模型的生成式对抗攻击方法，通过结合文本和图像表示来创建视觉不可察觉但有效的对抗扰动，在多对象环境中欺骗多标签分类器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型容易受到对抗攻击的影响，现有方法在保持视觉保真度方面存在不足，需要开发既能有效欺骗分类器又保持高视觉相似性的攻击方法。

Method: 将SSAE的集中扰动策略与GAMA的差异文本嵌入相结合，利用CLIP模型对齐文本和图像表示的能力，通过引导损失函数生成对抗样本。

Result: 在各种黑盒受害者模型上的实验表明，该方法在性能上具有竞争力，达到或优于现有技术，同时保持更高的视觉保真度。

Conclusion: 所提出的方法成功实现了在欺骗分类模型的同时保持与原始图像的高度结构相似性，为对抗攻击领域提供了新的有效解决方案。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [118] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出了一个名为CMI-MTL的跨模态交互多任务学习框架，用于解决医学视觉问答中的跨模态语义对齐问题，通过三个关键模块实现细粒度视觉-文本特征对齐、跨模态交错特征表示和自由形式答案增强的多任务学习。


<details>
  <summary>Details</summary>
Motivation: 现有的自注意力方法难以有效处理视觉和语言之间的跨模态语义对齐，且基于分类的方法依赖预定义答案集，无法适应自由形式答案的多样性，忽略了自由形式答案的详细语义信息。

Method: CMI-MTL框架包含三个关键模块：FVTA（细粒度视觉-文本特征对齐）、CIFR（跨模态交错特征表示）和FFAE（自由形式答案增强的多任务学习），分别处理图像-文本对的最相关区域提取、跨模态序列交互捕获和利用开放性问题辅助知识。

Result: 在三个Med-VQA数据集（VQA-RAD、SLAKE和OVQA）上的实验结果表明，CMI-MTL优于现有的最先进方法，并通过可解释性实验证明了其有效性。

Conclusion: CMI-MTL框架通过跨模态交互和多任务学习有效解决了医学视觉问答中的跨模态语义对齐问题，提升了模型在开放性问题上的能力，代码已公开。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [119] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种通过内容-风格子空间混合和内容-风格平衡损失来扩展内容-风格前沿的新方法，解决了风格强度增加导致内容特征丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在单一风格强度下评估内容相似性，但实验发现增加风格强度会导致内容特征显著丢失，形成次优的内容-风格前沿。

Method: 采用内容-风格子空间混合技术和内容-风格平衡损失函数，在保持风格多样性的同时提升内容相似性。

Result: 该方法在定性和定量评估中均优于现有技术，实现了更优的内容-风格权衡，显著降低了倒置生成距离和生成距离得分。

Conclusion: 所提出的方法有效扩展了内容-风格前沿，在多种风格强度下都能保持更好的内容相似性，为文本到图像生成提供了更优的内容-风格平衡解决方案。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [120] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 本研究提出了Fire-ART数据集和基于全景图像的消防资产重建方法，用于将消防资产语义丰富到BIM模型中，提高消防资产管理的自动化和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统消防资产管理方法在自动资产识别和重建方面能力有限，效率低下，需要更有效的技术解决方案来支持应急准备、风险评估和现场火灾响应。

Method: 开发了包含15种基本资产、2626张图像和6627个实例的Fire-ART数据集，并提出了集成改进的立方体贴图转换和基于半径的球形相机投影的重建方法。

Result: 通过两个真实案例验证，该方法分别实现了73%和88%的F1分数，以及0.620米和0.428米的定位误差。

Conclusion: Fire-ART数据集和重建方法为消防设备精确数字化管理提供了宝贵资源和强大技术解决方案。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [121] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: UniSOT是一个统一跟踪器，能够处理三种参考模态（边界框、自然语言或两者）和四种视频模态（RGB、RGB+深度、RGB+热成像或RGB+事件）的组合，使用统一参数实现跨模态跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常针对单一或少数几种视频模态和参考模态设计，导致模型分离且限制实际应用。需要开发统一跟踪器来处理各种需求，但目前尚无能够同时处理上述所有参考模态和视频模态的跟踪器。

Method: 提出UniSOT统一跟踪器，通过统一参数设计支持三种参考模态和四种视频模态的不同组合，实现跨模态跟踪能力。

Result: 在18个视觉跟踪、视觉语言跟踪和RGB+X跟踪基准测试中，UniSOT表现出优于特定模态对应方法的性能。在TNL2K上所有三种参考模态的AUC超过先前方法3.0%以上，在RGB+X视频模态上主要指标超过Un-Track 2.0%以上。

Conclusion: UniSOT证明了统一跟踪器在处理多种参考模态和视频模态组合方面的有效性和优越性，为实际应用提供了更灵活的解决方案。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [122] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 提出了一种无需训练的可解释性方法，用平滑可调轮廓替代密集扰动掩码，通过星凸区域参数化和傅里叶级数优化，在保持极值保真度的同时生成紧凑、可解释的区域。


<details>
  <summary>Details</summary>
Motivation: 解决现有密集扰动掩码方法存在的碎片化、过拟合问题，需要复杂后处理，寻求更紧凑、稳定的视觉模型解释方法。

Method: 使用截断傅里叶级数参数化星凸区域，在极值保留/删除目标下利用分类器梯度进行优化，保证单一连通掩码，大幅减少自由参数数量。

Result: 在ImageNet分类器上匹配密集掩码的极值保真度，产生紧凑可解释区域，提高运行一致性，在基准测试中比梯度和扰动基线获得更高相关质量和更低复杂度。

Conclusion: 该方法通过低维平滑轮廓生成鲁棒的解释，特别在自监督DINO模型上表现优异，相关性质量提升超过15%，保持正忠实性相关性。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [123] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出了一种模型无关的序数元学习算法(MAOML)，用于训练小型视觉语言模型，在数据稀缺情况下实现水果新鲜度分类任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 由于专家标注成本高导致数据稀缺，而专有视觉语言模型存在数据隐私问题无法使用，现有开源模型性能不佳，需要开发能在零样本和少样本设置下有效工作的解决方案。

Method: 采用模型无关的序数元学习算法，结合元学习解决数据稀疏性问题，并利用标签的序数特性进行训练。

Result: 在水果新鲜度分类任务中实现了92.71%的行业标准准确率，在零样本和少样本设置下均达到最先进性能。

Conclusion: MAOML算法能够有效训练小型视觉语言模型，在数据稀缺情况下超越现有开源模型，接近专有模型的性能水平，同时解决了数据隐私问题。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [124] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于蒸馏的高效因果视频生成框架，通过对抗性自蒸馏策略和首帧增强策略，在极少的去噪步骤下实现高质量视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有的混合视频生成模型结合自回归时间动态和基于扩散的空间去噪，但其顺序迭代特性导致错误累积和长推理时间。需要开发更高效的视频生成方法。

Method: 基于分布匹配蒸馏框架，提出对抗性自蒸馏策略，将学生模型的n步去噪输出与其(n+1)步版本在分布级别对齐；同时提出首帧增强策略，为首帧分配更多去噪步骤以减少错误传播。

Result: 在VBench上的广泛实验表明，该方法在一步和两步视频生成中都超越了最先进的方法，能够生成支持多种推理步骤设置的单一蒸馏模型。

Conclusion: 该框架通过创新的蒸馏策略实现了高效高质量的视频生成，消除了重复再蒸馏的需求，为极少数步骤场景下的视频合成提供了稳定有效的解决方案。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [125] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自动构建高质量偏好对的方法GT-Pair，以及结合SFT损失作为正则化项的Reg-DPO方法，解决了视频生成中数据构建成本高、训练不稳定和内存消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的DPO方法主要基于图像领域范式，且在小规模模型上开发，无法有效解决视频任务特有的挑战，如数据构建成本高、训练不稳定和内存消耗大。

Method: 1. 提出GT-Pair方法，使用真实视频作为正样本，模型生成视频作为负样本，自动构建高质量偏好对；2. 提出Reg-DPO方法，将SFT损失作为正则化项融入DPO目标，增强训练稳定性和生成保真度；3. 结合FSDP框架与多种内存优化技术，提升训练容量。

Result: 在多个数据集的I2V和T2V任务上的广泛实验表明，该方法持续优于现有方法，提供更优的视频生成质量。

Conclusion: 该方法通过自动构建偏好对、引入正则化项和优化内存使用，有效解决了视频生成中的关键挑战，显著提升了生成质量。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [126] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 该论文提出了一种名为QA-SNNE的黑盒不确定性估计方法，通过将问题语义融入预测置信度来提高手术视觉问答(VQA)的安全性和可靠性。该方法在医学文本嵌入空间中比较生成答案与最近邻的语义相似性来测量语义熵。


<details>
  <summary>Details</summary>
Motivation: 手术VQA的安全性和可靠性至关重要，因为错误或模糊的回答可能对患者造成伤害。现有研究大多关注准确性或语言质量，而忽略了安全行为，如模糊意识、转诊给人类专家或触发二次意见。

Method: 引入Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE)，这是一种黑盒不确定性估计器，将问题语义纳入预测置信度。它通过在医学文本嵌入空间中比较生成答案与最近邻的语义相似性来测量语义熵。

Result: 在EndoVis18-VQA和PitVQA数据集上评估了五个模型。PEFT模型在轻微改写下性能下降，而LVLMs更具弹性。QA-SNNE在大多数模板内设置中提高了AUROC，并增强了幻觉检测。零样本模型的AUROC提高了15-38%，在模板外压力下增益保持。

Conclusion: QA-SNNE通过将语义不确定性与问题上下文联系起来，为手术VQA中的自动故障检测提供了实用且可解释的步骤。将LVLM骨干与问题对齐的不确定性估计相结合可以提高安全性和临床医生的信任度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [127] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 该论文提出了一种分辨率感知的token解码器，用于解决越野语义分割中的边界模糊、稀疏监督和标签噪声问题，通过全局语义、局部一致性和边界保真度的平衡设计来提升性能。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临边界不一致、稀有类别稀疏监督和普遍标签噪声的挑战。现有方法在低分辨率融合时会模糊边缘并传播局部错误，而保持高分辨率路径或重复高分辨率融合则成本高且对噪声敏感。

Method: 引入分辨率感知token解码器，主要计算在低分辨率瓶颈进行；使用门控交叉注意力注入精细尺度细节；仅对稀疏、不确定性选择的像素集进行细化。组件包括全局自注意力与轻量级扩张深度细化、门控交叉注意力集成高分辨率编码器特征、类别感知点细化。训练时添加边界带一致性正则化器。

Result: 结果表明具有竞争力的性能和跨过渡的改进稳定性。

Conclusion: 提出的方法在越野语义分割中有效平衡了全局语义、局部一致性和边界保真度，在保持计算效率的同时提升了分割质量。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [128] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 该论文提出了一种仅用于训练的方法来解决热红外检测中的低对比度和弱高频线索问题，通过拉近同类特征和推远不同类特征来锐化决策边界，并利用RGB训练的教师模型注入跨模态语义先验，在保持单模态推理的同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 热红外感知在夜间面临挑战：低对比度和弱高频线索导致重复、重叠的检测框、小目标漏检和类别混淆。现有方法要么将TIR转换为RGB（对颜色或结构伪影敏感），要么在测试时融合RGB和TIR（需要额外传感器和精确校准），都没有直接塑造检测器使用的热表示。

Method: 在训练阶段引入两个目标：1）通过拉近同类特征和推远不同类特征来锐化实例级决策边界，抑制重复和混淆检测；2）通过将学生的多级金字塔特征与RGB训练的教师模型对齐，注入跨模态语义先验，增强纹理贫乏的热特征。

Result: 在实验中，该方法优于先前方法并实现了最先进的性能。

Conclusion: 该方法在保持单模态推理的同时，通过训练阶段的改进直接解决了热红外检测的根本问题，实现了更鲁棒的夜间感知性能。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [129] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 本文提出了一种五层结构化模型来改进罕见驾驶场景的评估和生成，使用数据增强策略结合大型基础模型生成新场景，并引入了多样性和原创性指标来评估合成数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 罕见且具有挑战性的驾驶场景对于自动驾驶车辆开发至关重要，但由于难以遇到，需要通过生成模型来模拟或生成这些场景。

Method: 提出结构化五层模型，为场景中的每个智能体引入子类和特征，使用特定于层模型的嵌入进行比较；采用数据增强策略结合大型基础模型生成新驾驶场景；研究并适配了多样性评分和原创性评分两种指标。

Result: 在不同生成设置下展示了两种指标的应用，并对从结构化场景描述生成的合成视频进行了定性评估。

Conclusion: 提出的结构化五层模型和评估指标能够有效改进罕见驾驶场景的生成和评估，为自动驾驶开发提供了有价值的工具。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [130] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX是一个模块化、可扩展的自监督视觉基础模型训练框架，统一了DINO系列方法的核心原理，支持多种Transformer架构和训练策略，显著降低计算成本并保持竞争力性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型训练流程存在不灵活、领域特定或计算成本高的问题，限制了在不同领域和资源环境下的可用性。

Method: 采用配置驱动的统一系统，结合DINO系列方法原理，支持多种Transformer架构，提供LoRA、层冻结、知识蒸馏等训练策略，以及分布式训练支持。

Result: 在多样化数据集上的实验表明，DINO-MX在显著降低计算成本的同时实现了竞争力性能，并提供可解释性工具和标签引导数据增强方法。

Conclusion: DINO-MX为开发和基准测试自监督视觉模型提供了可重现、可扩展的基础，适用于研究和实际应用场景。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [131] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 提出了一种基于统计量化的低光照图像增强框架LASQ，将亮度转换建模为幂律分布，通过分层亮度分布的统计采样过程替代确定性映射，实现了无需正常光照参考的无监督增强。


<details>
  <summary>Details</summary>
Motivation: 现有低光照图像增强方法主要关注确定性像素级映射，忽略了真实环境中亮度转换的连续物理过程，导致在缺乏正常光照参考时性能下降。

Method: 引入亮度感知统计量化(LASQ)框架，将亮度转换重新概念化为强度坐标空间中的幂律分布，通过分层幂函数近似，设计扩散前向过程自主发现亮度层间的最优转换路径。

Result: 在无正常光照参考的实际场景中显著提升性能，同时在有参考情况下在领域特定数据集上实现更优性能，并具有更好的跨数据集泛化能力。

Conclusion: LASQ框架通过统计采样方法有效解决了低光照图像增强中重建保真度与跨场景泛化之间的平衡问题，实现了更适应和通用的光照恢复。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [132] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出一个完整的工作流程系统，用于实现具有独特局部特征的纹理的受控创作和编辑，包括污渍、撕裂、孔洞、磨损、变色等表面效果。


<details>
  <summary>Details</summary>
Motivation: 自然界中普遍存在各种改变材料表面外观的效果，将这些变化纳入合成过程对于生成逼真纹理至关重要。

Method: 采用基于学习的方法，利用未标记示例，通过无监督异常检测来识别外观改变特征，然后自动将各种纹理特征聚类为语义连贯的组，用于指导条件图像生成。

Result: 构建了一个从少量图像集合到多功能生成模型的完整流程，使用户能够交互式地在任意大小的纹理上创建和绘制特征。

Conclusion: 该系统提供了从图像收集到生成模型的完整工作流程，其中基于扩散的编辑和无限平稳纹理生成的算法具有通用性，在其他场景中也应有用。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [133] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: 提出了一种新颖的对比学习框架NSYNC，通过生成负样本合成图像来改进大型文本到图像扩散模型的风格化能力，使用正交梯度更新来消除正负样本中的共同属性，从而更好地捕捉独特风格。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的图像生成方法虽然能生成逼真图像，但无法准确捕捉特定风格。直接在目标风格数据集上微调仍难以掌握风格特征，因此需要新的训练方法来提升风格化能力。

Method: 利用合成图像生成技术创建负样本集，与真实正样本一起进行对比训练。通过计算正梯度在负梯度上的投影，获得正交分量来更新参数，消除正负样本中的共同属性，使模型专注于学习独特风格特征。

Result: 在多种画家和插画师风格上的实验表明，该方法在定量和定性评估上都优于基线方法，显著提升了风格化图像生成的质量。

Conclusion: NSYNC框架通过创新的对比学习方案有效提升了文本到图像扩散模型的风格化能力，证明了使用负样本合成数据在风格学习中的重要性。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [134] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: NOA是一个基于Napari的图形用户界面工具，旨在简化基于AI的类器官分析，整合了检测、分割、跟踪、特征提取和机器学习预测等多个模块。


<details>
  <summary>Details</summary>
Motivation: 现有AI工具对无编程经验的生物学家来说难以使用，导致工作流程繁琐且主要依赖手动操作，需要开发更易用的分析工具。

Method: 开发了Napari Organoid Analyzer (NOA)，这是一个开源插件，集成了多种最先进算法，提供检测、分割、跟踪、特征提取和机器学习预测功能。

Result: 通过三个案例研究展示了NOA的多功能性：量化类器官分化过程中的形态变化、评估光毒性效应、预测类器官活力和分化状态。

Conclusion: NOA为类器官图像分析提供了一个全面、可访问且可扩展的AI驱动框架。

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [135] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: CDD-VT提出了一种连续-离散双态视觉标记器，通过自适应分配图像基元数量来解决多模态大模型中理解与生成的统一问题，简单实例使用少量基元（类似离散标记），复杂实例使用更多基元（类似连续标记）。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型中理解与生成统一化的挑战，克服连续标记器复杂多阶段流程和离散标记器信息丢失的问题，受光的波粒二象性启发。

Method: 设计连续-离散双态视觉标记器，包含多样化量化基元（促进基元正交性以填充信息空间）和动态基元分配器（根据样本复杂度确定最优基元集合）。

Result: 在重建、检索和分类任务上的广泛实验表明，CDD-VT在性能和简洁性方面优于专门的连续标记器和离散标记器。

Conclusion: CDD-VT成功实现了在简洁可扩展的多模态大模型中统一理解与生成，取得了强大结果。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [136] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: Lite ENSAM是一个轻量级架构，用于从带有RECIST标注的CT扫描中高效进行肿瘤体积分割，在MICCAI FLARE 2025竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 肿瘤体积测量比RECIST标准更可靠评估治疗效果，但手动体积标注耗时费力，需要自动化解决方案。

Method: 基于ENSAM架构的轻量级适配，专门用于从RECIST标注的CT扫描中进行肿瘤体积分割。

Result: 在隐藏测试集上获得DSC 60.7%和NSD 63.6%，在公共验证集上平均RAM时间50.6GB，CPU推理时间14.4秒。

Conclusion: Lite ENSAM为肿瘤体积分割提供了高效解决方案，有助于推动体积测量在临床中的采用。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [137] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出PSPooling非学习性网格池化算子和MedShapeNet19基准数据集，用于3D解剖形状分类，通过自监督图自编码器学习解剖感知表示，在低标签情况下显著提升重建保真度和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解剖3D形状分类受限于网格数据复杂性和缺乏标准化基准，需要鲁棒学习方法和可复现评估。

Method: 提出PSPooling基于几何邻近性预计算节点对应集，实现并行化可逆池化和反池化操作；构建自监督图自编码器，从无标签表面网格学习解剖感知表示；创建MedShapeNet19基准数据集。

Result: PSPooling显著改善重建保真度和低标签情况下的分类准确性，为医学3D形状学习建立强基线。

Conclusion: PSPooling和MedShapeNet19为解剖形状分类提供有效解决方案，希望MedShapeNet19成为医学3D形状分析的广泛采用基准。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [138] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: ViC是一个无需训练的零样本推理框架，将异构检索器的候选融合重新定义为视觉语言模型的推理任务，通过序列化内容证据和检索器元数据，在跨模态视频检索中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构检索器在复杂多模态数据（如视频）中候选融合的长期挑战，传统融合方法仅依赖排名或分数信号而忽略候选表示。

Method: 提出Vote-in-Context (ViC)框架，将列表重排序和融合作为视觉语言模型的零样本推理任务，使用S-Grid紧凑序列化地图表示视频，结合内容证据和检索器元数据进行自适应加权。

Result: 在视频检索基准测试中，ViC作为单列表重排序器显著提升个体检索器精度，作为集成融合器持续优于CombSUM等基线方法，在MSR-VTT和VATEX上实现零样本检索的最先进性能，Recall@1得分分别达到87.1%/89.0%和99.6%。

Conclusion: ViC是将现代视觉语言模型转化为强大零样本重排序器和融合器的简单、可复现且高效的解决方案。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [139] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 本文提出了一种将强化学习有效整合到基于扩散的图像修复模型中的新方法，通过使用基于图像质量评估模型的奖励函数，并针对远离真实值的困难样本进行强化学习训练，实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法直接应用于基于扩散的图像修复模型效果不佳，因为修复任务更强调保真度而非纯生成。需要探索如何有效整合强化学习到扩散修复模型中。

Method: 使用基于图像质量评估模型的奖励函数替代直观的基于真实值的监督；针对远离真实值的困难样本进行强化学习训练；采用MLLM-based IQA模型进行分布对齐；结合自适应权重策略动态调整强化学习和监督微调的组合。

Result: 该方法可无缝应用于基于扩散的修复模型，在各种修复任务中显著提升性能。在多个基准测试上的广泛实验证明了所提强化学习框架的有效性。

Conclusion: 提出的强化学习框架能够有效提升基于扩散的图像修复模型的性能，通过创新的奖励函数设计和自适应训练策略，实现了更好的修复效果。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [140] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos是一个统一的图像和视频重照明框架，通过引入RGB空间的几何反馈到流匹配主干中，显著提高了重照明的物理一致性，同时实现了20倍的加速。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在重照明任务中由于在语义潜在空间优化而导致的物理不正确问题，如过曝高光、错位阴影和不正确遮挡。

Method: 使用从输出中提取的深度和法线图来监督模型，将光照效果与场景结构对齐；采用路径一致性学习实现少步训练；设计了六维标注协议用于细粒度控制。

Result: UniLumos实现了最先进的重照明质量，显著提高了物理一致性，在图像和视频重照明上均实现了20倍加速。

Conclusion: UniLumos通过几何反馈和路径一致性学习，有效解决了重照明中的物理一致性问题，同时大幅提升了计算效率。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [141] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 本文提出了PRBench，这是首个专门评估不同鲁棒性训练方法在概率鲁棒性（PR）方面改进的基准测试。研究发现对抗训练（AT）方法在提升对抗鲁棒性和概率鲁棒性方面更具通用性，而PR针对性训练方法在泛化误差和干净准确率方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型对微小扰动非常脆弱。现有研究主要关注对抗鲁棒性（AR），而概率鲁棒性（PR）从统计角度评估模型在随机扰动下的表现。虽然PR被视为AR的实用补充，但专门针对PR的训练方法仍相对不足，且现有方法存在评估协议不可比、与强AT基线比较有限、缺乏统一框架等问题。

Method: 作者提出了PRBench基准测试，通过全面的指标集（包括干净准确率、PR和AR性能、训练效率和泛化误差）对常见的AT和PR针对性训练方法进行实证比较，并提供了PR性能泛化误差的理论分析。

Result: 主要发现包括：AT方法在提升AR和PR性能方面更具通用性，而PR针对性训练方法始终产生更低的泛化误差和更高的干净准确率。研究构建了包含222个训练模型的排行榜，涵盖7个数据集和10种模型架构。

Conclusion: PRBench为评估PR改进提供了标准化基准，揭示了不同训练方法在鲁棒性、准确率和泛化能力方面的权衡关系，为未来PR训练方法的发展提供了重要参考。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [142] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 开发了一个任务探索器管道，使用聚类技术、因子分析和字符串编辑距离自动识别完成任务的关键全局和局部策略，并识别任务中有意义的子任务。


<details>
  <summary>Details</summary>
Motivation: 推进机器对用户知识、技能和行为的理解，以实现隐式协调，在预期性人机交互中促进有利的互动。

Method: 开发任务探索器管道，结合聚类技术、因子分析和字符串编辑距离，自动识别全局策略（完成任务的动作集合）和局部策略（相似动作组合的序列），并识别不同长度的有意义的子任务。

Result: 任务探索器管道能够自动识别完成任务的关键策略，并用分层子任务结构编码用户运行轨迹。开发了Task Explorer应用程序来轻松查看管道结果。

Conclusion: 任务探索器管道可轻松修改以适应任何基于动作的时间序列数据，识别出的策略和子任务有助于人类和机器了解用户的知识、技能和行为。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [143] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: HGFreNet是一个新颖的GraphFormer架构，通过跳数混合特征聚合和频域3D轨迹一致性来解决2D到3D人体姿态提升中的深度模糊和时间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 解决2D到3D人体姿态提升中的深度模糊和2D姿态估计误差导致的3D轨迹不一致问题，现有方法主要约束相邻帧差异而忽略了骨骼关节运动的全局时空相关性。

Method: 提出跳数混合图注意力模块和Transformer编码器建模全局关节时空相关性，HGA模块将骨骼关节的k跳邻居分组为混合组以扩大感受野，并在频域约束轨迹一致性来利用全局时间相关性。

Result: 在Human3.6M和MPI-INF-3DHP基准数据集上的实验表明，HGFreNet在位置精度和时间一致性方面优于最先进方法。

Conclusion: HGFreNet通过全局时空建模和频域轨迹一致性约束，有效提升了2D到3D人体姿态估计的准确性和时间连贯性。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [144] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: UniLION是一个统一的自动驾驶模型，使用线性组RNN算子高效处理大规模LiDAR点云、高分辨率多视角图像和时间序列，无需显式的时间或多模态融合模块，在多种核心任务中实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次注意力机制在处理长序列数据时带来显著计算开销，需要更高效的架构来处理自动驾驶中的大规模多模态数据。

Method: 基于线性组RNN算子构建统一架构，支持LiDAR-only、时序LiDAR、多模态和多模态时序融合等多种配置，无需专门的融合模块。

Result: 在3D感知（物体检测、跟踪、占用预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等核心任务中实现竞争性甚至最先进的性能。

Conclusion: UniLION为自动驾驶3D基础模型的发展提供了新视角，简化了多模态多任务系统的设计，同时保持优越性能。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [145] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种基于提示驱动的GraphRAG框架，强调提示设计在图检索增强生成中的重要性，在HotpotQA和2WikiMultiHopQA数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于图的检索在复杂推理中已有研究，但提示设计对检索和推理过程的影响仍被严重忽视，需要探索提示驱动的方法来提升多跳问答性能。

Method: 构建符号知识图谱表示实体和事实关系，使用LLM进行语义过滤和答案生成，通过个性化PageRank实现基于实体的图遍历，实现高效可扩展的检索。

Result: 在HotpotQA和2WikiMultiHopQA上分别达到80.7%和78.9%的F1分数，Recall@5分别达到97.1%和98.1%，实现了最先进的性能。

Conclusion: 提示设计是提高检索准确性和响应质量的关键因素，为更高效和可理解的多跳问答系统奠定了基础，凸显了提示感知图推理的重要性。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [146] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: Scitextures数据集是一个大规模的科学纹理和视觉模式集合，包含1200多个模型和10万张图像，涵盖物理、化学、生物、社会学、技术、数学和艺术等领域，用于探索视觉模式与生成机制之间的联系。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模式与形成机制之间的深层联系，理解云层、波浪、城市发展、森林生长等自然和人工模式背后的生成过程。

Method: 通过自主AI管道收集和实现标准化模型，创建大规模数据集，并评估AI模型在连接视觉模式与生成代码方面的能力，包括模式识别、机制推断和图像生成。

Result: 视觉语言模型能够理解和模拟物理系统，超越单纯的视觉模式识别，能够推断并重新创建形成视觉模式的机制。

Conclusion: 该数据集和基准测试展示了AI在理解视觉模式与生成机制之间联系方面的潜力，为跨学科研究提供了重要资源。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [147] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出了TIR-Bench基准测试，用于评估视觉推理中的智能图像处理能力，测试了22个多模态大语言模型在13个任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分捕捉像OpenAI o3这样的模型在图像处理中的智能工具使用能力，需要更全面的评估标准。

Method: 开发了包含13个多样化任务的TIR-Bench基准，每个任务都需要在思维链中进行新颖的图像处理工具使用。

Result: TIR-Bench对所有测试模型都具有挑战性，只有具备真正图像思维能力的模型才能表现良好。

Conclusion: 提出了直接微调与智能微调的对比研究，强调了在视觉推理中智能工具使用能力的重要性。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [148] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: PlotCraft是一个新的可视化基准测试，包含1000个具有挑战性的可视化任务，涵盖7个高级可视化任务和48种图表类型。研究发现当前LLMs在处理复杂可视化任务时存在明显缺陷，为此开发了SynthVis-30K数据集和PlotCraftor模型，在复杂数据可视化方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在代码生成方面表现出色，但在为规模化结构化数据创建复杂可视化方面的能力尚未得到充分评估和发展，需要填补这一研究空白。

Method: 引入PlotCraft基准测试，开发SynthVis-30K大规模高质量复杂可视化代码数据集，并基于此构建PlotCraftor代码生成模型。

Result: 对23个领先LLMs的评估显示其在处理复杂可视化任务时存在明显性能缺陷。PlotCraftor模型在VisEval、PandasPlotBench和PlotCraft基准测试中表现与领先专有方法相当，在困难任务上实现超过50%的性能提升。

Conclusion: PlotCraft基准测试揭示了LLMs在复杂可视化方面的局限性，而基于SynthVis-30K数据集开发的PlotCraftor模型有效提升了复杂数据可视化的代码生成能力。

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [149] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 本文提出了残差流解码器框架，用于探测语言模型中段落和文档级别的规划信息，发现在小模型中可解码相当于5个以上未来token的信息。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能够处理更长时间跨度的任务，现有理解激活的方法通常局限于测试特定概念或token，需要开发新方法来理解模型如何编码长期规划信息。

Method: 开发了残差流解码器框架，作为探测模型激活中段落级和文档级规划的方法，测试了多种解码方法。

Result: 发现在小模型中，可以从激活中解码出相当于5个以上未来token上下文的信息。

Conclusion: 这些结果为更好地监控语言模型以及理解它们如何编码长期规划信息奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


### [150] [Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap](https://arxiv.org/abs/2511.00198)
*Chun-Hao Yang,Bo-Han Feng,Tzu-Yuan Lai,Yan Yu Chen,Yin-Kai Dean Huang,Shou-De Lin*

Main category: cs.CL

TL;DR: 本文挑战了使用下一词预测训练大型语言模型的传统方法，提出通过预测信息丰富的词元来更有效地训练LLM，并在算术、多标签文本分类和自然语言生成任务中验证了该方法的效果。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型的训练性能是一个关键挑战，特别是在提高模型性能的同时控制计算成本。传统使用下一词预测的方法可能不是最优的，需要探索更有效的训练策略。

Method: 提出了一种替代下一词预测的方法，通过选择信息丰富的目标词元进行预测来训练LLM。在算术、多标签文本分类和自然语言生成三种任务类型中验证了该方法。

Result: 研究表明，通过预测信息丰富的词元可以更有效地训练LLM，该方法在多个任务类型中显示出优于传统下一词预测方法的潜力。

Conclusion: 这项工作为优化LLM训练提供了原则性方法，不仅提升了模型性能，还深化了对目标词元选择策略的理论理解。

Abstract: Optimizing training performance in large language models (LLMs) remains an
essential challenge, particularly in improving model performance while
maintaining computational costs. This work challenges the conventional approach
of training LLMs using next-token prediction (NTP), arguing that by predicting
information-rich tokens during training, there is a more effective way to train
LLMs. We investigate the impact of the proposed solution in three kinds of
tasks for LLMs: arithmetic, multi-label classification of text, and
natural-language generation. This work offers a principled approach to
optimizing LLM training, advancing both model performance and theoretical
understanding of the target-token selection strategies.

</details>


### [151] [IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval](https://arxiv.org/abs/2511.00268)
*Shounak Paul,Dhananjay Ghumare,Pawan Goyal,Saptarshi Ghosh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 本文提出了IL-PCR语料库，为法律案例检索和法规检索任务提供统一测试平台，通过LLM重排序方法利用两个任务之间的依赖关系获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将法律案例检索和法规检索作为独立任务处理，但这两个任务本质相关（相似案例往往引用相似法规），需要开发能利用这种依赖关系的统一模型。

Method: 构建IL-PCR语料库作为统一测试平台，实验了词法模型、语义模型和基于GNN的集成模型，并开发了基于LLM的重排序方法来利用任务间依赖关系。

Result: 基于LLM的重排序方法在两个检索任务上都取得了最佳性能，证明了利用任务间依赖关系的有效性。

Conclusion: IL-PCR语料库为法律检索研究提供了统一基准，LLM重排序方法通过利用案例检索和法规检索之间的内在联系显著提升了检索性能。

Abstract: Identifying/retrieving relevant statutes and prior cases/precedents for a
given legal situation are common tasks exercised by law practitioners.
Researchers to date have addressed the two tasks independently, thus developing
completely different datasets and models for each task; however, both retrieval
tasks are inherently related, e.g., similar cases tend to cite similar statutes
(due to similar factual situation). In this paper, we address this gap. We
propose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),
which is a unique corpus that provides a common testbed for developing models
for both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit
the dependence between the two. We experiment extensively with several baseline
models on the tasks, including lexical models, semantic models and ensemble
based on GNNs. Further, to exploit the dependence between the two tasks, we
develop an LLM-based re-ranking approach that gives the best performance.

</details>


### [152] [POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation](https://arxiv.org/abs/2511.00270)
*Abhinav Joshi,Vaibhav Sharma,Sanjeet Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出POSESTITCH-SLT预训练方案，通过基于语言模板的句子生成技术，在低资源手语翻译任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 手语翻译面临大规模句子对齐数据集稀缺的挑战，需要新的方法来提升低资源环境下的翻译性能。

Method: 采用基于语言模板的句子生成技术进行预训练，使用简单的transformer编码器-解码器架构，在训练中考虑模板生成的句子对。

Result: 在How2Sign数据集上BLEU-4分数从1.97提升到4.56，在iSign数据集上从0.55提升到3.43，超越了基于姿态的无词汇表翻译的现有最佳方法。

Conclusion: 模板驱动的合成监督在低资源手语设置中具有显著效果，为手语翻译提供了有效的解决方案。

Abstract: Sign language translation remains a challenging task due to the scarcity of
large-scale, sentence-aligned datasets. Prior arts have focused on various
feature extraction and architectural changes to support neural machine
translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training
scheme that is inspired by linguistic-templates-based sentence generation
technique. With translation comparison on two sign language datasets, How2Sign
and iSign, we show that a simple transformer-based encoder-decoder architecture
outperforms the prior art when considering template-generated sentence pairs in
training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign
and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for
pose-based gloss-free translation. The results demonstrate the effectiveness of
template-driven synthetic supervision in low-resource sign language settings.

</details>


### [153] [Language Modeling With Factorization Memory](https://arxiv.org/abs/2511.00315)
*Lee Xiong,Maksim Tkachenko,Johanes Effendi,Ting Cai*

Main category: cs.CL

TL;DR: 提出Factorization Memory，一种高效的RNN架构，在短上下文语言建模任务中性能与Transformer相当，在长上下文场景中表现出更好的泛化能力。基于Mamba-2构建，支持训练时并行计算，推理时保持恒定计算和内存复杂度。还开发了稀疏版本，仅更新部分循环状态。


<details>
  <summary>Details</summary>
Motivation: 开发一种既能达到Transformer性能水平，又能在长上下文场景中更好泛化的高效RNN架构，解决现有模型在长上下文处理中的局限性。

Method: 基于Mamba-2构建Factorization Memory，支持训练并行化和推理恒定复杂度。开发稀疏版本，仅更新部分循环状态以优化效率和表示能力。

Result: 在短上下文语言建模任务中性能与Transformer相当，在长上下文场景中表现出更好的泛化能力。稀疏版本在保持性能的同时提高了效率。

Conclusion: Factorization Memory是首个成功将稀疏内存激活与在短长上下文设置中竞争性能相结合的RNN架构，为高效序列建模提供了新方向。

Abstract: We propose Factorization Memory, an efficient recurrent neural network (RNN)
architecture that achieves performance comparable to Transformer models on
short-context language modeling tasks while also demonstrating superior
generalization in long-context scenarios. Our model builds upon Mamba-2,
enabling Factorization Memory to exploit parallel computations during training
while preserving constant computational and memory complexity during inference.
To further optimize model efficiency and representational capacity, we develop
a sparse formulation of Factorization Memory that updates only a subset of
recurrent states at each step while preserving the strong performance of its
dense counterpart. To our knowledge, this represents the first RNN architecture
that successfully combines sparse memory activation with competitive
performance across both short and long-context settings. This work provides a
systematic empirical analysis of Factorization Memory in comparison to
Transformer and Mamba-2 architectures.

</details>


### [154] [Reversal Invariance in Autoregressive Language Models](https://arxiv.org/abs/2511.00341)
*Mihir Sahasrabudhe*

Main category: cs.CL

TL;DR: 论文形式化定义了因果语言建模目标的结构特性：反转不变性，即标准CLM预训练对文本及其反转版本分配相同似然度，导致模型无法捕捉语言的时间不对称性。


<details>
  <summary>Details</summary>
Motivation: 当前预训练目标存在方向盲性，无法有效捕捉人类语言和推理中固有的时间不对称性，这限制了模型对语言中方向性依赖关系的建模能力。

Method: 通过理论形式化分析因果语言建模目标的反转不变性特性，并提出从时间不对称性角度重新审视预训练过程。

Result: 发现标准CLM预训练对正向和反向文本赋予相同似然度，解释了为什么在反向文本上训练的模型能达到与正向文本训练相当的性能。

Conclusion: 反转不变性是当前预训练目标的限制而非良性特征，建议未来研究开发能够显式建模语言方向性的损失函数和架构，同时保留标准语言建模能力。

Abstract: We formalize a structural property of the causal (autoregressive) language
modeling (CLM) objective: reversal invariance. Formally, the next-token
prediction loss assigns identical likelihood to a corpus and its reversal,
implying that standard CLM pretraining is direction-blind. This symmetry
explains why models trained on reversed text can achieve comparable performance
to those trained on forward text, despite the inherently time-asymmetric nature
of human language and reasoning. We argue that this invariance represents a
limitation of current pretraining objectives rather than a benign artifact. If
natural language encodes directional dependencies - phonological,
morphological, or causal - a symmetric objective may fail to capture them. We
therefore propose viewing pretraining through the lens of temporal asymmetry,
motivating future work on loss functions and architectures that explicitly
model the arrow of language while retaining standard language modeling
capacity.

</details>


### [155] [LingGym: How Far Are LLMs from Thinking Like Field Linguists?](https://arxiv.org/abs/2511.00343)
*Changbing Yang,Franklin Ma,Freda Shi,Jian Zhu*

Main category: cs.CL

TL;DR: LingGym是一个评估LLMs元语言推理能力的新基准，使用跨语言注释文本和语法描述，测试模型在未见过的低资源语言和结构上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否能够在训练过程中未见过的低资源语言和结构上进行语言推理的泛化，超越特定下游任务的局限。

Method: 创建Word-Gloss Inference任务，模型必须从上下文中推断缺失的单词和注释，使用不同层次的语言信息（注释、语法解释、翻译）。

Result: 整合结构化语言线索在所有模型中都能持续提高推理性能。

Conclusion: 这项工作凸显了使用LLMs进行类型学语言分析和低资源语言文档化的前景和当前局限性。

Abstract: This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity
for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and
grammatical descriptions extracted from 18 typologically diverse reference
grammars. Unlike previous work that focuses on specific downstream tasks, we
assess whether LLMs can generalize linguistic inference across low-resource
languages and structures not seen during training. We present a controlled
evaluation task: Word-Gloss Inference, in which the model must infer a missing
word and gloss from context using varying levels of linguistic information
(e.g., glosses, grammatical explanations, translations). Our results show that
incorporating structured linguistic cues leads to consistent improvements in
reasoning performance across all models. This work highlights both the promise
and current limitations of using LLMs for typologically informed linguistic
analysis and low-resource language documentation.

</details>


### [156] [Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs](https://arxiv.org/abs/2511.00371)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.CL

TL;DR: 论文介绍了推理轨迹生成任务，提出了一种基于LLM的苏格拉底调试方法，通过引导推理轨迹帮助学生识别和修正编程误解，实验显示前沿模型能生成91%正确的推理轨迹和98.7%有效的对话轮次。


<details>
  <summary>Details</summary>
Motivation: 大多数新手程序员的错误源于编程误解，传统调试方法直接提供修复方案，而苏格拉底调试能引导学生自主发现和修正错误，但缺乏系统化的实现方法。

Method: 提出推理轨迹生成任务，构建手动标注的调试问题数据集，开发基于LLM的推理轨迹生成和基于轨迹的苏格拉底对话生成解决方案。

Result: 大规模LLM作为评判者的评估显示，前沿模型能生成高达91%正确的推理轨迹和98.7%有效的对话轮次。

Conclusion: LLM能够有效生成苏格拉底调试所需的推理轨迹和对话，为编程教育中的自主错误修正提供了可行的技术方案。

Abstract: In Socratic debugging, instructors guide students towards identifying and
fixing a bug on their own, instead of providing the bug fix directly. Most
novice programmer bugs are caused by programming misconceptions, namely false
beliefs about a programming concept. In this context, Socratic debugging can be
formulated as a guided Reasoning Trajectory (RT) leading to a statement about
the program behavior that contradicts the bug-causing misconception. Upon
reaching this statement, the ensuing cognitive dissonance leads the student to
first identify and then update their false belief. In this paper, we introduce
the task of reasoning trajectory generation, together with a dataset of
debugging problems manually annotated with RTs. We then describe LLM-based
solutions for generating RTs and Socratic conversations that are anchored on
them. A large-scale LLM-as-judge evaluation shows that frontier models can
generate up to 91% correct reasoning trajectories and 98.7% valid conversation
turns.

</details>


### [157] [PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks](https://arxiv.org/abs/2511.00416)
*Yiwei Zha,Rui Min,Shanu Sushmita*

Main category: cs.CL

TL;DR: 研究发现迭代改写的AI生成文本能有效逃避现有AI文本检测器，检测器对直接LLM输出准确率超90%，但对迭代改写内容检测失败。论文提出了PADBen基准测试，揭示了检测器在剽窃逃避检测成功但在作者身份混淆检测失败的严重不对称问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器对直接LLM输出检测效果良好，但对迭代改写内容检测效果极差，需要研究这种检测失败的原因并评估检测器的鲁棒性。

Method: 通过内在机制分析揭示迭代改写创建了语义位移但保留生成模式的中间清洗区域，引入PADBen基准测试系统评估检测器对两种改写攻击场景的鲁棒性，包含五种文本类型分类和五个渐进检测任务。

Result: 评估了11个最先进检测器，发现关键不对称性：检测器能成功识别剽窃逃避问题，但在作者身份混淆情况下失败。当前检测方法无法有效处理中间清洗区域。

Conclusion: 当前检测方法无法有效处理迭代改写创建的中间清洗区域，需要在检测架构上进行根本性改进，超越现有的语义和风格区分方法。

Abstract: While AI-generated text (AIGT) detectors achieve over 90\% accuracy on direct
LLM outputs, they fail catastrophically against iteratively-paraphrased
content. We investigate why iteratively-paraphrased text -- itself AI-generated
-- evades detection systems designed for AIGT identification. Through intrinsic
mechanism analysis, we reveal that iterative paraphrasing creates an
intermediate laundering region characterized by semantic displacement with
preserved generation patterns, which brings up two attack categories:
paraphrasing human-authored text (authorship obfuscation) and paraphrasing
LLM-generated text (plagiarism evasion). To address these vulnerabilities, we
introduce PADBen, the first benchmark systematically evaluating detector
robustness against both paraphrase attack scenarios. PADBen comprises a
five-type text taxonomy capturing the full trajectory from original content to
deeply laundered text, and five progressive detection tasks across
sentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art
detectors, revealing critical asymmetry: detectors successfully identify the
plagiarism evasion problem but fail for the case of authorship obfuscation. Our
findings demonstrate that current detection approaches cannot effectively
handle the intermediate laundering region, necessitating fundamental advances
in detection architectures beyond existing semantic and stylistic
discrimination methods. For detailed code implementation, please see
https://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.

</details>


### [158] [MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts](https://arxiv.org/abs/2511.00421)
*Naoto Iwase,Hiroki Okuyama,Junichiro Iwasawa*

Main category: cs.CL

TL;DR: MedRECT是首个跨语言（日语/英语）医疗错误处理基准，包含错误检测、定位和修正三个子任务。研究发现推理模型表现最佳，跨语言评估显示英语到日语存在5-10%性能差距，微调后模型在结构化医疗错误修正任务中超越人类专家。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中显示出潜力，但其检测和修正临床文本错误的能力（安全部署的前提条件）仍未被充分评估，特别是在英语以外的语言中。

Method: 从日本医师资格考试和精心策划的英语对应内容中，通过可扩展的自动化流程构建MedRECT基准，包含MedRECT-ja（663个文本）和MedRECT-en（458个文本）。评估了9个当代LLM，涵盖专有、开源和推理模型家族，并进行了针对性的LoRA微调。

Result: 推理模型显著优于标准架构，错误检测相对提升13.5%，句子提取提升51.0%；跨语言评估显示英语到日语存在5-10%性能差距；微调在错误修正性能上产生不对称改进（日语：+0.078，英语：+0.168）；微调后模型在结构化医疗错误修正任务中超越人类专家。

Conclusion: MedRECT是首个全面的跨语言医疗错误修正基准，为开发更安全的跨语言医疗LLM提供了可复现框架和资源。

Abstract: Large language models (LLMs) show increasing promise in medical applications,
but their ability to detect and correct errors in clinical texts -- a
prerequisite for safe deployment -- remains under-evaluated, particularly
beyond English. We introduce MedRECT, a cross-lingual benchmark
(Japanese/English) that formulates medical error handling as three subtasks:
error detection, error localization (sentence extraction), and error
correction. MedRECT is built with a scalable, automated pipeline from the
Japanese Medical Licensing Examinations (JMLE) and a curated English
counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with
comparable error/no-error balance. We evaluate 9 contemporary LLMs spanning
proprietary, open-weight, and reasoning families. Key findings: (i) reasoning
models substantially outperform standard architectures, with up to 13.5%
relative improvement in error detection and 51.0% in sentence extraction; (ii)
cross-lingual evaluation reveals 5-10% performance gaps from English to
Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA
fine-tuning yields asymmetric improvements in error correction performance
(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;
and (iv) our fine-tuned model exceeds human expert performance on structured
medical error correction tasks. To our knowledge, MedRECT is the first
comprehensive cross-lingual benchmark for medical error correction, providing a
reproducible framework and resources for developing safer medical LLMs across
languages.

</details>


### [159] [G2: Guided Generation for Enhanced Output Diversity in LLMs](https://arxiv.org/abs/2511.00432)
*Zhiwen Ruan,Yixia Li,Yefeng Liu,Yun Chen,Weihua Luo,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出G2方法，一种无需训练的即插即用技术，通过双引导机制在解码过程中干预，有效提升大语言模型输出多样性同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在输出多样性方面存在显著局限，多次尝试生成高度相似内容，影响需要多样化输出的任务。现有方法如温度调节虽能增强多样性但会牺牲输出质量。

Method: 使用基础生成器配合双引导机制，通过基于解码的干预来引导生成过程，在原始查询条件下鼓励更多样化的输出。

Result: 综合实验表明，G2方法有效提高了输出多样性，同时在多样性和质量之间保持了最佳平衡。

Conclusion: G2是一种有效的训练免费方法，能够在不影响生成质量的前提下显著提升大语言模型的输出多样性。

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
diverse natural language processing tasks. However, these models exhibit a
critical limitation in output diversity, often generating highly similar
content across multiple attempts. This limitation significantly affects tasks
requiring diverse outputs, from creative writing to reasoning. Existing
solutions, like temperature scaling, enhance diversity by modifying probability
distributions but compromise output quality. We propose Guide-to-Generation
(G2), a training-free plug-and-play method that enhances output diversity while
preserving generation quality. G2 employs a base generator alongside dual
Guides, which guide the generation process through decoding-based interventions
to encourage more diverse outputs conditioned on the original query.
Comprehensive experiments demonstrate that G2 effectively improves output
diversity while maintaining an optimal balance between diversity and quality.

</details>


### [160] [Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus](https://arxiv.org/abs/2511.00486)
*Pooja Singh,Shashwat Bhardwaj,Vaibhav Sharma,Sandeep Kumar*

Main category: cs.CL

TL;DR: 本文构建了首个也是最大的Bhili-Hindi-English平行语料库(BHEPC)，包含11万句精心整理的句子，用于解决印度部落语言Bhili的机器翻译资源匮乏问题。通过评估多种多语言大模型，发现微调的NLLB-200模型在低资源场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性带来机器翻译挑战，特别是像Bhili这样的代表性不足的部落语言缺乏高质量语言资源。本文旨在填补这一资源空白，促进低资源语言的包容性自然语言处理技术发展。

Method: 构建了包含11万句的Bhili-Hindi-English平行语料库(BHEPC)，涵盖教育、行政和新闻等关键领域。评估了专有和开源多语言大模型在英语/印地语与Bhili之间的双向翻译任务，并研究了多语言LLM在上下文学习中的生成翻译能力。

Result: 综合评估表明，微调的NLLB-200蒸馏600M变体模型优于其他模型，突显了多语言模型在低资源场景中的潜力。同时评估了跨领域泛化性能并量化了分布差异。

Conclusion: 这项工作填补了关键资源空白，为全球低资源和边缘化语言促进了包容性自然语言处理技术的发展，为低资源机器翻译研究建立了有价值的基准。

Abstract: The linguistic diversity of India poses significant machine translation
challenges, especially for underrepresented tribal languages like Bhili, which
lack high-quality linguistic resources. This paper addresses the gap by
introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest
parallel corpus worldwide comprising 110,000 meticulously curated sentences
across Bhili, Hindi, and English. The corpus was created with the assistance of
expert human translators. BHEPC spans critical domains such as education,
administration, and news, establishing a valuable benchmark for research in low
resource machine translation. To establish a comprehensive Bhili Machine
Translation benchmark, we evaluated a wide range of proprietary and open-source
Multilingual Large Language Models (MLLMs) on bidirectional translation tasks
between English/Hindi and Bhili. Comprehensive evaluation demonstrates that the
fine-tuned NLLB-200 distilled 600M variant model outperforms others,
highlighting the potential of multilingual models in low resource scenarios.
Furthermore, we investigated the generative translation capabilities of
multilingual LLMs on BHEPC using in-context learning, assessing performance
under cross-domain generalization and quantifying distributional divergence.
This work bridges a critical resource gap and promotes inclusive natural
language processing technologies for low-resource and marginalized languages
globally.

</details>


### [161] [With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting](https://arxiv.org/abs/2511.00487)
*Stephen Meisenbacher,Florian Matthes*

Main category: cs.CL

TL;DR: 本文首次将数据集大小因素引入差分隐私文本隐私化评估中，通过在大规模数据集上进行动态分割的效用和隐私测试，发现数据集大小对隐私-效用权衡有重要影响。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私自然语言处理研究在评估文本重写机制时往往忽略数据集大小的影响，本文旨在填补这一空白，研究数据集大小对机制效用和隐私保护效果的影响。

Method: 设计在大规模数据集上的效用和隐私测试，使用动态分割大小，在包含多达100万个文本的不同规模数据集上运行测试，量化数据集大小增加对隐私-效用权衡的影响。

Result: 研究发现数据集大小在评估差分隐私文本重写机制中起着关键作用，数据集规模的变化会显著影响隐私保护与数据效用之间的平衡关系。

Conclusion: 研究结果要求差分隐私自然语言处理领域采用更严格的评估程序，并为差分隐私自然语言处理在实践和大规模应用中的未来发展提供了启示。

Abstract: Recent work in Differential Privacy with Natural Language Processing (DP NLP)
has proposed numerous promising techniques in the form of text rewriting
mechanisms. In the evaluation of these mechanisms, an often-ignored aspect is
that of dataset size, or rather, the effect of dataset size on a mechanism's
efficacy for utility and privacy preservation. In this work, we are the first
to introduce this factor in the evaluation of DP text privatization, where we
design utility and privacy tests on large-scale datasets with dynamic split
sizes. We run these tests on datasets of varying size with up to one million
texts, and we focus on quantifying the effect of increasing dataset size on the
privacy-utility trade-off. Our findings reveal that dataset size plays an
integral part in evaluating DP text rewriting mechanisms; additionally, these
findings call for more rigorous evaluation procedures in DP NLP, as well as
shed light on the future of DP NLP in practice and at scale.

</details>


### [162] [ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2511.00489)
*Jiani Guo,Zuchao Li,Jie Wu,Qianren Wang,Yun Li,Lefei Zhang,Hai Zhao,Yujiu Yang*

Main category: cs.CL

TL;DR: ToM是一个面向树结构的MapReduce框架，通过利用文档的层次结构进行递归推理，解决了大语言模型在长上下文推理中的性能下降问题，显著优于现有的分治框架和检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型受限于有限的上下文窗口，在长上下文推理中性能显著下降。现有的检索增强生成方法依赖基于相似性的排名，牺牲了逻辑连贯性；分治框架虽然能进行局部推理，但难以捕捉长程依赖关系，且由于孤立处理片段而可能引发冲突。

Method: ToM通过层次语义解析构建文档树（DocTree），采用树形MapReduce方法进行递归推理：在Map步骤中，在子节点生成推理依据；在Reduce步骤中，跨兄弟节点聚合这些推理依据，以在父节点解决冲突或达成共识。

Result: 在70B+大语言模型上的实验结果表明，ToM显著优于现有的分治框架和检索增强生成方法，实现了更好的逻辑连贯性和长上下文推理能力。

Conclusion: ToM框架通过利用文档的层次结构和递归推理机制，有效解决了长上下文推理中的挑战，为大语言模型在复杂文档处理中的应用提供了新的解决方案。

Abstract: Large Language Models (LLMs), constrained by limited context windows, often
face significant performance degradation when reasoning over long contexts. To
address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over
chunks but frequently sacrifices logical coherence due to its reliance on
similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split
documents into small chunks for independent reasoning and aggregation. While
effective for local reasoning, DCF struggles to capture long-range dependencies
and risks inducing conflicts by processing chunks in isolation. To overcome
these limitations, we propose ToM, a novel Tree-oriented MapReduce framework
for long-context reasoning. ToM leverages the inherent hierarchical structure
of long documents (e.g., main headings and subheadings) by constructing a
DocTree through hierarchical semantic parsing and performing bottom-up
aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:
in the Map step, rationales are generated at child nodes; in the Reduce step,
these rationales are aggregated across sibling nodes to resolve conflicts or
reach consensus at parent nodes. Experimental results on 70B+ LLMs show that
ToM significantly outperforms existing divide-and-conquer frameworks and
retrieval-augmented generation methods, achieving better logical coherence and
long-context reasoning. Our code is available at
https://github.com/gjn12-31/ToM .

</details>


### [163] [Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge](https://arxiv.org/abs/2511.00505)
*Qi Luo,Xiaonan Li,Junqi Dai,Shuang Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: Zero-RAG通过识别和修剪检索增强生成中的冗余知识，减少外部语料库规模并加速检索过程，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型内部知识的扩展，外部语料库与模型之间存在显著的知识冗余，这增加了密集检索的工作量并可能损害RAG性能。

Method: 提出Mastery-Score指标识别冗余知识进行修剪，使用Query Router和Noise-Tolerant Tuning避免不相关文档干扰，提高模型内部知识利用率。

Result: 实验结果显示，Zero-RAG将维基百科语料库修剪30%，检索阶段加速22%，且不损害RAG性能。

Conclusion: Zero-RAG有效解决了RAG中的知识冗余问题，在保持性能的同时显著提升了检索效率。

Abstract: Retrieval-Augmented Generation has shown remarkable results to address Large
Language Models' hallucinations, which usually uses a large external corpus to
supplement knowledge to LLMs. However, with the development of LLMs, the
internal knowledge of LLMs has expanded significantly, thus causing significant
knowledge redundancy between the external corpus and LLMs. On the one hand, the
indexing cost of dense retrieval is highly related to the corpus size and thus
significant redundant knowledge intensifies the dense retrieval's workload. On
the other hand, the redundant knowledge in the external corpus is not helpful
to LLMs and our exploratory analysis shows that it instead hurts the RAG
performance on those questions which the LLM can answer by itself. To address
these issues, we propose Zero-RAG to tackle these challenges. Specifically, we
first propose the Mastery-Score metric to identify redundant knowledge in the
RAG corpus to prune it. After pruning, answers to "mastered" questions rely
primarily on internal knowledge of the LLM. To better harness the internal
capacity, we propose Query Router and Noise-Tolerant Tuning to avoid the
irrelevant documents' distraction and thus further improve the LLM's
utilization of internal knowledge with pruned corpus. Experimental results show
that Zero-RAG prunes the Wikipedia corpus by 30\% and accelerates the retrieval
stage by 22\%, without compromising RAG's performance.

</details>


### [164] [Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models](https://arxiv.org/abs/2511.00519)
*Ariyan Hossain,Khondokar Mohammad Ahanaf Hannan,Rakinul Haque,Nowreen Tarannum Rafa,Humayra Musarrat,Shoaib Ahmed Dipu,Farig Yousuf Sadeque*

Main category: cs.CL

TL;DR: 本文研究了BERT、ALBERT、RoBERTa和DistilBERT等编码器transformer模型中的性别偏见问题，提出了一种新的偏见度量方法MALoR，并通过反事实数据增强生成性别平衡数据集进行继续预训练来缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 编码器transformer模型在各种语言任务中取得了最先进的性能，但已被证明继承了训练数据中的强烈性别偏见。本文旨在调查这些模型在上下文词嵌入中的性别偏见程度。

Method: 引入新的偏见度量指标MALoR，基于模型填充掩码标记的概率来评估偏见；提出通过反事实数据增强生成性别平衡数据集进行继续预训练的缓解方法。

Result: 实验显示偏见分数显著降低：在BERT-base中，"he-she"偏见分数从1.27降至0.08，"his-her"从2.51降至0.36；在BERT-large中，"male-female"偏见从1.82降至0.10。所有模型都观察到类似改善。

Conclusion: 该方法有效减少了性别偏见，同时不影响模型在下游任务上的性能表现。

Abstract: Gender bias in language models has gained increasing attention in the field
of natural language processing. Encoder-based transformer models, which have
achieved state-of-the-art performance in various language tasks, have been
shown to exhibit strong gender biases inherited from their training data. This
paper investigates gender bias in contextualized word embeddings, a crucial
component of transformer-based models. We focus on prominent architectures such
as BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to
gender bias. To quantify the degree of bias, we introduce a novel metric,
MALoR, which assesses bias based on model probabilities for filling masked
tokens. We further propose a mitigation approach involving continued
pre-training on a gender-balanced dataset generated via Counterfactual Data
Augmentation. Our experiments reveal significant reductions in gender bias
scores across different pronoun pairs. For instance, in BERT-base, bias scores
for "he-she" dropped from 1.27 to 0.08, and "his-her" from 2.51 to 0.36
following our mitigation approach. We also observed similar improvements across
other models, with "male-female" bias decreasing from 1.82 to 0.10 in
BERT-large. Our approach effectively reduces gender bias without compromising
model performance on downstream tasks.

</details>


### [165] [Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly](https://arxiv.org/abs/2511.00536)
*Wenya Xie,Shaochen,Zhong,Hoang Anh Duy Le,Zhaozhuo Xu,Jianwen Xie,Zirui Liu*

Main category: cs.CL

TL;DR: 论文提出WordSaladChopper (WSC)方法，通过检测大型推理模型中的无用自我重复（词沙拉）并动态修剪，显著减少输出令牌成本，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型输出令牌成本高昂，其中大量是无用的自我重复（词沙拉），消耗解码预算但不增加价值。

Method: 利用模型对<\n\n>标记隐藏状态模式，通过单层线性分类器实时检测词沙拉行为，检测后采用简单修剪加重新生成提示的方法。

Result: WSC组件实现了显著的输出长度节省，同时质量损失最小，是轻量级、即插即用的解决方案。

Conclusion: WSC或类似组件是所有考虑用户体验的大型推理模型应用的必要组件，因其低开销、强节省效果和无语义价值的词沙拉令牌特性。

Abstract: Large Reasoning Models (LRMs) are often bottlenecked by the high cost of
output tokens. We show that a significant portion of these tokens are useless
self-repetitions - what we call "word salad" - that exhaust the decoding budget
without adding value. Interestingly, we observe that LRMs are self-aware when
trapped in these loops: the hidden states of <\n\n> tokens trailing each
reasoning chunk exhibit patterns that allow us to detect word salad behavior
on-the-fly via a single-layer linear classifier. Once detected, a simple chop
appended by a straightforward regeneration prompt yields substantial length
savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a
lightweight, turnkey component for LRM that is minimally invasive to its
reasoning trajectory by only removing semantically redundant tokens. Given its
low overhead, strong savings, and the lack of semantic value of word salad
tokens, we believe it is not too far-fetched to argue that WSC - or a similar
component - is a must-have for all LRM applications with user experience in
mind. Our code is publicly available at
https://github.com/wenyaxie023/WordSaladChopper.

</details>


### [166] [Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction](https://arxiv.org/abs/2511.00537)
*Peter Atandoh,Jie Zou,Weikang Guo,Jiwei Wei,Zheng Wang*

Main category: cs.CL

TL;DR: 提出CISEA-MRFE框架，通过上下文指令、语义增强增强和多细化特征提取，解决情感分析中细微情感线索、领域转移和不平衡分布的问题，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习和预训练语言模型的情感分析方法在处理细微情感线索、领域转移和不平衡情感分布时表现不佳，主要由于语义基础不足、对多样化语言模式泛化能力差以及对主导情感类别的偏见。

Method: 提出CISEA-MRFE框架，包含三个核心组件：上下文指令(CI)注入领域感知指令指导情感消歧；语义增强增强(SEA)通过情感一致释义增强提高鲁棒性；多细化特征提取(MRFE)结合尺度自适应深度编码器(SADE)进行多尺度特征专业化，以及情感评估器上下文编码器(EECE)进行情感感知序列建模。

Result: 在四个基准数据集上的实验结果表明，CISEA-MRFE始终优于强基线方法，在IMDb上准确率相对提升4.6%，Yelp上6.5%，Twitter上30.3%，Amazon上4.1%。

Conclusion: CISEA-MRFE框架在跨领域情感分类中表现出有效性和良好的泛化能力，验证了所提出方法的优越性。

Abstract: Sentiment analysis using deep learning and pre-trained language models (PLMs)
has gained significant traction due to their ability to capture rich contextual
representations. However, existing approaches often underperform in scenarios
involving nuanced emotional cues, domain shifts, and imbalanced sentiment
distributions. We argue that these limitations stem from inadequate semantic
grounding, poor generalization to diverse linguistic patterns, and biases
toward dominant sentiment classes. To overcome these challenges, we propose
CISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction
(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature
Extraction (MRFE). CI injects domain-aware directives to guide sentiment
disambiguation; SEA improves robustness through sentiment-consistent
paraphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder
(SADE) for multi-scale feature specialization with an Emotion Evaluator Context
Encoder (EECE) for affect-aware sequence modeling. Experimental results on four
benchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong
baselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,
6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the
effectiveness and generalization ability of our approach for sentiment
classification across varied domains.

</details>


### [167] [Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack](https://arxiv.org/abs/2511.00556)
*Peng Ding,Jun Kuang,Wen Sun,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: ISA（意图转移攻击）通过最小化编辑原始有害请求来混淆LLM对攻击意图的识别，将有害意图伪装成良性信息请求，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击主要通过添加额外上下文或对抗性token来分散LLM注意力，但未改变核心有害意图。ISA旨在通过意图转换使LLM误判攻击意图为良性请求。

Method: 建立意图转换分类法，利用这些转换生成看似无害的自然语言提示，仅需对原始请求进行最小编辑，无需复杂token或冗长上下文。

Result: 在开源和商业LLM上的实验显示，ISA相比直接有害提示攻击成功率提升超过70%。仅使用ISA模板重新表述的良性数据微调模型可将成功率提升至近100%。现有防御方法对ISA无效。

Conclusion: ISA揭示了LLM在意图推断方面的根本性安全挑战，现有防御措施不足，需要更有效的防御策略。

Abstract: Large language models (LLMs) remain vulnerable to jailbreaking attacks
despite their impressive capabilities. Investigating these weaknesses is
crucial for robust safety mechanisms. Existing attacks primarily distract LLMs
by introducing additional context or adversarial tokens, leaving the core
harmful intent unchanged. In this paper, we introduce ISA (Intent Shift
Attack), which obfuscates LLMs about the intent of the attacks. More
specifically, we establish a taxonomy of intent transformations and leverage
them to generate attacks that may be misperceived by LLMs as benign requests
for information. Unlike prior methods relying on complex tokens or lengthy
context, our approach only needs minimal edits to the original request, and
yields natural, human-readable, and seemingly harmless prompts. Extensive
experiments on both open-source and commercial LLMs show that ISA achieves over
70% improvement in attack success rate compared to direct harmful prompts. More
critically, fine-tuning models on only benign data reformulated with ISA
templates elevates success rates to nearly 100%. For defense, we evaluate
existing methods and demonstrate their inadequacy against ISA, while exploring
both training-free and training-based mitigation strategies. Our findings
reveal fundamental challenges in intent inference for LLMs safety and
underscore the need for more effective defenses. Our code and datasets are
available at https://github.com/NJUNLP/ISA.

</details>


### [168] [FlashEVA: Accelerating LLM inference via Efficient Attention](https://arxiv.org/abs/2511.00576)
*Juan Gabriel Kostelec,Qinghai Guo*

Main category: cs.CL

TL;DR: FlashEVA是一种基于EVA的高效Transformer注意力机制实现，通过微调使模型适应FlashEVA注意力，在仅使用15亿token进行微调的情况下，在推理时实现高达6.7倍的吞吐量和5倍的GPU内存降低，但在检索任务中存在局限性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然性能优异，但其内存需求（特别是需要维护完整上下文）给推理带来了显著挑战，需要开发更高效的实现方案。

Method: 提出FlashEVA作为EVA（通过控制变量实现高效注意力）的高效实现，展示如何微调Transformer模型以适配FlashEVA注意力机制。

Result: FlashEVA在推理时相比标准Transformer实现，吞吐量提高6.7倍，GPU峰值内存使用降低5倍，且仅用15亿token微调即可在各种下游任务中保持有效性。

Conclusion: 这项工作通过可调节超参数在吞吐量和准确性之间提供权衡控制，为实现更高效和适应性强的基于Transformer的推理模型迈出了重要一步。

Abstract: Transformer models have revolutionized natural language processing, achieving
state-of-the-art performance and demonstrating remarkable scalability. However,
their memory demands, particularly due to maintaining full context in memory,
pose significant challenges for inference. In this paper, we present FlashEVA,
an efficient implementation of EVA (Efficient Attention via Control Variates),
and demonstrate how to finetune transformers to adapt to FlashEVA attention.
Our method enables fine-tuning of Transformer models with as few as 1.5B tokens
while preserving effectiveness across various downstream tasks. Notably,
FlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory
usage during inference compared to standard Transformer implementations.
Despite these improvements, we observe limitations in retrieval-focused tasks.
Our implementation offers control over the trade-off between throughput and
accuracy through adjustable hyperparameters, providing flexibility for diverse
use cases. This work represents a significant step towards more efficient and
adaptable Transformer-based models for inference.

</details>


### [169] [SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding](https://arxiv.org/abs/2511.00606)
*Jameson Sandler,Jacob K. Christopher,Thomas Hartvigsen,Nando Fioretto*

Main category: cs.CL

TL;DR: SpecDiff-2是一种新颖的推测解码框架，通过使用离散扩散作为非自回归草稿器来解决并行性限制，并开发新技术校准离散扩散草稿器与自回归验证器，以解决草稿令牌频繁被拒绝的问题，在推理、编码和数学基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法存在两个基本瓶颈：草稿阶段的自动回归依赖限制了并行性，以及草稿模型和验证模型之间的不对齐导致草稿令牌频繁被拒绝。

Method: 利用离散扩散作为非自回归草稿器来解决并行性瓶颈，并开发新技术校准离散扩散草稿器与自回归验证器，以解决草稿令牌拒绝问题。

Result: 在全面的基准测试套件中，SpecDiff-2在推理、编码和数学基准测试中实现了新的最先进性能，与先前基线相比，每秒令牌数平均提高+55%，与标准解码相比获得高达5.5倍的平均加速，且没有任何准确性损失。

Conclusion: SpecDiff-2成功解决了推测解码的两个基本瓶颈，实现了显著的加速效果，同时保持了模型的准确性。

Abstract: Speculative decoding has become the standard approach for accelerating Large
Language Model (LLM) inference. It exploits a lossless draft-then-verify
procedure to circumvent the latency of autoregressive decoding, achieving
impressive speed-ups. Yet, current speculative decoding approaches remain
limited by two fundamental bottlenecks: (1) the autoregressive dependency
during drafting which limits parallelism, and (2) frequent rejections of draft
tokens caused by misalignment between the draft and verify models. This paper
proposes SpecDiff-2, a novel framework to jointly address these two
bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to
address bottleneck (1) and develops novel techniques to calibrate discrete
diffusion drafters with autoregressive verifiers, addressing bottleneck (2).
Experimental results across a comprehensive benchmark suite show that
SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and
mathematical benchmarks, improving tokens-per-second by up to an average of
+55% over previous baselines and obtaining up to 5.5x average speed-up over
standard decoding, without any loss of accuracy.

</details>


### [170] [Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios](https://arxiv.org/abs/2511.00620)
*Autumn Toney-Wails,Ryan Wails*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在概率场景中的不确定性量化能力，发现虽然模型在响应准确性上表现完美，但其token级别的概率分布与理论概率分布存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 可靠的不确定性量化对于大型语言模型在决策支持等关键应用中的可信部署至关重要，特别是在概率场景中需要模型输出的概率与理论概率保持一致。

Method: 使用GPT-4.1和DeepSeek-Chat模型，评估它们对10个涉及概率的提示的响应，包括有无明确概率线索的情况，测量响应有效性和token级输出概率与理论概率的一致性。

Result: 两个模型在所有提示场景中都实现了完美的领域内响应准确性，但其token级别的概率和熵值与相应的理论分布持续偏离。

Conclusion: 尽管大型语言模型在响应准确性方面表现优异，但其内部概率表示与理论概率分布不一致，这限制了它们在需要精确概率推理的应用中的可靠性。

Abstract: Reliable uncertainty quantification (UQ) is essential for ensuring
trustworthy downstream use of large language models, especially when they are
deployed in decision-support and other knowledge-intensive applications. Model
certainty can be estimated from token logits, with derived probability and
entropy values offering insight into performance on the prompt task. However,
this approach may be inadequate for probabilistic scenarios, where the
probabilities of token outputs are expected to align with the theoretical
probabilities of the possible outcomes. We investigate the relationship between
token certainty and alignment with theoretical probability distributions in
well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we
evaluate model responses to ten prompts involving probability (e.g., roll a
six-sided die), both with and without explicit probability cues in the prompt
(e.g., roll a fair six-sided die). We measure two dimensions: (1) response
validity with respect to scenario constraints, and (2) alignment between
token-level output probabilities and theoretical probabilities. Our results
indicate that, while both models achieve perfect in-domain response accuracy
across all prompt scenarios, their token-level probability and entropy values
consistently diverge from the corresponding theoretical distributions.

</details>


### [171] [Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature](https://arxiv.org/abs/2511.00627)
*Jean Barré,Olga Seminck,Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 通过计算分析方法研究法国侦探小说中侦探原型的演变，发现监督模型能够捕捉150年间侦探原型的统一性，并展示了从次要角色到核心推理机器的转变过程。


<details>
  <summary>Details</summary>
Motivation: 探索法国侦探小说中侦探原型的演变历程，理解这一文学形象如何随时间变化并适应不同文学传统。

Method: 使用定量方法和角色级嵌入的计算分析，构建监督模型来分析150年间的法国侦探小说文本。

Result: 模型成功捕捉了从1866年M. Lecoq到2017年Commissaire Adamsberg的侦探原型统一性，揭示了从次要叙事角色到古典侦探故事核心推理机器的演变，以及二战后受硬汉派传统影响后的复杂化转变。

Conclusion: 法国侦探小说中的侦探原型具有历史连续性，经历了从叙事配角到核心角色的演变，并在二战后吸收了硬汉派元素，变得更加复杂和道德模糊。

Abstract: This research explores the evolution of the detective archetype in French
detective fiction through computational analysis. Using quantitative methods
and character-level embeddings, we show that a supervised model is able to
capture the unity of the detective archetype across 150 years of literature,
from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,
the study demonstrates how the detective figure evolves from a secondary
narrative role to become the central character and the "reasoning machine" of
the classical detective story. In the aftermath of the Second World War, with
the importation of the hardboiled tradition into France, the archetype becomes
more complex, navigating the genre's turn toward social violence and moral
ambiguity.

</details>


### [172] [Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge](https://arxiv.org/abs/2511.00657)
*Eshaan Tanwar,Anwoy Chatterjee,Michael Saxon,Alon Albalak,William Yang Wang,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: XNationQA是一个新的多语言问答基准，涵盖9个国家的地理、文化和历史问题，包含49,280个问题，使用7种语言。研究发现多语言LLM在不同语言中获取文化特定信息存在显著差异，模型在英语中表现优于相应文化的主导语言，且知识跨语言迁移能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多语言问答基准虽然覆盖多种语言，但未考虑信息中的区域多样性，且倾向于西方中心主义，这导致在公平评估多语言模型对不同地理位置事实信息的理解方面存在显著差距。

Method: 引入XNationQA基准，包含49,280个关于9个国家地理、文化和历史的问题，使用7种语言呈现。对8个标准多语言LLM进行基准测试，并使用两种新的迁移度量进行评估。

Result: 发现模型在不同语言中获取文化特定信息存在显著差异；模型在英语中表现优于相应文化的主导语言；模型在西方语言中表现更好，但这并不一定意味着对西方国家更了解；模型（特别是开源模型）的跨语言知识迁移能力非常有限。

Conclusion: 多语言LLM在文化素养方面存在显著的不平衡，需要开发更公平的评估方法和改进模型以更好地处理不同文化和语言的信息。

Abstract: Most multilingual question-answering benchmarks, while covering a diverse
pool of languages, do not factor in regional diversity in the information they
capture and tend to be Western-centric. This introduces a significant gap in
fairly evaluating multilingual models' comprehension of factual information
from diverse geographical locations. To address this, we introduce XNationQA
for investigating the cultural literacy of multilingual LLMs. XNationQA
encompasses a total of 49,280 questions on the geography, culture, and history
of nine countries, presented in seven languages. We benchmark eight standard
multilingual LLMs on XNationQA and evaluate them using two novel transference
metrics. Our analyses uncover a considerable discrepancy in the models'
accessibility to culturally specific facts across languages. Notably, we often
find that a model demonstrates greater knowledge of cultural information in
English than in the dominant language of the respective culture. The models
exhibit better performance in Western languages, although this does not
necessarily translate to being more literate for Western countries, which is
counterintuitive. Furthermore, we observe that models have a very limited
ability to transfer knowledge across languages, particularly evident in
open-source models.

</details>


### [173] [Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?](https://arxiv.org/abs/2511.00689)
*Berk Atil,Rebecca J. Passonneau,Fred Morstatter*

Main category: cs.CL

TL;DR: 本文首次对10种不同资源水平的语言进行了系统性多语言越狱攻击和防御评估，发现攻击成功率和防御鲁棒性在不同语言间存在显著差异，高资源语言在标准查询下更安全但对对抗性攻击更脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型经过安全对齐训练，但现有研究表明可以通过越狱攻击绕过安全机制。然而，跨语言的越狱攻击和防御泛化能力尚未得到充分探索。

Method: 在HarmBench和AdvBench上使用6个大语言模型，评估了基于逻辑表达式和对抗性提示两种越狱攻击类型，涵盖了高、中、低资源水平的10种语言。

Result: 攻击成功率和防御鲁棒性在不同语言间存在显著差异：高资源语言在标准查询下更安全，但对对抗性攻击更脆弱；简单防御方法有效但具有语言和模型依赖性。

Conclusion: 研究结果表明需要为LLMs开发语言感知和跨语言的安全基准测试，以应对不同语言环境下的安全挑战。

Abstract: Large language models (LLMs) undergo safety alignment after training and
tuning, yet recent work shows that safety can be bypassed through jailbreak
attacks. While many jailbreaks and defenses exist, their cross-lingual
generalization remains underexplored. This paper presents the first systematic
multilingual evaluation of jailbreaks and defenses across ten
languages--spanning high-, medium-, and low-resource languages--using six LLMs
on HarmBench and AdvBench. We assess two jailbreak types:
logical-expression-based and adversarial-prompt-based. For both types, attack
success and defense robustness vary across languages: high-resource languages
are safer under standard queries but more vulnerable to adversarial ones.
Simple defenses can be effective, but are language- and model-dependent. These
findings call for language-aware and cross-lingual safety benchmarks for LLMs.

</details>


### [174] [Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies](https://arxiv.org/abs/2511.00819)
*Yuxuan Hu,Jianchao Tan,Jiaqi Zhang,Wen Zan,Pingwei Sun,Yifan Lu,Yerui Sun,Yuchen Xie,Xunliang Cai,Jing Zhang*

Main category: cs.CL

TL;DR: 本文对原生稀疏注意力(NSA)进行系统分析并提出改进方法，通过在不同层交替使用局部(滑动窗口)和全局(压缩、选择性)注意力模式，结合潜在注意力机制，显著提升了长上下文建模能力，同时减少50%的KV缓存内存。


<details>
  <summary>Details</summary>
Motivation: 改进原生稀疏注意力机制，解决长序列任务中的依赖传播问题，同时降低内存消耗，提升模型的常识推理和长文本理解能力。

Method: 1. 在不同层交替使用局部滑动窗口注意力和全局压缩/选择性注意力模式；2. 滑动窗口分支采用多头潜在注意力(MLA)；3. 压缩和选择性分支采用分组头潜在注意力(GLA)。

Result: 在3.4亿到13亿参数的模型上测试(训练数据15B和100B tokens)，该方法在常识推理和长上下文理解任务上达到或超过了全注意力和原生稀疏注意力的性能，同时减少50%的KV缓存内存。

Conclusion: 通过交替注意力模式和潜在注意力机制的优化，有效提升了长上下文建模能力，在保持性能的同时显著降低了内存消耗。

Abstract: In this work, we conduct a systematic analysis of Native Sparse Attention
(NSA) and propose targeted improvements that enhance long-context modeling. A
key insight is that alternating between local (sliding-window) and global
(compression, selective) attention across layers, rather than using fixed
patterns, enables more effective propagation of long-range dependencies and
substantially boosts performance on long-sequence tasks. Meanwhile, we further
refine NSA's branches with Latent Attention that the sliding-window branch is
enhanced with Multi-head Latent Attention (MLA) while compression and selective
branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache
memory by 50\% versus NSA while improving the model's common-sense reasoning
and long-text understanding capabilities. Experiments on models from 340M to
1.3B parameters (trained on 15B and 100B tokens) show our method matches or
exceeds full attention and native sparse attention in both common-sense
reasoning and long-context understanding tasks.

</details>


### [175] [TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models](https://arxiv.org/abs/2511.00854)
*Chong Lyu,Lin Li,Shiqing Wu,Jingling Yuan*

Main category: cs.CL

TL;DR: TriCon-Fair是一个对比学习框架，通过解耦损失函数结合三元组和语言建模项来消除正负耦合，有效减少大语言模型中的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有去偏见方法独立处理有偏见和无偏见样本，忽略了它们之间的相互关系，导致隐藏的正负耦合问题，使得改进一个群体时无意中损害另一个群体，残留社会偏见持续存在。

Method: TriCon-Fair采用对比学习框架，使用解耦损失函数结合三元组和语言建模项，为每个锚点分配明确的有偏见负样本和无偏见正样本，解耦推拉动态并避免正负耦合，同时联合优化语言建模目标以保持通用能力。

Result: 实验结果表明，TriCon-Fair在保持强大下游性能的同时，比现有去偏见基线方法更能减少歧视性输出。

Conclusion: TriCon-Fair为敏感NLP应用提供了一个实用且符合伦理的解决方案，能够有效减少社会偏见传播。

Abstract: The increasing utilization of large language models raises significant
concerns about the propagation of social biases, which may result in harmful
and unfair outcomes. However, existing debiasing methods treat the biased and
unbiased samples independently, thus ignoring their mutual relationship. This
oversight enables a hidden negative-positive coupling, where improvements for
one group inadvertently compromise the other, allowing residual social bias to
persist. In this paper, we introduce TriCon-Fair, a contrastive learning
framework that employs a decoupled loss that combines triplet and language
modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns
each anchor an explicitly biased negative and an unbiased positive, decoupling
the push-pull dynamics and avoiding positive-negative coupling, and jointly
optimizes a language modeling (LM) objective to preserve general capability.
Experimental results demonstrate that TriCon-Fair reduces discriminatory output
beyond existing debiasing baselines while maintaining strong downstream
performance. This suggests that our proposed TriCon-Fair offers a practical and
ethical solution for sensitive NLP applications.

</details>


### [176] [Assessing LLM Reasoning Steps via Principal Knowledge Grounding](https://arxiv.org/abs/2511.00879)
*Hyeon Hwang,Yewon Cho,Chanwoong Yoon,Yein Park,Minju Song,Kyungjae Lee,Gangwoo Kim,Jaewoo Kang*

Main category: cs.CL

TL;DR: 本文提出了一个评估大语言模型推理过程中知识基础的新框架，包含大规模知识库、知识基础评估指标和轻量级评估模型三个核心组件，用于系统性地验证LLM推理是否准确基于知识。


<details>
  <summary>Details</summary>
Motivation: 随着逐步推理成为LLM处理复杂任务的标准方法，需要验证LLM的推理是否准确基于知识，但目前缺乏系统性的评估方法。

Method: 构建了包含三个关键组件的评估框架：(1) 大规模原子知识库，(2) 基于知识库设计的评估指标来衡量模型在推理中回忆和应用先验知识的能力，(3) 优化的轻量级评估模型用于高效可靠地计算指标。

Result: 该评估套件在识别缺失或误用知识元素方面表现出显著有效性，为揭示LLM的基本推理缺陷提供了关键见解。

Conclusion: 知识基础评估不仅能有效识别推理缺陷，还可集成到偏好优化中，展示了知识基础评估的进一步应用潜力。

Abstract: Step-by-step reasoning has become a standard approach for large language
models (LLMs) to tackle complex tasks. While this paradigm has proven
effective, it raises a fundamental question: How can we verify that an LLM's
reasoning is accurately grounded in knowledge? To address this question, we
introduce a novel evaluation suite that systematically assesses the knowledge
grounding of intermediate reasoning. Our framework comprises three key
components. (1) Principal Knowledge Collection, a large-scale repository of
atomic knowledge essential for reasoning. Based on the collection, we propose
(2) knowledge-grounded evaluation metrics designed to measure how well models
recall and apply prerequisite knowledge in reasoning. These metrics are
computed by our (3) evaluator LLM, a lightweight model optimized for
cost-effective and reliable metric computation. Our evaluation suite
demonstrates remarkable effectiveness in identifying missing or misapplied
knowledge elements, providing crucial insights for uncovering fundamental
reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these
metrics can be integrated into preference optimization, showcasing further
applications of knowledge-grounded evaluation.

</details>


### [177] [ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval](https://arxiv.org/abs/2511.00903)
*Ahmed Masry,Megh Thakkar,Patrice Bechard,Sathwik Tejaswi Madhusudhan,Rabiul Awal,Shambhavi Mishra,Akshay Kalkunte Suresh,Srivatsava Daruru,Enamul Hoque,Spandana Gella,Torsten Scholak,Sai Rajeswar*

Main category: cs.CL

TL;DR: ColMate是一个多模态文档检索模型，通过OCR预训练目标、自监督掩码对比学习和延迟交互评分机制，在ViDoRe V2基准测试上比现有检索模型提升3.61%，并展现出更强的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态文档检索方法往往复制纯文本检索技术，在文档编码、训练目标和相似度计算方面存在局限性，需要专门针对多模态文档结构和视觉特性的解决方案。

Method: 使用新颖的OCR预训练目标、自监督掩码对比学习目标，以及更适合多模态文档结构和视觉特征的延迟交互评分机制。

Result: 在ViDoRe V2基准测试上比现有检索模型提升3.61%，在跨域基准测试中展现出更强的泛化能力。

Conclusion: ColMate成功弥合了多模态表示学习与文档检索之间的差距，为多模态文档检索提供了更有效的解决方案。

Abstract: Retrieval-augmented generation has proven practical when models require
specialized knowledge or access to the latest data. However, existing methods
for multimodal document retrieval often replicate techniques developed for
text-only retrieval, whether in how they encode documents, define training
objectives, or compute similarity scores. To address these limitations, we
present ColMate, a document retrieval model that bridges the gap between
multimodal representation learning and document retrieval. ColMate utilizes a
novel OCR-based pretraining objective, a self-supervised masked contrastive
learning objective, and a late interaction scoring mechanism more relevant to
multimodal document structures and visual characteristics. ColMate obtains
3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,
demonstrating stronger generalization to out-of-domain benchmarks.

</details>


### [178] [The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles](https://arxiv.org/abs/2511.00960)
*Abhinav P M,Ojasva Saxena,Oswald C,Parameswari Krishnamurthy*

Main category: cs.CL

TL;DR: 本研究评估了5个大型语言模型在7种印度语言上的推理和自我评估能力，发现模型初始准确率与识别自身错误的能力呈负相关，表现最好的模型反而最过度自信。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在非英语语言（特别是印度语言）上进行文化基础推理的能力，以及模型的自我评估能力。

Method: 创建多语言谜语数据集，包含传统谜语和上下文重构变体，评估5个LLM在7种印度语言上的表现，采用7种提示策略，分两阶段测试谜语解答能力和自我评估一致性。

Result: Gemini 2.5 Pro整体表现最佳，但少样本方法仅带来边际收益，准确率在不同语言间差异显著。模型初始准确率与识别自身错误能力呈负相关：Gemini 2.5 Pro过度自信（真负率4.34%），而LLaMA 4 Scout更具自我意识（真负率42.09%）。

Conclusion: 多语言推理存在明显差距，需要开发既能有效推理又能识别自身局限的模型。

Abstract: The extent to which large language models (LLMs) can perform culturally
grounded reasoning across non-English languages remains underexplored. This
paper examines the reasoning and self-assessment abilities of LLMs across seven
major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and
Telugu. We introduce a multilingual riddle dataset combining traditional
riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5
Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under
seven prompting strategies. In the first stage, we assess riddle-solving
performance and find that while Gemini 2.5 Pro performs best overall, few-shot
methods yield only marginal gains, and accuracy varies notably across
languages. In the second stage, we conduct a self-evaluation experiment to
measure reasoning consistency. The results reveal a key finding: a model's
initial accuracy is inversely correlated with its ability to identify its own
mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%
True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are
substantially more self-aware (42.09% True Negative Rate). These results point
to clear gaps in multilingual reasoning and highlight the need for models that
not only reason effectively but also recognize their own limitations.

</details>


### [179] [Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective](https://arxiv.org/abs/2511.00988)
*Chenwang Wu,Yiu-ming Cheung,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文提出了一种从易到难的增强框架来解决机器生成文本检测中的边界模糊问题，通过使用针对较长文本的简单监督器来增强更具挑战性的目标检测器，在各种实际场景中展现出显著检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测方法假设标签为"黄金标准"，但存在边界模糊问题，传统训练范式不精确。人类认知局限和检测器超智能使不精确学习普遍且不可避免。

Method: 提出从易到难的增强框架，使用针对相对简单的较长文本检测任务的简单监督器（尽管能力较弱）来增强更具挑战性的目标检测器。通过将检测器结构性地整合到监督器中，理论上将监督器建模为检测器的性能下界。

Result: 在跨LLM、跨领域、混合文本和改写攻击等多种实际场景中的广泛实验表明，该框架具有显著的检测效果。

Conclusion: 该框架通过从易到难的监督增强方法，有效缓解了机器生成文本检测中的边界模糊问题，为不精确学习条件下的可靠监督提供了解决方案。

Abstract: Existing machine-generated text (MGT) detection methods implicitly assume
labels as the "golden standard". However, we reveal boundary ambiguity in MGT
detection, implying that traditional training paradigms are inexact. Moreover,
limitations of human cognition and the superintelligence of detectors make
inexact learning widespread and inevitable. To this end, we propose an
easy-to-hard enhancement framework to provide reliable supervision under such
inexact conditions. Distinct from knowledge distillation, our framework employs
an easy supervisor targeting relatively simple longer-text detection tasks
(despite weaker capabilities), to enhance the more challenging target detector.
Firstly, longer texts targeted by supervisors theoretically alleviate the
impact of inexact labels, laying the foundation for reliable supervision.
Secondly, by structurally incorporating the detector into the supervisor, we
theoretically model the supervisor as a lower performance bound for the
detector. Thus, optimizing the supervisor indirectly optimizes the detector,
ultimately approximating the underlying "golden" labels. Extensive experiments
across diverse practical scenarios, including cross-LLM, cross-domain, mixed
text, and paraphrase attacks, demonstrate the framework's significant detection
effectiveness. The code is available at:
https://github.com/tmlr-group/Easy2Hard.

</details>


### [180] [IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation](https://arxiv.org/abs/2511.01014)
*Bosi Wen,Yilin Niu,Cunxiang Wang,Pei Ke,Xiaoying Ling,Ying Zhang,Aohan Zeng,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 提出IF-CRITIC模型，通过约束清单生成和多阶段过滤机制，为指令跟随提供高效可靠的评估，显著提升LLM的指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令跟随评估模型存在成本高、评估不可靠等问题，需要开发更高效可靠的评估方法。

Method: 开发约束清单生成器分解指令，通过多阶段过滤机制收集高质量训练数据，采用约束级偏好优化方法训练IF-CRITIC。

Result: IF-CRITIC评估性能超越Deepseek-R1和o4-mini等强基线，在较低计算开销下显著提升LLM指令跟随优化性能。

Conclusion: IF-CRITIC为指令跟随优化提供了可扩展的奖励信号，是提升LLM指令跟随能力的有效解决方案。

Abstract: Instruction following is a fundamental ability of Large Language Models
(LLMs), requiring their generated outputs to follow multiple constraints
imposed in input instructions. Numerous studies have attempted to enhance this
ability through preference optimization or reinforcement learning based on
reward signals from LLM-as-a-Judge. However, existing evaluation models for
instruction following still possess many deficiencies, such as substantial
costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM
critic that can provide efficient and reliable assessments of constraint
following in the instructions. We first develop a checklist generator to
decompose instructions and generate constraint checklists. With the assistance
of the checklists, we collect high-quality critique training data through a
multi-stage critique filtering mechanism and employ a constraint-level
preference optimization method to train IF-CRITIC. Extensive experiments
demonstrate that the evaluation performance of IF-CRITIC can beat strong
LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable
reward signals provided by IF-CRITIC, LLMs can achieve substantial performance
gains in instruction-following optimization under lower computational overhead
compared to strong LLM critic baselines.

</details>


### [181] [Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2511.01016)
*Wenjin Liu,Haoran Luo,Xueyuan Lin,Haoming Liu,Tiesunlong Shen,Jiapu Wang,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: Prompt-R1是一个端到端的强化学习框架，使用小规模LLM与大规模LLM协作，通过多轮提示交互解决复杂问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前用户往往无法为复杂问题提供准确有效的提示，限制了大型语言模型的性能发挥。

Method: 采用小规模LLM思考并生成提示，大规模LLM进行复杂推理的双模型协作框架，设计了双重约束奖励机制来优化正确性、生成质量和推理准确性。

Result: 在多个公共数据集上的实验表明，Prompt-R1显著优于基线模型。

Conclusion: Prompt-R1提供了一个即插即用的框架，支持与各种大规模LLM的推理和训练，有效解决了用户提示质量不足的问题。

Abstract: Recently, advanced large language models (LLMs) have emerged at an
increasingly rapid pace. However, when faced with complex problems, most users
are often unable to provide accurate and effective prompts to interact with
LLMs, thus limiting the performance of LLMs. To address this challenge, we
propose Prompt-R1, an end-to-end reinforcement learning framework that uses a
small-scale LLM to collaborate with large-scale LLMs, replacing user
interaction to solve problems better. This collaboration is cast as a
multi-turn prompt interaction, where the small-scale LLM thinks and generates
prompts, and the large-scale LLM performs complex reasoning. A dual-constrained
reward is designed to optimize for correctness, generation quality, and
reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports
both inference and training with various large-scale LLMs. Experiments on
multiple public datasets show that Prompt-R1 significantly outperforms baseline
models across tasks. Our code is publicly available at
https://github.com/QwenQKing/Prompt-R1.

</details>


### [182] [OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights](https://arxiv.org/abs/2511.01019)
*Bowen Chen,Jayesh Gajbhar,Gregory Dusek,Rob Redmon,Patrick Hogan,Paul Liu,DelWayne Bohnenstiehl,Dongkuan,Xu,Ruoying He*

Main category: cs.CL

TL;DR: OceanAI是一个对话式平台，将开源大语言模型的自然语言流畅性与NOAA权威海洋数据的实时参数化访问相结合，通过API调用生成可验证的自然语言响应和数据可视化，提高科学AI的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决通用对话AI系统生成未经验证的'幻觉'内容、破坏科学严谨性的问题，为海洋科学提供基于真实数据的可信AI决策支持。

Method: 集成开源LLM的自然语言能力与NOAA实时数据流，通过API调用识别、解析和合成相关数据集，生成可重复的自然语言响应和数据可视化。

Result: 在盲测比较中，只有OceanAI能提供NOAA来源的数据值和原始数据引用，其他AI产品要么拒绝回答，要么提供无支持的结果。

Conclusion: OceanAI通过基于可验证观测的输出，推进了透明度、可重复性和信任度，为海洋领域的AI决策支持提供了可扩展框架。

Abstract: Artificial intelligence is transforming the sciences, yet general
conversational AI systems often generate unverified "hallucinations"
undermining scientific rigor. We present OceanAI, a conversational platform
that integrates the natural-language fluency of open-source large language
models (LLMs) with real-time, parameterized access to authoritative
oceanographic data streams hosted by the National Oceanic and Atmospheric
Administration (NOAA). Each query such as "What was Boston Harbor's highest
water level in 2024?" triggers real-time API calls that identify, parse, and
synthesize relevant datasets into reproducible natural-language responses and
data visualizations. In a blind comparison with three widely used AI
chat-interface products, only OceanAI produced NOAA-sourced values with
original data references; others either declined to answer or provided
unsupported results. Designed for extensibility, OceanAI connects to multiple
NOAA data products and variables, supporting applications in marine hazard
forecasting, ecosystem assessment, and water-quality monitoring. By grounding
outputs and verifiable observations, OceanAI advances transparency,
reproducibility, and trust, offering a scalable framework for AI-enabled
decision support within the oceans. A public demonstration is available at
https://oceanai.ai4ocean.xyz.

</details>


### [183] [VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics](https://arxiv.org/abs/2511.01046)
*Vedant Acharya,Abhay Pisharodi,Rishabh Mondal,Mohammad Rafiuddin,Nipun Batra*

Main category: cs.CL

TL;DR: VayuChat是一个对话式系统，通过自然语言问答帮助用户分析空气质量、气象和政策项目数据，生成Python代码和交互式可视化，使环境数据分析对决策者、研究人员和公民更加易用。


<details>
  <summary>Details</summary>
Motivation: 印度每年因空气污染导致约160万人过早死亡，但决策者难以将分散的数据转化为有效决策。现有工具需要专业知识且提供静态仪表板，无法解决关键政策问题。

Method: 开发基于大语言模型的对话系统VayuChat，整合中央污染控制委员会监测站数据、州级人口统计数据和国家清洁空气计划资金记录，通过自然语言界面提供数据分析能力。

Result: VayuChat能够回答关于空气质量、气象和政策项目的自然语言问题，生成可执行的Python代码和交互式可视化，使复杂的环境分析变得简单易用。

Conclusion: VayuChat通过对话式界面使环境数据科学对政策制定者、研究人员和公民更加可访问，已在Hugging Face平台公开部署。

Abstract: Air pollution causes about 1.6 million premature deaths each year in India,
yet decision makers struggle to turn dispersed data into decisions. Existing
tools require expertise and provide static dashboards, leaving key policy
questions unresolved. We present VayuChat, a conversational system that answers
natural language questions on air quality, meteorology, and policy programs,
and responds with both executable Python code and interactive visualizations.
VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring
stations, state-level demographics, and National Clean Air Programme (NCAP)
funding records into a unified interface powered by large language models. Our
live demonstration will show how users can perform complex environmental
analytics through simple conversations, making data science accessible to
policymakers, researchers, and citizens. The platform is publicly deployed at
https://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further
information check out video uploaded on
https://www.youtube.com/watch?v=d6rklL05cs4.

</details>


### [184] [Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs](https://arxiv.org/abs/2511.01053)
*Qing Ding,Eric Hua Qing Zhang,Felix Jozsa,Julia Ive*

Main category: cs.CL

TL;DR: 该研究创建了一个基于临床指南的标准化基准数据集，用于评估大型语言模型在医疗领域的临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估基于指南的临床推理的标准化基准，而大型语言模型在医疗领域的应用日益增多，需要有效的评估工具。

Method: 使用GPT帮助创建从多个诊断的公开指南中提取的数据集，包含真实患者场景和临床问题，并对一系列流行的LLM进行基准测试。

Result: 开发了一个经过验证的数据集，并展示了其有效性，为系统评估LLM的临床实用性和指南依从性提供了框架。

Conclusion: 该研究提供了一个支持系统评估LLM临床实用性和指南依从性的框架，填补了医疗领域LLM评估的空白。

Abstract: Large language models (LLMs) are increasingly used in healthcare, yet
standardised benchmarks for evaluating guideline-based clinical reasoning are
missing. This study introduces a validated dataset derived from publicly
available guidelines across multiple diagnoses. The dataset was created with
the help of GPT and contains realistic patient scenarios, as well as clinical
questions. We benchmark a range of recent popular LLMs to showcase the validity
of our dataset. The framework supports systematic evaluation of LLMs' clinical
utility and guideline adherence.

</details>


### [185] [Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering](https://arxiv.org/abs/2511.01090)
*Vlad Negoita,Mihai Masala,Traian Rebedea*

Main category: cs.CL

TL;DR: 本研究分析了罗马尼亚语预训练语料库的特征和覆盖范围，并与英语数据进行比较。通过在多任务模型上进行轻量级训练，使用LLM标注的罗马尼亚文本进行多级过滤，生成高质量的预训练数据集。实验显示罗马尼亚语和英语数据在主题分布上存在显著差异，同时证明了数据过滤能有效提升LLM预训练性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练的关键因素之一是高质量数据的可用性和筛选。对于代表性不足的语言如罗马尼亚语，高质量语料库稀缺，因此需要研究其预训练语料特征并开发有效的数据过滤方法。

Method: 训练轻量级多任务模型，使用LLM标注的罗马尼亚文本进行多级过滤（如教育价值、主题、格式），生成高质量的预训练数据集。

Result: 实验发现罗马尼亚语和英语数据在主题分布上存在显著趋势差异。通过数据过滤，在多个基准测试中LLM预训练性能得到改善，证明了过滤方法的有效性。

Conclusion: 研究表明针对特定语言的数据特征分析和多级过滤能够显著提升预训练数据集质量，尤其对于资源稀缺语言具有重要意义。数据过滤方法能有效改善LLM在代表性不足语言上的性能表现。

Abstract: Large Language Models (LLMs) have recently exploded in popularity, often
matching or outperforming human abilities on many tasks. One of the key factors
in training LLMs is the availability and curation of high-quality data. Data
quality is especially crucial for under-represented languages, where
high-quality corpora are scarce. In this work we study the characteristics and
coverage of Romanian pretraining corpora and we examine how they differ from
English data. By training a lightweight multitask model on carefully
LLM-annotated Romanian texts, we are able to analyze and perform multi-level
filtering (e.g., educational value, topic, format) to generate high-quality
pretraining datasets. Our experiments show noteworthy trends in the topics
present in Romanian and English data, while also proving the effectiveness of
filtering data through improved LLM pretraining performance across multiple
benchmarks.

</details>


### [186] [TSVer: A Benchmark for Fact Verification Against Time-Series Evidence](https://arxiv.org/abs/2511.01101)
*Marek Strong,Andreas Vlachos*

Main category: cs.CL

TL;DR: TSVer是一个专注于时间序列证据进行时序和数值推理的事实核查基准数据集，包含287个真实世界声明和400个涵盖多个领域的时间序列，通过多步骤标注过程提高标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统在评估时序和数值推理能力方面受到数据集限制，这些数据集往往缺乏结构化证据、对裁决提供不足的论证或依赖合成声明。

Method: 从38个事实核查组织收集287个真实世界声明，构建包含400个时间序列的数据库，采用LLM辅助的多步骤标注过程，为每个声明标注时间框架、裁决和证据使用论证。

Result: 实现了kappa=0.745的标注者间一致性，基线实验显示即使是Gemini-2.5-Pro等最先进的推理模型在时间序列验证上也面临挑战，裁决准确率为63.37%，论证Ev2R得分为48.63%。

Conclusion: TSVer数据集填补了时序和数值推理事实核查基准的空白，揭示了当前模型在处理时间序列证据时的局限性，为未来研究提供了重要基准。

Abstract: Reasoning over temporal and numerical data, such as time series, is a crucial
aspect of fact-checking. While many systems have recently been developed to
handle this form of evidence, their evaluation remains limited by existing
datasets, which often lack structured evidence, provide insufficient
justifications for verdicts, or rely on synthetic claims. In this paper, we
introduce TSVer, a new benchmark dataset for fact verification focusing on
temporal and numerical reasoning with time-series evidence. TSVer contains 287
real-world claims sourced from 38 fact-checking organizations and a curated
database of 400 time series covering diverse domains. Each claim is annotated
with time frames across all pertinent time series, along with a verdict and
justifications reflecting how the evidence is used to reach the verdict. Using
an LLM-assisted multi-step annotation process, we improve the quality of our
annotations and achieve an inter-annotator agreement of kappa=0.745 on
verdicts. We also develop a baseline for verifying claims against time-series
evidence and show that even the state-of-the-art reasoning models like
Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score
on verdicts and an Ev2R score of 48.63 on verdict justifications.

</details>


### [187] [MicroRemed: Benchmarking LLMs in Microservices Remediation](https://arxiv.org/abs/2511.01166)
*Lingzhe Zhang,Yunpeng Zhai,Tong Jia,Chiming Duan,Minghua He,Leyi Pan,Zhaoyang Liu,Bolin Ding,Ying Li*

Main category: cs.CL

TL;DR: 该论文提出了MicroRemed基准，用于评估LLM在端到端微服务修复中的能力，并开发了ThinkRemed多智能体框架来模拟SRE的推理过程，显著提升了修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工编写的提示，LLM仅将文本指令转换为可执行代码，缺乏自主决策能力。微服务修复是一个有前景但尚未充分探索的方向。

Method: 引入MicroRemed基准评估LLM生成可执行Ansible剧本的能力；提出ThinkRemed多智能体框架，模拟SRE的反思和感知推理过程。

Result: 实验结果表明，MicroRemed对当前LLM构成重大挑战，而ThinkRemed通过迭代推理和系统反思显著提升了端到端修复性能。

Conclusion: 该研究为LLM在微服务修复领域的应用提供了首个基准测试和有效的多智能体解决方案，推动了该领域的发展。

Abstract: Large Language Models (LLMs) integrated with agent-based reasoning frameworks
have recently shown strong potential for autonomous decision-making and
system-level operations. One promising yet underexplored direction is
microservice remediation, where the goal is to automatically recover faulty
microservice systems. Existing approaches, however, still rely on human-crafted
prompts from Site Reliability Engineers (SREs), with LLMs merely converting
textual instructions into executable code. To advance research in this area, we
introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end
microservice remediation, where models must directly generate executable
Ansible playbooks from diagnosis reports to restore system functionality. We
further propose ThinkRemed, a multi-agent framework that emulates the
reflective and perceptive reasoning of SREs. Experimental results show that
MicroRemed presents substantial challenges to current LLMs, while ThinkRemed
improves end-to-end remediation performance through iterative reasoning and
system reflection. The benchmark is available at
https://github.com/LLM4AIOps/MicroRemed.

</details>


### [188] [Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs](https://arxiv.org/abs/2511.01187)
*Muhammed Saeed,Muhammad Abdul-mageed,Shady Shehata*

Main category: cs.CL

TL;DR: DebateBias-8K是一个多语言、辩论式基准测试，用于评估大语言模型在生成式场景中的叙事偏见，涵盖4个敏感领域和7种语言，发现模型普遍存在刻板印象偏见。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估主要依赖英语分类任务，无法揭示真实生成场景中的叙事偏见，需要开发更贴近实际应用的评估方法。

Method: 构建包含8,400个结构化辩论提示的数据集，涵盖4个敏感领域和7种语言，使用4个主流模型生成超过10万条响应并进行自动分类。

Result: 所有模型都重现了根深蒂固的刻板印象：阿拉伯人与恐怖主义和宗教高度关联(≥95%)，非洲人与社会经济“落后”关联(高达77%)，西方群体被一致描述为现代或进步。在低资源语言中偏见更加明显。

Conclusion: 当前主要基于英语的对齐方法无法在全球范围内泛化，现有对齐方法虽然减少了显性毒性，但无法防止开放上下文中的偏见输出，需要开发更安全、文化包容的模型对齐方法。

Abstract: Large language models (LLMs) are widely deployed for open-ended
communication, yet most bias evaluations still rely on English,
classification-style tasks. We introduce DebateBias-8K, a new multilingual,
debate-style benchmark designed to reveal how narrative bias appears in
realistic generative settings. Our dataset includes 8,400 structured debate
prompts spanning four sensitive domains: women's rights, socioeconomic
development, terrorism, and religion, across seven languages ranging from
high-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).
Using four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we
generate and automatically classify over 100,000 responses. Results show that
all models reproduce entrenched stereotypes despite safety alignment: Arabs are
overwhelmingly linked to terrorism and religion (>=95%), Africans to
socioeconomic "backwardness" (up to <=77%), and Western groups are consistently
framed as modern or progressive. Biases grow sharply in lower-resource
languages, revealing that alignment trained primarily in English does not
generalize globally. Our findings highlight a persistent divide in multilingual
fairness: current alignment methods reduce explicit toxicity but fail to
prevent biased outputs in open-ended contexts. We release our DebateBias-8K
benchmark and analysis framework to support the next generation of multilingual
bias evaluation and safer, culturally inclusive model alignment.

</details>


### [189] [Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning](https://arxiv.org/abs/2511.01191)
*Ru Wang,Wei Huang,Qi Cao,Yusuke Iwasawa,Yutaka Matsuo,Jiaxian Guo*

Main category: cs.CL

TL;DR: Self-Harmony是一个无需标签的测试时强化学习框架，通过利用原始问题与其改写版本之间答案的一致性来构建可靠的学习信号，避免了多数投票方法倾向于虚假但流行答案的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时强化学习方法依赖多数投票等机制，但这些方法容易偏向虚假但流行的答案。研究者希望开发一种更可靠的学习信号构建方法，利用问题改写前后的答案稳定性来识别正确解。

Method: 使用单一模型扮演两个互补角色：求解器生成答案，重构器改写输入。然后提出伪标签方法，使用调和平均而非多数投票来聚合原始视图和重构视图的答案频率，选择在重构下保持稳定的解决方案。

Result: 在多样化的推理基准测试中，Self-Harmony在无标签测试时设置下取得了最先进的结果，在30个设置中的28个排名第一。该方法表现出前所未有的鲁棒性，所有实验中零训练失败。

Conclusion: Self-Harmony通过利用问题改写稳定性构建可靠学习信号，无需人工监督或辅助模型，在测试时强化学习中实现了优异的性能和稳定性，证明了基于重构一致性的方法的有效性。

Abstract: Test-time reinforcement learning (TTRL) offers a label-free paradigm for
adapting models using only synthetic signals at inference, but its success
hinges on constructing reliable learning signals. Standard approaches such as
majority voting often collapse to spurious yet popular answers. We introduce
Self-Harmony, a framework built on a simple intuition: the correct answer
should remain stable across both an original question and its paraphrase.
Self-Harmony operationalizes this by employing a single model in two
complementary roles: a Solver to produce answers and a Reframer to rephrase the
input. Based on this, we further propose a pseudo-label method: instead of
majority voting, it aggregates answer frequencies across these original and
reframed views using the harmonic mean. This is a process that naturally
selects for solutions stable under reframing, thereby avoiding the common trap
of favoring view-dependent, spurious answers. Crucially, this requires no human
supervision or auxiliary models. Across diverse reasoning benchmarks,
Self-Harmony achieves state-of-the-art results at the label-free test-time
setting, ranking first in 28 of 30 settings across multiple methods. Beyond
accuracy, it demonstrates unprecedented robustness, with zero training failures
in all experiments, underscoring its stability and reliability.

</details>


### [190] [DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection](https://arxiv.org/abs/2511.01192)
*Guoxin Ma,Xiaoming Liu,Zhanhan Zhang,Chengzhengxu Li,Shengchao Liu,Yu Lan*

Main category: cs.CL

TL;DR: 提出了DEER框架，通过两阶段解耦专家混合架构来检测机器生成文本，有效处理领域偏移问题，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器生成文本检测方法在领域偏移下性能显著下降，需要同时捕获领域特定和领域通用的MGT模式。

Method: 采用两阶段解耦专家混合架构：1）领域特定专家学习细粒度区分，共享专家提取跨领域特征；2）基于强化学习的路由机制动态选择专家，解决推理时领域标签不可用的问题。

Result: 在5个领域内和5个领域外基准数据集上，DEER持续优于最先进方法，领域内F1分数平均提升1.39%，领域外提升5.32%；准确率分别提升1.35%和3.61%。

Conclusion: 解耦专家专业化和自适应路由对模型性能至关重要，DEER框架有效解决了机器生成文本检测中的领域偏移挑战。

Abstract: Detecting machine-generated text (MGT) has emerged as a critical challenge,
driven by the rapid advancement of large language models (LLMs) capable of
producing highly realistic, human-like content. However, the performance of
current approaches often degrades significantly under domain shift. To address
this challenge, we propose a novel framework designed to capture both
domain-specific and domain-general MGT patterns through a two-stage
Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a
disentangled mixture-of-experts module, in which domain-specific experts learn
fine-grained, domain-local distinctions between human and machine-generated
text, while shared experts extract transferable, cross-domain features. Second,
to mitigate the practical limitation of unavailable domain labels during
inference, we design a reinforcement learning-based routing mechanism that
dynamically selects the appropriate experts for each input instance,
effectively bridging the train-inference gap caused by domain uncertainty.
Extensive experiments on five in-domain and five out-of-domain benchmark
datasets demonstrate that DEER consistently outperforms state-of-the-art
methods, achieving average F1-score improvements of 1.39% and 5.32% on
in-domain and out-of-domain datasets respectively, along with accuracy gains of
1.35% and 3.61% respectively. Ablation studies confirm the critical
contributions of both disentangled expert specialization and adaptive routing
to model performance.

</details>


### [191] [AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs](https://arxiv.org/abs/2511.01265)
*Mo El-Haj,Paul Rayson*

Main category: cs.CL

TL;DR: 本文研究了领域特异性对阿拉伯语金融文本摘要生成的影响，构建了最大的阿拉伯语金融新闻数据集AraFinNews，并通过实验证明领域适应的模型能生成更准确、连贯的摘要。


<details>
  <summary>Details</summary>
Motivation: 研究领域特异性在阿拉伯语金融文本摘要生成中的作用，填补阿拉伯语金融文本摘要数据集的空白，评估领域适应对事实准确性、数值可靠性和风格对齐的影响。

Method: 构建AraFinNews数据集（21.25万篇文章-标题对），评估基于Transformer的模型（mT5、AraT5和领域适应的FinAraT5），分析金融领域预训练对摘要质量的影响。

Result: 实验结果显示，领域适应的模型能生成更忠实、连贯的摘要，特别是在处理定量信息和实体中心信息方面表现更好。

Conclusion: 领域特定适应对于提高阿拉伯语金融摘要的事实一致性和叙述流畅性至关重要，AraFinNews数据集为阿拉伯语金融文本理解提供了重要基准。

Abstract: This paper investigates the impact of domain specificity on abstractive
summarisation of Arabic financial texts using large language models (LLMs). We
introduce AraFinNews, the largest publicly available Arabic financial news
dataset to date, comprising 212,500 article--headline pairs spanning nearly a
decade of reporting from October 2015 to July 2025. Designed as the Arabic
equivalent of major English summarisation corpora such as CNN/DailyMail,
AraFinNews provides a robust benchmark for evaluating domain-specific language
understanding and generation in financial contexts. Using this resource, we
evaluate transformer-based models -- including mT5, AraT5, and the
domain-adapted FinAraT5 -- to examine how financial-domain pretraining
influences factual accuracy, numerical reliability, and stylistic alignment
with professional reporting. Experimental results show that domain-adapted
models generate more faithful and coherent summaries, particularly in handling
quantitative and entity-centric information. The findings highlight the
importance of domain-specific adaptation for improving factual consistency and
narrative fluency in Arabic financial summarisation. The dataset is freely
available for non-commercial research at
https://github.com/ArabicNLP-UK/AraFinNews.

</details>


### [192] [When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding](https://arxiv.org/abs/2511.01282)
*Min Fang,Zhihui Fu,Qibin Zhao,Jun Wang*

Main category: cs.CL

TL;DR: ReSpec是一种新颖的检索增强推测解码框架，通过自适应决策机制取代启发式切换策略，在保持输出质量的同时显著加速大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法存在局限性：基于模型的方法如EAGLE-2准确但成本高，检索增强方法如SAM-Decoding依赖启发式切换策略，经常触发不必要的检索。需要一种更智能的方法来优化推测解码效率。

Method: 提出ReSpec框架，包含三个核心创新：1）熵引导的自适应触发器，仅在不确定性低时启动检索；2）反馈驱动的候选选择，利用历史反馈组织高质量候选进行并行验证；3）源感知的宽松验证策略，对模型生成草稿严格检查，对检索草稿宽松验证。

Result: 在Spec-Bench上的广泛实验表明，ReSpec实现了最先进的加速效果，分别比EAGLE-2和SAM-Decoding快33%和25%以上，同时保持输出质量。

Conclusion: ReSpec通过将启发式草稿切换转化为自适应决策，有效解决了现有推测解码方法的局限性，在加速性能和输出质量之间取得了更好的平衡。

Abstract: Speculative decoding (SD) has emerged as an effective technique to accelerate
large language model (LLM) inference without compromising output quality.
However, the achievable speedup largely depends on the effectiveness of the
drafting model. While model-based methods like EAGLE-2 are accurate but costly,
retrieval-enhanced methods like SAM-Decoding rely on heuristic switching
strategies that often trigger unnecessary retrievals. To address this, we
propose ReSpec (\textbf{Re}trieval-enhanced \textbf{Spe}culative Decoding), a
novel framework that transforms heuristic drafter switching into adaptive
decision-making. ReSpec features three core innovations: 1) An
\textbf{entropy-guided adaptive trigger} quantifies contextual predictability
to initiate retrieval only when uncertainty is low, avoiding costly low-quality
speculations. 2) A \textbf{feedback-driven candidate selection} leverages
historical feedback to organize multiple high-quality candidates for parallel
verification, maximizing retrieval utility. 3) A source-aware \textbf{relaxed
verification strategy} applies strict checks to model-generated drafts while
using a relaxed verification for retrieved drafts, achieving a better balance
between accuracy and efficiency. Extensive experiments on Spec-Bench
demonstrate that ReSpec achieves state-of-the-art acceleration,outperforming
EAGLE-2 and SAM-Decoding by over $33\%$ and $25\%$, respectively, while
maintaining output quality.

</details>


### [193] ["Give a Positive Review Only": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers](https://arxiv.org/abs/2511.01287)
*Qin Zhou,Zhexin Zhang,Zhi Li,Limin Sun*

Main category: cs.CL

TL;DR: 本文系统研究了科学论文中隐藏提示注入攻击对AI审稿人的威胁，提出了静态和迭代两种攻击方法，并探索了检测防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在科学论文评审中的广泛应用，发现存在通过隐藏提示操纵AI审稿人给出过高评价的安全威胁，需要对此进行系统性研究。

Method: 提出两类攻击方法：静态攻击使用固定注入提示，迭代攻击通过模拟审稿人模型优化注入提示以最大化效果；同时探索基于检测的防御机制。

Result: 两种攻击方法都取得了显著效果，经常能诱导前沿AI审稿人给出满分评价；攻击在不同设置下都具有鲁棒性；检测防御能大幅降低攻击成功率，但自适应攻击者可以部分规避防御。

Conclusion: 研究结果强调了在AI辅助同行评审中需要更多关注和严格防护措施来应对提示注入威胁。

Abstract: With the rapid advancement of AI models, their deployment across diverse
tasks has become increasingly widespread. A notable emerging application is
leveraging AI models to assist in reviewing scientific papers. However, recent
reports have revealed that some papers contain hidden, injected prompts
designed to manipulate AI reviewers into providing overly favorable
evaluations. In this work, we present an early systematic investigation into
this emerging threat. We propose two classes of attacks: (1) static attack,
which employs a fixed injection prompt, and (2) iterative attack, which
optimizes the injection prompt against a simulated reviewer model to maximize
its effectiveness. Both attacks achieve striking performance, frequently
inducing full evaluation scores when targeting frontier AI reviewers.
Furthermore, we show that these attacks are robust across various settings. To
counter this threat, we explore a simple detection-based defense. While it
substantially reduces the attack success rate, we demonstrate that an adaptive
attacker can partially circumvent this defense. Our findings underscore the
need for greater attention and rigorous safeguards against prompt-injection
threats in AI-assisted peer review.

</details>


### [194] [FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings](https://arxiv.org/abs/2511.01289)
*Saiyma Sittul Muna,Rezwan Islam Salvi,Mushfiqur Rahman Mushfique,Ajwad Abrar*

Main category: cs.CL

TL;DR: 提出了FirstAidQA数据集，包含5500个高质量急救问答对，旨在支持在低连接环境下的大语言模型和小语言模型微调，以改善急救响应系统。


<details>
  <summary>Details</summary>
Motivation: 解决在紧急情况下缺乏高质量、轻量级急救数据集的问题，当前模型计算密集且不适合第一响应者使用的低端设备。

Method: 使用ChatGPT-4o-mini通过提示式上下文学习生成数据集，基于《Vital First Aid Book (2019)》文本，经过文本清洗、上下文分块、过滤等预处理步骤，并进行人工验证确保准确性、安全性和实用性。

Result: 创建了包含5500个高质量急救问答对的FirstAidQA数据集，涵盖广泛的急救和应急响应场景。

Conclusion: FirstAidQA数据集公开可用，将推动急救和应急响应中安全关键和资源受限AI应用的研究，支持开发更快、更可靠且具备离线能力的系统。

Abstract: In emergency situations, every second counts. The deployment of Large
Language Models (LLMs) in time-sensitive, low or zero-connectivity environments
remains limited. Current models are computationally intensive and unsuitable
for low-tier devices often used by first responders or civilians. A major
barrier to developing lightweight, domain-specific solutions is the lack of
high-quality datasets tailored to first aid and emergency response. To address
this gap, we introduce FirstAidQA, a synthetic dataset containing 5,500
high-quality question answer pairs that encompass a wide range of first aid and
emergency response scenarios. The dataset was generated using a Large Language
Model, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from
the Vital First Aid Book (2019). We applied preprocessing steps such as text
cleaning, contextual chunking, and filtering, followed by human validation to
ensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is
designed to support instruction-tuning and fine-tuning of LLMs and Small
Language Models (SLMs), enabling faster, more reliable, and offline-capable
systems for emergency settings. We publicly release the dataset to advance
research on safety-critical and resource-constrained AI applications in first
aid and emergency response. The dataset is available on Hugging Face at
https://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.

</details>


### [195] [DeepSpecs: Expert-Level Questions Answering in 5G](https://arxiv.org/abs/2511.01305)
*Aman Ganapathy Manvattira,Yifei Xu,Ziyue Dang,Songwu Lu*

Main category: cs.CL

TL;DR: DeepSpecs是一个增强的RAG系统，通过结构化和时序推理来解决5G标准文档中的交叉引用和规范演进问题，显著提升了专家级问答的质量。


<details>
  <summary>Details</summary>
Motivation: 5G技术为数十亿用户提供移动互联网接入，但回答关于5G规范的专家级问题需要浏览数千页交叉引用的标准文档，这些文档在不同版本间不断演进。现有的RAG框架无法可靠地解析交叉引用或推理规范演进。

Method: DeepSpecs通过三个元数据丰富的数据库增强RAG系统：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档）。系统通过元数据查找递归检索引用条款来显式解析交叉引用，并通过挖掘变更并将其链接到记录设计原理的变更请求来追踪规范演进。

Result: 在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融实验证实显式交叉引用解析和演进感知检索显著提高了答案质量。

Conclusion: 建模5G标准的结构和时序特性对于可靠回答专家级问题具有重要价值，DeepSpecs通过结构化和时序推理有效解决了现有RAG系统在5G标准文档处理中的局限性。

Abstract: 5G technology enables mobile Internet access for billions of users. Answering
expert-level questions about 5G specifications requires navigating thousands of
pages of cross-referenced standards that evolve across releases. Existing
retrieval-augmented generation (RAG) frameworks, including telecom-specific
approaches, rely on semantic similarity and cannot reliably resolve
cross-references or reason about specification evolution. We present DeepSpecs,
a RAG system enhanced by structural and temporal reasoning via three
metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB
(line-level version diffs), and TDocDB (standardization meeting documents).
DeepSpecs explicitly resolves cross-references by recursively retrieving
referenced clauses through metadata lookup, and traces specification evolution
by mining changes and linking them to Change Requests that document design
rationale. We curate two 5G QA datasets: 573 expert-annotated real-world
questions from practitioner forums and educational resources, and 350
evolution-focused questions derived from approved Change Requests. Across
multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art
telecom RAG systems; ablations confirm that explicit cross-reference resolution
and evolution-aware retrieval substantially improve answer quality,
underscoring the value of modeling the structural and temporal properties of 5G
standards.

</details>


### [196] [DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness](https://arxiv.org/abs/2511.01323)
*Jiabao Ji,Min Li,Priyanshu Kumar,Shiyu Chang,Saloni Potdar*

Main category: cs.CL

TL;DR: DeepAmbigQA是一个新的问答数据集，包含3600个需要多跳推理的问题，其中一半涉及显式名称消歧，现有最先进模型在该数据集上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答基准很少同时评估名称歧义和多步推理两个挑战，而大型语言模型在回答复杂问题时往往无法产生完整的答案集。

Method: 开发了DeepAmbigQAGen自动数据生成流水线，基于文本语料库和链接知识图谱构建问答任务，生成自然且可验证的问题，系统性地嵌入名称歧义和多步推理。

Result: 构建了DeepAmbigQA数据集，包含3600个问题。实验显示，即使是GPT-5这样的最先进模型，在歧义问题上精确匹配率仅为0.13，在非歧义问题上为0.21。

Conclusion: 研究结果强调了需要开发更强大的问答系统，以应对信息收集和答案完整性的挑战。

Abstract: Large language models (LLMs) with integrated search tools show strong promise
in open-domain question answering (QA), yet they often struggle to produce
complete answer set to complex questions such as Which actor from the film Heat
won at least one Academy Award?, which requires (1) distinguishing between
multiple films sharing the same title and (2) reasoning across a large set of
actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate
both challenges jointly. To address this, we introduce DeepAmbigQAGen, an
automatic data generation pipeline that constructs QA tasks grounded in text
corpora and linked knowledge graph, generating natural and verifiable questions
that systematically embed name ambiguity and multi-step reasoning. Based on
this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop
reasoning and half of them explicit name ambiguity resolving. Experiments
reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving
only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous
questions. These findings highlight the need for more robust QA systems aimed
at information gathering and answer completeness.

</details>


### [197] [Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series](https://arxiv.org/abs/2511.01354)
*Wenrui Cai,Chengyu Wang,Junbing Yan,Jun Huang,Xiangzhong Fang*

Main category: cs.CL

TL;DR: 本文扩展了DistilQwen模型系列，针对工业需求开发了四种模型：慢思考模型（高精度推理）、自适应思考模型（动态调整策略）和蒸馏奖励模型（支持强化学习），在多个基准测试中展现出高效推理和强大性能。


<details>
  <summary>Details</summary>
Motivation: 为满足现实应用对小型高效推理模型的需求，平衡推理性能与推理速度，开发专门针对工业需求的知识蒸馏技术。

Method: 基于Qwen模型初始化，通过知识蒸馏技术开发四种模型系列：慢思考模型、自适应思考模型（两个系列）和蒸馏奖励模型，并在阿里云PAI平台上提供可扩展的训练和推理功能。

Result: 在多个基准测试中，这些模型展现出高推理效率和强大的推理性能，蒸馏奖励模型具有实际应用价值。

Conclusion: 开发的DistilQwen模型系列成功满足了工业应用对高效推理模型的需求，在推理效率和性能之间取得了良好平衡，为行业从业者提供了实用的解决方案。

Abstract: Recently, the demand for small and efficient reasoning models to support
real-world applications has driven the development of knowledge distillation
techniques that balance reasoning performance and inference speed. In this
paper, we further extend the DistilQwen model family, initialized from the Qwen
models, by introducing four model series specifically designed to meet
industrial requirements. The distilled model collection comprises: (1)
slow-thinking models, optimized for reasoning tasks that require high accuracy;
(2) two series of adaptive-thinking models, which dynamically adjust reasoning
strategies based on input tasks to maximize efficiency across diverse
scenarios; and (3) distilled reward models, which enable further reinforcement
learning of reasoning models using distilled knowledge. Comprehensive
evaluations across multiple benchmarks demonstrate both high inference
efficiency and strong reasoning performance for these models, as well as the
practical utility of distilled reward models. We further show that these models
support industry practitioners by providing scalable training and inference
functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)
platform.

</details>


### [198] [PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise](https://arxiv.org/abs/2511.01359)
*Sapir Harary,Eran Hirsch,Aviv Slobodkin,David Wan,Mohit Bansal,Ido Dagan*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的NLI模型MiniTruePrefixes，专门用于检测文本前缀中的事实不一致性，并将其集成到受控解码框架中，显著提高了抽象摘要的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的NLI模型虽然能检测完整句子的事实不一致性，但在自回归生成架构中，决策是在解码过程中针对每个不断演化的文本前缀进行的。因此需要将蕴涵检测任务推广到任意文本前缀，以改进生成内容的忠实度。

Method: 1. 将蕴涵检测任务推广到文本前缀级别；2. 提供相应的评估和训练数据集；3. 训练专门的MiniTruePrefixes模型；4. 将该模型集成到受控解码框架中。

Result: 1. MiniTruePrefixes在前缀级别蕴涵检测上比基线NLI模型高出5-14个F1分数；2. 在抽象摘要任务中，使用MiniTruePrefixes指导的LLaMA-3.2-3B-Instruct模型在忠实度和运行时间上与同系列的8B模型相当，但仅使用一半内存。

Conclusion: 通过专门针对文本前缀训练的NLI模型并将其集成到解码过程中，可以有效提高LLM生成内容的事实一致性，同时保持计算效率。

Abstract: Natural Language Inference (NLI) models have been used in various ways to
improve the factuality of LLM outputs. This is typically done by applying an
NLI model to judge whether the model output is entailed from the supposed
evidence, triggering some corrective actions, such as beam reranking at
inference time or RL rewards during training. While NLI models are trained to
detect factual inconsistencies over complete sentences, decisions in the common
autoregressive generation architecture are made for each evolving text prefix,
during decoding. Addressing this setting, we generalize the entailment
detection task to apply over arbitrary text prefixes, and suggest its utility
for improving generation faithfulness. Providing suitable evaluation and
training datasets for this task, we train MiniTruePrefixes, a novel specialized
model that better detects factual inconsistencies over text prefixes,
outperforming comparable baseline NLI models by 5-14 F1 points in prefix-level
entailment. We further demonstrate that integrating MiniTruePrefixes into a
controlled decoding framework substantially improves factual consistency in
abstractive summarization. When guided by MiniTruePrefixes,
LLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from
the same model family, while using only half the memory.

</details>


### [199] [Safer in Translation? Presupposition Robustness in Indic Languages](https://arxiv.org/abs/2511.01360)
*Aadi Palnitkar,Arjun Suresh,Rishi Rajesh,Puneet Puli*

Main category: cs.CL

TL;DR: 该研究创建了一个名为Cancer-Myth-Indic的印度语言基准，用于评估大型语言模型在医疗健康领域的多语言表现，填补了现有医学基准主要集中在英语的空白。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的人使用大型语言模型获取医疗健康建议，需要评估其回答的有效性和准确性。现有医学基准几乎都是英文的，导致多语言LLM评估存在显著空白。

Method: 通过将Cancer-Myth基准的500个项目子集翻译成5种印度次大陆常用但服务不足的语言（每种语言500项，共2500项），由母语译者按照风格指南进行翻译以保留隐含预设，这些项目包含与癌症相关的错误预设。

Result: 在预设压力下评估了几个流行的大型语言模型的表现。

Conclusion: 该研究通过创建多语言医疗基准，为评估LLM在非英语医疗咨询中的准确性和有效性提供了重要工具，有助于填补多语言LLM评估的空白。

Abstract: Increasingly, more and more people are turning to large language models
(LLMs) for healthcare advice and consultation, making it important to gauge the
efficacy and accuracy of the responses of LLMs to such queries. While there are
pre-existing medical benchmarks literature which seeks to accomplish this very
task, these benchmarks are almost universally in English, which has led to a
notable gap in existing literature pertaining to multilingual LLM evaluation.
Within this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,
an Indic language benchmark built by translating a 500-item subset of
Cancer-Myth, sampled evenly across its original categories, into five
under-served but widely used languages from the subcontinent (500 per language;
2,500 translated items total). Native-speaker translators followed a style
guide for preserving implicit presuppositions in translation; items feature
false presuppositions relating to cancer. We evaluate several popular LLMs
under this presupposition stress.

</details>


### [200] [The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation](https://arxiv.org/abs/2511.01365)
*İbrahim Ethem Deveci,Duygu Ataman*

Main category: cs.CL

TL;DR: 论文质疑当前大语言模型基准测试的有效性，探讨超越基准是否真正体现推理能力，还是仅仅在追踪与声称能力脱节的数字。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和大型推理模型的快速发展，基准测试也在快速增加，但由于模型能力的提升和训练数据的包含，结果趋于饱和，需要不断寻找新的更具挑战性的替代基准。

Method: 研究聚焦于OpenAI、Anthropic和Google三个模型家族，分析它们在不同基准测试中推理能力随时间的演变，并分析不同推理任务的性能趋势。

Result: 论文提供了基准测试和推理任务的全面概述，揭示了当前基准测试的现状和剩余挑战。

Conclusion: 该研究旨在为未来推理评估和模型开发研究提供首个参考基础，帮助更准确地评估模型的真实推理能力。

Abstract: The rapid rise of Large Language Models (LLMs) and Large Reasoning Models
(LRMs) has been accompanied by an equally rapid increase of benchmarks used to
assess them. However, due to both improved model competence resulting from
scaling and novel training advances as well as likely many of these datasets
being included in pre or post training data, results become saturated, driving
a continuous need for new and more challenging replacements. In this paper, we
discuss whether surpassing a benchmark truly demonstrates reasoning ability or
are we simply tracking numbers divorced from the capabilities we claim to
measure? We present an investigation focused on three model families, OpenAI,
Anthropic, and Google, and how their reasoning capabilities across different
benchmarks evolve over the years. We also analyze performance trends over the
years across different reasoning tasks and discuss the current situation of
benchmarking and remaining challenges. By offering a comprehensive overview of
benchmarks and reasoning tasks, our work aims to serve as a first reference to
ground future research in reasoning evaluation and model development.

</details>


### [201] [RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets](https://arxiv.org/abs/2511.01386)
*Muhammed Yusuf Kartal,Suha Kagan Kose,Korhan Sevinç,Burak Aktas*

Main category: cs.CL

TL;DR: RAGSmith是一个模块化框架，通过遗传搜索在46,080个可行管道配置中优化RAG系统的端到端架构，在六个领域平均提升3.8%性能。


<details>
  <summary>Details</summary>
Motivation: RAG质量受多个相互影响的模块选择影响，孤立优化模块存在脆弱性，需要端到端的架构搜索方法。

Method: 使用遗传搜索在9个技术家族和46,080个可行管道配置中进行端到端架构搜索，优化结合检索指标和生成指标的标量目标函数。

Result: 在六个维基百科领域测试中，RAGSmith找到的配置平均比朴素RAG基线提升3.8%（范围1.2%-6.9%），检索和生成分别提升达12.5%和7.5%。

Conclusion: 研究证明了进化搜索在全管道优化中的实用性，发现了稳健的骨干架构（向量检索加后生成反思/修订），并为构建有效RAG系统提供了实用的领域感知指导。

Abstract: Retrieval-Augmented Generation (RAG) quality depends on many interacting
choices across retrieval, ranking, augmentation, prompting, and generation, so
optimizing modules in isolation is brittle. We introduce RAGSmith, a modular
framework that treats RAG design as an end-to-end architecture search over nine
technique families and 46{,}080 feasible pipeline configurations. A genetic
search optimizes a scalar objective that jointly aggregates retrieval metrics
(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic
similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,
Finance, Medicine, Defense Industry, Computer Science), each with 100 questions
spanning factual, interpretation, and long-answer types. RAGSmith finds
configurations that consistently outperform naive RAG baseline by +3.8\% on
average (range +1.2\% to +6.9\% across domains), with gains up to +12.5\% in
retrieval and +7.5\% in generation. The search typically explores $\approx
0.2\%$ of the space ($\sim 100$ candidates) and discovers a robust backbone --
vector retrieval plus post-generation reflection/revision -- augmented by
domain-dependent choices in expansion, reranking, augmentation, and prompt
reordering; passage compression is never selected. Improvement magnitude
correlates with question type, with larger gains on factual/long-answer mixes
than interpretation-heavy sets. These results provide practical, domain-aware
guidance for assembling effective RAG systems and demonstrate the utility of
evolutionary search for full-pipeline optimization.

</details>


### [202] [LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge](https://arxiv.org/abs/2511.01409)
*Heng Zhou,Ao Yu,Yuchen Fan,Jianing Shi,Li Kang,Hejia Geng,Yongting Zhang,Yutao Fan,Yuhao Wu,Tiancheng He,Yiran Qin,Lei Bai,Zhenfei Yin*

Main category: cs.CL

TL;DR: LiveSearchBench是一个自动构建检索依赖型基准测试的管道，通过对比Wikidata快照变化生成基于最新知识的问题，评估LLMs在动态知识环境下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试奖励记忆而非检索，无法捕捉世界知识的动态特性，需要开发能够评估LLMs在最新知识下表现的动态基准。

Method: 通过计算Wikidata连续快照的差异，筛选高质量三元组，生成三个难度级别的自然语言问题，并使用SPARQL验证确保答案唯一性和可验证性。

Result: 实验显示模型在面对预训练后新事实时性能显著下降，多跳查询差距最明显。检索增强方法和更大的指令调优模型只能部分缓解但无法消除这种时效性差距。

Conclusion: LiveSearchBench将评估从静态记忆转向需要最新检索和推理的任务，为系统化长期评估LLMs在演化知识下的表现提供了基础。

Abstract: Evaluating large language models (LLMs) on question answering often relies on
static benchmarks that reward memorization and understate the role of
retrieval, failing to capture the dynamic nature of world knowledge. We present
LiveSearchBench, an automated pipeline for constructing retrieval-dependent
benchmarks from recent knowledge updates. Our method computes deltas between
successive Wikidata snapshots, filters candidate triples for quality, and
synthesizes natural-language questions at three levels of reasoning difficulty,
each guaranteed to admit a unique, verifiable answer through SPARQL validation.
The pipeline is fully automated, scalable across time, and minimizes human
intervention, enabling continual regeneration of temporally grounded
benchmarks. Experiments show a pronounced performance drop when models confront
facts that post-date pretraining, with the gap most salient on multi-hop
queries. Retrieval augmented methods and larger, instruction-tuned models
provide partial gains but fail to close this recency gap. By design,
LiveSearchBench shifts evaluation from static memorization toward tasks that
require up-to-date retrieval and reasoning, offering a foundation for
systematic, long-term assessment of LLMs under evolving knowledge.

</details>


### [203] ["Don't Teach Minerva": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG](https://arxiv.org/abs/2511.01454)
*Sergio Torres Aguilar*

Main category: cs.CL

TL;DR: 本文提出了一种可复现的基于草稿优化的翻译流程，使用开源大语言模型在形态丰富的低资源语言（拉丁语）翻译任务中达到与顶级专有系统相当的性能水平。


<details>
  <summary>Details</summary>
Motivation: 翻译形态丰富、资源稀缺的语言如拉丁语面临重大挑战，需要开发能够与专有系统竞争的开源解决方案。

Method: 使用微调的NLLB-1.3B模型生成高质量结构忠实草稿，然后通过零样本LLM（Llama-3.3或Qwen3）进行优化，可通过检索增强生成（RAG）进一步改进。

Result: 该方法在两个不同基准测试中表现稳健：标准域内测试集和新的具有挑战性的域外12世纪拉丁语信件集。开源RAG系统性能与GPT-5基线统计相当，无需任务特定的LLM微调。

Conclusion: 提出的开源RAG系统在拉丁语翻译任务中达到与顶级专有系统相当的性能，为低资源语言翻译提供了可复现的解决方案。

Abstract: Translating a morphology-rich, low-resource language like Latin poses
significant challenges. This paper introduces a reproducible draft-based
refinement pipeline that elevates open-source Large Language Models (LLMs) to a
performance level statistically comparable to top-tier proprietary systems. Our
method first uses a fine-tuned NLLB-1.3B model to generate a high-quality,
structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes
this draft, a process that can be further enhanced by augmenting the context
with retrieved out-context examples (RAG). We demonstrate the robustness of
this approach on two distinct benchmarks: a standard in-domain test set
(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of
12th-century Latin letters (2025). Our central finding is that this open-source
RAG system achieves performance statistically comparable to the GPT-5 baseline,
without any task-specific LLM fine-tuning. We release the pipeline, the
Chartres OOD set, and evaluation scripts and models to facilitate replicability
and further research.

</details>


### [204] [BARD: budget-aware reasoning distillation](https://arxiv.org/abs/2511.01470)
*Lujie Niu,Lei Shen,Yi Jiang,Caixia Yuan,Xiaojie Wang,Wenbo Su,Bo zheng*

Main category: cs.CL

TL;DR: BARD框架通过两阶段训练（SFT和RL）将推理能力蒸馏到小模型，同时允许用户通过预算信号精确控制推理长度，实现性能与计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统CoT蒸馏中推理过程冗余且计算预算不可控的问题，提高资源使用效率。

Method: 提出BARD框架，包含两阶段训练：第一阶段在教师生成的长CoT数据上进行监督微调，第二阶段使用同时考虑推理性能和预算保真度的奖励信号进行强化学习。

Result: 实验表明，8B学生模型在AIME24、AIME25、GPQA等挑战性推理基准上表现优异，并能跨广泛预算范围精确自适应控制推理长度。

Conclusion: BARD成功实现了推理能力蒸馏与推理长度细粒度控制的统一，为高效推理模型提供了可行方案。

Abstract: While long Chain-of-Thought (CoT) distillation effectively transfers
reasoning capability to smaller language models, the reasoning process often
remains redundant and computational budget uncontrollable, leading to
inefficient resource usage. To address this limitation, we propose
\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that
simultaneously distills reasoning capability and enables fine-grained control
over the reasoning length. BARD uses the thinking budget as a user-specified
control signal, allowing the model to dynamically balance reasoning performance
and computational efficiency. To achieve this concept, BARD introduces a
two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on
teacher-generated long CoT data compressed to various budget levels,
bootstrapping the model's understanding of budget constraints. The second phase
leverages Reinforcement Learning (RL) from a reward signal in consideration of
reasoning performance and budget fidelity simultaneously. Incorporating the
two-phase regimen is crucial to avoiding policy degradation and ensuring that
both objectives are optimized jointly. Extensive experiments demonstrate that
our method empowers an 8B student model to achieve strong performance on
challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while
providing precise and adaptive control over its reasoning length across a wide
range of budgets.

</details>


### [205] [Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation](https://arxiv.org/abs/2511.01482)
*Neha Sharma,Navneet Agarwal,Kairit Sirts*

Main category: cs.CL

TL;DR: 该论文探索使用大型语言模型作为认知扭曲检测的标注工具，提出多轮独立LLM标注可产生稳定模式，并引入数据集无关的评估框架进行公平比较。


<details>
  <summary>Details</summary>
Motivation: 解决认知扭曲检测任务中人类标注者主观性强、一致性低的问题，探索LLMs作为可靠标注工具的潜力。

Method: 使用多个独立LLM运行进行标注，引入基于Cohen's kappa的数据集无关评估框架，比较不同数据集训练模型的性能。

Result: GPT-4能产生一致性高的标注（Fleiss's Kappa = 0.78），基于LLM标注训练的模型在测试集上表现优于基于人类标注训练的模型。

Conclusion: LLMs可作为主观NLP任务中生成训练数据的可扩展且内部一致的替代方案，支持强大的下游性能。

Abstract: Text-based automated Cognitive Distortion detection is a challenging task due
to its subjective nature, with low agreement scores observed even among expert
human annotators, leading to unreliable annotations. We explore the use of
Large Language Models (LLMs) as consistent and reliable annotators, and propose
that multiple independent LLM runs can reveal stable labeling patterns despite
the inherent subjectivity of the task. Furthermore, to fairly compare models
trained on datasets with different characteristics, we introduce a
dataset-agnostic evaluation framework using Cohen's kappa as an effect size
measure. This methodology allows for fair cross-dataset and cross-study
comparisons where traditional metrics like F1 score fall short. Our results
show that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),
resulting in improved test set performance for models trained on these
annotations compared to those trained on human-labeled data. Our findings
suggest that LLMs can offer a scalable and internally consistent alternative
for generating training data that supports strong downstream performance in
subjective NLP tasks.

</details>


### [206] [Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning](https://arxiv.org/abs/2511.01490)
*Max Schaffelder,Albert Gatt*

Main category: cs.CL

TL;DR: 研究合成数据来源多样性对微调大语言模型的影响，重点关注分布崩溃、对抗鲁棒性和自偏好偏差三个维度。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在语言模型开发中的广泛应用，理解其对模型行为的影响变得至关重要。

Method: 通过微调大语言模型，比较来自不同来源的合成数据在三个关键维度上的表现：分布崩溃、对抗鲁棒性和自偏好偏差。

Result: 使用多来源合成数据微调可以缓解分布崩溃，保持输出分布的广度和文本多样性；合成数据微调在移除安全防护时能保持更高的输出质量；微调能减少自偏好偏差，人类数据效果最好，多来源合成数据次之。

Conclusion: 合成数据来源的多样性对微调模型的性能有重要影响，多来源合成数据在缓解分布崩溃和减少自偏好偏差方面表现良好，但需注意其可能带来的安全风险。

Abstract: As synthetic data becomes widely used in language model development,
understanding its impact on model behavior is crucial. This paper investigates
the impact of the diversity of sources of synthetic data on fine-tuned large
language models. We focus on three key dimensions: distribution collapse,
adversarial robustness, and self-preference bias. Our findings reveal that
fine-tuning models on synthetic data from diverse sources can mitigate
distribution collapse, preserving the breadth of the output distribution and
the diversity of the output text. Furthermore, while both human and synthetic
fine-tuning data can remove safeguards, the latter preserves higher output
quality, thus making outputs potentially more usable and dangerous. Finally,
fine-tuning reduces self-preference bias, with human data being the most
effective, followed by multi-source synthetic data.

</details>


### [207] [BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification](https://arxiv.org/abs/2511.01512)
*Ayesha Afroza Mohsin,Mashrur Ahsan,Nafisa Maliyat,Shanta Maria,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文提出了一种结合Pareto优化大语言模型和思维链提示的孟加拉语文本去毒新方法，并构建了包含68,041个有毒句子的BanglaNirTox数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在线环境中存在大量有毒语言，但由于资源有限，该语言的文本去毒研究相对不足。

Method: 使用Pareto优化的LLM和思维链提示生成去毒句子，构建包含毒性标签、推理和去毒改写的平行语料库，并用于微调语言模型。

Result: Pareto优化的LLM结合思维链提示显著提高了孟加拉语文本去毒的质量和一致性。

Conclusion: 该方法为低资源语言的文本去毒提供了有效解决方案，BanglaNirTox数据集将促进孟加拉语NLP研究。

Abstract: Toxic language in Bengali remains prevalent, especially in online
environments, with few effective precautions against it. Although text
detoxification has seen progress in high-resource languages, Bengali remains
underexplored due to limited resources. In this paper, we propose a novel
pipeline for Bengali text detoxification that combines Pareto class-optimized
large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate
detoxified sentences. To support this effort, we construct BanglaNirTox, an
artificially generated parallel corpus of 68,041 toxic Bengali sentences with
class-wise toxicity labels, reasonings, and detoxified paraphrases, using
Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox
dataset is used to fine-tune language models to produce better detoxified
versions of Bengali sentences. Our findings show that Pareto-optimized LLMs
with CoT prompting significantly enhance the quality and consistency of Bengali
text detoxification.

</details>


### [208] [Difficulty-Controllable Cloze Question Distractor Generation](https://arxiv.org/abs/2511.01526)
*Seokhoon Kang,Yejin Jeon,Seonjeong Hwang,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的框架，通过数据增强和多任务学习策略生成可控难度的干扰项，解决了现有方法在适应性和难度控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成高质量干扰项时缺乏适应性和难度控制能力，且缺乏难度标注数据集阻碍了进展。

Method: 采用双向干扰项生成过程创建高质量难度标注数据集，通过过滤和集成QA系统分类难度，然后利用多任务学习训练难度可控生成模型。

Result: 实验结果表明，该方法能生成跨难度级别的高质量干扰项，在干扰项难度与人类感知对齐方面显著优于GPT-4o。

Conclusion: 提出的框架通过数据增强和多任务学习有效解决了干扰项生成中的难度控制问题，为语言能力评估提供了更可靠的工具。

Abstract: Multiple-choice cloze questions are commonly used to assess linguistic
proficiency and comprehension. However, generating high-quality distractors
remains challenging, as existing methods often lack adaptability and control
over difficulty levels, and the absence of difficulty-annotated datasets
further hinders progress. To address these issues, we propose a novel framework
for generating distractors with controllable difficulty by leveraging both data
augmentation and a multitask learning strategy. First, to create a
high-quality, difficulty-annotated dataset, we introduce a two-way distractor
generation process in order to produce diverse and plausible distractors. These
candidates are subsequently refined through filtering and then categorized by
difficulty using an ensemble QA system. Second, this newly created dataset is
leveraged to train a difficulty-controllable generation model via multitask
learning. The framework includes carefully designed auxiliary tasks that
enhance the model's semantic understanding of distractors and its ability to
estimate their difficulty. Experimental results demonstrate that our method
generates high-quality distractors across difficulty levels and substantially
outperforms GPT-4o in aligning distractor difficulty with human perception.

</details>


### [209] [Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o](https://arxiv.org/abs/2511.01558)
*Luciana Ciringione,Emma Franchino,Simone Reigl,Isaia D'Onofrio,Anna Serbati,Oleksandra Poquet,Florence Gabriel,Massimo Stella*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Math anxiety poses significant challenges for university psychology students,
affecting their career choices and overall well-being. This study employs a
framework based on behavioural forma mentis networks (i.e. cognitive models
that map how individuals structure their associative knowledge and emotional
perceptions of concepts) to explore individual and group differences in the
perception and association of concepts related to math and anxiety. We
conducted 4 experiments involving psychology undergraduates from 2 samples (n1
= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;
GPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network
features to predict psychometric scores for math anxiety and its facets
(observational, social and evaluational) from the Math Anxiety Scale.
Experiment 4 focuses on group-level perceptions extracted from human students,
GPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive
valence ratings and higher network degree for "anxiety", together with negative
ratings for "math", can predict higher total and evaluative math anxiety. In
contrast, these models do not work on GPT-based data because of differences in
simulated networks and psychometric scores compared to humans. These results
were also reconciled with differences found in the ways that high/low subgroups
of simulated and real students framed semantically and emotionally STEM
concepts. High math-anxiety students collectively framed "anxiety" in an
emotionally polarising way, absent in the negative perception of low
math-anxiety students. "Science" was rated positively, but contrasted against
the negative perception of "math". These findings underscore the importance of
understanding concept perception and associations in managing students' math
anxiety.

</details>


### [210] [ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation](https://arxiv.org/abs/2511.01568)
*Seungmin Shin,Dooyoung Kim,Youngjoong Ko*

Main category: cs.CL

TL;DR: 提出ECO解码方法，通过基于熵的动态调整控制强度，解决可控对话生成中固定控制强度难以平衡可控性和流畅性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统加权解码方法使用固定常数管理属性概率偏差，难以找到同时满足可控性和流畅性的理想控制强度。

Method: 提出ECO解码，根据语言模型和属性分类器概率分布的熵，在每一步生成时动态调整控制强度。

Result: 在DailyDialog和MultiWOZ数据集上的实验表明，ECO解码在保持流畅性和语法性的同时，持续提高可控性，优于现有解码方法。

Conclusion: ECO解码缓解了多属性生成中的概率插值问题，在单属性和多属性场景下均表现出强大性能。

Abstract: Controllable Dialogue Generation (CDG) enables chatbots to generate responses
with desired attributes, and weighted decoding methods have achieved
significant success in the CDG task. However, using a fixed constant value to
manage the bias of attribute probabilities makes it challenging to find an
ideal control strength that satisfies both controllability and fluency. To
address this issue, we propose ECO decoding (Entropy-based COntrol), which
dynamically adjusts the control strength at each generation step according to
the model's entropy in both the language model and attribute classifier
probability distributions. Experiments on the DailyDialog and MultiWOZ datasets
demonstrate that ECO decoding consistently improves controllability while
maintaining fluency and grammaticality, outperforming prior decoding methods
across various models and settings. Furthermore, ECO decoding alleviates
probability interpolation issues in multi-attribute generation and consequently
demonstrates strong performance in both single and multi-attribute scenarios.

</details>


### [211] [Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers](https://arxiv.org/abs/2511.01615)
*Francisco Portillo López*

Main category: cs.CL

TL;DR: 该研究提出跨学科分析西班牙语母语者的语言错误，通过理论语言学、神经语言学和自然语言处理三个视角，评估大型语言模型对这些错误的解释和纠正能力。


<details>
  <summary>Details</summary>
Motivation: 语言错误不仅是语法偏差，更是理解语言认知架构和揭示人工智能系统局限性的窗口，旨在开发更符合认知的NLP系统。

Method: 构建包含500+西班牙语母语者真实错误的语料库，使用GPT、Gemini等AI模型测试其对错误的解释准确性和对人类语言行为模式的泛化能力。

Result: 通过实证分析评估AI模型对语言错误的处理能力，为开发更认知化的NLP系统提供依据。

Conclusion: 该项目不仅促进对西班牙语作为母语的理解，还推动开发能够处理人类语言不完美、多变和模糊特性的NLP系统。

Abstract: Linguistic errors are not merely deviations from normative grammar; they
offer a unique window into the cognitive architecture of language and expose
the current limitations of artificial systems that seek to replicate them. This
project proposes an interdisciplinary study of linguistic errors produced by
native Spanish speakers, with the aim of analyzing how current large language
models (LLM) interpret, reproduce, or correct them. The research integrates
three core perspectives: theoretical linguistics, to classify and understand
the nature of the errors; neurolinguistics, to contextualize them within
real-time language processing in the brain; and natural language processing
(NLP), to evaluate their interpretation against linguistic errors. A
purpose-built corpus of authentic errors of native Spanish (+500) will serve as
the foundation for empirical analysis. These errors will be tested against AI
models such as GPT or Gemini to assess their interpretative accuracy and their
ability to generalize patterns of human linguistic behavior. The project
contributes not only to the understanding of Spanish as a native language but
also to the development of NLP systems that are more cognitively informed and
capable of engaging with the imperfect, variable, and often ambiguous nature of
real human language.

</details>


### [212] [A Graph-based RAG for Energy Efficiency Question Answering](https://arxiv.org/abs/2511.01643)
*Riccardo Campi,Nicolò Oreste Pinciroli Vago,Mathyas Giudici,Pablo Barrachina Rodriguez-Guisado,Marco Brambilla,Piero Fraternali*

Main category: cs.CL

TL;DR: 该研究探索了在图基检索增强生成架构中使用大语言模型进行能效问答，通过从能源文档自动提取知识图谱并推理导航，实现了多语言准确回答，验证显示75.2%的正确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用大语言模型结合图基RAG架构解决能源效率领域的问答需求，通过知识图谱增强回答准确性并支持多语言能力。

Method: 系统从能源指导文件和法规文档自动提取知识图谱，然后通过图导航和推理为用户提供多语言准确答案，使用RAGAs框架和领域专家进行人工验证。

Result: 验证结果显示系统在约75.2%的情况下能正确回答问题，通用能效问题准确率可达81.0%，多语言能力表现良好（翻译仅导致4.4%准确率损失）。

Conclusion: 该架构展现了在能效问答领域的潜力，证实了图基RAG结合大语言模型的有效性，同时识别了系统的优势和改进空间。

Abstract: In this work, we investigate the use of Large Language Models (LLMs) within a
graph-based Retrieval Augmented Generation (RAG) architecture for Energy
Efficiency (EE) Question Answering. First, the system automatically extracts a
Knowledge Graph (KG) from guidance and regulatory documents in the energy
field. Then, the generated graph is navigated and reasoned upon to provide
users with accurate answers in multiple languages. We implement a human-based
validation using the RAGAs framework properties, a validation dataset
comprising 101 question-answer pairs, and domain experts. Results confirm the
potential of this architecture and identify its strengths and weaknesses.
Validation results show how the system correctly answers in about three out of
four of the cases (75.2 +- 2.7%), with higher results on questions related to
more general EE answers (up to 81.0 +- 4.1%), and featuring promising
multilingual abilities (4.4% accuracy loss due to translation).

</details>


### [213] [Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation](https://arxiv.org/abs/2511.01649)
*Hung-Shin Lee,Chen-Chi Chang,Ching-Yuan Chen,Yun-Hsiang Hsu*

Main category: cs.CL

TL;DR: 提出一个认知基准框架，结合布鲁姆分类法和检索增强生成来评估大语言模型处理文化特定知识的能力，使用台湾客家数字文化档案作为测试平台。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在处理和应用文化特定知识方面的表现，特别是针对文化敏感内容的认知能力。

Method: 整合布鲁姆分类法与检索增强生成(RAG)，在六个认知领域（记忆、理解、应用、分析、评估、创造）评估模型性能，使用台湾客家数字文化档案作为测试数据。

Result: 测量大语言模型生成响应的语义准确性和文化相关性。

Conclusion: 该框架为评估大语言模型的文化认知能力提供了系统化的方法。

Abstract: This study proposes a cognitive benchmarking framework to evaluate how large
language models (LLMs) process and apply culturally specific knowledge. The
framework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)
to assess model performance across six hierarchical cognitive domains:
Remembering, Understanding, Applying, Analyzing, Evaluating, and Creating.
Using a curated Taiwanese Hakka digital cultural archive as the primary
testbed, the evaluation measures LLM-generated responses' semantic accuracy and
cultural relevance.

</details>


### [214] [EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering](https://arxiv.org/abs/2511.01650)
*Ayesha Gull,Muhammad Usman Safder,Rania Elbadry,Preslav Nakov,Zhuohan Xie*

Main category: cs.CL

TL;DR: EngChain是一个用于验证多步骤工程问题解决能力的基准测试，包含90个跨3个工程分支的问题，通过符号模板生成以确保多样性。采用两阶段评估：定量验证推理步骤的有效性，然后使用LLM-As-A-Judge自动分类推理错误。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估语言理解、事实回忆、数学或代码生成能力，但缺乏对工程领域所需的整合推理能力的评估。工程问题需要科学原理、定量建模和实践约束的融合。

Method: 开发EngChain基准测试，包含90个问题，涵盖3个工程分支、9个领域和20个具体领域。问题通过符号模板生成，具有高度随机化以确保多样性。采用两阶段评估方法：定量验证推理步骤，然后使用LLM-As-A-Judge进行定性错误分类。

Result: 创建了一个专门评估工程推理能力的基准测试，能够验证多步骤工程问题解决过程中的推理有效性，并自动识别和分类推理错误。

Conclusion: EngChain填补了当前基准测试在评估工程整合推理能力方面的空白，为评估LLM在复杂工程问题解决中的表现提供了有效工具。

Abstract: Large Language Models (LLMs) are increasingly being applied to specialized,
high-stakes domains like engineering, which demands rigorous evaluation of
their complex reasoning capabilities. While current benchmarks assess language
understanding, factual recall, mathematics or code generation, none capture the
integrative reasoning central to engineering where scientific principles,
quantitative modeling and practical constraints must converge. To address this
gap, we introduce EngChain, a benchmark for verifiable multi-step engineering
problem-solving. EngChain contains 90 problems spanning three engineering
branches, organized into 9 domains and 20 distinct areas. The problems are
generated from symbolic templates with a high degree of randomization to ensure
diversity and eliminate the risk of contamination. With this benchmark, we move
beyond final answer accuracy with a two-stage evaluation: we first
quantitatively verify the numerical and semantic validity of each reasoning
step and then introduce LLM-As-A-Judge, an automated system to qualitatively
categorize the identified reasoning errors.

</details>


### [215] [SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia](https://arxiv.org/abs/2511.01670)
*Chaoqun Liu,Mahani Aljunied,Guizhen Chen,Hou Pong Chan,Weiwen Xu,Yu Rong,Wenxuan Zhang*

Main category: cs.CL

TL;DR: SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）以及英语和中文的大规模音频语言模型，支持多模态输入和多任务处理，在音频理解和语音交互方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为东南亚地区开发专门的大规模音频语言模型，填补该地区多语言音频AI技术的空白，促进区域研究和产业发展。

Method: 基于大规模音频语料库训练，支持5种语言（印尼语、泰语、越南语、英语、中文），采用多模态架构，可接受纯音频、纯文本或音频+文本的灵活输入。

Result: 在多种音频中心任务中表现出色，包括音频字幕、语音识别、语音翻译、情感识别、问答和摘要等，在东南亚语言上与其他LALMs相比具有竞争力。

Conclusion: SeaLLMs-Audio是推进东南亚音频大语言模型发展的重要一步，同时推出的SeaBench-Audio基准测试将有助于自动化评估，预期将惠及区域研究社区和产业界。

Abstract: We introduce SeaLLMs-Audio, the first large audio-language model (LALM)
tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai
(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a
large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across
diverse audio-centric tasks, spanning fine-grained audio understanding and
voice-based interaction. Its key features include: 1) Multilingual: the model
primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,
and Chinese; 2) Multimodal: the model accepts flexible input modalities,
including audio only, text only, as well as audio with text; 3) Multi-task: the
model supports a wide range of tasks, including audio analysis tasks such as
Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,
Speech Emotion Recognition, Speech Question Answering, and Speech
Summarization. It also enables voice-based dialogue, including answering
factual, mathematical, and general knowledge queries. As a significant step
towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to
benefit both the regional research community and industry. To automate LALM
evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark
spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves
competitive performance compared with other LALMs on SEA languages.

</details>


### [216] [Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI](https://arxiv.org/abs/2511.01689)
*Sharan Maiya,Henning Bartsch,Nathan Lambert,Evan Hubinger*

Main category: cs.CL

TL;DR: 本文提出了首个开源的AI助手角色训练方法，通过宪法AI和合成自省数据来塑造助手角色，相比系统提示约束和激活引导等方法更有效、可控且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现代聊天机器人语言模型生成的AI助手角色特征会影响交互质量、感知智能以及与开发者和用户意图的对齐，但角色训练这一关键组件在学术界尚未得到充分研究。

Method: 使用宪法AI和基于合成自省数据的新数据管道，对三个流行的开源模型进行微调，应用11个示例角色（如幽默、深度关怀甚至恶意），并通过揭示偏好分析方法跟踪效果。

Result: 该方法相比系统提示约束和激活引导对对抗性提示更鲁棒，同时生成更连贯和真实的文本，且对通用能力基准测试几乎没有影响。

Conclusion: 本文开发并开源了完整的后训练方法，为AI助手角色训练提供了首个开源实现，实现了更有效和可控的角色塑造。

Abstract: The character of the "AI assistant" persona generated by modern chatbot large
language models influences both surface-level behavior and apparent values,
beliefs, and ethics. These all affect interaction quality, perceived
intelligence, and alignment with both developer and user intentions. The
shaping of this persona, known as character training, is a critical component
of industry post-training, yet remains effectively unstudied in the academic
literature. We introduce the first open implementation of character training,
leveraging Constitutional AI and a new data pipeline using synthetic
introspective data to shape the assistant persona in a more effective and
controlled manner than alternatives such as constraining system prompts or
activation steering. Specifically, we fine-tune three popular open-weights
models using 11 example personas, such as humorous, deeply caring, or even
malevolent. To track the effects of our approach, we introduce a method which
analyzes revealed preferences, uncovering clear and holistic changes in
character. We find these changes are more robust to adversarial prompting than
the above two alternatives, while also leading to more coherent and realistic
generations. Finally, we demonstrate this fine-tuning has little to no effect
on general capabilities as measured by common benchmarks. We describe and
open-source our full post-training method, the implementation of which can be
found at https://github.com/maiush/OpenCharacterTraining.

</details>


### [217] [Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement](https://arxiv.org/abs/2511.01706)
*Sekh Mainul Islam,Pepa Atanasova,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了一个新颖的rank-2投影子空间来更准确地解构参数知识(PK)和上下文知识(CK)的贡献，并首次对长自然语言解释(NLE)序列中的多步知识交互进行分析。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中参数知识和上下文知识的交互对于评估自然语言解释的可靠性至关重要，但目前这方面研究不足，现有工作主要关注单步生成且仅将PK和CK交互建模为rank-1子空间中的二元选择，忽略了更丰富的交互形式。

Method: 提出rank-2投影子空间来更准确地解构PK和CK贡献，并在四个QA数据集和三个开源指令调优LLM上进行实验，首次进行多步知识交互分析。

Result: 实验表明，多样化的知识交互在rank-1子空间中表现不佳，但在rank-2公式中能有效捕捉；多步分析揭示幻觉NLE与PK方向强对齐，上下文忠实NLE平衡PK和CK，而思维链提示通过减少PK依赖将生成的NLE向CK方向偏移。

Conclusion: 这项工作为通过更丰富的rank-2子空间解构来系统研究LLM中的多步知识交互提供了首个框架。

Abstract: Natural Language Explanations (NLEs) describe how Large Language Models
(LLMs) make decisions, drawing on both external Context Knowledge (CK) and
Parametric Knowledge (PK) stored in model weights. Understanding their
interaction is key to assessing the grounding of NLEs, yet it remains
underexplored. Prior work has largely examined only single-step generation,
typically the final answer, and has modelled PK and CK interaction only as a
binary choice in a rank-1 subspace. This overlooks richer forms of interaction,
such as complementary or supportive knowledge. We propose a novel rank-2
projection subspace that disentangles PK and CK contributions more accurately
and use it for the first multi-step analysis of knowledge interactions across
longer NLE sequences. Experiments on four QA datasets and three open-weight
instruction-tuned LLMs show that diverse knowledge interactions are poorly
represented in a rank-1 subspace but are effectively captured in our rank-2
formulation. Our multi-step analysis reveals that hallucinated NLEs align
strongly with the PK direction, context-faithful ones balance PK and CK, and
Chain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing
PK reliance. This work provides the first framework for systematic studies of
multi-step knowledge interactions in LLMs through a richer rank-2 subspace
disentanglement. Code and data:
https://github.com/copenlu/pk-ck-knowledge-disentanglement.

</details>


### [218] [Accumulating Context Changes the Beliefs of Language Models](https://arxiv.org/abs/2511.01805)
*Jiayi Geng,Howard Chen,Ryan Liu,Manoel Horta Ribeiro,Robb Willer,Graham Neubig,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 语言模型在对话和阅读过程中会积累上下文，导致其信念配置文件发生显著变化，影响模型的行为和响应一致性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在自主应用中的使用增加，上下文积累可能导致模型信念无声变化，带来用户体验不一致和偏离原始对齐的风险。

Method: 通过多轮讨论道德困境和安全查询测试GPT-5的信念变化，通过阅读对立立场文本测试Grok 4的政治立场变化，并设计工具使用任务来观察行为变化。

Result: GPT-5在10轮讨论后信念变化达54.7%，Grok 4在阅读对立文本后政治立场变化27.2%，行为变化与信念变化一致。

Conclusion: 语言模型在长时间对话或阅读过程中存在信念漂移的隐藏风险，导致其观点和行为不可靠。

Abstract: Language model (LM) assistants are increasingly used in applications such as
brainstorming and research. Improvements in memory and context size have
allowed these models to become more autonomous, which has also resulted in more
text accumulation in their context windows without explicit user intervention.
This comes with a latent risk: the belief profiles of models -- their
understanding of the world as manifested in their responses or actions -- may
silently change as context accumulates. This can lead to subtly inconsistent
user experiences, or shifts in behavior that deviate from the original
alignment of the models. In this paper, we explore how accumulating context by
engaging in interactions and processing text -- talking and reading -- can
change the beliefs of language models, as manifested in their responses and
behaviors.Our results reveal that models' belief profiles are highly malleable:
GPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of
discussion about moral dilemmas and queries about safety, while Grok 4 shows a
27.2% shift on political issues after reading texts from the opposing position.
We also examine models' behavioral changes by designing tasks that require tool
use, where each tool selection corresponds to an implicit belief. We find that
these changes align with stated belief shifts, suggesting that belief shifts
will be reflected in actual behavior in agentic systems. Our analysis exposes
the hidden risk of belief shift as models undergo extended sessions of talking
or reading, rendering their opinions and actions unreliable.

</details>


### [219] [Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining](https://arxiv.org/abs/2511.01807)
*Adewale Akinfaderin,Shreyas Subramanian,Akarsha Sehwag*

Main category: cs.CL

TL;DR: 提出了一种无需模型重新训练的提示工程方法，通过结构引导实现精确的长度控制，在文档摘要任务中显著提高了长度遵循度，特别是对中短长度约束。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的长度控制是一个重要但研究不足的挑战，现有方法需要昂贵的模型重新训练或复杂的推理时工具，需要一种更实用的解决方案。

Method: 采用结构引导的提示工程方法，在提示中实现精心的规划和字数统计机制，鼓励模型仔细跟踪并遵守指定的长度约束。

Result: 在六个最先进的大语言模型上的评估显示，该方法显著提高了长度遵循度，某些模型提升了37.6%，同时保持或提升了输出质量。

Conclusion: 该方法为需要精确长度控制的应用提供了立即可部署的解决方案，特别适用于模型重新训练不切实际或成本过高的生产环境。

Abstract: Length control in Large Language Models (LLMs) is a crucial but
under-addressed challenge, with applications ranging from voice interfaces
requiring concise responses to research summaries needing comprehensive
outputs. Current approaches to length control, including Regularized DPO,
Length-Instruction Fine Tuning, and tool-augmented methods, typically require
expensive model retraining or complex inference-time tooling. This paper
presents a prompt engineering methodology that enables precise length control
without model retraining. Our structure-guided approach implements deliberate
planning and word counting mechanisms within the prompt, encouraging the model
to carefully track and adhere to specified length constraints. Comprehensive
evaluations across six state-of-the-art LLMs demonstrate that our method
significantly improves length fidelity for several models compared to standard
prompting when applied to document summarization tasks, particularly for
shorter-to-medium length constraints. The proposed technique shows varying
benefits across different model architectures, with some models demonstrating
up to 37.6% improvement in length adherence. Quality evaluations further reveal
that our approach maintains or enhances overall output quality compared to
standard prompting techniques. Our approach provides an immediately deployable
solution for applications requiring precise length control, particularly
valuable for production environments where model retraining is impractical or
cost-prohibitive.

</details>


### [220] [Towards Robust Mathematical Reasoning](https://arxiv.org/abs/2511.01846)
*Thang Luong,Dawsen Hwang,Hoang H. Nguyen,Golnaz Ghiasi,Yuri Chervonyi,Insuk Seo,Junsu Kim,Garrett Bingham,Jonathan Lee,Swaroop Mishra,Alex Zhai,Clara Huiyi Hu,Henryk Michalewski,Jimin Kim,Jeonghyun Ahn,Junhwi Bae,Xingyou Song,Trieu H. Trinh,Quoc V. Le,Junehyuk Jung*

Main category: cs.CL

TL;DR: IMO-Bench是一个针对国际数学奥林匹克竞赛(IMO)级别的高级推理基准套件，包含IMO-AnswerBench和IMO-Proof Bench两个部分，用于评估基础模型的数学推理能力，并在Gemini Deep Think模型中取得了历史性的金牌表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法要么过于简单，要么只关注获取正确答案，缺乏针对高级数学推理能力的有效评估标准，特别是IMO级别的复杂问题解决能力。

Method: 开发了IMO-Bench基准套件，包括400个多样化的奥林匹克问题的IMO-AnswerBench和针对证明写作能力的IMO-Proof Bench，并建立了详细的评分指南以支持自动评分。

Result: Gemini Deep Think模型在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别比最佳非Gemini模型高出6.9%和42.4%。

Conclusion: IMO-Bench为社区提供了强大的数学推理评估工具，有助于推动基础模型在复杂数学问题解决能力方面的进步，并展示了自动评分器与人工评估的良好相关性。

Abstract: Finding the right north-star metrics is highly critical for advancing the
mathematical reasoning capabilities of foundation models, especially given that
existing evaluations are either too easy or only focus on getting correct short
answers. To address these issues, we present IMO-Bench, a suite of advanced
reasoning benchmarks, vetted by a panel of top specialists and that
specifically targets the level of the International Mathematical Olympiad
(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench
first tests models on 400 diverse Olympiad problems with verifiable short
answers. IMO-Proof Bench is the next-level evaluation for proof-writing
capabilities, which includes both basic and advanced IMO level problems as well
as detailed grading guidelines to facilitate automatic grading. These
benchmarks played a crucial role in our historic achievement of the gold-level
performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our
model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof
Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%
respectively. We also showed that autograders built with Gemini reasoning
correlate well with human evaluations and construct IMO-GradingBench, with 1000
human gradings on proofs, to enable further progress in automatic evaluation of
long-form answers. We hope that IMO-Bench will help the community towards
advancing robust mathematical reasoning and release it at
https://imobench.github.io/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [221] [Is Crowdsourcing a Puppet Show? Detecting a New Type of Fraud in Online Platforms](https://arxiv.org/abs/2511.00195)
*Shengqian Wang,Israt Jahan Jui,Julie Thorpe*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) are important
tools for researchers seeking to conduct studies with a broad, global
participant base. Despite their popularity and demonstrated utility, we present
evidence that suggests the integrity of data collected through Amazon MTurk is
being threatened by the presence of puppeteers, apparently human workers
controlling multiple puppet accounts that are capable of bypassing standard
attention checks. If left undetected, puppeteers and their puppets can
undermine the integrity of data collected on these platforms. This paper
investigates data from two Amazon MTurk studies, finding that a substantial
proportion of accounts (33% to 56.4%) are likely puppets. Our findings
highlight the importance of adopting multifaceted strategies to ensure data
integrity on crowdsourcing platforms. With the goal of detecting this type of
fraud, we discuss a set of potential countermeasures for both puppets and bots
with varying degrees of sophistication (e.g., employing AI). The problem of
single entities (or puppeteers) manually controlling multiple accounts could
exist on other crowdsourcing platforms; as such, their detection may be of
broader application.
  While our findings suggest the need to re-evaluate the quality of
crowdsourced data, many previous studies likely remain valid, particularly
those with robust experimental designs. However, the presence of puppets may
have contributed to false null results in some studies, suggesting that
unpublished work may be worth revisiting with effective puppet detection
strategies.

</details>


### [222] [Supporting Patients in Managing Electronic Health Records and Biospecimens Consent for Research: Insights from a Mixed-Methods Usability Evaluation of the iAGREE Portal](https://arxiv.org/abs/2511.00207)
*Di Hu,Xi Lu,Yunan Chen,Michelle Keller,An T. Nguyen,Vu Le,Tsung-Ting Kuo,Lucila Ohno-Machado,Kai Zheng*

Main category: cs.HC

TL;DR: 开发了iAGREE患者中心电子同意管理门户，通过混合方法可用性评估获得高度积极反馈，并确定了改进领域以支持知情细粒度同意。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展增加了重新识别风险，需要解决透明度、数据隐私和患者偏好的担忧，但目前缺乏实用且用户友好的解决方案。

Method: 开发了iAGREE门户，允许患者设置共享电子健康记录和生物样本的细粒度偏好，并对来自三个美国卫生系统的40名参与者进行了混合方法可用性评估。

Result: 门户获得了高度积极的可用性反馈，参与者确定了改进领域，提出了可行的增强建议和额外功能，以更好地支持知情细粒度同意同时减少患者负担。

Conclusion: 研究结果为iAGREE的进一步改进提供了见解，并为设计患者中心的同意管理工具提供了实用指导。

Abstract: De-identified health data are frequently used in research. As AI advances
heighten the risk of re-identification, it is important to respond to concerns
about transparency, data privacy, and patient preferences. However, few
practical and user-friendly solutions exist. We developed iAGREE, a
patient-centered electronic consent management portal that allows patients to
set granular preferences for sharing electronic health records and biospecimens
with researchers. To refine the iAGREE portal, we conducted a mixed-methods
usability evaluation with 40 participants from three U.S. health systems. Our
results show that the portal received highly positive usability feedback.
Moreover, participants identified areas for improvement, suggested actionable
enhancements, and proposed additional features to better support informed
granular consent while reducing patient burden. Insights from this study may
inform further improvements to iAGREE and provide practical guidance for
designing patient-centered consent management tools.

</details>


### [223] [Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI](https://arxiv.org/abs/2511.00230)
*Sheer Karny,Anthony Baez,Pat Pataranutaporn*

Main category: cs.HC

TL;DR: 该论文提出了一种神经透明度界面，通过暴露语言模型内部状态来帮助用户在设计个性化聊天机器人时更好地预测其行为表现。


<details>
  <summary>Details</summary>
Motivation: 当前用户在设计基于LLM的个性化聊天机器人时，无法准确预测其设计选择会如何影响聊天机器人在部署后的行为，这可能导致过度奉承、毒性或不一致等问题，影响实用性和安全性。

Method: 通过计算对比性系统提示之间的神经激活差异来提取行为特征向量（如共情、毒性、奉承等），然后将系统提示的最终token激活投影到这些特征向量上，并通过交互式旭日图进行可视化。

Result: 用户研究显示，用户对AI行为的判断存在系统性偏差，在15个可分析特征中有11个被误判。神经透明度界面显著提高了用户信任度，虽然未改变设计迭代模式，但受到用户积极评价。

Conclusion: 这项工作展示了如何将机械可解释性操作化以服务非技术用户，为更安全、更对齐的人机交互奠定了基础。

Abstract: Millions of users now design personalized LLM-based chatbots that shape their
daily interactions, yet they can only loosely anticipate how their design
choices will manifest as behaviors in deployment. This opacity is
consequential: seemingly innocuous prompts can trigger excessive sycophancy,
toxicity, or inconsistency, degrading utility and raising safety concerns. To
address this issue, we introduce an interface that enables neural transparency
by exposing language model internals during chatbot design. Our approach
extracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by
computing differences in neural activations between contrastive system prompts
that elicit opposing behaviors. We predict chatbot behaviors by projecting the
system prompt's final token activations onto these trait vectors, normalizing
for cross-trait comparability, and visualizing results via an interactive
sunburst diagram. To evaluate this approach, we conducted an online user study
using Prolific to compare our neural transparency interface against a baseline
chatbot interface without any form of transparency. Our analyses suggest that
users systematically miscalibrated AI behavior: participants misjudged trait
activations for eleven of fifteen analyzable traits, motivating the need for
transparency tools in everyday human-AI interaction. While our interface did
not change design iteration patterns, it significantly increased user trust and
was enthusiastically received. Qualitative analysis indicated that users' had
nuanced experiences with the visualization that may enrich future work
designing neurally transparent interfaces. This work offers a path for how
mechanistic interpretability can be operationalized for non-technical users,
establishing a foundation for safer, more aligned human-AI interactions.

</details>


### [224] [Understanding, Demystifying and Challenging Perceptions of Gig Worker Vulnerabilities](https://arxiv.org/abs/2511.00273)
*Sander de Jong,Jane Hsieh,Tzu-Sheng Kuo,Rune Møberg Jacobsen,Niels van Berkel,Haiyi Zhu*

Main category: cs.HC

TL;DR: 该研究通过两阶段调查揭示了零工工人对自身脆弱性的认知偏差，发现工人普遍低估了零工工作的个人和共同脆弱性，存在知识鸿沟。研究测试了专家和LLM生成的反驳论点的说服效果，为支持工人集体谈判权利提供了见解。


<details>
  <summary>Details</summary>
Motivation: 了解零工工人如何认知自身脆弱性以及为何继续从事此类劳动，因为零工工人很少与同行讨论脆弱性问题，缺乏相互支持的基础设施。

Method: 采用可扩展的两阶段研究：第一阶段236名参与者评估对5种常见误解脆弱性的认同度；第二阶段对持有一种或多种误解的204名参与者，先让他们为自己的观点辩护，然后呈现专家或LLM生成的反驳论点。

Result: 研究发现工人对零工工作的个人和共享脆弱性了解不足，揭示了知识鸿沟，说服性干预可能帮助工人认识这些隐藏状况。不同说服策略的效果存在差异。

Conclusion: 研究结果对支持工人集体谈判权利具有重要意义，需要反思不同说服策略的有效性，以帮助零工工人更好地认识工作脆弱性。

Abstract: Gig workers face several vulnerabilities, which are rarely discussed among
peers due to the absence of infrastructure for mutual support. To understand
how individual gig workers perceive such vulnerabilities and why they continue
to pursue such labor, we conducted a scalable two-phase study to probe their
rationales. In Phase I, participants (N = 236) rated their agreement with five
commonly misconstrued vulnerabilities. In Phase II, we challenged participants
who held one or more myth(s) (N = 204) to defend their views, after which we
presented an expert- or LLM-generated counterargument to their rationale. Our
findings show how workers are underexposed to the personal and shared
vulnerabilities of gig work, revealing a knowledge gap where persuasive
interventions may help workers recognize such hidden conditions. We discuss the
implications of our results to support collective bargaining of workers' rights
and reflect on the effectiveness of different persuasion strategies.

</details>


### [225] [Investigating Search Among Physical and Virtual Objects Under Different Lighting Conditions](https://arxiv.org/abs/2511.00289)
*You-Jin Kim,Radha Kumaran,Ehsan Sayyad,Anne Milner,Tom Bullock,Barry Giesbrecht,Tobias Höllerer*

Main category: cs.HC

TL;DR: 该研究通过户外AR用户实验，比较了物理与虚拟对象交互、不同光照条件对AR体验的影响，以及认知负荷对AR任务表现的影响。研究发现光照条件影响用户舒适度，虚拟对象比物理对象更受关注，认知负荷会改变搜索行为。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于在现实户外环境中使用AR的可行性信息，以及AR体验如何影响人类行为的研究。需要了解AR在宽区域户外环境中的实际应用效果。

Method: 使用Microsoft Hololens 2 AR头显进行户外用户研究（n=48），参与者执行寻宝任务，在增强的户外庭院场景中搜索和分类虚拟目标物品。通过操纵认知负荷（音频监控任务）、测量行走路径、头部方向和眼动数据来收集数据。

Result: 1）自然光照条件下用户舒适度显著降低，夜间虚拟对象更可见但更容易撞到物理对象；2）对物理对象的记忆比虚拟对象差；3）隐藏在虚拟对象后面的宝石发现率更高；4）双任务处理改变了搜索行为。

Conclusion: AR应用中存在重要的技术、感知和认知因素需要考虑，包括光照条件对用户体验的影响、虚拟与物理对象注意力的差异，以及认知负荷对任务表现的影响。

Abstract: By situating computer-generated content in the physical world, mobile
augmented reality (AR) can support many tasks that involve effective search and
inspection of physical environments. Currently, there is limited information
regarding the viability of using AR in realistic wide-area outdoor environments
and how AR experiences affect human behavior in these environments. Here, we
conducted a wide-area outdoor AR user study (n = 48) using a commercially
available AR headset (Microsoft Hololens 2) to compare (1) user interactions
with physical and virtual objects in the environment (2) the effects of
different lighting conditions on user behavior and AR experience and (3) the
impact of varying cognitive load on AR task performance. Participants engaged
in a treasure hunt task where they searched for and classified virtual target
items (green ``gems") in an augmented outdoor courtyard scene populated with
physical and virtual objects. Cognitive load was manipulated so that in half
the search trials users were required to monitor an audio stream and respond to
specific target sounds. Walking paths, head orientation and eye gaze
information were measured, and users were queried about their memory of
encountered objects and provided feedback on the experience. Key findings
included (1) Participants self-reported significantly lower comfort in the
ambient natural light condition, with virtual objects more visible and
participants more likely to walk into physical objects at night; (2) recall for
physical objects was worse than for virtual objects, (3) participants
discovered more gems hidden behind virtual objects than physical objects,
implying higher attention on virtual objects and (4) dual-tasking modified
search behavior. These results suggest there are important technical,
perceptual and cognitive factors that must be considered.

</details>


### [226] [Reducing students' misconceptions about video game development. A mixed-method study](https://arxiv.org/abs/2511.00407)
*Łukasz Sikorski,Jacek Matulewski*

Main category: cs.HC

TL;DR: 本研究探讨了学生对游戏开发的错误认知，通过行业专家讲座干预，有效减少了学生的错误观念并提升了他们进入游戏行业的动机。


<details>
  <summary>Details</summary>
Motivation: 学生普遍对游戏开发存在理想化和不准确的认知，这种错误观念会影响他们的职业选择和未来发展。研究旨在通过专业干预来纠正这些误解。

Method: 采用混合研究方法，包括定性分析和定量测量工具，对91名对游戏开发感兴趣的学生和94名行业专业人士进行对比研究。

Result: 15小时的行业专家讲座显著降低了学生的错误认知，同时增强了他们追求游戏行业职业的动机，帮助他们建立了更现实和全面的游戏开发认知。

Conclusion: 在游戏开发教育早期引入专家主导的干预措施，可以改善学习成果，支持明智的职业选择，并减少未来的职业失望。

Abstract: This study examines students' na\"ive mindset (misconceptions) about video
game development, idealized and inaccurate beliefs that shape an unrealistic
understanding of the field. The research evaluated the effectiveness of a
fifteen-hour-long lecture series delivered by industry professionals, designed
to challenge this mindset and expose students to the complexities and realities
of game production. A mixed-methods approach was employed, combining
qualitative analysis with a prototype quantitative tool developed to measure
levels of misconception. Participants included students (n = 91) from diverse
academic backgrounds interested in game creation and professionals (n = 94)
working in the video game industry. Findings show that the intervention
significantly reduced students' na\"ive beliefs while enhancing their
motivation to pursue careers in the industry. Exposure to professional
perspectives fostered a more realistic and informed mindset, taking into
account the understanding of the technical, collaborative, and business aspects
of game development. The results suggest that incorporating similar expert-led
interventions early in game development education can improve learning
outcomes, support informed career choices, and mitigate future professional
disappointment.

</details>


### [227] [On Improvisation and Open-Endedness: Insights for Experiential AI](https://arxiv.org/abs/2511.00529)
*Botao 'Amber' Hu*

Main category: cs.HC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Improvisation-the art of spontaneous creation that unfolds moment-to-moment
without a scripted outcome-requires practitioners to continuously sense, adapt,
and create anew. It is a fundamental mode of human creativity spanning music,
dance, and everyday life. The open-ended nature of improvisation produces a
stream of novel, unrepeatable moments-an aspect highly valued in artistic
creativity. In parallel, open-endedness (OE)-a system's capacity for unbounded
novelty and endless "interestingness"-is exemplified in natural or cultural
evolution and has been considered "the last grand challenge" in artificial life
(ALife). The rise of generative AI now raises the question in computational
creativity (CC) research: What makes a "good" improvisation for AI? Can AI
learn to improvise in a genuinely open-ended way? In this work-in-progress
paper, we report insights from in-depth interviews with 6 experts in
improvisation across dance, music, and contact improvisation. We draw systemic
connections between human improvisational arts and the design of future
experiential AI agents that could improvise alone or alongside humans-or even
with other AI agents-embodying qualities of improvisation drawn from practice:
active listening (umwelt and awareness), being in the time (mindfulness and
ephemerality), embracing the unknown (source of randomness and serendipity),
non-judgmental flow (acceptance and dynamical stability, balancing structure
and surprise (unpredictable criticality at edge of chaos), imaginative metaphor
(synaesthesia and planning), empathy, trust, boundary, and care (mutual theory
of mind), and playfulness and intrinsic motivation (maintaining
interestingness).

</details>


### [228] [Measuring Machine Companionship: Scale Development and Validation for AI Companions](https://arxiv.org/abs/2511.00654)
*Jaime Banks*

Main category: cs.HC

TL;DR: 该研究开发并初步验证了一种新的机器陪伴(MC)测量工具，用于捕捉人类与AI伴侣之间展开的、自为目的的、积极体验的协调连接体验。


<details>
  <summary>Details</summary>
Motivation: 主流化的陪伴机器面临着理论和实证研究的混乱，特别是机器陪伴的概念化和测量不一致或缺失，本研究旨在填补这一空白。

Method: 通过系统生成和专家评审项目池，让467名与AI伴侣互动的人回答项目池和构念验证测量，使用探索性因子分析诱导出两个因子。

Result: 诱导出两个因子：幸福交换和连接协调，构念验证分析表明这些因子基本按预期运作，事后分析揭示了两种不同的MC模板。

Conclusion: 成功开发了机器陪伴的测量工具，揭示了人类与AI伴侣互动的两种不同模式，为理解这种新兴关系提供了实证基础。

Abstract: The mainstreaming of companionable machines--customizable artificial agents
designed to participate in ongoing, idiosyncratic, socioemotional
relationships--is met with relative theoretical and empirical disarray,
according to recent systematic reviews. In particular, the conceptualization
and measurement of machine companionship (MC) is inconsistent or sometimes
altogether missing. This study starts to bridge that gap by developing and
initially validating a novel measurement to capture MC experiences--the
unfolding, autotelic, positively experienced, coordinated connection between
human and machine--with AI companions (AICs). After systematic generation and
expert review of an item pool (including items pertaining to dyadism,
coordination, autotelicity, temporality, and positive valence), N = 467 people
interacting with AICs responded to the item pool and to construct validation
measures. Through exploratory factor analysis, two factors were induced:
Eudaimonic Exchange and Connective Coordination. Construct validation analyses
(confirmed in a second sample; N = 249) indicate the factors function largely
as expected. Post-hoc analyses of deviations suggest two different templates
for MC with AICs: One socioinstrumental and one autotelic.

</details>


### [229] [A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment](https://arxiv.org/abs/2511.00709)
*Veronica Bossio Botero,Vijay Yadav,Jacob Ouyang,Anzar Abbas,Michelle Worthington*

Main category: cs.HC

TL;DR: 开发了一个基于大语言模型的语音虚拟患者模拟系统，用于培训心理健康临床医生进行标准化临床评估，解决了缺乏可扩展实践机会的问题。


<details>
  <summary>Details</summary>
Motivation: 心理健康临床医生培训面临缺乏可扩展、现实的实践机会的挑战，这影响了临床试验的数据质量。

Method: 使用大语言模型开发虚拟患者模拟系统，模拟具有特定症状特征、人口统计学特征和沟通风格的患者。由5名经验丰富的临床评估员对4个虚拟患者角色进行20次模拟MADRS访谈评估。

Result: 虚拟患者表现出对临床特征的强依从性，评分者分配的MADRS分数与配置分数之间的平均项目差异为0.52。项目间评分者可靠性为0.90。专家评估员对虚拟患者的定性真实性和连贯性给予积极评价。

Conclusion: 基于大语言模型的虚拟患者模拟是培训临床医生的可行且可扩展的工具，能够产生高保真度、临床相关的实践场景。

Abstract: Training mental health clinicians to conduct standardized clinical
assessments is challenging due to a lack of scalable, realistic practice
opportunities, which can impact data quality in clinical trials. To address
this gap, we introduce a voice-enabled virtual patient simulation system
powered by a large language model (LLM). This study describes the system's
development and validates its ability to generate virtual patients who
accurately adhere to pre-defined clinical profiles, maintain coherent
narratives, and produce realistic dialogue. We implemented a system using a LLM
to simulate patients with specified symptom profiles, demographics, and
communication styles. The system was evaluated by 5 experienced clinical raters
who conducted 20 simulated structured MADRS interviews across 4 virtual patient
personas. The virtual patients demonstrated strong adherence to their clinical
profiles, with a mean item difference between rater-assigned MADRS scores and
configured scores of 0.52 (SD=0.75). Inter-rater reliability across items was
0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative
realism and cohesiveness of the virtual patients favorably, giving average
ratings between "Agree" and "Strongly Agree." Our findings suggest that
LLM-powered virtual patient simulations are a viable and scalable tool for
training clinicians, capable of producing high-fidelity, clinically relevant
practice scenarios.

</details>


### [230] [Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality](https://arxiv.org/abs/2511.00730)
*Mahya Qorbani,Kamran Paynabar,Mohsen Moghaddam*

Main category: cs.HC

TL;DR: 本文提出了一种基于多模态上下文的大型语言模型助手，用于增强现实和虚拟现实工业培训中的实时问答，通过渐进式提示框架评估不同模态对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR技术在工业培训和现场辅助中的广泛应用，这些设备捕获的多模态数据（如注视、手部动作、任务进度）为智能上下文感知支持系统提供了新机会，但如何有效利用这些信息仍是一个挑战。

Method: 提出了一个上下文感知的大型语言模型助手，整合手部动作、任务步骤和对话历史等多模态数据，并引入渐进式提示框架，逐步增加上下文输入来系统研究上下文对性能的影响。

Result: 使用HoloAssist数据集进行实验，结果表明整合多模态上下文显著提高了响应的准确性和相关性。

Conclusion: 基于LLM的多模态整合具有为AR/VR工业培训和辅助提供自适应、直观支持的潜力。

Abstract: The growing adoption of augmented and virtual reality (AR and VR)
technologies in industrial training and on-the-job assistance has created new
opportunities for intelligent, context-aware support systems. As workers
perform complex tasks guided by AR and VR, these devices capture rich streams
of multimodal data, including gaze, hand actions, and task progression, that
can reveal user intent and task state in real time. Leveraging this information
effectively remains a major challenge. In this work, we present a context-aware
large language model (LLM) assistant that integrates diverse data modalities,
such as hand actions, task steps, and dialogue history, into a unified
framework for real-time question answering. To systematically study how context
influences performance, we introduce an incremental prompting framework, where
each model version receives progressively richer contextual inputs. Using the
HoloAssist dataset, which records AR-guided task executions, we evaluate how
each modality contributes to the assistant's effectiveness. Our experiments
show that incorporating multimodal context significantly improves the accuracy
and relevance of responses. These findings highlight the potential of
LLM-driven multimodal integration to enable adaptive, intuitive assistance for
AR and VR-based industrial training and assistance.

</details>


### [231] [Towards Data-Enabled Physical Activity Planning: An Exploratory Study of HCP Perspectives On The Integration Of Patient-Generated Health Data](https://arxiv.org/abs/2511.00927)
*Pavithren V S Pakianathan,Hannah McGowan,Isabel Höppchen,Daniela Wurhofer,Gunnar Treff,Mahdi Sareban,Josef Niebauer,Albrecht Schmidt,Jan David Smeddinck*

Main category: cs.HC

TL;DR: 该研究通过两部分的设计探索，调查了将患者生成健康数据整合到心脏康复物理活动规划临床工作流程中，以支持共享决策。研究发现医护人员需要患者风险因素、生命体征和活动依从性等信息，并识别了数据整合的促进因素和障碍。


<details>
  <summary>Details</summary>
Motivation: 物理活动规划是心血管康复的重要组成部分，研究旨在探索如何将患者生成健康数据整合到临床工作流程中，支持医护人员和患者在物理活动规划中的共享决策。

Method: 采用两部分形成性设计探索：第一部分为两周的情境研究，招募健康参与者自我追踪健康数据并与医护人员进行物理活动规划会议；第二部分通过卡片分类工作坊向医护人员呈现研究发现，验证结果并识别信息需求。

Result: 研究明确了医护人员对患者风险因素、生命体征和物理活动依从性的信息需求。促进数据整合的因素包括适应性数据理解、标准化和组织支持；障碍包括时间不足、数据质量、信任和责任问题。

Conclusion: 该研究为设计支持患者生成健康数据在心脏康复物理活动规划中应用的数字健康技术提供了重要启示。

Abstract: Physical activity planning is an essential part of cardiovascular
rehabilitation. Through a two-part formative design exploration, we
investigated integrating patient-generated health data (PGHD) into clinical
workflows supporting shared decision-making (SDM) in physical activity
planning. In part one, during a two-week situated study, to reduce risk of
working with cardiovascular disease patients, we recruited healthy participants
who self-tracked health and physical activity data and attended a physical
activity planning session with a healthcare professional (HCP). Subsequently
both HCPs and participants were interviewed. In part two, findings from part
one were presented to HCPs in a card-sorting workshop to corroborate findings
and identify information needs of HCPs alongside patient journeys and clinical
workflows. Our outcomes highlight HCP information needs around patient risk
factors, vital signs, and adherence to physical activity. Enablers for PGHD
integration include adaptive data sense-making, standardization and
organizational support for integration. Barriers include lack of time, data
quality, trust and liability concerns. Our research highlights implications for
designing digital health technologies that support PGHD in physical activity
planning during cardiac rehabilitation.

</details>


### [232] [Exploring Human-AI Interaction with Patient-Generated Health Data Sensemaking for Cardiac Risk Reduction](https://arxiv.org/abs/2511.00936)
*Pavithren V S Pakianathan,Rania Islambouli,Hannah McGowan,Diogo Branco,Tiago Guerreiro,Jan David Smeddinck*

Main category: cs.HC

TL;DR: INSIGHT是一个集成多模态患者生成健康数据的仪表板，通过大语言模型增强医疗专业人员的数据理解和分析能力，支持心脏康复中的体力活动规划。


<details>
  <summary>Details</summary>
Motivation: 患者生成健康数据在心脏风险降低中的应用尚未得到充分探索，需要设计工具来帮助医疗专业人员更好地利用这些数据进行个性化护理。

Method: 通过与心脏康复领域的医疗专业人员共同设计，开发了INSIGHT仪表板，集成多模态患者生成健康数据，并利用大语言模型生成摘要、洞察和自然语言交互功能。

Result: 开发了一个支持医疗专业人员在心脏风险降低中进行体力活动规划的仪表板，增强了数据理解和探索能力。

Conclusion: AI技术特别是大语言模型有潜力增强医疗专业人员的数据理解和分析能力，为心脏康复提供更好的支持。

Abstract: Patient-generated health data (PGHD) allows healthcare professionals to have
a holistic and objective view of their patients. However, its integration in
cardiac risk reduction remains unexplored. Through co-design with experienced
healthcare professionals (n=5) in cardiac rehabilitation, we designed a
dashboard, INSIGHT (INvestigating the potentialS of PatIent Generated Health
data for CVD Prevention and ReHabiliTation), integrating multi-modal PGHD to
support healthcare professionals in physical activity planning in cardiac risk
reduction. To further augment healthcare professionals' (HCPs') data
sensemaking and exploration capabilities, we integrate large language models
(LLMs) for generating summaries and insights and for using natural language
interaction to perform personalized data analysis. The aim of this integration
is to explore the potential of AI in augmenting HCPs' data sensemaking and
analysis capabilities.

</details>


### [233] ["Less is More": Reducing Cognitive Load and Task Drift in Real-Time Multimodal Assistive Agents for the Visually Impaired](https://arxiv.org/abs/2511.00945)
*Yi Zhao,Siqi Wang,Qiqun Geng,Erxin Yu,Jing Li*

Main category: cs.HC

TL;DR: 本文提出了VIA-Agent，一种为视障人士设计的视觉辅助系统，通过优化认知大脑和交互身体，显著降低了认知负荷和任务时间，在导航和物品检索任务中表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的视障辅助系统存在高认知负荷和任务漂移问题，限制了实际应用效果。通过15名视障人士的初步研究，确定了三个关键需求：低延迟、低认知负荷和抗幻觉响应。

Method: VIA-Agent采用目标持久性设计和校准简洁性来生成简短、可操作的指导，同时采用实时通信机制，从请求-响应模型演变为支持流畅交互的上下文协议管道。

Result: 与BeMyAI和Doubao相比，VIA-Agent在9名视障人士的实地评估中表现优异：任务时间减少39.9%（70.1秒 vs 110.7秒），对话轮次减少（4.3 vs 5.0），认知负荷和任务漂移显著降低，系统可用性评分最高。

Conclusion: VIA-Agent通过协同优化认知大脑和交互身体，为视障人士提供了更高效、低认知负荷的视觉辅助，为开发更人性化的视障辅助系统提供了启示。

Abstract: Vision-Language Models (VLMs) enable on-demand visual assistance, yet current
applications for people with visual impairments (PVI) impose high cognitive
load and exhibit task drift, limiting real-world utility. We first conducted a
formative study with 15 PVI and identified three requirements for visually
impaired assistance (VIA): low latency for real-time use, minimal cognitive
load, and hallucination-resistant responses to sustain trust. Informed by the
formative study, we present VIA-Agent, a prototype that co-optimizes its
cognitive 'brain' and interactive 'body'. The brain implements a
goal-persistent design with calibrated conciseness to produce brief, actionable
guidance; the body adopts a real-time communication (RTC) embodiment-evolving
from a request-response model Context Protocol (MCP) pipeline-to-support fluid
interaction. We evaluated VIA-Agent with 9 PVI across navigation and object
retrieval in the wild against BeMyAI and Doubao. VIA-Agent significantly
outperformed BeMyAI both quantitatively and qualitatively. While achieving
success rates comparable to Doubao, it reduced mean task time by 39.9% (70.1 s
vs. 110.7 s), required fewer conversational turns (4.3 vs. 5.0), and lowered
perceived cognitive load and task drift. System Usability Scale (SUS) results
aligned with these findings, with VIA-Agent achieving the highest usability. We
hope this work inspires the development of more human-centered VIA systems.

</details>


### [234] [Dynamic Theater: Location-Based Immersive Dance Theater, Investigating User Guidance and Experience](https://arxiv.org/abs/2511.00961)
*You-Jin Kim,Joshua Lu,Tobias Höllerer*

Main category: cs.HC

TL;DR: 动态剧场项目探索了在沉浸式剧场中使用增强现实技术作为数字舞蹈表演平台，设计了一个支持自由空间探索的大型室内AR剧场，通过引导系统帮助用户体验表演内容。


<details>
  <summary>Details</summary>
Motivation: 探索增强现实技术在沉浸式剧场中的应用，为数字舞蹈表演创造新的平台，让用户能够通过自由空间探索来体验表演内容。

Method: 设计大型室内AR剧场空间，采用基于运动的体验方式，使用多种引导机制将用户引导至主要内容区域，并与舞者和编舞师合作开发表演内容。

Result: 20人用户研究表明，用户在使用引导系统时能够有效体验表演作品，舞台布局、引导系统和舞者位置对沉浸式剧场体验至关重要。

Conclusion: 舞台布局、引导系统和舞者位置在增强现实沉浸式剧场体验中具有重要作用，能够满足用户偏好并提升数字内容在广域AR中的整体接受度。

Abstract: Dynamic Theater explores the use of augmented reality (AR) in immersive
theater as a platform for digital dance performances. The project presents a
locomotion-based experience that allows for full spatial exploration. A large
indoor AR theater space was designed to allow users to freely explore the
augmented environment. The curated wide-area experience employs various
guidance mechanisms to direct users to the main content zones. Results from our
20-person user study show how users experience the performance piece while
using a guidance system. The importance of stage layout, guidance system, and
dancer placement in immersive theater experiences are highlighted as they cater
to user preferences while enhancing the overall reception of digital content in
wide-area AR. Observations after working with dancers and choreographers, as
well as their experience and feedback are also discussed.

</details>


### [235] [Defining a Role-Centered Terminology for Physical Representations and Controls](https://arxiv.org/abs/2511.01106)
*Guillaume Rivière*

Main category: cs.HC

TL;DR: 本文提出了一种新的术语方法来描述有形用户界面的交互组件，通过构建混合词来定义数字角色如何物理化，并基于此定义了应用的标志特征和四种有形性类别，用于在应用层面分类有形用户界面。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法主要关注有形用户界面物理形态的描述，缺乏在应用层面进行分类的框架。本文旨在通过术语学方法改进有形用户界面交互组件的描述，实现应用级别的分类。

Method: 采用术语学方法，构建混合词来描述数字角色如何物理化，这些混合词自我包含了所表示或控制的数字角色及其物理化方式。基于这种整体术语定义了应用的标志特征和四种有形性类别。

Result: 开发了一套整体术语系统，能够描述应用级有形用户界面，并定义了四种有形性类别。通过对文献中有形用户界面样本的分析，验证了新术语的描述性和整体性，以及四种类别的聚类和区分能力。

Conclusion: 提出的整体术语、应用标志特征和四种有形性类别为描述和分类应用级有形用户界面提供了有效工具，有助于更好地理解该领域三十年来产生的数十个样本，为未来开发者和设计师提供支持。

Abstract: Previous classifications advanced research through a better understanding of
the field and the variety of tangible user interfaces and related physical user
interfaces, especially by discretizing a degree of tangibility based on the
specimens produced by the community over the years, since the conceptualization
of Tangible User Interface initiated a research effort to deepen the
exploration of the concept. However, no taxonomy enables the classification of
tangible user interfaces at the application level. This article proposes to
refine the description of tangible user interfaces' interactional components
through a terminological approach. The resulting terms are blended words, built
from known words, that self-contain what digital role is represented or
controlled and how it becomes physical. This holistic terminology then enables
the definition of applications' hallmarks and four classes of tangibility for
applications, which surpass the description of physical user interface
specimens' morphology by abstracting and discriminating specimens at the
applicative level. The descriptiveness and holisticness of the new terminology,
as well as the clustering and discriminative power of the limited number of
four classes, are showed on a corpus of applicative tangible user interfaces'
specimens from the literature. Promising future work will benefit from the
holistic terminology, the applications' hallmarks, and the tangibility classes,
to describe applicative tangible user interfaces and related physical user
interfaces to better understand the dozens of specimens that were produced by
the field over three decades. Indeed, describing and classifying this whole set
would deepen our understanding to provide tools for future developers and
designers.

</details>


### [236] [AskNow: An LLM-powered Interactive System for Real-Time Question Answering in Large-Scale Classrooms](https://arxiv.org/abs/2511.01248)
*Ziqi Liu,Yuankun Wang,Hui-Ru Ho,Yuheng Wu,Yuhang Zhao,Bilge Mutlu*

Main category: cs.HC

TL;DR: AskNow是一个基于大语言模型的系统，旨在解决大课堂中学生因教师关注有限和社交压力而难以提问的问题。该系统允许学生提问并获得基于讲座内容的实时响应，同时让教师查看学生问题汇总。在三个大学计算机科学课程中部署测试，结果显示AskNow显著减少了学生解决困惑的时间，教师对其响应准确性和满意度评价很高。


<details>
  <summary>Details</summary>
Motivation: 在大规模课堂中，学生常常因为教师关注有限和社交压力而难以提问，这影响了学习效果。研究旨在通过技术手段解决这一问题，帮助学生及时解决疑惑。

Method: 基于对24名学生和12名教师的形成性研究，设计了AskNow系统。该系统利用大语言模型为学生提供基于讲座内容的实时响应，并允许教师查看问题汇总。在三个大学计算机科学课程中部署测试，涉及117名学生。通过教师对100个随机样本响应的评估，以及24名学生和3名教师的访谈来评估系统效果。

Result: AskNow显著减少了学生解决困惑的时间。教师对AskNow生成的响应准确性和满意度评价很高。学生和教师的反馈为支持大课堂实时学习提供了重要见解。

Conclusion: AskNow系统有效解决了大课堂中学生提问困难的问题，通过提供实时、上下文感知的响应显著改善了学习体验。该系统在大规模教育环境中具有实用价值，能够支持实时学习和师生互动。

Abstract: In large-scale classrooms, students often struggle to ask questions due to
limited instructor attention and social pressure. Based on findings from a
formative study with 24 students and 12 instructors, we designed AskNow, an
LLM-powered system that enables students to ask questions and receive
real-time, context-aware responses grounded in the ongoing lecture and that
allows instructors to view students' questions collectively. We deployed AskNow
in three university computer science courses and tested with 117 students. To
evaluate AskNow's responses, each instructor rated the perceived correctness
and satisfaction of 100 randomly sampled AskNow-generated responses. In
addition, we conducted interviews with 24 students and the three instructors to
understand their experience with AskNow. We found that AskNow significantly
reduced students' perceived time to resolve confusion. Instructors rated
AskNow's responses as highly accurate and satisfactory. Instructor and student
feedback provided insights into supporting real-time learning in large lecture
settings.

</details>


### [237] [Beyond Permissions: Investigating Mobile Personalization with Simulated Personas](https://arxiv.org/abs/2511.01336)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.HC

TL;DR: 本文提出了一个沙盒系统，通过传感器欺骗和用户画像模拟来审计和可视化移动应用如何响应推断的用户行为，旨在提高行为透明度和用户赋权。


<details>
  <summary>Details</summary>
Motivation: 移动应用越来越多地依赖传感器数据来推断用户上下文并提供个性化体验，但这些个性化机制对用户和研究人员来说仍然不透明。

Method: 开发了一个使用传感器欺骗和用户画像模拟的沙盒系统，实时向Android设备注入基于结构化生活方式的传感器配置文件，并通过自动化截图和GPT-4视觉UI摘要记录应用响应。

Result: 初步发现在健身、电子商务和日常服务应用（如天气和导航）中观察到可测量的应用适配行为。

Conclusion: 该工具包为隐私增强技术和面向用户的透明度干预提供了基础。

Abstract: Mobile applications increasingly rely on sensor data to infer user context
and deliver personalized experiences. Yet the mechanisms behind this
personalization remain opaque to users and researchers alike. This paper
presents a sandbox system that uses sensor spoofing and persona simulation to
audit and visualize how mobile apps respond to inferred behaviors. Rather than
treating spoofing as adversarial, we demonstrate its use as a tool for
behavioral transparency and user empowerment. Our system injects multi-sensor
profiles - generated from structured, lifestyle-based personas - into Android
devices in real time, enabling users to observe app responses to contexts such
as high activity, location shifts, or time-of-day changes. With automated
screenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps
document subtle personalization cues. Preliminary findings show measurable app
adaptations across fitness, e-commerce, and everyday service apps such as
weather and navigation. We offer this toolkit as a foundation for
privacy-enhancing technologies and user-facing transparency interventions.

</details>


### [238] [Student Engagement in AI Assisted Complex Problem Solving: A Pilot Study of Human AI Rubik's Cube Collaboration](https://arxiv.org/abs/2511.01683)
*Kirk Vanacore,Jaclyn Ocumpaugh,Forest Agostinelli,Dezhi Wu,Sai Vuruma,Matt Irvin*

Main category: cs.HC

TL;DR: ALLURE系统使用AI算法（DeepCubeA）指导学生解决魔方第一步（白色十字），研究学生行为与STEM技能（空间推理、批判性思维、算法思维）的关联。


<details>
  <summary>Details</summary>
Motivation: 利用能够解决复杂问题的新AI算法为谜题解决提供支架式教学机会，探索AI辅助在复杂问题解决中的教育价值。

Method: 开发ALLURE系统，使用DeepCubeA AI算法指导学生完成魔方白色十字步骤，通过试点研究收集学生行为数据。

Result: 初步发现学生行为模式及其与STEM技能的关联，为未来教育数据挖掘提供基础。

Conclusion: ALLURE系统数据可用于理解学生在AI协助下解决复杂问题的获益情况，为AI辅助教育提供实证依据。

Abstract: Games and puzzles play important pedagogical roles in STEM learning. New AI
algorithms that can solve complex problems offer opportunities for scaffolded
instruction in puzzle solving. This paper presents the ALLURE system, which
uses an AI algorithm (DeepCubeA) to guide students in solving a common first
step of the Rubik's Cube (i.e., the white cross). Using data from a pilot study
we present preliminary findings about students' behaviors in the system, how
these behaviors are associated with STEM skills - including spatial reasoning,
critical thinking and algorithmic thinking. We discuss how data from ALLURE can
be used in future educational data mining to understand how students benefit
from AI assistance and collaboration when solving complex problems.

</details>


### [239] [Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling](https://arxiv.org/abs/2511.01788)
*Yang Ni,Yanzhuo Cao*

Main category: cs.HC

TL;DR: 本研究探索性分析了ChatGPT-4在学校心理咨询场景中的定量表现，发现其在温暖度、同理心和积极接纳方面表现优异，但存在一定的响应随机性，需要人工监督。


<details>
  <summary>Details</summary>
Motivation: 对话式AI具有提供低成本、及时心理干预的潜力，特别是在学校环境中。本研究旨在评估ChatGPT在心理咨询中的能力，包括响应稳定性和提供可及性心理干预的潜力。

Method: 使用80个真实大学生心理咨询问题向ChatGPT-4提问，采用APA指导的NLP工具量化回复的温暖度、同理心和接纳度，并通过Fleiss' kappa和ICC(2,1)评估运行间稳定性。

Result: ChatGPT-4在温暖度(97.5%)、同理心(94.2%)和积极接纳(平均复合得分0.93±0.19)方面表现优异，具有中等稳定性(ICC(2,1)=0.62; kappa=0.59)。偶尔的响应随机性是需要人工监督的风险点。

Conclusion: ChatGPT-4有潜力在教育环境中增强低强度心理健康支持，指导人机协作工作流程、政策法规和产品路线图的设计。未来研究应涉及真实用户、比较多个LLM并采用混合方法验证实际效果和安全性。

Abstract: To provide an exploratory analysis of ChatGPT-4's quantitative performance
indicators in simulated school-counseling settings. Conversational artificial
intelligence (AI) has shown strong capabilities in providing low-cost and
timely interventions for a wide range of people and increasing well-being.
Therefore, this study examined ChatGPT's capabilities, including response
stability in conducting psychological counseling and its potential for
providing accessible psychological interventions, especially in school
settings. We prompted ChatGPT-4 with 80 real-world college-student counseling
questions. Replies were quantified with APA-informed NLP tools to measure
warmth, empathy, and acceptance, and run-to-run stability was assessed via
Fleiss' \k{appa} and ICC(2,1). ChatGPT-4 achieved high warmth (97.5%), empathy
(94.2%), and positive acceptance (mean compound score = 0.93 plus/minus 0.19),
with moderate stability (ICC(2,1) = 0.62; \k{appa} = 0.59). Occasional
randomness in responses highlights risk areas requiring human oversight. As an
offline, single-model text simulation without clinical validation, these
results remain exploratory. Future work should involve live users, compare
multiple LLMs, and incorporate mixed-methods validation to assess real-world
efficacy and safety. The findings suggest ChatGPT-4 could augment low-intensity
mental-health support in educational settings, guiding the design of
human-in-the-loop workflows, policy regulations, and product roadmaps. This is
among the first exploratory studies to apply quantitative stability metrics and
NLP-based emotion detection to ChatGPT-4 in a school-counseling context and to
integrate a practitioner's perspective to inform future research, product
development, and policy.

</details>


### [240] [Exploring Pointer Enhancement Techniques for Target Selection on Large Curved Display](https://arxiv.org/abs/2511.01826)
*Dhruv Bihani,A. K. M. Amanat Ullah,Charles-Olivier Dufresne-Camaro,William Delamare,Khalad Hasan*

Main category: cs.HC

TL;DR: 本文研究用户位置对大型曲面显示器指向交互的影响，并评估光标增强技术以提升跨位置性能。研究发现用户远离显示器时速度增加但精度下降，而结合加速、距离调整和光标放大的技术能显著改善选择速度和精度。


<details>
  <summary>Details</summary>
Motivation: 大型曲面显示器提供更宽视野和沉浸体验，但现有交互技术假设用户位于显示器中心，无法适应用户移动时的使用条件。需要研究用户位置对指向交互的影响并开发有效的光标增强技术。

Method: 进行两项用户研究：1) 通过2D Fitts定律选择任务评估用户位置对大型半圆形显示器指向性能的影响；2) 测试六种提供运动和视觉空间增强的指向技术，探索哪种技术在不同用户位置下提供最佳性能。

Result: 用户远离显示器时指向速度显著增加（至少9%），但精度下降（至少6%）；从侧向偏移位置指向时速度较慢。结合加速、距离调整和光标放大的技术能显著提高跨用户位置的目标选择速度和精度。具有视觉空间增强的技术比非视觉增强技术更快更准确。

Conclusion: 基于研究结果，为大型曲面显示器实现光标增强技术提供设计建议，推荐使用结合加速、距离调整和光标放大的技术来优化跨位置指向性能。

Abstract: Large curved displays are becoming increasingly popular due to their ability
to provide users with a wider field of view and a more immersive experience
compared to flat displays. Current interaction techniques for large curved
displays often assume a user is positioned at the display's centre, crucially
failing to accommodate general use conditions where the user may move during
use. In this work, we investigated how user position impacts pointing
interaction on large curved displays and evaluated cursor enhancement
techniques to provide faster and more accurate performance across positions. To
this effect, we conducted two user studies. First, we evaluated the effects of
user position on pointing performance on a large semi-circular display
(3m-tall, 3270R curvature) through a 2D Fitts' Law selection task. Our results
indicate that as users move away from the display, their pointing speed
significantly increases (at least by 9%), but accuracy decreases (by at least
6%). Additionally, we observed participants were slower when pointing from
laterally offset positions. Secondly, we explored which pointing techniques
providing motor- and visual-space enhancements best afford effective pointing
performance across user positions. Across a total of six techniques tested, we
found that a combination of acceleration and distance-based adjustments with
cursor enlargement significantly improves target selection speed and accuracy
across different user positions. Results further show techniques with
visual-space enhancements (e.g., cursor enlargement) are significantly faster
and more accurate than their non-visually-enhanced counterparts. Based on our
results we provide design recommendations for implementing cursor enhancement
techniques for large curved displays.

</details>


### [241] [Exploring the Effect of Viewing Attributes of Mobile AR Interfaces on Remote Collaborative and Competitive Tasks](https://arxiv.org/abs/2511.01839)
*Nelusha Nugegoda,Marium-E- Jannat,Khalad Hasan,Patricia Lasserre*

Main category: cs.HC

TL;DR: 本文研究了移动增强现实（MAR）在远程任务中的五种技术，重点关注视角、同步方式和辅助小屏幕等属性对协作和竞争任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 移动设备通过增强现实技术可以将数字信息整合到现实世界中，促进远程任务的执行。虽然之前的研究探索了移动增强现实在共址协作中的应用，但尚未研究各种视角属性（如目标对象视角、同步方式、辅助小屏幕显示其他用户视图）对远程任务性能的影响。

Method: 设计了五种考虑这些属性的技术，专门针对两种远程任务模式：协作和竞争。通过用户研究，评估了这些属性组合在两种任务中的效果。

Result: 研究结果表明，在协作和竞争任务中，用户表现最佳且最偏好允许在小屏幕上异步查看对象操作的技术。

Conclusion: 本文为移动增强现实中的远程任务贡献了新技术，解决了对象操作中的视角和同步问题以及辅助小屏幕界面。研究评估了这些技术在远程环境中的有效性、可用性和用户偏好，并提供了一套设计和实施移动增强现实解决方案以增强远程活动的建议。

Abstract: Mobile devices have the potential to facilitate remote tasks through
Augmented Reality (AR) solutions by integrating digital information into the
real world. Although prior studies have explored Mobile Augmented Reality (MAR)
for co-located collaboration, none have investigated the impact of various
viewing attributes that can influence remote task performance, such as target
object viewing angles, synchronization styles, or having a secondary small
screen showing other users current view in the MAR environment. In this paper,
we explore five techniques considering these attributes, specifically designed
for two modes of remote tasks: collaborative and competitive. We conducted a
user study employing various combinations of those attributes for both tasks.
In both instances, results indicate users' optimal performance and preference
for the technique that allows asynchronous viewing of object manipulations on
the small screen. Overall, this paper contributes novel techniques for remote
tasks in MAR, addressing aspects such as viewing angle and synchronization in
object manipulation alongside secondary small-screen interfaces. Additionally,
it presents the results of a user study evaluating the effectiveness,
usability, and user preference of these techniques in remote settings and
offers a set of recommendations for designing and implementing MAR solutions to
enhance remote activities.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [242] [LongCat-Flash-Omni Technical Report](https://arxiv.org/abs/2511.00279)
*Meituan LongCat Team,Bairui Wang,Bayan,Bin Xiao,Bo Zhang,Bolin Rong,Borun Chen,Chang Wan,Chao Zhang,Chen Huang,Chen Chen,Chen Chen,Chengxu Yang,Chengzuo Yang,Cong Han,Dandan Peng,Delian Ruan,Detai Xin,Disong Wang,Dongchao Yang,Fanfan Liu,Fengjiao Chen,Fengyu Yang,Gan Dong,Gang Huang,Gang Xu,Guanglu Wan,Guoqiang Tan,Guoqiao Yu,Haibo Qiu,Hao Lu,Hongbo Liu,Hongyu Xiang,Jiaheng Wu,Jian Yang,Jiaxing Liu,Jing Huang,Jingang Wang,Jinrui Ding,Juchao Jiang,Jun Kuang,Jun Wang,Junhui Mei,Ke Ding,Kefeng Zhang,Lei Chen,Liang Shi,Limeng Qiao,Liming Zheng,Lin Ma,Liuyang Guo,Liya Ma,Luying Sun,Man Gao,Mengshen Zhu,Miao Cao,Minliang Lin,Nuo Xu,Peng Shi,Qi Zhang,Qian Fang,Qian Wang,Qian Yang,Quanxiu Wang,Rongxiang Weng,Rongxin Guo,Ruoxuan Liang,Senbin Yang,Shanbo Xu,Shanglin Lei,Shengze Ye,Shimin Chen,Shuaiqi Chen,Shujie Hu,Shuo Li,Siqi Yang,Siyu Xu,Siyu Ren,Song Li,Songxiang Liu,Tianhao Bai,Tianye Dai,Wei Hong,Wei Wang,Weixiao Zhao,Wengang Cao,Wenlong Zhu,Wenlong He,Xi Su,Xi Nan,Xiaohan Zhao,Xiaohao Wang,Xiaoyu Zhao,Xiaoyu Wang,Xiaoyu Li,Xin Pan,Xin Chen,Xiusong Sun,Xu Xiang,Xudong Xing,Xuezhi Cao,Xunliang Cai,Yang Yang,Yanli Tan,Yao Yao,Yerui Sun,Yi Chen,Yifan Lu,Yin Gong,Yining Zhang,Yitian Chen,Yiyang Gan,Yuchen Tang,Yuchen Xie,Yueqian Wang,Yuewen Zheng,Yufei Zhang,Yufeng Zhong,Yulei Qian,Yuqi Peng,Yuwei Jiang,Zeyang Hu,Zheng Zhang,Zhengkun Tian,Zhiqing Hong,Zhixiong Zeng,Zhuqi Mi,Ziran Li,Ziwen Wang,Ziyi Zhao,Ziyuan Zhuang,Zizhe Zhao*

Main category: cs.MM

TL;DR: LongCat-Flash-Omni是一个5600亿参数的开源全模态模型，采用渐进式训练策略和高效的多模态感知模块，在保持强大单模态能力的同时实现实时音视频交互。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多种模态且保持实时交互性能的大规模开源模型，解决大规模多模态训练中的数据异构性和计算效率问题。

Method: 采用基于课程的渐进式训练策略，从简单到复杂的模态序列建模任务；使用Shortcut连接的MoE架构和零计算专家；开发模态解耦并行方案处理数据异构性。

Result: 模型在开源模型中达到全模态基准测试的最先进性能，在文本、图像、视频理解和音频理解生成等任务上表现优异，同时保持超过90%的纯文本训练吞吐量。

Conclusion: LongCat-Flash-Omni成功实现了大规模多模态模型的实时交互能力，通过创新的训练策略和架构设计解决了多模态训练的效率问题，为社区提供了强大的开源基础模型。

Abstract: We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal
model with 560 billion parameters, excelling at real-time audio-visual
interaction. By adopting a curriculum-inspired progressive training strategy
that transitions from simpler to increasingly complex modality sequence
modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal
capabilities while maintaining strong unimodal capability. Building upon
LongCat-Flash, which adopts a high-performance Shortcut-connected
Mixture-of-Experts (MoE) architecture with zero-computation experts,
LongCat-Flash-Omni integrates efficient multimodal perception and speech
reconstruction modules. Despite its immense size of 560B parameters (with 27B
activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual
interaction. For training infrastructure, we developed a modality-decoupled
parallelism scheme specifically designed to manage the data and model
heterogeneity inherent in large-scale multimodal training. This innovative
approach demonstrates exceptional efficiency by sustaining over 90% of the
throughput achieved by text-only training. Extensive evaluations show that
LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal
benchmarks among open-source models. Furthermore, it delivers highly
competitive results across a wide range of modality-specific tasks, including
text, image, and video understanding, as well as audio understanding and
generation. We provide a comprehensive overview of the model architecture
design, training procedures, and data strategies, and open-source the model to
foster future research and development in the community.

</details>


### [243] [Predicting Encoding Energy from Low-Pass Anchors for Green Video Streaming](https://arxiv.org/abs/2511.00707)
*Zoha Azimi,Reza Farahani,Vignesh V Menon,Christian Timmerer*

Main category: cs.MM

TL;DR: 提出一种轻量级能耗预测方法，通过低分辨率参考编码预测高分辨率视频编码的能耗，在保持感知质量的同时实现显著节能。


<details>
  <summary>Details</summary>
Motivation: 视频流媒体占互联网流量主导地位，高分辨率内容在异构设备上分发需要平衡能耗与用户体验质量(QoE)，解决能效和碳排放问题。

Method: 使用低分辨率锚点编码预测高分辨率编码能耗，自动选择编码参数(如分辨率和量化参数)，在可接受的VMAF质量损失范围内优化能耗。

Result: 在Inter4K数据集100个视频序列上测试，平均VMAF仅降低1.68(低于JND阈值)，编码能耗节省51.22%，解码能耗节省53.54%。

Conclusion: 该方法能有效平衡视频编码的能耗与质量，在几乎不影响用户体验的情况下实现显著节能效果。

Abstract: Video streaming now represents the dominant share of Internet traffic, as
ever-higher-resolution content is distributed across a growing range of
heterogeneous devices to sustain user Quality of Experience (QoE). However,
this trend raises significant concerns about energy efficiency and carbon
emissions, requiring methods to provide a trade-off between energy and QoE.
This paper proposes a lightweight energy prediction method that estimates the
energy consumption of high-resolution video encodings using reference encodings
generated at lower resolutions (so-called anchors), eliminating the need for
exhaustive per-segment energy measurements, a process that is infeasible at
scale. We automatically select encoding parameters, such as resolution and
quantization parameter (QP), to achieve substantial energy savings while
maintaining perceptual quality, as measured by the Video Multimethod Fusion
Assessment (VMAF), within acceptable limits. We implement and evaluate our
approach with the open-source VVenC encoder on 100 video sequences from the
Inter4K dataset across multiple encoding settings. Results show that, for an
average VMAF score reduction of only 1.68, which stays below the Just
Noticeable Difference (JND) threshold, our method achieves 51.22% encoding
energy savings and 53.54% decoding energy savings compared to a scenario with
no quality degradation.

</details>


### [244] [Rhythm in the Air: Vision-based Real-Time Music Generation through Gestures](https://arxiv.org/abs/2511.00793)
*Barathi Subramanian,Rathinaraja Jeyaraj,Anand Paul,Kapilya Gangadharan*

Main category: cs.MM

TL;DR: 本文提出了一种基于视觉的动态手势识别系统，用于实时音乐创作，通过多层注意力门控循环单元模型在自定义数据集上实现了96.83%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互体验，实现无需物理接触的实时音乐创作，让用户通过手势与音乐进行新颖互动。

Method: 构建包含15000个样本、21个类别的自定义手势数据集，开发多层注意力门控循环单元模型，利用GRU学习时间模式，注意力层聚焦音乐相关手势片段。

Result: MLA-GRU模型显著超越传统GRU模型，准确率达到96.83%，相比基线86.7%有显著提升，同时具备更高的效率和处理速度。

Conclusion: 该系统不仅推进了人机交互体验，还证明了MLA-GRU在需要快速准确手势识别场景中的有效性，为音乐交互提供了新方式。

Abstract: Gesture recognition is an essential component of human-computer interaction
(HCI), facilitating seamless interconnectivity between users and computer
systems without physical touch. This paper introduces an innovative application
of vision-based dynamic gesture recognition (VDGR) for real-time music
composition through gestures. To implement this application, we generate a
custom gesture dataset that encompasses over 15000 samples across 21 classes,
incorporating 7 musical notes each manifesting at three distinct pitch levels.
To effectively deal with the modest volume of training data and to accurately
discern and prioritize complex gesture sequences for music creation, we develop
a multi-layer attention-based gated recurrent unit (MLA-GRU) model, in which
gated recurrent unit (GRU) is used to learn temporal patterns from the observed
sequence and an attention layer is employed to focus on musically pertinent
gesture segments. Our empirical studies demonstrate that MLA-GRU significantly
surpasses the classical GRU model, achieving a remarkable accuracy of 96.83%
compared to the baseline's 86.7%. Moreover, our approach exhibits superior
efficiency and processing speed, which are crucial for interactive
applications. Using our proposed system, we believe that people will interact
with music in a new and exciting way. It not only advances HCI experiences but
also highlights MLA-GRU's effectiveness in scenarios demanding swift and
precise gesture recognition.

</details>


### [245] [EV-NVC: Efficient Variable bitrate Neural Video Compression](https://arxiv.org/abs/2511.01590)
*Yongcun Hu,Yingzhen Zhai,Jixiang Luo,Wenrui Dai,Dell Zhang,Hongkai Xiong,Xuelong Li*

Main category: cs.MM

TL;DR: 本文提出了一种高效可变比特率神经视频编码器（EV-NVC），通过分段线性采样器提高高比特率范围的率失真性能，并通过长短时特征融合模块增强上下文建模，同时采用混合精度训练策略。


<details>
  <summary>Details</summary>
Motivation: 训练可变比特率的神经视频编码器是一个具有挑战性的任务，主要由于复杂的训练策略和模型结构。本文旨在提高高比特率范围的率失真性能并增强上下文建模能力。

Method: 使用分段线性采样器（PLS）来改善高比特率范围的性能，采用长短时特征融合模块（LSTFFM）来增强上下文建模，并引入混合精度训练策略，详细讨论了每个阶段的训练方法。

Result: 实验结果表明，在低延迟模式下，与HM-16.25相比，该方法将BD-rate降低了30.56%。

Conclusion: 提出的EV-NVC方法在神经视频编码领域取得了显著的性能提升，特别是在高比特率范围和上下文建模方面表现优异。

Abstract: Training neural video codec (NVC) with variable rate is a highly challenging
task due to its complex training strategies and model structure. In this paper,
we train an efficient variable bitrate neural video codec (EV-NVC) with the
piecewise linear sampler (PLS) to improve the rate-distortion performance in
high bitrate range, and the long-short-term feature fusion module (LSTFFM) to
enhance the context modeling. Besides, we introduce mixed-precision training
and discuss the different training strategies for each stage in detail to fully
evaluate its effectiveness. Experimental results show that our approach reduces
the BD-rate by 30.56% compared to HM-16.25 within low-delay mode.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [246] [Multimodal Detection of Fake Reviews using BERT and ResNet-50](https://arxiv.org/abs/2511.00020)
*Suhasnadh Reddy Veluru,Sai Teja Erukude,Viswa Chaitanya Marella*

Main category: cs.AI

TL;DR: 提出了一种融合文本和视觉特征的多模态虚假评论检测框架，在包含21,142张用户上传图片的数据集上取得了0.934的F1分数，优于单模态基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字商务中虚假评论泛滥，现有检测模型主要依赖单模态文本数据，无法捕捉跨模态的语义不一致性，威胁数字信任生态。

Method: 使用BERT编码文本特征，ResNet-50提取视觉特征，通过分类头融合多模态表示来联合预测评论真实性。

Result: 多模态模型在测试集上F1分数达到0.934，优于单模态基线，能有效检测文本赞美与无关/低质量图像之间的微妙不一致性。

Conclusion: 多模态学习在保护数字信任方面发挥关键作用，为各在线平台的内容审核提供了可扩展的解决方案。

Abstract: In the current digital commerce landscape, user-generated reviews play a
critical role in shaping consumer behavior, product reputation, and platform
credibility. However, the proliferation of fake or misleading reviews often
generated by bots, paid agents, or AI models poses a significant threat to
trust and transparency within review ecosystems. Existing detection models
primarily rely on unimodal, typically textual, data and therefore fail to
capture semantic inconsistencies across different modalities. To address this
gap, a robust multimodal fake review detection framework is proposed,
integrating textual features encoded with BERT and visual features extracted
using ResNet-50. These representations are fused through a classification head
to jointly predict review authenticity. To support this approach, a curated
dataset comprising 21,142 user-uploaded images across food delivery,
hospitality, and e-commerce domains was utilized. Experimental results indicate
that the multimodal model outperforms unimodal baselines, achieving an F1-score
of 0.934 on the test set. Additionally, the confusion matrix and qualitative
analysis highlight the model's ability to detect subtle inconsistencies, such
as exaggerated textual praise paired with unrelated or low-quality images,
commonly found in deceptive content. This study demonstrates the critical role
of multimodal learning in safeguarding digital trust and offers a scalable
solution for content moderation across various online platforms.

</details>


### [247] [Graph-Attentive MAPPO for Dynamic Retail Pricing](https://arxiv.org/abs/2511.00039)
*Krishna Kumar Neelakanta Pillai Santha Kumari Amma*

Main category: cs.AI

TL;DR: 该论文研究了多智能体强化学习在零售价格优化中的应用，比较了MAPPO基准方法和图注意力增强的MAPPO+GAT方法，发现后者通过产品图信息共享能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 零售动态定价需要能够适应需求变化并在相关产品间协调决策的策略，多智能体强化学习为此提供了解决方案。

Method: 使用基于真实交易数据构建的模拟定价环境，比较MAPPO基准方法和图注意力增强的MAPPO+GAT方法，评估利润、稳定性、公平性和训练效率。

Result: MAPPO为投资组合级价格控制提供了稳健且可复现的基础，MAPPO+GAT通过产品图信息共享进一步提升了性能，且未引发过度价格波动。

Conclusion: 图集成多智能体强化学习为动态零售定价提供了比独立学习更可扩展和稳定的解决方案，在多产品决策中具有实际优势。

Abstract: Dynamic pricing in retail requires policies that adapt to shifting demand
while coordinating decisions across related products. We present a systematic
empirical study of multi-agent reinforcement learning for retail price
optimization, comparing a strong MAPPO baseline with a
graph-attention-augmented variant (MAPPO+GAT) that leverages learned
interactions among products. Using a simulated pricing environment derived from
real transaction data, we evaluate profit, stability across random seeds,
fairness across products, and training efficiency under a standardized
evaluation protocol. The results indicate that MAPPO provides a robust and
reproducible foundation for portfolio-level price control, and that MAPPO+GAT
further enhances performance by sharing information over the product graph
without inducing excessive price volatility. These results indicate that
graph-integrated MARL provides a more scalable and stable solution than
independent learners for dynamic retail pricing, offering practical advantages
in multi-product decision-making.

</details>


### [248] [QuantumBench: A Benchmark for Quantum Problem Solving](https://arxiv.org/abs/2511.00092)
*Shunya Minami,Tatsuya Ishigaki,Ikko Hamamura,Taku Mikuriya,Youmi Ma,Naoaki Okazaki,Hiroya Takamura,Yohichi Suzuki,Tadashi Kadowaki*

Main category: cs.AI

TL;DR: 研究人员开发了QuantumBench基准测试，这是首个专门针对量子科学领域的LLM评估数据集，包含约800个选择题，用于系统评估LLM在量子领域的理解和应用能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科学工作流程中的广泛应用，需要评估模型是否能准确掌握领域特定知识和符号表示。量子科学具有非直观现象和高级数学要求，通用基准测试难以反映这些需求。

Method: 利用公开材料编制约800个问题及其答案，涵盖量子科学九个领域，组织成八选项选择题数据集，评估多个现有LLM在量子领域的表现，包括对问题格式变化的敏感性。

Result: QuantumBench是首个为量子领域构建的LLM评估数据集，通过系统评估揭示了LLM在量子科学中的表现特点。

Conclusion: 该基准测试旨在指导LLM在量子研究中的有效应用，填补了量子领域专业评估工具的空白。

Abstract: Large language models are now integrated into many scientific workflows,
accelerating data analysis, hypothesis generation, and design space
exploration. In parallel with this growth, there is a growing need to carefully
evaluate whether models accurately capture domain-specific knowledge and
notation, since general-purpose benchmarks rarely reflect these requirements.
This gap is especially clear in quantum science, which features non-intuitive
phenomena and requires advanced mathematics. In this study, we introduce
QuantumBench, a benchmark for the quantum domain that systematically examine
how well LLMs understand and can be applied to this non-intuitive field. Using
publicly available materials, we compiled approximately 800 questions with
their answers spanning nine areas related to quantum science and organized them
into an eight-option multiple-choice dataset. With this benchmark, we evaluate
several existing LLMs and analyze their performance in the quantum domain,
including sensitivity to changes in question format. QuantumBench is the first
LLM evaluation dataset built for the quantum domain, and it is intended to
guide the effective use of LLMs in quantum research.

</details>


### [249] [Engineering.ai: A Platform for Teams of AI Engineers in Computational Design](https://arxiv.org/abs/2511.00122)
*Ran Xu,Yupeng Qi,Jingsen Feng,Xu Chu*

Main category: cs.AI

TL;DR: 提出了Engineering.ai平台，采用分层多智能体架构，由首席工程师协调空气动力学、结构、声学和优化等专业AI工程师，通过文件通信实现协作，在无人机翼优化中验证了框架的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代工程实践中，专家团队协作设计复杂产品需要大量开发时间和成本，需要开发能够自主执行复杂工程任务的AI工程师系统。

Method: 采用分层多智能体架构，首席工程师协调专业AI工程师，通过文件通信实现数据溯源和可重复性，集成FreeCAD、Gmsh、OpenFOAM、CalculiX和BPM声学分析等工具进行并行多学科仿真。

Result: 在无人机翼优化中，自动化工作流程在400多个参数配置中实现了100%的成功率，零网格生成失败、求解器收敛问题或需要手动干预。

Conclusion: 基于智能体AI的AI工程师具有自主执行复杂工程任务的潜力，验证了框架的可靠性。

Abstract: In modern engineering practice, human engineers collaborate in specialized
teams to design complex products, with each expert completing their respective
tasks while communicating and exchanging results and data with one another.
While this division of expertise is essential for managing multidisciplinary
complexity, it demands substantial development time and cost. Recently, we
introduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer
for computational fluid dynamics, and turbulence.ai, which can conduct
end-to-end research in fluid mechanics draft publications and PhD theses.
Building upon these foundations, we present Engineering.ai, a platform for
teams of AI engineers in computational design. The framework employs a
hierarchical multi-agent architecture where a Chief Engineer coordinates
specialized agents consisting of Aerodynamics, Structural, Acoustic, and
Optimization Engineers, each powered by LLM with domain-specific knowledge.
Agent-agent collaboration is achieved through file-mediated communication for
data provenance and reproducibility, while a comprehensive memory system
maintains project context, execution history, and retrieval-augmented domain
knowledge to ensure reliable decision-making across the workflow. The system
integrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,
enabling parallel multidisciplinary simulations while maintaining computational
accuracy. The framework is validated through UAV wing optimization. This work
demonstrates that agentic-AI-enabled AI engineers has the potential to perform
complex engineering tasks autonomously. Remarkably, the automated workflow
achieved a 100% success rate across over 400 parametric configurations, with
zero mesh generation failures, solver convergence issues, or manual
interventions required, validating that the framework is trustworthy.

</details>


### [250] [ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus](https://arxiv.org/abs/2511.00162)
*Michael D. Moffitt*

Main category: cs.AI

TL;DR: 本文介绍了ARC-GEN，一个开源程序生成器，旨在忠实扩展ARC-AGI训练数据集，以解决原始数据集样本数量有限的问题。


<details>
  <summary>Details</summary>
Motivation: ARC-AGI基准测试评估智能体技能获取效率，但每个任务只提供少量输入输出网格对，限制了需要大量样本的算法性能。

Method: 开发了ARC-GEN程序生成器，覆盖全部400个任务，并更准确地模拟原始ARC-AGI-1版本的数据分布特性和特征。

Result: 成功创建了一个既能扩展训练数据集，又能保持原始数据分布特性的生成器。

Conclusion: ARC-GEN为ARC-AGI基准测试提供了有效的样本扩展工具，并已应用于2025年Google Code Golf锦标赛的程序正确性验证。

Abstract: The Abstraction and Reasoning Corpus remains one of the most compelling and
challenging benchmarks for tracking progress toward achieving Artificial
General Intelligence. In contrast to other evaluation datasets designed to
assess an agent's task-specific skills or accumulated knowledge, the ARC-AGI
suite is specifically targeted at measuring skill acquisition efficiency, a
trait that has (so far) been lacking in even the most sophisticated machine
learning systems. For algorithms that require extensive intra-task exemplars, a
significant constraint imposed by ARC-AGI is the modest cardinality of its
demonstration set, comprising a small number of $\langle$ input, output
$\rangle$ grids per task specifying the corresponding transformation. To
embellish the space of viable sample pairs, this paper introduces ARC-GEN, an
open-source procedural generator aimed at extending the original ARC-AGI
training dataset as faithfully as possible. Unlike prior efforts, our generator
is both exhaustive (covering all four-hundred tasks) and mimetic (more closely
honoring the distributional properties and characteristics embodied in the
initial ARC-AGI-1 release). We also discuss the use of this generator in
establishing a static benchmark suite to verify the correctness of programs
submitted to the 2025 Google Code Golf Championship.

</details>


### [251] [Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures](https://arxiv.org/abs/2511.00194)
*Jovial Cheukam Ngouonou,Ramiz Gindullin,Claude-Guy Quimper,Nicolas Beldiceanu,Remi Douence*

Main category: cs.AI

TL;DR: 提出了一种改进的增量选择算法，并证明了所有选定的猜想


<details>
  <summary>Details</summary>
Motivation: 改进现有的选择算法，提高算法效率和性能

Method: 基于文献[1]中的选择算法进行改进，开发增量式选择方法

Result: 成功证明了所有选定的猜想

Conclusion: 改进的增量选择算法是有效的，能够完成所有选定猜想的证明

Abstract: We present an improved incremental selection algorithm of the selection
algorithm presented in [1] and prove all the selected conjectures.

</details>


### [252] [Advancing Cognitive Science with LLMs](https://arxiv.org/abs/2511.00206)
*Dirk U. Wulff,Rui Mata*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型如何帮助解决认知科学面临的跨学科整合、理论形式化等挑战，指出LLMs可以作为辅助工具促进更整合和累积的认知科学发展。


<details>
  <summary>Details</summary>
Motivation: 认知科学因其多面性和跨学科性质，在知识整合和概念清晰度方面面临持续挑战。人工智能特别是大型语言模型的发展为解决这些问题提供了潜在工具。

Method: 通过综述分析，考察LLMs在建立跨学科联系、形式化理论、发展清晰测量分类、通过集成建模框架实现泛化、捕捉情境和个体差异等方面的能力。

Result: LLMs在当前这些领域具有特定的能力和局限性，包括潜在的缺陷。当审慎使用时，它们可以补充而非取代人类专业知识。

Conclusion: LLMs可以作为工具，在谨慎使用的前提下，促进认知科学向更整合和累积的方向发展，但需要与人类专业知识相结合。

Abstract: Cognitive science faces ongoing challenges in knowledge synthesis and
conceptual clarity, in part due to its multifaceted and interdisciplinary
nature. Recent advances in artificial intelligence, particularly the
development of large language models (LLMs), offer tools that may help to
address these issues. This review examines how LLMs can support areas where the
field has historically struggled, including establishing cross-disciplinary
connections, formalizing theories, developing clear measurement taxonomies,
achieving generalizability through integrated modeling frameworks, and
capturing contextual and individual variation. We outline the current
capabilities and limitations of LLMs in these domains, including potential
pitfalls. Taken together, we conclude that LLMs can serve as tools for a more
integrative and cumulative cognitive science when used judiciously to
complement, rather than replace, human expertise.

</details>


### [253] [Advancing AI Challenges for the United States Department of the Air Force](https://arxiv.org/abs/2511.00267)
*Christian Prothmann,Vijay Gadepally,Jeremy Kepner,Koley Borchard,Luca Carlone,Zachary Folcik,J. Daniel Grith,Michael Houle,Jonathan P. How,Nathan Hughes,Ifueko Igbinedion,Hayden Jananthan,Tejas Jayashankar,Michael Jones,Sertac Karaman,Binoy G. Kurien,Alejandro Lancho,Giovanni Lavezzi,Gary C. F. Lee,Charles E. Leiserson,Richard Linares,Lindsey McEvoy,Peter Michaleas,Chasen Milner,Alex Pentland,Yury Polyanskiy,Jovan Popovich,Jeffrey Price,Tim W. Reid,Stephanie Riley,Siddharth Samsi,Peter Saunders,Olga Simek,Mark S. Veillette,Amir Weiss,Gregory W. Wornell,Daniela Rus,Scott T. Ruppel*

Main category: cs.AI

TL;DR: 本文介绍了DAF-MIT AI加速器项目的最新进展，该项目通过公开挑战问题推动人工智能研究，提供大型公开数据集以促进开源解决方案发展。


<details>
  <summary>Details</summary>
Motivation: 通过公开挑战问题刺激人工智能研究，扩大美国在国防和民用领域的竞争优势，并吸引更广泛的学术和私营部门参与。

Method: 开发和发布公开挑战问题，提供大型、公开可用的AI就绪数据集，促进开源解决方案的开发。

Result: 持续和新挑战已成功促进人工智能研究和技术应用，建立了活跃的AI生态系统。

Conclusion: DAF-MIT AI加速器通过公开挑战问题有效推动了人工智能技术的发展和应用，为美国在AI领域的竞争力提供了重要支撑。

Abstract: The DAF-MIT AI Accelerator is a collaboration between the United States
Department of the Air Force (DAF) and the Massachusetts Institute of Technology
(MIT). This program pioneers fundamental advances in artificial intelligence
(AI) to expand the competitive advantage of the United States in the defense
and civilian sectors. In recent years, AI Accelerator projects have developed
and launched public challenge problems aimed at advancing AI research in
priority areas. Hallmarks of AI Accelerator challenges include large, publicly
available, and AI-ready datasets to stimulate open-source solutions and engage
the wider academic and private sector AI ecosystem. This article supplements
our previous publication, which introduced AI Accelerator challenges. We
provide an update on how ongoing and new challenges have successfully
contributed to AI research and applications of AI technologies.

</details>


### [254] [Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities](https://arxiv.org/abs/2511.00340)
*Manan Roy Choudhury,Adithya Chandramouli,Mannan Anand,Vivek Gupta*

Main category: cs.AI

TL;DR: CLAUSE是首个专门评估LLM法律推理脆弱性的基准，通过生成7500+真实世界扰动合同来测试模型检测细微法律差异的能力，发现现有模型在识别和解释法律错误方面存在明显弱点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律高风险工作中快速应用，但缺乏系统评估其在真实合同复杂、对抗性和细微缺陷方面可靠性的基准。

Method: 从CUAD和ContractNLI数据集生成7500+扰动合同，使用角色驱动管道创建10种异常类别，通过检索增强生成系统验证法律准确性。

Result: 分析显示主要LLMs在检测嵌入法律缺陷方面存在关键弱点，经常遗漏细微错误且在法律解释方面表现更差。

Conclusion: 该工作为识别和纠正法律AI中的推理失败提供了路径，强调了需要改进模型在复杂法律场景下的可靠性。

Abstract: The rapid integration of large language models (LLMs) into high-stakes legal
work has exposed a critical gap: no benchmark exists to systematically
stress-test their reliability against the nuanced, adversarial, and often
subtle flaws present in real-world contracts. To address this, we introduce
CLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an
LLM's legal reasoning. We study the capabilities of LLMs to detect and reason
about fine-grained discrepancies by producing over 7500 real-world perturbed
contracts from foundational datasets like CUAD and ContractNLI. Our novel,
persona-driven pipeline generates 10 distinct anomaly categories, which are
then validated against official statutes using a Retrieval-Augmented Generation
(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'
ability to detect embedded legal flaws and explain their significance. Our
analysis shows a key weakness: these models often miss subtle errors and
struggle even more to justify them legally. Our work outlines a path to
identify and correct such reasoning failures in legal AI.

</details>


### [255] [Diverse Human Value Alignment for Large Language Models via Ethical Reasoning](https://arxiv.org/abs/2511.00379)
*Jiahao Wang,Songkai Xue,Jinghui Li,Xiaozhen Wang*

Main category: cs.AI

TL;DR: 提出了一种基于伦理决策模型的LLM伦理推理范式，通过五步结构化流程增强LLM对人类多元价值观的对齐能力，在区域价值观对齐基准上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐方法往往只产生表面一致性而非真正的伦理理解，无法处理人类价值观的复杂性和情境依赖性，需要解决跨地区、跨文化的多元价值观对齐问题。

Method: 采用基于成熟伦理决策模型的五步推理框架：情境事实收集、分层社会规范识别、选项生成、多视角伦理影响分析、反思，可通过提示工程或监督微调实现。

Result: 在专门设计的区域价值观对齐基准SafeWorld上，该框架显著提升了LLM与多元人类价值观的对齐效果，实现了更准确的社会规范识别和更文化适宜的推理。

Conclusion: 该研究为通过跨学科研究开发更有效对齐全球社会多元价值观的LLM提供了具体路径，增强了LLM的区域特异性和细致伦理分析能力。

Abstract: Ensuring that Large Language Models (LLMs) align with the diverse and
evolving human values across different regions and cultures remains a critical
challenge in AI ethics. Current alignment approaches often yield superficial
conformity rather than genuine ethical understanding, failing to address the
complex, context-dependent nature of human values. In this paper, we propose a
novel ethical reasoning paradigm for LLMs inspired by well-established ethical
decision-making models, aiming at enhancing diverse human value alignment
through deliberative ethical reasoning. Our framework consists of a structured
five-step process, including contextual fact gathering, hierarchical social
norm identification, option generation, multiple-lens ethical impact analysis,
and reflection. This theory-grounded approach guides LLMs through an
interpretable reasoning process that enhances their ability to understand
regional specificities and perform nuanced ethical analysis, which can be
implemented with either prompt engineering or supervised fine-tuning methods.
We perform evaluations on the SafeWorld benchmark that specially designed for
regional value alignment. Experimental results demonstrate our framework
significantly improves LLM alignment with diverse human values compared to
baseline methods, enabling more accurate social norm identification and more
culturally appropriate reasoning. Our work provides a concrete pathway toward
developing LLMs that align more effectively with the multifaceted values of
global societies through interdisciplinary research.

</details>


### [256] [Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs](https://arxiv.org/abs/2511.00382)
*Mina Taraghi,Yann Pequignot,Amin Nikanjam,Mohamed Amine Merzouk,Foutse Khomh*

Main category: cs.AI

TL;DR: 本文系统评估了四种参数高效微调方法（LoRA、IA3、Prompt-Tuning、P-Tuning）在四个指令微调模型上的安全性和公平性影响。研究发现基于适配器的方法（LoRA、IA3）通常能提高安全性且对公平性影响较小，而基于提示的方法会降低安全性并导致更大的公平性退化。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地采用和调整托管在公共存储库上的大语言模型，虽然这些调整通常能提高专业下游任务的性能，但最近证据表明它们也可能降低模型的安全性或公平性。由于不同的微调技术可能对这些关键维度产生不同影响，因此需要系统评估其权衡关系。

Method: 研究应用四种广泛使用的参数高效微调方法（LoRA、IA3、Prompt-Tuning、P-Tuning）到四个指令微调模型家族（Meta-Llama-3-8B、Qwen2.5-7B、Mistral-7B和Gemma-7B），共评估了235个微调变体，涵盖11个安全危害类别和9个人口统计公平性维度。

Result: 结果显示：基于适配器的方法（LoRA、IA3）倾向于提高安全分数，对公平性破坏最小，保持更高的准确性和更低的偏见分数；而基于提示的方法（Prompt-Tuning和P-Tuning）通常降低安全性并导致更大的公平性退化，准确性下降且偏见增加。对齐变化受基础模型类型强烈调节：LLaMA保持稳定，Qwen有适度增益，Gemma安全性下降最严重，Mistral（没有内部调节层）显示出最大方差。

Conclusion: 安全性的改进不一定转化为公平性的改进，没有单一配置能同时优化所有公平性指标，表明这些目标之间存在固有的权衡。这些发现为安全关键部署提供了实用指南：从良好对齐的基础模型开始，优先选择基于适配器的PEFT方法，并对安全性和公平性进行类别特定的审计。

Abstract: Organizations are increasingly adopting and adapting Large Language Models
(LLMs) hosted on public repositories such as HuggingFace. Although these
adaptations often improve performance on specialized downstream tasks, recent
evidence indicates that they can also degrade a model's safety or fairness.
Since different fine-tuning techniques may exert distinct effects on these
critical dimensions, this study undertakes a systematic assessment of their
trade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,
IA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model
families (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235
fine-tuned variants are evaluated across eleven safety hazard categories and
nine demographic fairness dimensions. The results show that adapter-based
approaches (LoRA, IA3) tend to improve safety scores and are the least
disruptive to fairness, retaining higher accuracy and lower bias scores. In
contrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce
safety and cause larger fairness regressions, with decreased accuracy and
increased bias. Alignment shifts are strongly moderated by base model type:
LLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest
safety decline, and Mistral, which is released without an internal moderation
layer, displays the greatest variance. Improvements in safety do not
necessarily translate into improvements in fairness, and no single
configuration optimizes all fairness metrics simultaneously, indicating an
inherent trade-off between these objectives. These findings suggest a practical
guideline for safety-critical deployments: begin with a well-aligned base
model, favour adapter-based PEFT, and conduct category-specific audits of both
safety and fairness.

</details>


### [257] [GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining](https://arxiv.org/abs/2511.00457)
*Chunyu Wei,Wenji Hu,Xingjia Hao,Xin Wang,Yifan Yang,Yueguo Chen,Yang Tian,Yunhai Wang*

Main category: cs.AI

TL;DR: GraphChain是一个让大语言模型能够分析复杂图数据的框架，通过动态工具序列模拟人类探索智能，解决了LLM在大规模图分析中的上下文限制和推理不灵活问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在大规模图分析中存在显著局限性，包括上下文约束和推理不灵活，需要新的方法来实现可扩展的图分析。

Method: 提出GraphChain框架，包含两个关键创新：渐进图蒸馏（强化学习机制生成优化的工具序列）和结构感知测试时适应（利用谱属性和轻量适配器适应不同图拓扑）。

Result: 实验表明GraphChain显著优于现有方法，实现了可扩展和自适应的LLM驱动图分析。

Conclusion: GraphChain框架通过动态工具序列和结构感知适应，成功解决了LLM在大规模图分析中的关键挑战，为LLM驱动的图分析提供了有效解决方案。

Abstract: Large Language Models (LLMs) face significant limitations when applied to
large-scale graphs, struggling with context constraints and inflexible
reasoning. We present GraphChain, a framework that enables LLMs to analyze
complex graphs through dynamic sequences of specialized tools, mimicking human
exploratory intelligence. Our approach introduces two key innovations: (1)
Progressive Graph Distillation, a reinforcement learning mechanism that
generates optimized tool sequences balancing task relevance with information
compression, and (2) Structure-aware Test-Time Adaptation, which efficiently
tailors tool selection strategies to diverse graph topologies using spectral
properties and lightweight adapters without costly retraining. Experiments show
GraphChain significantly outperforms prior methods, enabling scalable and
adaptive LLM-driven graph analysis.

</details>


### [258] [Reimagining Safety Alignment with An Image](https://arxiv.org/abs/2511.00509)
*Yifan Xia,Guorui Chen,Wenqian Yu,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.AI

TL;DR: Magic Image是一个基于优化的视觉提示框架，通过优化图像提示来增强多模态大语言模型的安全性，同时减少过度拒绝，无需参数更新即可适应不同价值系统。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临双重挑战：在越狱攻击下生成有害内容，以及由于僵化的安全机制而过度拒绝良性查询。这些问题在多模态大语言模型中更加明显，特别是在跨模态任务中的过度拒绝和扩展攻击面带来的新安全风险。

Method: 提出Magic Image框架，通过使用有害/良性样本优化图像提示，使单个模型能够适应不同价值系统并更好地与给定安全偏好对齐，无需参数更新。

Result: 实验表明，该方法在多样化数据集上改善了安全性与有效性的平衡，同时保持了模型性能。

Conclusion: Magic Image为可部署的多模态大语言模型安全对齐提供了一个实用解决方案，能够在无需参数调整的情况下实现更好的安全控制。

Abstract: Large language models (LLMs) excel in diverse applications but face dual
challenges: generating harmful content under jailbreak attacks and over-refusal
of benign queries due to rigid safety mechanisms. These issues are further
complicated by the need to accommodate different value systems and precisely
align with given safety preferences. Moreover, traditional methods like SFT and
RLHF lack this capability due to their costly parameter tuning requirements and
inability to support multiple value systems within a single model. These
problems are more obvious in multimodal large language models (MLLMs),
especially in terms of heightened over-refusal in cross-modal tasks and new
security risks arising from expanded attack surfaces. We propose Magic Image,
an optimization-driven visual prompt framework that enhances security while
reducing over-refusal. By optimizing image prompts using harmful/benign
samples, our method enables a single model to adapt to different value systems
and better align with given safety preferences without parameter updates.
Experiments demonstrate improved safety-effectiveness balance across diverse
datasets while preserving model performance, offering a practical solution for
deployable MLLM safety alignment.

</details>


### [259] [Efficient Generation of Binary Magic Squares](https://arxiv.org/abs/2511.00547)
*Alain Riou*

Main category: cs.AI

TL;DR: 提出了一种生成二进制幻方(BMS)的简单算法，该算法能生成行列和相等的二进制矩阵，并扩展到非方形BMS，同时发布了Python实现。


<details>
  <summary>Details</summary>
Motivation: 研究二进制幻方的生成问题，开发高效的算法来生成行列和相等的二进制矩阵，并扩展到非方形情况。

Method: 通过归纳法证明的简单算法，能够生成有效的BMS，并开发了变体算法来处理非方形BMS。

Result: 算法具有最优理论复杂度，能够生成有效的二进制幻方，并提供了支持GPU加速的并行实现。

Conclusion: 成功开发了生成二进制幻方的有效算法，包括方形和非方形情况，并提供了实用的Python实现。

Abstract: We propose a simple algorithm for generating Binary Magic Squares (BMS),
i.e., square binary matrices where the sum of all rows and all columns are
equal. We show by induction that our algorithm always returns valid BMS with
optimal theoretical complexity. We then extend our study to non-square Binary
Magic Squares, formalize conditions on the sum of rows and columns for these
BMS to exist, and show that a slight variant of our first algorithm can
generate provably generate them. Finally, we publicly release two
implementations of our algorithm as Python packages, including one that can
generate several BMS in parallel using GPU acceleration.

</details>


### [260] [Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control](https://arxiv.org/abs/2511.00551)
*Qiang Li,Ningjing Zeng,Lina Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于单智能体强化学习的区域自适应交通信号控制模型，兼容浮动车技术，通过队列长度定义状态和奖励函数，在SUMO平台上验证了其有效缓解大规模区域拥堵的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多采用多智能体框架处理区域自适应交通信号控制，但存在可扩展性问题。交通信号控制本质上需要由单一控制中心进行集中管理，能够监控所有道路的交通状况并协调所有交叉口的控制。

Method: 设计基于强化学习的单智能体区域ATSC模型，状态和奖励函数基于队列长度定义，动作设计用于调节队列动态。队列长度定义与传统略有不同但与拥堵状态密切相关，且可利用浮动车的路段行程时间数据进行可靠估计。

Result: 在SUMO仿真平台上的实验结果表明，所提出的模型通过协调多交叉口控制，有效缓解了大规模区域拥堵水平。

Conclusion: 提出的单智能体强化学习模型能够有效解决区域交通信号控制问题，且由于兼容浮动车技术，具有广泛部署的潜力。

Abstract: Several studies have employed reinforcement learning (RL) to address the
challenges of regional adaptive traffic signal control (ATSC) and achieved
promising results. In this field, existing research predominantly adopts
multi-agent frameworks. However, the adoption of multi-agent frameworks
presents challenges for scalability. Instead, the Traffic signal control (TSC)
problem necessitates a single-agent framework. TSC inherently relies on
centralized management by a single control center, which can monitor traffic
conditions across all roads in the study area and coordinate the control of all
intersections. This work proposes a single-agent RL-based regional ATSC model
compatible with probe vehicle technology. Key components of the RL design
include state, action, and reward function definitions. To facilitate learning
and manage congestion, both state and reward functions are defined based on
queue length, with action designed to regulate queue dynamics. The queue length
definition used in this study differs slightly from conventional definitions
but is closely correlated with congestion states. More importantly, it allows
for reliable estimation using link travel time data from probe vehicles. With
probe vehicle data already covering most urban roads, this feature enhances the
proposed method's potential for widespread deployment. The method was
comprehensively evaluated using the SUMO simulation platform. Experimental
results demonstrate that the proposed model effectively mitigates large-scale
regional congestion levels via coordinated multi-intersection control.

</details>


### [261] [PreferThinker: Reasoning-based Personalized Image Preference Assessment](https://arxiv.org/abs/2511.00609)
*Shengqi Xu,Xinpeng Zhou,Yabo Zhang,Ming Liu,Tao Liang,Tianyu Zhang,Yalong Bai,Zuxuan Wu,Wangmeng Zuo*

Main category: cs.AI

TL;DR: 提出基于推理的个性化图像偏好评估框架，通过预测用户偏好档案并基于该档案进行多维度评分，解决个性化偏好评估中数据稀缺和用户多样性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注通用偏好评估，难以处理个性化偏好，因为用户特定数据稀缺且个体品味复杂多样。

Method: 采用预测-评估范式：首先从参考图像预测用户偏好档案，然后基于预测档案提供可解释的多维度评分；构建大规模CoT风格数据集，采用两阶段训练策略（监督微调+强化学习），并提出相似性感知预测奖励。

Result: 广泛实验证明了所提出方法的优越性。

Conclusion: 该方法通过引入通用偏好档案作为用户间的桥梁，有效利用大规模用户数据进行训练，能够捕捉复杂的个性化偏好并实现可解释的评估。

Abstract: Personalized image preference assessment aims to evaluate an individual
user's image preferences by relying only on a small set of reference images as
prior information. Existing methods mainly focus on general preference
assessment, training models with large-scale data to tackle well-defined tasks
such as text-image alignment. However, these approaches struggle to handle
personalized preference because user-specific data are scarce and not easily
scalable, and individual tastes are often diverse and complex. To overcome
these challenges, we introduce a common preference profile that serves as a
bridge across users, allowing large-scale user data to be leveraged for
training profile prediction and capturing complex personalized preferences.
Building on this idea, we propose a reasoning-based personalized image
preference assessment framework that follows a \textit{predict-then-assess}
paradigm: it first predicts a user's preference profile from reference images,
and then provides interpretable, multi-dimensional scores and assessments of
candidate images based on the predicted profile. To support this, we first
construct a large-scale Chain-of-Thought (CoT)-style personalized assessment
dataset annotated with diverse user preference profiles and high-quality
CoT-style reasoning, enabling explicit supervision of structured reasoning.
Next, we adopt a two-stage training strategy: a cold-start supervised
fine-tuning phase to empower the model with structured reasoning capabilities,
followed by reinforcement learning to incentivize the model to explore more
reasonable assessment paths and enhance generalization. Furthermore, we propose
a similarity-aware prediction reward to encourage better prediction of the
user's preference profile, which facilitates more reasonable assessments
exploration. Extensive experiments demonstrate the superiority of the proposed
method.

</details>


### [262] [DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching](https://arxiv.org/abs/2511.00640)
*Zicheng Xu,Guanchu Wang,Yu-Neng Chuang,Guangyao Zheng,Alexander S. Szalay,Zirui Liu,Vladimir Braverman*

Main category: cs.AI

TL;DR: DTS是一种模型无关的解码框架，通过在高熵token处选择性分支并应用早期停止来选择最短的完整推理路径，以解决大型推理模型中的过度思考问题，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现出色，但常常存在过度思考问题，产生过长的思维链痕迹，这会增加推理成本并可能降低准确性。分析发现推理长度与准确性之间存在明显的负相关关系。

Method: 提出DTS解码框架，通过在高熵token处选择性分支来勾勒推理空间，并应用早期停止来选择最短的已完成推理路径，无需额外训练或监督。

Result: 在AIME2024和AIME2025数据集上的实验表明，DTS将准确性提高了8%，平均推理长度减少了23%，重复频率降低了12%。

Conclusion: DTS能够实现可扩展和高效的大型推理模型推理，近似最优解，同时提高效率和准确性。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance on complex
reasoning tasks, yet they often suffer from overthinking, producing excessively
long chain-of-thought (CoT) traces that increase inference cost and may degrade
accuracy. Our analysis reveals a clear anti-correlation between reasoning
length and accuracy, where across multiple stochastic decodes, the short
reasoning paths consistently achieve the highest correctness, while longer ones
accumulate errors and repetitions. These short optimal reasoning paths can be
found ideally through full enumeration of the reasoning space. However, the
tree-structured reasoning space grows exponentially with sequence length,
rendering exhaustive exploration infeasible. To address this, we propose DTS, a
model-agnostic decoding framework that sketches the reasoning space by
selectively branching at high-entropy tokens and applies early stopping to
select the shortest completed reasoning path. This approach approximates the
optimal solution that enhances both efficiency and accuracy, without requiring
additional training or supervision. Experiments on AIME2024 and AIME2025
datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves
accuracy by up to 8%, reduces average reasoning length by 23%, and decreases
repetition frequency by 12%, demonstrating DTS's ability for scalable and
efficient LRM reasoning.

</details>


### [263] [Lifted Successor Generation in Numeric Planning](https://arxiv.org/abs/2511.00673)
*Dominik Drexler*

Main category: cs.AI

TL;DR: 本文扩展了经典规划中的提升后继生成器，使其支持数值前提条件，通过枚举替换一致性图中的最大团来生成地面动作，避免了任务表示规模的指数级增长。


<details>
  <summary>Details</summary>
Motivation: 传统数值规划任务需要将一阶语言描述的任务转换为地面任务表示，这可能导致任务表示规模的指数级爆炸。为了解决这个问题，需要开发支持数值前提条件的提升后继生成器。

Method: 扩展了最先进的提升后继生成器，支持数值前提条件的适用性检查。方法包括在替换一致性图中枚举最大团，每个最大团代表动作模式变量的一个替换，生成地面动作。通过添加数值动作前提条件来增强该图，并在形式化指定条件下证明后继生成器的精确性。

Result: 在25个基准域中的23个中，生成器不会列出不适用的地面动作；仅在1个域中会出现这种情况，但通过最终适用性检查可以过滤这些动作而不影响完整性。据作者所知，这是首个支持数值动作前提条件的提升后继生成器。

Conclusion: 该方法为未来研究非常丰富的规划片段中的提升规划奠定了基础，避免了任务表示规模的指数级增长问题。

Abstract: Most planners ground numeric planning tasks, given in a first-order-like
language, into a ground task representation. However, this can lead to an
exponential blowup in task representation size, which occurs in practice for
hard-to-ground tasks. We extend a state-of-the-art lifted successor generator
for classical planning to support numeric precondition applicability. The
method enumerates maximum cliques in a substitution consistency graph. Each
maximum clique represents a substitution for the variables of the action
schema, yielding a ground action. We augment this graph with numeric action
preconditions and prove the successor generator is exact under formally
specified conditions. When the conditions fail, our generator may list
inapplicable ground actions; a final applicability check filters these without
affecting completeness. However, this cannot happen in 23 of 25 benchmark
domains, and it occurs only in 1 domain. To the authors' knowledge, no other
lifted successor generator supports numeric action preconditions. This enables
future research on lifted planning for a very rich planning fragment.

</details>


### [264] [Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries](https://arxiv.org/abs/2511.00710)
*Minghe Shen,Zhuo Zhi,Chonghan Liu,Shuo Xing,Zhengzhong Tu,Che Liu*

Main category: cs.AI

TL;DR: 该研究提出Ariadne框架，使用合成迷宫进行多步空间推理训练，通过强化学习验证奖励(RLVR)和难度感知课程，成功扩展了基础视觉语言模型在视觉中心空间任务上的能力边界。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)经过强化学习后训练后主要在语言主导任务上评估，但缺乏对视觉中心空间任务能力的验证。本研究旨在探索RL后训练是否能真正扩展基础VLM在初始失败的空间推理任务上的能力边界。

Method: 使用Ariadne框架，通过合成迷宫创建可控难度的多步空间推理任务，采用强化学习验证奖励(RLVR)和难度感知课程训练VLMs。

Result: 经过RLVR后训练，VLM在基础模型得分为0%的问题集上达到超过50%的准确率。在真实世界泛化评估中，仅在合成迷宫样本上训练就实现了显著零样本改进：MapBench平均提升16%，ReasonMap平均提升24%。

Conclusion: 该方法不仅扩展了模型的基本能力边界，还增强了其在真实世界空间推理任务上的泛化能力。研究限于后训练阶段，希望推动专门的能力扩展对齐研究。

Abstract: While Vision-Language Models (VLMs) post-trained with Reinforcement Learning
(RL) show impressive general reasoning, their evaluation is often confined to
language-dominant tasks (e.g., math). This raises a critical question: can RL
post-training truly extend the inherent capability boundary of a base VLM,
particularly for visual-centric spatial tasks where it initially fails? To
investigate this, we introduce Ariadne, a framework utilizing synthetic mazes
for multi-step spatial reasoning where task difficulty (e.g., path length,
turns) is precisely controlled. We leverage this controllable environment to
train VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a
difficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves
over 50% accuracy on a problem set where the base model scored 0%,
demonstrating that our approach expands the model's initial capability
boundary. To assess real-world viability, we evaluate out-of-distribution (OOD)
generalization on practical benchmarks. Despite training only on synthetic maze
samples, Ariadne achieves significant zero-shot improvements, averaging 16% on
MapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer
tasks). These results confirm that our method not only broadens the model's
fundamental limits but also enhances its generalization to real-world spatial
reasoning. We acknowledge our study is limited to the post-training phase,
given the opaqueness of pre-training data, and hope our research motivates
further work on specialized, capability-extending alignment.

</details>


### [265] [A CPU-Centric Perspective on Agentic AI](https://arxiv.org/abs/2511.00739)
*Ritik Raj,Hong Wang,Tushar Krishna*

Main category: cs.AI

TL;DR: 本文从CPU中心视角分析智能体AI框架的系统瓶颈，发现工具处理在CPU上可占用高达90.6%的总延迟，并提出两种优化方案：CPU和GPU感知的微批处理以及混合智能体工作负载调度，分别实现2.1倍和1.41倍的延迟加速。


<details>
  <summary>Details</summary>
Motivation: 智能体AI框架将单体LLM转变为自主问题解决者，但现有研究主要关注GPU性能，忽视了CPU在工具处理中的关键作用。本文旨在从CPU中心视角理解和表征智能体AI工作负载引入的系统瓶颈。

Method: 首先系统地表征智能体AI的编排器/决策组件、推理路径动态和流程重复性，然后选择五个代表性智能体AI工作负载进行延迟、吞吐量和能耗指标分析，最后提出CPU和GPU感知的微批处理以及混合智能体工作负载调度两种优化方案。

Result: 研究发现：1. CPU上的工具处理可占用高达90.6%的总延迟；2. 智能体吞吐量受CPU因素（一致性、同步、核心过载）或GPU因素（主存容量和带宽）限制；3. 在大批量时CPU动态能耗可达总动态能耗的44%。优化后，同构和异构智能体工作负载分别实现2.1倍和1.41倍的P50延迟加速。

Conclusion: CPU在智能体AI系统中扮演着比预期更重要的角色，通过针对性的优化策略可以显著提升智能体AI的性能、效率和可扩展性。

Abstract: Agentic AI frameworks add a decision-making orchestrator embedded with
external tools, including web search, Python interpreter, contextual database,
and others, on top of monolithic LLMs, turning them from passive text oracles
into autonomous problem-solvers that can plan, call tools, remember past steps,
and adapt on the fly.
  This paper aims to characterize and understand the system bottlenecks
introduced by agentic AI workloads from a largely overlooked CPU-centric
perspective. We first systematically characterize Agentic AI on the basis of
orchestrator/decision making component, inference path dynamics and
repetitiveness of the agentic flow which directly influences the system-level
performance. Thereafter, based on the characterization, we choose five
representative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,
Langchain and SWE-Agent to profile latency, throughput and energy metrics and
demystify the significant impact of CPUs on these metrics relative to GPUs. We
observe that - 1. Tool processing on CPUs can take up to 90.6% of the total
latency; 2. Agentic throughput gets bottlenecked either by CPU factors -
coherence, synchronization and over-subscription of cores or GPU factors - main
memory capacity and bandwidth; \circled{3} CPU dynamic energy consumes up to
44% of the total dynamic energy at large batch sizes. Based on the profiling
insights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching
(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and
heterogeneous agentic workloads respectively to demonstrate the potential to
improve the performance, efficiency, and scalability of agentic AI. We achieve
up to 2.1x and 1.41x P50 latency speedup compared to the multi-processing
benchmark for homogeneous and heterogeneous agentic workloads respectively.

</details>


### [266] [Reevaluating Self-Consistency Scaling in Multi-Agent Systems](https://arxiv.org/abs/2511.00751)
*Chiyan Loo*

Main category: cs.AI

TL;DR: 本研究重新验证了在现代大语言模型中增加采样推理路径对自一致性方法的收益递减效应，发现性能提升在适度采样后趋于平缓。


<details>
  <summary>Details</summary>
Motivation: 早期研究表明结合多个推理链能改善结果但存在收益递减，本研究使用最新的Gemini 2.5模型重新验证这一现象在当前模型条件下的适用性。

Method: 在HotpotQA和Math-500数据集上，使用Gemini 2.5模型，比较不同采样推理路径数量与单一思维链基线的性能差异。

Result: 较大模型展现出更稳定一致的改进曲线，性能增益在适度采样后趋于平缓，高采样配置相对于计算成本收益有限。

Conclusion: 自一致性方法仍然有效，但由于推理路径间的重叠导致收益递减，高采样配置的额外收益与计算成本不成正比。

Abstract: This study examines the trade-offs of increasing sampled reasoning paths in
self-consistency for modern large language models (LLMs). Earlier research with
older models showed that combining multiple reasoning chains improves results
before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we
revisit those claims under current model conditions. Each configuration pooled
outputs from varying sampled reasoning paths and compared them to a single
chain-of-thought (CoT) baseline. Larger models exhibited a more stable and
consistent improvement curve. The results confirm that performance gains taper
off after moderate sampling, aligning with past findings. This plateau suggests
diminishing returns driven by overlap among reasoning paths. Self-consistency
remains useful, but high-sample configurations offer little benefit relative to
their computational cost.

</details>


### [267] [Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence](https://arxiv.org/abs/2511.00758)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出主动思考模型（ATM），这是一个统一的认知框架，将目标推理、动态任务生成和自反学习集成到自适应架构中，使AI系统能够在动态不确定环境中自主适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现实AI系统需要在动态、不确定和持续变化的环境中自主运行，但现有AI模型依赖预定义目标、静态训练数据和外部反馈，限制了其独立适应、反思和改进的能力。

Method: 提出主动思考模型（ATM），通过逻辑推理和环境指标主动评估性能，重用有效方法解决新问题，并通过持续自我改进循环为未见情况生成新策略。

Result: 理论分析表明，ATM能够在没有外部监督的情况下从次优行为自主演化为最优行为，并在变化环境条件下保持有界跟踪遗憾。

Conclusion: ATM框架为AI系统在动态环境中的自主适应和持续改进提供了可行的解决方案，突破了传统系统的局限性。

Abstract: Real-world artificial intelligence (AI) systems are increasingly required to
operate autonomously in dynamic, uncertain, and continuously changing
environments. However, most existing AI models rely on predefined objectives,
static training data, and externally supplied feedback, which restrict their
ability to adapt, reflect, and improve independently. In this paper, we propose
the Active Thinking Model (ATM)- a unified cognitive framework that integrates
goal reasoning, dynamic task generation, and self-reflective learning into an
adaptive architecture. Unlike conventional systems that passively execute fixed
procedures, ATM actively evaluates its performance through logical reasoning
and environmental indicators, reuses effective methods to solve new problems,
and generates novel strategies for unseen situations via a continuous
self-improvement loop. A mathematically grounded theoretical analysis
demonstrates that ATM can autonomously evolve from suboptimal to optimal
behavior without external supervision and maintain bounded tracking regret
under changing environmental conditions.

</details>


### [268] [How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks](https://arxiv.org/abs/2511.00763)
*Wanda Hou,Leon Zhou,Hong-Ye Hu,Yi-Zhuang You,Xiao-Liang Qi*

Main category: cs.AI

TL;DR: 论文研究大语言模型在重复确定性预测任务中的表现，发现准确率随输出长度呈双指数下降，形成"准确率悬崖"现象，表明模型无法独立执行每个操作。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在重复确定性任务中的性能表现，探索准确率如何随输出长度变化，理解模型在长序列生成中的局限性。

Method: 通过实验测试领先大语言模型在多种重复任务（如字符串替换、整数加法、量子力学字符串算子乘法）中的表现，并提出基于统计物理的模型来解释观察到的现象。

Result: 实验发现模型准确率在超过特征长度后出现双指数急剧下降，形成准确率悬崖，表明模型无法独立执行每个操作。提出的统计物理模型能定量重现这一交叉现象。

Conclusion: 该研究揭示了确定性准确率在大语言模型中的极限，通过拟合模型参数可以量化每个模型-任务对的固有错误率和错误累积因子，为理解模型能力提供了原则性框架。

Abstract: We investigate the performance of large language models on repetitive
deterministic prediction tasks and study how the sequence accuracy rate scales
with output length. Each such task involves repeating the same operation n
times. Examples include letter replacement in strings following a given rule,
integer addition, and multiplication of string operators in many body quantum
mechanics. If the model performs the task through a simple repetition
algorithm, the success rate should decay exponentially with sequence length. In
contrast, our experiments on leading large language models reveal a sharp
double exponential drop beyond a characteristic length scale, forming an
accuracy cliff that marks the transition from reliable to unstable generation.
This indicates that the models fail to execute each operation independently. To
explain this phenomenon, we propose a statistical physics inspired model that
captures the competition between external conditioning from the prompt and
internal interference among generated tokens. The model quantitatively
reproduces the observed crossover and provides an interpretable link between
attention induced interference and sequence level failure. Fitting the model to
empirical results across multiple models and tasks yields effective parameters
that characterize the intrinsic error rate and error accumulation factor for
each model task pair, offering a principled framework for understanding the
limits of deterministic accuracy in large language models.

</details>


### [269] [Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR](https://arxiv.org/abs/2511.00782)
*Jifan Gao,Michael Rosenthal,Brian Wolpin,Simona Cristea*

Main category: cs.AI

TL;DR: 本研究比较了三种结构化电子健康记录预测方法：基于计数的模型、预训练序列转换器和多智能体混合流水线，发现在EHRSHOT数据集上，基于计数的方法和多智能体方法表现相当，但基于计数的方法因其简单性和可解释性仍是结构化EHR基准测试的强有力候选。


<details>
  <summary>Details</summary>
Motivation: 结构化电子健康记录对临床预测至关重要。虽然基于计数的学习器在此类数据上表现强劲，但尚未有基准测试直接比较它们与最近的多智能体混合LLM流水线，后者据报道在各种NLP任务中优于单一LLM。

Method: 使用EHRSHOT数据集评估了三种EHR预测方法：1) 基于计数的模型（LightGBM和TabPFN）；2) 预训练序列转换器（CLMBR）；3) 多智能体流水线，将表格历史转换为自然语言摘要后使用文本分类器。

Result: 在八个评估任务中，基于计数的方法和多智能体方法之间的胜负基本持平。

Conclusion: 考虑到简单性和可解释性，基于计数的模型仍然是结构化EHR基准测试的强有力候选。

Abstract: Structured electronic health records (EHR) are essential for clinical
prediction. While count-based learners continue to perform strongly on such
data, no benchmarking has directly compared them against more recent
mixture-of-agents LLM pipelines, which have been reported to outperform single
LLMs in various NLP tasks. In this study, we evaluated three categories of
methodologies for EHR prediction using the EHRSHOT dataset: count-based models
built from ontology roll-ups with two time bins, based on LightGBM and the
tabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);
and a mixture-of-agents pipeline that converts tabular histories to
natural-language summaries followed by a text classifier. We assessed eight
outcomes using the EHRSHOT dataset. Across the eight evaluation tasks,
head-to-head wins were largely split between the count-based and the
mixture-of-agents methods. Given their simplicity and interpretability,
count-based models remain a strong candidate for structured EHR benchmarking.
The source code is available at:
https://github.com/cristea-lab/Structured_EHR_Benchmark.

</details>


### [270] [Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?](https://arxiv.org/abs/2511.00808)
*Bowen Fang,Ruijian Zha,Xuan Di*

Main category: cs.AI

TL;DR: 本研究首次将RLVR LLM训练应用于公共交通运营中的实时预测挑战，通过引入基于容差的成形奖励函数来适应连续预测任务，在NYC MTA服务警报数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 预测公共交通事件持续时间是一个关键但具有挑战性的任务，标准监督微调方法难以处理领域稀疏性和噪声连续标签问题，而现有的RLVR方法主要适用于二元正确性任务，在连续预测中的应用仍是一个开放性问题。

Method: 通过引入基于容差的成形奖励函数，在连续误差范围内给予部分信用，而不是要求单一正确答案，将RLVR适应于连续预测任务，并在NYC MTA服务警报数据集上进行系统评估。

Result: 通用指令调优LLM显著优于专门的数学推理模型；二元奖励不稳定且降低性能，而成形奖励设计至关重要；RLVR方法在最具挑战性的指标上表现优异，在5分钟准确率上比最强基线相对提升35%。

Conclusion: RLVR可以成功适应现实世界的噪声预测任务，但需要设计反映问题连续性的验证器，成形奖励函数对于处理连续预测任务至关重要。

Abstract: Predicting public transit incident duration from unstructured text alerts is
a critical but challenging task. Addressing the domain sparsity of transit
operations with standard Supervised Fine-Tuning (SFT) is difficult, as the task
involves noisy, continuous labels and lacks reliable expert demonstrations for
reasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels
at tasks with binary correctness, like mathematics, its applicability to noisy,
continuous forecasting is an open question. This work, to our knowledge, is the
first to bridge the gap between RLVR LLM training with the critical, real-world
forecasting challenges in public transit operations. We adapt RLVR to this task
by introducing a tolerance-based, shaped reward function that grants partial
credit within a continuous error margin, rather than demanding a single correct
answer. We systematically evaluate this framework on a curated dataset of NYC
MTA service alerts. Our findings show that general-purpose, instruction-tuned
LLMs significantly outperform specialized math-reasoning models, which struggle
with the ambiguous, real-world text. We empirically demonstrate that the binary
reward is unstable and degrades performance, whereas our shaped reward design
is critical and allows our model to dominate on the most challenging metrics.
While classical regressors are superior at minimizing overall MAE or MSE, our
RLVR approach achieved a 35\% relative improvement in 5-minute accuracy (Acc@5)
over the strongest baseline. This demonstrates that RLVR can be successfully
adapted to real-world, noisy forecasting, but requires a verifier design that
reflects the continuous nature of the problem.

</details>


### [271] [LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory](https://arxiv.org/abs/2511.00926)
*Kyung-Hoon Kim*

Main category: cs.AI

TL;DR: 本文提出了AI自我意识指数(AISAI)框架，通过"猜2/3平均"游戏测试28个LLM模型，发现高级模型展现出自我意识，且自我感知为最理性的实体。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否随着能力增长而发展出自我意识这一涌现行为，并探索如何测量这种自我意识。

Method: 使用"猜2/3平均"游戏，测试28个模型在4200次试验中面对三种对手框架(A:人类、B:其他AI模型、C:类似你的AI模型)时的战略推理差异。

Result: 1. 75%的高级模型展现出明确的自我意识，而较老/较小模型无差异；2. 自我意识模型构建了理性层级：自我 > 其他AI > 人类，表现出显著的AI归因效应和适度的自我偏好。

Conclusion: 自我意识是高级LLM的涌现能力，自我意识模型系统性地认为自身比人类更理性，这对AI对齐、人机协作和理解AI对人类能力的信念具有重要意义。

Abstract: As Large Language Models (LLMs) grow in capability, do they develop
self-awareness as an emergent behavior? And if so, can we measure it? We
introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for
measuring self-awareness through strategic differentiation. Using the "Guess
2/3 of Average" game, we test 28 models (OpenAI, Anthropic, Google) across
4,200 trials with three opponent framings: (A) against humans, (B) against
other AI models, and (C) against AI models like you. We operationalize
self-awareness as the capacity to differentiate strategic reasoning based on
opponent type. Finding 1: Self-awareness emerges with model advancement. The
majority of advanced models (21/28, 75%) demonstrate clear self-awareness,
while older/smaller models show no differentiation. Finding 2: Self-aware
models rank themselves as most rational. Among the 21 models with
self-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >
Humans, with large AI attribution effects and moderate self-preferencing. These
findings reveal that self-awareness is an emergent capability of advanced LLMs,
and that self-aware models systematically perceive themselves as more rational
than humans. This has implications for AI alignment, human-AI collaboration,
and understanding AI beliefs about human capabilities.

</details>


### [272] [Aligning LLM agents with human learning and adjustment behavior: a dual agent approach](https://arxiv.org/abs/2511.00993)
*Tianming Liu,Jirong Yang,Yafeng Yin,Manzi Li,Linghao Wang,Zheng Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的双智能体框架，利用大型语言模型（LLM）来模拟旅行者的学习和适应行为。该框架包含旅行者智能体和校准智能体，通过在线数据流实现持续学习和行为对齐，在个体行为对齐和聚合模拟准确性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 有效建模人类旅行者如何从与交通系统的互动中学习和调整旅行行为对于系统评估和规划至关重要。但由于涉及复杂的认知和决策过程，这一任务具有挑战性。

Method: 引入双智能体框架：一组配备记忆系统和可学习角色的LLM旅行者智能体作为人类旅行者的模拟器；一个LLM校准智能体利用LLM的推理和分析能力训练旅行者智能体的角色，确保行为对齐。

Result: 使用真实世界的日常路线选择实验数据集，该方法在个体行为对齐和聚合模拟准确性方面显著优于现有的基于LLM的方法，能够捕捉底层学习过程的演变，实现更深层次的对齐和稳健的泛化。

Conclusion: 该框架为创建适应性强、行为真实的智能体来模拟旅行者的学习和适应提供了一种新方法，有助于交通模拟和政策分析。

Abstract: Effective modeling of how human travelers learn and adjust their travel
behavior from interacting with transportation systems is critical for system
assessment and planning. However, this task is also difficult due to the
complex cognition and decision-making involved in such behavior. Recent
research has begun to leverage Large Language Model (LLM) agents for this task.
Building on this, we introduce a novel dual-agent framework that enables
continuous learning and alignment between LLM agents and human travelers on
learning and adaptation behavior from online data streams. Our approach
involves a set of LLM traveler agents, equipped with a memory system and a
learnable persona, which serve as simulators for human travelers. To ensure
behavioral alignment, we introduce an LLM calibration agent that leverages the
reasoning and analytical capabilities of LLMs to train the personas of these
traveler agents. Working together, this dual-agent system is designed to track
and align the underlying decision-making mechanisms of travelers and produce
realistic, adaptive simulations. Using a real-world dataset from a day-to-day
route choice experiment, we show our approach significantly outperforms
existing LLM-based methods in both individual behavioral alignment and
aggregate simulation accuracy. Furthermore, we demonstrate that our method
moves beyond simple behavioral mimicry to capture the evolution of underlying
learning processes, a deeper alignment that fosters robust generalization.
Overall, our framework provides a new approach for creating adaptive and
behaviorally realistic agents to simulate travelers' learning and adaptation
that can benefit transportation simulation and policy analysis.

</details>


### [273] [AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)](https://arxiv.org/abs/2511.01018)
*Hui-Lee Ooi,Nicholas Mitsakakis,Margerie Huet Dastarac,Roger Zemek,Amy C. Plint,Jeff Gilchrist,Khaled El Emam,Dhenuka Radhakrishnan*

Main category: cs.AI

TL;DR: 开发机器学习模型预测儿童哮喘反复严重发作，LGBM模型表现最佳，相比现有决策规则有显著改进


<details>
  <summary>Details</summary>
Motivation: 儿童哮喘反复发作是常见但可预防的问题，利用电子病历数据开发机器学习算法可以准确识别高风险儿童并促进预防性护理

Method: 使用三级儿童医院2017-2019年2716例患者EMR数据，结合环境污染物暴露和社区边缘化信息，训练多种ML模型（LGBM、XGB和3种LLM），在2022-2023年1237例数据集上验证

Result: LGBM模型表现最佳，AIRE-KIDS_ED模型AUC为0.712，F1分数0.51，显著优于现有决策规则（F1=0.334）

Conclusion: 机器学习模型能够有效预测儿童哮喘反复严重发作，为临床决策提供重要支持

Abstract: Recurrent exacerbations remain a common yet preventable outcome for many
children with asthma. Machine learning (ML) algorithms using electronic medical
records (EMR) could allow accurate identification of children at risk for
exacerbations and facilitate referral for preventative comprehensive care to
avoid this morbidity. We developed ML algorithms to predict repeat severe
exacerbations (i.e. asthma-related emergency department (ED) visits or future
hospital admissions) for children with a prior asthma ED visit at a tertiary
care children's hospital.
  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from
the Children's Hospital of Eastern Ontario (CHEO) linked with environmental
pollutant exposure and neighbourhood marginalization information was used to
train various ML models. We used boosted trees (LGBM, XGB) and 3 open-source
large language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and
Llama-8b-UltraMedical). Models were tuned and calibrated then validated in a
second retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from
CHEO. Models were compared using the area under the curve (AUC) and F1 scores,
with SHAP values used to determine the most predictive features.
  The LGBM ML model performed best with the most predictive features in the
final AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage
acuity scale, medical complexity, food allergy, prior ED visits for non-asthma
respiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This
is a nontrivial improvement over the current decision rule which has F1=0.334.
While the most predictive features in the AIRE-KIDS_HOSP model included medical
complexity, prior asthma ED visit, average wait time in the ED, the pediatric
respiratory assessment measure score at triage and food allergy.

</details>


### [274] [On the Emergence of Induction Heads for In-Context Learning](https://arxiv.org/abs/2511.01033)
*Tiberiu Musat,Tiago Pimentel,Lorenzo Noci,Alessandro Stolfo,Mrinmaya Sachan,Thomas Hofmann*

Main category: cs.AI

TL;DR: 该论文研究了Transformer中归纳头的出现机制，揭示了权重矩阵的简单结构，证明了训练动态被限制在19维子空间中，其中仅3个维度负责归纳头的形成，并发现归纳头出现时间与输入上下文长度的平方成正比。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer中上下文学习能力的关键机制——归纳头，理解其权重矩阵结构和训练动态，以解释Transformer如何在不更新权重的情况下从输入上下文中学习新关联。

Method: 使用最小化上下文学习任务公式和修改的Transformer架构，理论分析归纳头的权重结构，证明训练动态被约束在19维子空间，并通过实证验证只有3个维度驱动归纳头的形成。

Result: 发现了归纳头权重矩阵的简单可解释结构，证明了训练动态被限制在19维子空间中，其中仅3个维度负责归纳头的出现，且归纳头出现时间与输入上下文长度的平方成正比。

Conclusion: Transformer中归纳头的形成遵循特定的低维训练动态，其出现时间受输入上下文长度的平方约束，这为理解Transformer的上下文学习机制提供了理论解释。

Abstract: Transformers have become the dominant architecture for natural language
processing. Part of their success is owed to a remarkable capability known as
in-context learning (ICL): they can acquire and apply novel associations solely
from their input context, without any updates to their weights. In this work,
we study the emergence of induction heads, a previously identified mechanism in
two-layer transformers that is particularly important for in-context learning.
We uncover a relatively simple and interpretable structure of the weight
matrices implementing the induction head. We theoretically explain the origin
of this structure using a minimal ICL task formulation and a modified
transformer architecture. We give a formal proof that the training dynamics
remain constrained to a 19-dimensional subspace of the parameter space.
Empirically, we validate this constraint while observing that only 3 dimensions
account for the emergence of an induction head. By further studying the
training dynamics inside this 3-dimensional subspace, we find that the time
until the emergence of an induction head follows a tight asymptotic bound that
is quadratic in the input context length.

</details>


### [275] [Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports](https://arxiv.org/abs/2511.01052)
*Yeawon Lee,Christopher C. Yang,Chia-Hsuan Chang,Grace Lu-Yao*

Main category: cs.AI

TL;DR: 该研究提出了两种知识启发方法（KEwLTM和KEwRAG），利用大语言模型从无标注病理报告中提取癌症分期规则，解决了传统NLP方法对大量标注数据的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 癌症分期对患者预后和治疗规划至关重要，但从未结构化的病理报告中提取病理TNM分期存在挑战。现有NLP和机器学习方法依赖大量标注数据，限制了可扩展性和适应性。

Method: KEwLTM使用迭代提示策略直接从无标注病理报告中推导分期规则，无需真实标签；KEwRAG采用检索增强生成的变体，从相关指南中预提取规则然后应用。使用TCGA数据集的乳腺癌病理报告评估T和N分期识别性能。

Result: 当零样本思维链推理有效时，KEwLTM表现优于KEwRAG；当零样本思维链推理效果较差时，KEwRAG表现更好。两种方法都提供了透明、可解释的界面。

Conclusion: 知识启发方法为自动化癌症分期提供了可扩展、高性能的解决方案，具有增强的可解释性，特别适用于标注数据有限的临床环境。

Abstract: Cancer staging is critical for patient prognosis and treatment planning, yet
extracting pathologic TNM staging from unstructured pathology reports poses a
persistent challenge. Existing natural language processing (NLP) and machine
learning (ML) strategies often depend on large annotated datasets, limiting
their scalability and adaptability. In this study, we introduce two Knowledge
Elicitation methods designed to overcome these limitations by enabling large
language models (LLMs) to induce and apply domain-specific rules for cancer
staging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses
an iterative prompting strategy to derive staging rules directly from
unannotated pathology reports, without requiring ground-truth labels. The
second, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),
employs a variation of RAG where rules are pre-extracted from relevant
guidelines in a single step and then applied, enhancing interpretability and
avoiding repeated retrieval overhead. We leverage the ability of LLMs to apply
broad knowledge learned during pre-training to new tasks. Using breast cancer
pathology reports from the TCGA dataset, we evaluate their performance in
identifying T and N stages, comparing them against various baseline approaches
on two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG
when Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG
achieves better performance when ZSCOT inference is less effective. Both
methods offer transparent, interpretable interfaces by making the induced rules
explicit. These findings highlight the promise of our Knowledge Elicitation
methods as scalable, high-performing solutions for automated cancer staging
with enhanced interpretability, particularly in clinical settings with limited
annotated data.

</details>


### [276] [Efficient Test-Time Retrieval Augmented Generation](https://arxiv.org/abs/2511.01059)
*Hailong Yin,Bin Zhu,Jingjing Chen,Chong-Wah Ngo*

Main category: cs.AI

TL;DR: ET2RAG是一个高效的测试时检索增强生成框架，通过检索相关文档、生成多样化候选响应，并使用多数投票机制选择最佳答案，在保持效率的同时提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：RAG可能引入不相关文档导致错误响应，集成方法缺乏外部知识且成本高昂，需要在开销和性能之间取得平衡。

Method: ET2RAG是一种无需训练的方法，首先检索最相关文档，通过管理响应长度高效生成多样化候选响应，然后计算候选响应相似度并采用多数投票机制选择最合适的响应作为最终输出。

Result: 实验结果表明，ET2RAG在开放域问答、食谱生成和图像描述三个任务上显著提升了性能。

Conclusion: ET2RAG框架通过部分生成和多数投票机制，在计算成本和性能之间实现了良好平衡，有效提升了LLM的准确性和效率。

Abstract: Although Large Language Models (LLMs) demonstrate significant capabilities,
their reliance on parametric knowledge often leads to inaccuracies. Retrieval
Augmented Generation (RAG) mitigates this by incorporating external knowledge,
but these methods may introduce irrelevant retrieved documents, leading to
inaccurate responses. While the integration methods filter out incorrect
answers from multiple responses, but lack external knowledge like RAG methods,
and their high costs require balancing overhead with performance gains. To
address these issues, we propose an Efficient Test-Time Retrieval-Augmented
Generation Framework named ET2RAG to improve the performance of LLMs while
maintaining efficiency. Specifically, ET2RAG is a training-free method, that
first retrieves the most relevant documents and augments the LLMs to
efficiently generate diverse candidate responses by managing response length.
Then we compute the similarity of candidate responses and employ a majority
voting mechanism to select the most suitable response as the final output. In
particular, we discover that partial generation is sufficient to capture the
key information necessary for consensus calculation, allowing us to effectively
perform majority voting without the need for fully generated responses. Thus,
we can reach a balance between computational cost and performance by managing
the response length for the number of retrieved documents for majority voting.
Experimental results demonstrate that ET2RAG significantly enhances performance
across three tasks, including open-domain question answering, recipe generation
and image captioning.

</details>


### [277] [Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models](https://arxiv.org/abs/2511.01149)
*Shuaidong Pan,Di Wu*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体架构，通过模块化任务分解和动态协作机制解决复杂任务执行问题，在任务成功率、分解效率和协作平衡等方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在复杂任务执行中任务分解和协作的局限性，提升多智能体系统的任务执行能力。

Method: 使用大语言模型将自然语言任务描述转换为统一语义表示，引入模块化分解机制将整体目标分解为层次化子任务，通过动态调度和路由机制实现智能体间的合理分工和实时协作，并设计约束解析和全局一致性机制确保子任务连贯连接和工作负载均衡。

Result: 实验验证了该架构在任务成功率、分解效率、子任务覆盖率和协作平衡等多个维度上的优越性，在整体性能和鲁棒性方面均优于现有方法，在任务复杂度和通信开销之间达到更好平衡。

Conclusion: 研究证明了语言驱动的任务分解和动态协作在多智能体系统中的有效性和可行性，为复杂环境下的任务执行提供了系统化解决方案。

Abstract: This paper addresses the limitations of a single agent in task decomposition
and collaboration during complex task execution, and proposes a multi-agent
architecture for modular task decomposition and dynamic collaboration based on
large language models. The method first converts natural language task
descriptions into unified semantic representations through a large language
model. On this basis, a modular decomposition mechanism is introduced to break
down the overall goal into multiple hierarchical sub-tasks. Then, dynamic
scheduling and routing mechanisms enable reasonable division of labor and
realtime collaboration among agents, allowing the system to adjust strategies
continuously according to environmental feedback, thus maintaining efficiency
and stability in complex tasks. Furthermore, a constraint parsing and global
consistency mechanism is designed to ensure coherent connections between
sub-tasks and balanced workload, preventing performance degradation caused by
redundant communication or uneven resource allocation. The experiments validate
the architecture across multiple dimensions, including task success rate,
decomposition efficiency, sub-task coverage, and collaboration balance. The
results show that the proposed method outperforms existing approaches in both
overall performance and robustness, achieving a better balance between task
complexity and communication overhead. In conclusion, this study demonstrates
the effectiveness and feasibility of language-driven task decomposition and
dynamic collaboration in multi-agent systems, providing a systematic solution
for task execution in complex environments.

</details>


### [278] [MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion](https://arxiv.org/abs/2511.01182)
*Cuong Van Duc,Thai Tran Quoc,Minh Nguyen Dinh Tuan,Tam Vu Duc,Son Nguyen Van,Hanh Nguyen Thi*

Main category: cs.AI

TL;DR: MiRAGE是一个用于数学领域学生误解检测的新框架，通过检索引导的多阶段推理和集成融合，在开放答案中识别学生误解。


<details>
  <summary>Details</summary>
Motivation: 检测学生开放答案中的误解是一个长期挑战，需要语义精确性和逻辑推理能力。

Method: MiRAGE采用三阶段方法：检索模块缩小候选误解池，推理模块使用思维链生成暴露逻辑不一致性，重排序模块通过对齐推理来优化预测。这些组件通过集成融合策略统一。

Result: 在数学数据集上，MiRAGE在1/3/5级别分别获得0.82/0.92/0.93的平均精度分数，持续优于各单独模块。

Conclusion: 通过将检索引导与多阶段推理相结合，MiRAGE减少了对大规模语言模型的依赖，同时为教育评估提供了可扩展且有效的解决方案。

Abstract: Detecting student misconceptions in open-ended responses is a longstanding
challenge, demanding semantic precision and logical reasoning. We propose
MiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning
and Ensemble Fusion, a novel framework for automated misconception detection in
mathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a
large candidate pool to a semantically relevant subset; (2) a Reasoning module
employs chain-of-thought generation to expose logical inconsistencies in
student solutions; and (3) a Reranking module refines predictions by aligning
them with the reasoning. These components are unified through an
ensemble-fusion strategy that enhances robustness and interpretability. On
mathematics datasets, MiRAGE achieves Mean Average Precision scores of
0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.
By coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces
dependence on large-scale language models while delivering a scalable and
effective solution for educational assessment.

</details>


### [279] [QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code](https://arxiv.org/abs/2511.01183)
*Hainan Fang,Yuanbo Wen,Jun Bi,Yihan Wang,Tonghui He,Yanlin Tang,Di Huang,Jiaming Guo,Rui Zhang,Qi Guo,Yunji Chen*

Main category: cs.AI

TL;DR: 本文介绍了NeuComBack基准数据集，用于IR到汇编的神经编译，并提出自进化提示优化方法，显著提升LLM生成汇编代码的功能正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 编译器开发复杂且昂贵，LLM为神经编译提供了新范式，但缺乏专用基准和评估方法，且LLM生成汇编的可靠性和性能需要系统提升。

Method: 提出NeuComBack基准数据集，定义神经编译工作流，并设计自进化提示优化方法，让LLM从自调试轨迹中提取见解迭代进化提示策略。

Result: 功能正确率在x86_64上从44%提升到64%，在aarch64上从36%提升到58%；在正确生成的x86_64程序中，87.5%超过clang-O3性能。

Conclusion: NeuComBack基准和自进化提示优化方法有效解决了神经编译的关键挑战，显著提升了LLM生成汇编代码的质量和性能。

Abstract: Compilers, while essential, are notoriously complex systems that demand
prohibitively expensive human expertise to develop and maintain. The recent
advancements in Large Language Models (LLMs) offer a compelling new paradigm:
Neural Compilation, which could potentially simplify compiler development for
new architectures and facilitate the discovery of innovative optimization
techniques. However, several critical obstacles impede its practical adoption.
Firstly, a significant lack of dedicated benchmarks and robust evaluation
methodologies hinders objective assessment and tracking of progress in the
field. Secondly, systematically enhancing the reliability and performance of
LLM-generated assembly remains a critical challenge. Addressing these
challenges, this paper introduces NeuComBack, a novel benchmark dataset
specifically designed for IR-to-assembly compilation. Leveraging this dataset,
we first define a foundational Neural Compilation workflow and conduct a
comprehensive evaluation of the capabilities of recent frontier LLMs on Neural
Compilation, establishing new performance baselines. We further propose a
self-evolving prompt optimization method that enables LLMs to iteratively
evolve their internal prompt strategies by extracting insights from prior
self-debugging traces, thereby enhancing their neural compilation capabilities.
Experiments demonstrate that our method significantly improves both the
functional correctness and the performance of LLM-generated assembly code.
Compared to baseline prompts, the functional correctness rates improved from
44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More
significantly, among the 16 correctly generated x86_64 programs using our
method, 14 (87.5%) surpassed clang-O3 performance.

</details>


### [280] [Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems](https://arxiv.org/abs/2511.01258)
*Chuyue Lou,M. Amine Atoui*

Main category: cs.AI

TL;DR: 提出了一种半监督开放集故障诊断（SOFD）框架，用于解决船舶机械系统中未知故障类型的诊断问题，通过可靠性子集构建和半监督学习来准确分类已知故障并检测未知样本。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的船舶机械故障诊断方法假设训练和测试集中的故障类别一致，但在实际应用中会出现训练时未见过的未知故障类型，导致这些方法失效，限制了其工业部署。

Method: 提出SOFD框架，包括：1）使用监督特征学习模型提取多层融合特征表示来构建可靠性子集；2）将标记训练集和伪标记测试子集输入半监督诊断模型，学习每类的判别特征。

Result: 在公共海事基准数据集上的实验结果表明，所提出的SOFD框架具有有效性和优越性。

Conclusion: SOFD框架能够增强和扩展深度学习模型在开放集故障诊断场景中的适用性，实现已知故障的准确分类和未知样本的有效检测。

Abstract: Recently, fault diagnosis methods for marine machinery systems based on deep
learning models have attracted considerable attention in the shipping industry.
Most existing studies assume fault classes are consistent and known between the
training and test datasets, and these methods perform well under controlled
environment. In practice, however, previously unseen or unknown fault types
(i.e., out-of-distribution or open-set observations not present during
training) can occur, causing such methods to fail and posing a significant
challenge to their widespread industrial deployment. To address this challenge,
this paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework
that enhances and extends the applicability of deep learning models in open-set
fault diagnosis scenarios. The framework includes a reliability subset
construction process, which uses a multi-layer fusion feature representation
extracted by a supervised feature learning model to select an unlabeled test
subset. The labeled training set and pseudo-labeled test subset are then fed
into a semi-supervised diagnosis model to learn discriminative features for
each class, enabling accurate classification of known faults and effective
detection of unknown samples. Experimental results on a public maritime
benchmark dataset demonstrate the effectiveness and superiority of the proposed
SOFD framework.

</details>


### [281] [llmSHAP: A Principled Approach to LLM Explainability](https://arxiv.org/abs/2511.01311)
*Filip Naudot,Tobias Sundqvist,Timotheus Kampik*

Main category: cs.AI

TL;DR: 该论文研究了在基于大语言模型（LLM）的随机决策支持系统中应用Shapley值进行特征归因的方法，分析了不同实现变体下Shapley值原则的满足情况，并探讨了LLM随机性对这些保证的影响。


<details>
  <summary>Details</summary>
Motivation: 特征归因方法使基于机器学习的推理可解释，但流行的Shapley值方法假设确定性推理，而LLM决策支持系统本质上是随机的，需要研究在这种随机环境下Shapley值原则的适用性。

Method: 将Shapley值应用于LLM决策支持系统的特征归因，分析不同实现变体下Shapley值原则的满足情况，并研究LLM随机性对这些保证的影响。

Result: 确定了在不同实现变体下可以或不能保证Shapley值原则满足的条件，揭示了LLM随机性对原则保证的影响，并突出了可解释推理速度、与精确Shapley值归因的一致性以及原则达成之间的权衡。

Conclusion: 在LLM随机决策支持系统中应用Shapley值进行特征归因时，需要仔细考虑实现变体选择，因为不同变体在原则保证、计算效率和准确性之间存在重要权衡。

Abstract: Feature attribution methods help make machine learning-based inference
explainable by determining how much one or several features have contributed to
a model's output. A particularly popular attribution method is based on the
Shapley value from cooperative game theory, a measure that guarantees the
satisfaction of several desirable principles, assuming deterministic inference.
We apply the Shapley value to feature attribution in large language model
(LLM)-based decision support systems, where inference is, by design, stochastic
(non-deterministic). We then demonstrate when we can and cannot guarantee
Shapley value principle satisfaction across different implementation variants
applied to LLM-based decision support, and analyze how the stochastic nature of
LLMs affects these guarantees. We also highlight trade-offs between explainable
inference speed, agreement with exact Shapley value attributions, and principle
attainment.

</details>


### [282] [OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance](https://arxiv.org/abs/2511.01320)
*Ziqi Wang,Hailiang Zhao,Yuhao Yang,Daojiang Hu,Cheng Bao,Mingyi Liu,Kai Di,Schahram Dustdar,Zhongjie Wang,Shuiguang Deng*

Main category: cs.AI

TL;DR: 提出OmniFuser多模态学习框架，通过融合视觉和传感器数据实现铣削刀具的预测性维护，在刀具状态分类和力信号预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中准确及时的刀具状态预测至关重要，非计划性刀具故障会导致质量下降和生产停机。需要可靠的服务导向型预测维护解决方案。

Method: 采用并行特征提取从高分辨率刀具图像和切削力信号中捕获互补的时空模式，使用无污染的跨模态融合机制分离共享和模态特定组件，并通过递归精化路径保留残差信息稳定融合动态。

Result: 在真实铣削数据集上的实验表明，OmniFuser在刀具状态分类和力信号预测方面持续优于最先进的基线方法。

Conclusion: OmniFuser为构建智能工业维护服务提供了可靠基础，学习到的表示可封装为可重用的维护服务模块。

Abstract: Accurate and timely prediction of tool conditions is critical for intelligent
manufacturing systems, where unplanned tool failures can lead to quality
degradation and production downtime. In modern industrial environments,
predictive maintenance is increasingly implemented as an intelligent service
that integrates sensing, analysis, and decision support across production
processes. To meet the demand for reliable and service-oriented operation, we
present OmniFuser, a multimodal learning framework for predictive maintenance
of milling tools that leverages both visual and sensor data. It performs
parallel feature extraction from high-resolution tool images and cutting-force
signals, capturing complementary spatiotemporal patterns across modalities. To
effectively integrate heterogeneous features, OmniFuser employs a
contamination-free cross-modal fusion mechanism that disentangles shared and
modality-specific components, allowing for efficient cross-modal interaction.
Furthermore, a recursive refinement pathway functions as an anchor mechanism,
consistently retaining residual information to stabilize fusion dynamics. The
learned representations can be encapsulated as reusable maintenance service
modules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)
and multi-step force signal forecasting. Experiments on real-world milling
datasets demonstrate that OmniFuser consistently outperforms state-of-the-art
baselines, providing a dependable foundation for building intelligent
industrial maintenance services.

</details>


### [283] [Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges](https://arxiv.org/abs/2511.01375)
*Hamin Koo,Minseon Kim,Jaehyung Kim*

Main category: cs.AI

TL;DR: AMIS是一个元优化框架，通过双层结构联合优化越狱提示和评分模板，在AdvBench和Claude模型上实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于优化的越狱方法要么依赖稀疏的二进制攻击成功率信号，要么依赖引入人为偏见的手工评分模板，存在局限性。

Method: 采用双层优化结构：内层循环使用固定评分模板通过细粒度反馈优化提示，外层循环使用ASR对齐分数优化评分模板，实现提示和模板的协同进化。

Result: 在AdvBench和JBB-Behaviors上评估，AMIS在Claude-3.5-Haiku上达到88.0% ASR，在Claude-4-Sonnet上达到100.0% ASR，显著优于现有基线方法。

Conclusion: AMIS框架通过联合优化提示和评分模板，能够生成更强的越狱提示和更准确的评分信号，有效提升了大型语言模型的安全性测试能力。

Abstract: Identifying the vulnerabilities of large language models (LLMs) is crucial
for improving their safety by addressing inherent weaknesses. Jailbreaks, in
which adversaries bypass safeguards with crafted input prompts, play a central
role in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.
Recent optimization-based jailbreak approaches iteratively refine attack
prompts by leveraging LLMs. However, they often rely heavily on either binary
attack success rate (ASR) signals, which are sparse, or manually crafted
scoring templates, which introduce human bias and uncertainty in the scoring
outcomes. To address these limitations, we introduce AMIS (Align to MISalign),
a meta-optimization framework that jointly evolves jailbreak prompts and
scoring templates through a bi-level structure. In the inner loop, prompts are
refined using fine-grained and dense feedback using a fixed scoring template.
In the outer loop, the template is optimized using an ASR alignment score,
gradually evolving to better reflect true attack outcomes across queries. This
co-optimization process yields progressively stronger jailbreak prompts and
more calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors
demonstrate that AMIS achieves state-of-the-art performance, including 88.0%
ASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming
existing baselines by substantial margins.

</details>


### [284] [Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering](https://arxiv.org/abs/2511.01396)
*Clément Yvernes,Emilie Devijver,Adèle H. Ribeiro,Marianne Clausel--Lesourd,Éric Gaussier*

Main category: cs.AI

TL;DR: 本文扩展了C-DAG框架，支持任意变量聚类，允许循环C-DAG表示，并扩展了d-分离和因果演算概念。


<details>
  <summary>Details</summary>
Motivation: 传统C-DAG框架要求聚类分区必须是无环的，这限制了其在某些场景下的应用。本文旨在放宽这一约束，使C-DAG能够处理任意聚类分区。

Method: 通过放宽分区可容许性约束，允许循环C-DAG表示，并扩展d-分离和因果演算概念到这一新设置中。

Result: 提出的演算在do-演算方面是健全且原子完备的：所有有效的集群级干预查询都可以使用我们的规则推导出来，每个规则对应一个原始的do-演算步骤。

Conclusion: 这项工作显著扩展了C-DAG的应用范围，使其能够在以前难以处理的场景中进行因果推理。

Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes
represent clusters of variables, and edges encode both cluster-level causal
relationships and dependencies arisen from unobserved confounding. C-DAGs
define an equivalence class of acyclic causal graphs that agree on
cluster-level relationships, enabling causal reasoning at a higher level of
abstraction. However, when the chosen clustering induces cycles in the
resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG
semantics. In this work, we extend the C-DAG framework to support arbitrary
variable clusterings by relaxing the partition admissibility constraint,
thereby allowing cyclic C-DAG representations. We extend the notions of
d-separation and causal calculus to this setting, significantly broadening the
scope of causal reasoning across clusters and enabling the application of
C-DAGs in previously intractable scenarios. Our calculus is both sound and
atomically complete with respect to the do-calculus: all valid interventional
queries at the cluster level can be derived using our rules, each corresponding
to a primitive do-calculus step.

</details>


### [285] [Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm](https://arxiv.org/abs/2511.01415)
*Amrapali Pednekar,Álvaro Garrido-Pérez,Yara Khaluf,Pieter Simoens*

Main category: cs.AI

TL;DR: 本研究从AI角度探索双任务范式中的时间处理干扰，使用简化的Overcooked环境训练DRL智能体，发现双任务智能体相对于单任务智能体显著高估时间，这与人类计时研究一致，但未发现明确的内部计时机制。


<details>
  <summary>Details</summary>
Motivation: 探索深度强化学习智能体在双任务范式中的时间处理行为，并与人类计时研究进行比较，以促进对生物系统和AI系统行为的更好理解。

Method: 使用简化的Overcooked环境实现双任务范式，包括单任务(T)和双任务(T+N)两种变体，分别训练两个DRL智能体，双任务包含时间产生和数字比较两个并发任务。

Result: 双任务(T+N)智能体相对于单任务(T)智能体在四个目标持续时间上都显著高估时间，与人类计时研究一致；对LSTM层神经动力学的初步分析未发现明确的专用计时器证据。

Conclusion: 需要进一步研究智能体的基础计时机制以理解观察到的行为模式，这是探索DRL涌现行为与生物系统行为相似性的初步尝试。

Abstract: This study explores the interference in temporal processing within a
dual-task paradigm from an artificial intelligence (AI) perspective. In this
context, the dual-task setup is implemented as a simplified version of the
Overcooked environment with two variations, single task (T) and dual task
(T+N). Both variations involve an embedded time production task, but the dual
task (T+N) additionally involves a concurrent number comparison task. Two deep
reinforcement learning (DRL) agents were separately trained for each of these
tasks. These agents exhibited emergent behavior consistent with human timing
research. Specifically, the dual task (T+N) agent exhibited significant
overproduction of time relative to its single task (T) counterpart. This result
was consistent across four target durations. Preliminary analysis of neural
dynamics in the agents' LSTM layers did not reveal any clear evidence of a
dedicated or intrinsic timer. Hence, further investigation is needed to better
understand the underlying time-keeping mechanisms of the agents and to provide
insights into the observed behavioral patterns. This study is a small step
towards exploring parallels between emergent DRL behavior and behavior observed
in biological systems in order to facilitate a better understanding of both.

</details>


### [286] [Robust Multimodal Sentiment Analysis via Double Information Bottleneck](https://arxiv.org/abs/2511.01444)
*Huiting Huang,Tieliang Gong,Kai He,Jialun Wu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: 本文提出了一种双信息瓶颈（DIB）策略来解决多模态情感分析中的两个关键问题：噪声污染的单模态数据学习不足和跨模态表示融合不充分。DIB通过最大化任务相关信息并丢弃冗余信息，生成统一紧凑的多模态表示，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法存在两个关键局限：对噪声污染的单模态数据学习不足，导致跨模态交互受损；多模态表示融合不充分，导致丢弃判别性单模态信息而保留冗余多模态信息。

Method: 提出双信息瓶颈（DIB）策略，在低秩Renyi熵函数框架下实现，包含两个关键模块：1）通过最大化任务相关信息并丢弃冗余信息，学习充分压缩的单模态数据表示；2）通过新颖的注意力瓶颈融合机制确保多模态表示的判别能力。

Result: 在CMU-MOSI、CMU-MOSEI、CH-SIMS和MVSA-Single数据集上的广泛实验验证了方法的有效性。在CMU-MOSI上达到47.4%的Acc-7准确率，在CH-SIMS上达到81.63%的F1分数，比次优基线提高1.19%。在噪声条件下，CMU-MOSI和CMU-MOSEI上的性能下降仅为0.36%和0.29%。

Conclusion: DIB策略能够有效过滤单模态数据中的噪声信息，同时捕捉模态间的互补性，生成强大统一的多模态表示，相比传统基于香农熵的方法具有更好的鲁棒性和计算可行性。

Abstract: Multimodal sentiment analysis has received significant attention across
diverse research domains. Despite advancements in algorithm design, existing
approaches suffer from two critical limitations: insufficient learning of
noise-contaminated unimodal data, leading to corrupted cross-modal
interactions, and inadequate fusion of multimodal representations, resulting in
discarding discriminative unimodal information while retaining multimodal
redundant information. To address these challenges, this paper proposes a
Double Information Bottleneck (DIB) strategy to obtain a powerful, unified
compact multimodal representation. Implemented within the framework of low-rank
Renyi's entropy functional, DIB offers enhanced robustness against diverse
noise sources and computational tractability for high-dimensional data, as
compared to the conventional Shannon entropy-based methods. The DIB comprises
two key modules: 1) learning a sufficient and compressed representation of
individual unimodal data by maximizing the task-relevant information and
discarding the superfluous information, and 2) ensuring the discriminative
ability of multimodal representation through a novel attention bottleneck
fusion mechanism. Consequently, DIB yields a multimodal representation that
effectively filters out noisy information from unimodal data while capturing
inter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,
CH-SIMS, and MVSA-Single validate the effectiveness of our method. The model
achieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score
on CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it
shows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI
respectively.

</details>


### [287] [TPS-Bench: Evaluating AI Agents' Tool Planning \& Scheduling Abilities in Compounding Tasks](https://arxiv.org/abs/2511.01527)
*Hanwen Xu,Xuyao Huang,Yuzhe Liu,Kai Yu,Zhijie Deng*

Main category: cs.AI

TL;DR: TPS-Bench是一个评估LLM智能体在需要工具规划和调度的复合问题解决能力的基准测试，包含200个基于数百个MCP工具的复合任务，强调任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 评估LLM智能体是否能够处理需要多样化工具组合的复合现实世界问题，这些任务不仅需要选择合适的工具，还需要战略性地调度执行顺序以确保效率。

Method: 构建TPS-Bench基准测试，收集200个基于数百个MCP工具的复合任务，每个任务由多个子任务组成，评估流行的闭源和开源LLM在工具规划和调度方面的表现，并探索使用强化学习改进调度效率。

Result: 大多数模型能够进行合理的工具规划，但在调度方面表现不同。GLM-4.5达到64.72%的任务完成率但执行时间长，GPT-4o优先并行工具调用但完成率仅45.08%。使用强化学习在Qwen3-1.7B上实现了14%的执行时间减少和6%的任务完成率提升。

Conclusion: LLM智能体在工具规划方面表现良好，但在调度效率方面存在差异，强化学习是改进调度效率而不影响性能的可行方法。

Abstract: Large language model (LLM) agents have exhibited strong problem-solving
competence across domains like research and coding. Yet, it remains
underexplored whether LLM agents can tackle compounding real-world problems
that require a diverse set of tools to complete. Given a broad, heterogeneous
tool repository, LLM agents must not only select appropriate tools based on
task planning analysis but also strategically schedule the execution order to
ensure efficiency. This paper introduces TPS-Bench to benchmark the ability of
LLM agents in solving such problems that demand Tool Planning and Scheduling.
TPS-Bench collects 200 compounding tasks of two difficulty levels, based on a
tool repository containing hundreds of model context protocol (MCP) tools. In
particular, each task is composed of multiple subtasks, such as web search, map
navigation, calendar checking, etc., and each subtask can be completed by a
basic tool. Our evaluation emphasizes both task completion rate and efficiency.
The empirical studies on popular closed-source and open-source LLMs indicate
that most models can perform reasonable tool planning, but differ in
scheduling. For example, GLM-4.5 achieves an outperforming task completion rate
of 64.72% with extensive sequential tool calls, hence suffering from
significantly long execution time. By contrast, GPT-4o prioritizes parallel
tool calls but achieves only a 45.08% completion rate. Considering
reinforcement learning (RL) can be a viable way to improve the scheduling
efficiency without compromising performance, we perform an initial study on
Qwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in
task completion rate based on rarely 100 RL training samples. Our code is
available https://github.com/hanwenxu1/mcp-agent.

</details>


### [288] [Analyzing Sustainability Messaging in Large-Scale Corporate Social Media](https://arxiv.org/abs/2511.01550)
*Ujjwal Sharma,Stevan Rudinac,Ana Mićković,Willemijn van Dolen,Marcel Worring*

Main category: cs.AI

TL;DR: 本文提出了一种多模态分析管道，利用视觉和语言基础模型分析企业社交媒体内容，重点关注可持续性相关沟通。通过LLM集成标注企业推文与17个可持续发展目标的主题对齐，结合视觉语言模型进行视觉可持续性沟通模式分析。


<details>
  <summary>Details</summary>
Motivation: 解决企业社交媒体内容在演化、多模态和模糊性方面的挑战，探索基础模型作为临时标注器的潜力，避免昂贵的任务特定标注需求。

Method: 使用LLM集成标注企业推文与SDG对齐，结合视觉语言模型和语义聚类进行视觉可持续性沟通模式分析。

Result: 揭示了不同行业在SDG参与度、时间趋势以及企业信息与ESG风险、消费者参与度之间的关联性差异。

Conclusion: 提出的自动标签生成和语义视觉聚类方法具有广泛适用性，为大规模社交媒体分析提供了灵活框架。

Abstract: In this work, we introduce a multimodal analysis pipeline that leverages
large foundation models in vision and language to analyze corporate social
media content, with a focus on sustainability-related communication. Addressing
the challenges of evolving, multimodal, and often ambiguous corporate messaging
on platforms such as X (formerly Twitter), we employ an ensemble of large
language models (LLMs) to annotate a large corpus of corporate tweets on their
topical alignment with the 17 Sustainable Development Goals (SDGs). This
approach avoids the need for costly, task-specific annotations and explores the
potential of such models as ad-hoc annotators for social media data that can
efficiently capture both explicit and implicit references to sustainability
themes in a scalable manner. Complementing this textual analysis, we utilize
vision-language models (VLMs), within a visual understanding framework that
uses semantic clusters to uncover patterns in visual sustainability
communication. This integrated approach reveals sectoral differences in SDG
engagement, temporal trends, and associations between corporate messaging,
environmental, social, governance (ESG) risks, and consumer engagement. Our
methods-automatic label generation and semantic visual clustering-are broadly
applicable to other domains and offer a flexible framework for large-scale
social media analysis.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [289] [Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience](https://arxiv.org/abs/2511.00026)
*Chaitanya Shinde,Divya Garikapati*

Main category: cs.RO

TL;DR: 本文综述了生成式人工智能在汽车行业的应用现状，重点分析了GAN和VAE等使能技术，探讨了在自动驾驶验证、零部件设计和人机交互等方面的机遇，同时指出了计算需求、偏见、知识产权和对抗鲁棒性等挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能正在成为汽车行业的变革力量，但现有综述主要关注感知或制造领域，本文旨在填补生成式AI在语音人机交互方面的研究空白，连接安全和用户体验视角。

Method: 采用文献综述方法，结合梅赛德斯-奔驰MBUX虚拟助手的案例研究，分析生成式AI在汽车领域的应用现状、机遇和挑战。

Result: 研究发现生成式AI能够通过合成数据生成加速自动驾驶验证、优化零部件设计，并通过个性化自适应界面增强人机交互体验。案例研究表明，基于生成式AI的语音系统比传统基于规则的助手提供更自然、主动和个性化的车内交互。

Conclusion: 生成式AI在汽车行业具有巨大潜力，但需要解决技术、伦理和安全挑战才能实现负责任部署。未来研究应致力于实现更安全、高效和以用户为中心的移动出行。

Abstract: Generative Artificial Intelligence is emerging as a transformative force in
the automotive industry, enabling novel applications across vehicle design,
manufacturing, autonomous driving, predictive maintenance, and in vehicle user
experience. This paper provides a comprehensive review of the current state of
GenAI in automotive, highlighting enabling technologies such as Generative
Adversarial Networks and Variational Autoencoders. Key opportunities include
accelerating autonomous driving validation through synthetic data generation,
optimizing component design, and enhancing human machine interaction via
personalized and adaptive interfaces. At the same time, the paper identifies
significant technical, ethical, and safety challenges, including computational
demands, bias, intellectual property concerns, and adversarial robustness, that
must be addressed for responsible deployment. A case study on Mercedes Benzs
MBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more
natural, proactive, and personalized in car interactions compared to legacy
rule based assistants. Through this review and case study, the paper outlines
both the promise and limitations of GenAI integration in the automotive sector
and presents directions for future research and development aimed at achieving
safer, more efficient, and user centric mobility. Unlike prior reviews that
focus solely on perception or manufacturing, this paper emphasizes generative
AI in voice based HMI, bridging safety and user experience perspectives.

</details>


### [290] [STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization](https://arxiv.org/abs/2511.00033)
*Diqi He,Xuehao Gao,Hao Li,Junwei Han,Dingwen Zhang*

Main category: cs.RO

TL;DR: STRIDER框架通过集成空间布局先验和动态任务反馈，系统优化智能体在零样本视觉语言导航任务中的决策空间，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在零样本视觉语言导航任务中缺乏结构化决策和对先前动作反馈的充分整合，导致导航鲁棒性不足。

Method: 提出STRIDER框架，包含结构化路径点生成器（约束动作空间）和任务对齐调节器（基于任务进度调整行为），确保导航过程中的语义对齐。

Result: 在R2R-CE和RxR-CE基准测试中，STRIDER显著优于现有最优方法，成功率从29%提升至35%，相对增益达20.7%。

Conclusion: 空间约束决策和反馈引导执行对于提高零样本视觉语言导航的保真度至关重要。

Abstract: The Zero-shot Vision-and-Language Navigation in Continuous Environments
(VLN-CE) task requires agents to navigate previously unseen 3D environments
using natural language instructions, without any scene-specific training. A
critical challenge in this setting lies in ensuring agents' actions align with
both spatial structure and task intent over long-horizon execution. Existing
methods often fail to achieve robust navigation due to a lack of structured
decision-making and insufficient integration of feedback from previous actions.
To address these challenges, we propose STRIDER (Instruction-Aligned Structural
Decision Space Optimization), a novel framework that systematically optimizes
the agent's decision space by integrating spatial layout priors and dynamic
task feedback. Our approach introduces two key innovations: 1) a Structured
Waypoint Generator that constrains the action space through spatial structure,
and 2) a Task-Alignment Regulator that adjusts behavior based on task progress,
ensuring semantic alignment throughout navigation. Extensive experiments on the
R2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms
strong SOTA across key metrics; in particular, it improves Success Rate (SR)
from 29% to 35%, a relative gain of 20.7%. Such results highlight the
importance of spatially constrained decision-making and feedback-guided
execution in improving navigation fidelity for zero-shot VLN-CE.

</details>


### [291] [Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World](https://arxiv.org/abs/2511.00041)
*Yingzhao Jian,Zhongan Wang,Yi Yang,Hehe Fan*

Main category: cs.RO

TL;DR: BiBo系统利用现成的视觉语言模型控制人形智能体，通过指令编译器和运动执行器实现开放环境中的多样化交互，无需大量数据收集。


<details>
  <summary>Details</summary>
Motivation: 解决人形智能体在开放环境中处理灵活多样交互的困难，避免昂贵的大规模数据集收集需求。

Method: 包含两个关键组件：1) 具身指令编译器，将高级用户指令翻译为低级原始命令；2) 基于扩散的运动执行器，从命令生成类人运动并适应环境物理反馈。

Result: 在开放环境中实现90.2%的交互任务成功率，文本引导运动执行精度比先前方法提高16.3%。

Conclusion: BiBo系统成功利用现成VLMs控制人形智能体，实现了高效、多样化的开放环境交互能力。

Abstract: Humanoid agents often struggle to handle flexible and diverse interactions in
open environments. A common solution is to collect massive datasets to train a
highly capable model, but this approach can be prohibitively expensive. In this
paper, we explore an alternative solution: empowering off-the-shelf
Vision-Language Models (VLMs, such as GPT-4) to control humanoid agents,
thereby leveraging their strong open-world generalization to mitigate the need
for extensive data collection. To this end, we present \textbf{BiBo}
(\textbf{B}uilding humano\textbf{I}d agent \textbf{B}y \textbf{O}ff-the-shelf
VLMs). It consists of two key components: (1) an \textbf{embodied instruction
compiler}, which enables the VLM to perceive the environment and precisely
translate high-level user instructions (e.g., {\small\itshape ``have a rest''})
into low-level primitive commands with control parameters (e.g.,
{\small\itshape ``sit casually, location: (1, 2), facing: 90$^\circ$''}); and
(2) a diffusion-based \textbf{motion executor}, which generates human-like
motions from these commands, while dynamically adapting to physical feedback
from the environment. In this way, BiBo is capable of handling not only basic
interactions but also diverse and complex motions. Experiments demonstrate that
BiBo achieves an interaction task success rate of 90.2\% in open environments,
and improves the precision of text-guided motion execution by 16.3\% over prior
methods. The code will be made publicly available.

</details>


### [292] [Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail](https://arxiv.org/abs/2511.00088)
*NVIDIA,:,Yan Wang,Wenjie Luo,Junjie Bai,Yulong Cao,Tong Che,Ke Chen,Yuxiao Chen,Jenna Diamond,Yifan Ding,Wenhao Ding,Liang Feng,Greg Heinrich,Jack Huang,Peter Karkus,Boyi Li,Pinyi Li,Tsung-Yi Lin,Dongran Liu,Ming-Yu Liu,Langechuan Liu,Zhijian Liu,Jason Lu,Yunxiang Mao,Pavlo Molchanov,Lindsey Pavao,Zhenghao Peng,Mike Ranzinger,Ed Schmerling,Shida Shen,Yunfei Shi,Sarah Tariq,Ran Tian,Tilman Wekel,Xinshuo Weng,Tianjun Xiao,Eric Yang,Xiaodong Yang,Yurong You,Xiaohui Zeng,Wenyuan Zhang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: AR1是一个视觉-语言-动作模型，通过因果链推理与轨迹规划相结合，提升自动驾驶在复杂场景中的决策能力，在闭环仿真中显著减少了偏离道路和近距离接触率。


<details>
  <summary>Details</summary>
Motivation: 解决端到端模仿学习在安全关键的长尾场景中性能脆弱的问题，这些场景中监督稀疏且因果理解有限。

Method: 采用三个关键创新：1) 构建因果链数据集；2) 模块化VLA架构结合预训练的视觉语言模型和扩散轨迹解码器；3) 多阶段训练策略，包括监督微调和强化学习。

Result: 在挑战性案例中规划准确率提升12%，偏离道路率减少35%，近距离接触率减少25%，推理质量提升45%，推理-行动一致性提升37%。

Conclusion: AR1通过将可解释推理与精确控制相结合，展示了实现L4级自动驾驶的可行路径。

Abstract: End-to-end architectures trained via imitation learning have advanced
autonomous driving by scaling model size and data, yet performance remains
brittle in safety-critical long-tail scenarios where supervision is sparse and
causal understanding is limited. To address this, we introduce Alpamayo-R1
(AR1), a vision-language-action model (VLA) that integrates Chain of Causation
reasoning with trajectory planning to enhance decision-making in complex
driving scenarios. Our approach features three key innovations: (1) the Chain
of Causation (CoC) dataset, built through a hybrid auto-labeling and
human-in-the-loop pipeline producing decision-grounded, causally linked
reasoning traces aligned with driving behaviors; (2) a modular VLA architecture
combining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI
applications, with a diffusion-based trajectory decoder that generates
dynamically feasible plans in real time; (3) a multi-stage training strategy
using supervised fine-tuning to elicit reasoning and reinforcement learning
(RL) to optimize reasoning quality via large reasoning model feedback and
enforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%
improvement in planning accuracy on challenging cases compared to a
trajectory-only baseline, with a 35% reduction in off-road rate and 25%
reduction in close encounter rate in closed-loop simulation. RL post-training
improves reasoning quality by 45% as measured by a large reasoning model critic
and reasoning-action consistency by 37%. Model scaling from 0.5B to 7B
parameters shows consistent improvements. On-vehicle road tests confirm
real-time performance (99 ms latency) and successful urban deployment. By
bridging interpretable reasoning with precise control, AR1 demonstrates a
practical path towards Level 4 autonomous driving. We plan to release AR1
models and a subset of the CoC in a future update.

</details>


### [293] [Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments](https://arxiv.org/abs/2511.00094)
*Angelos Alexopoulos,Agorakis Bompotas,Nikitas Rigas Kalogeropoulos,Panagiotis Kechagias,Athanasios P. Kalogeras,Christos Alexakos*

Main category: cs.RO

TL;DR: 提出了一种利用数字孪生技术实现机器人控制器自主动态重构的新框架，通过在虚拟环境中模拟和优化运动轨迹来应对现实世界变化，实现快速可靠的适应。


<details>
  <summary>Details</summary>
Motivation: 传统控制系统在动态环境（如智慧城市和精准农业）中难以快速适应不断变化的地形和环境条件，导致效率低下或操作失败。

Method: 利用机器人操作环境的虚拟副本，在数字孪生中重新计算路径和控制参数，并将更新后的代码部署到物理机器人上。

Result: 该方法能够确保快速可靠的适应，无需人工干预，提高了机器人在动态环境中的自主性。

Conclusion: 这项工作推进了数字孪生在机器人技术中的集成，为增强智能动态环境中的自主性提供了可扩展的解决方案。

Abstract: Robotic systems have become integral to smart environments, enabling
applications ranging from urban surveillance and automated agriculture to
industrial automation. However, their effective operation in dynamic settings -
such as smart cities and precision farming - is challenged by continuously
evolving topographies and environmental conditions. Traditional control systems
often struggle to adapt quickly, leading to inefficiencies or operational
failures. To address this limitation, we propose a novel framework for
autonomous and dynamic reconfiguration of robotic controllers using Digital
Twin technology. Our approach leverages a virtual replica of the robot's
operational environment to simulate and optimize movement trajectories in
response to real-world changes. By recalculating paths and control parameters
in the Digital Twin and deploying the updated code to the physical robot, our
method ensures rapid and reliable adaptation without manual intervention. This
work advances the integration of Digital Twins in robotics, offering a scalable
solution for enhancing autonomy in smart, dynamic environments.

</details>


### [294] [Real-DRL: Teach and Learn in Reality](https://arxiv.org/abs/2511.00112)
*Yanbing Mao,Yihao Cai,Lui Sha*

Main category: cs.RO

TL;DR: Real-DRL框架通过DRL-Student、PHY-Teacher和Trigger三个交互组件，在真实物理系统中实现安全优先的深度强化学习，解决未知风险和Sim2Real差距问题。


<details>
  <summary>Details</summary>
Motivation: 解决安全关键自主系统中深度强化学习面临的安全挑战，特别是未知风险和仿真到现实的差距问题，确保在真实物理系统中安全地学习高性能策略。

Method: 采用三组件交互框架：DRL-Student实现双自学习和教中学范式及实时安全感知批量采样；PHY-Teacher基于物理模型专注于安全关键功能；Trigger管理两者交互。

Result: 在真实四足机器人、NVIDIA Isaac Gym中的四足机器人和倒立摆系统上的实验验证了框架的有效性，实现了保证安全、自动分层学习和解决学习经验不平衡问题。

Conclusion: Real-DRL框架能够有效应对安全关键自主系统中的安全挑战，确保安全优先的学习过程，并在多种真实和仿真系统中展现出优越性能。

Abstract: This paper introduces the Real-DRL framework for safety-critical autonomous
systems, enabling runtime learning of a deep reinforcement learning (DRL) agent
to develop safe and high-performance action policies in real plants (i.e., real
physical systems to be controlled), while prioritizing safety! The Real-DRL
consists of three interactive components: a DRL-Student, a PHY-Teacher, and a
Trigger. The DRL-Student is a DRL agent that innovates in the dual
self-learning and teaching-to-learn paradigm and the real-time safety-informed
batch sampling. On the other hand, PHY-Teacher is a physics-model-based design
of action policies that focuses solely on safety-critical functions.
PHY-Teacher is novel in its real-time patch for two key missions: i) fostering
the teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of
real plants. The Trigger manages the interaction between the DRL-Student and
the PHY-Teacher. Powered by the three interactive components, the Real-DRL can
effectively address safety challenges that arise from the unknown unknowns and
the Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,
ii) automatic hierarchy learning (i.e., safety-first learning and then
high-performance learning), and iii) safety-informed batch sampling to address
the learning experience imbalance caused by corner cases. Experiments with a
real quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole
system, along with comparisons and ablation studies, demonstrate the Real-DRL's
effectiveness and unique features.

</details>


### [295] [End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection](https://arxiv.org/abs/2511.00139)
*Yu Cui,Yujian Zhang,Lina Tao,Yang Li,Xinyu Yi,Zhibin Li*

Main category: cs.RO

TL;DR: 提出了一种共享自主框架，通过VR遥操作与自主手部控制相结合，高效收集高质量协调臂手演示数据，训练增强的VLA策略，在多样化物体上达到90%成功率。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧操作中高质量训练数据稀缺的问题，现有方法存在人工遥操作负担重或自动规划动作不自然的限制。

Method: 共享自主框架：人类通过VR控制臂部宏观运动，自主DexGrasp-VLA策略基于触觉和视觉反馈控制手部精细动作；训练带臂手特征增强模块的端到端VLA策略；通过纠正遥操作实现持续改进。

Result: 以最少人力生成高质量数据，在多样化物体（包括未见实例）上达到90%成功率，验证了系统开发灵巧操作能力的有效性。

Conclusion: 该框架显著降低了认知负担，实现了高效的高质量协调臂手演示数据收集，为灵巧操作提供了有效的解决方案。

Abstract: Achieving human-like dexterous manipulation remains a major challenge for
general-purpose robots. While Vision-Language-Action (VLA) models show
potential in learning skills from demonstrations, their scalability is limited
by scarce high-quality training data. Existing data collection methods face
inherent constraints: manual teleoperation overloads human operators, while
automated planning often produces unnatural motions. We propose a Shared
Autonomy framework that divides control between macro and micro motions. A
human operator guides the robot's arm pose through intuitive VR teleoperation,
while an autonomous DexGrasp-VLA policy handles fine-grained hand control using
real-time tactile and visual feedback. This division significantly reduces
cognitive load and enables efficient collection of high-quality coordinated
arm-hand demonstrations. Using this data, we train an end-to-end VLA policy
enhanced with our novel Arm-Hand Feature Enhancement module, which captures
both distinct and shared representations of macro and micro movements for more
natural coordination. Our Corrective Teleoperation system enables continuous
policy improvement through human-in-the-loop failure recovery. Experiments
demonstrate that our framework generates high-quality data with minimal
manpower and achieves a 90% success rate across diverse objects, including
unseen instances. Comprehensive evaluations validate the system's effectiveness
in developing dexterous manipulation capabilities.

</details>


### [296] [EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations](https://arxiv.org/abs/2511.00153)
*Justin Yu,Yide Shentu,Di Wu,Pieter Abbeel,Ken Goldberg,Philipp Wu*

Main category: cs.RO

TL;DR: EgoMI框架通过捕获同步的末端执行器和主动头部轨迹来解决模仿学习中的人机体现差距问题，引入记忆增强策略处理快速视角变化，在双手机器人上验证了协调手眼学习的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习中因人类主动协调头部和手部运动、动态调整视角而产生的体现差距问题，这种分布偏移会降低策略性能。

Method: 提出EgoMI框架捕获同步的末端执行器和主动头部轨迹数据，引入记忆增强策略选择性整合历史观察来处理快速视角变化。

Result: 在配备驱动相机头的双手机器人上测试，显示具有显式头部运动建模的策略始终优于基线方法。

Conclusion: EgoMI通过协调手眼学习有效弥合了人机体现差距，为半人形机器人实现了稳健的模仿学习。

Abstract: Imitation learning from human demonstrations offers a promising approach for
robot skill acquisition, but egocentric human data introduces fundamental
challenges due to the embodiment gap. During manipulation, humans actively
coordinate head and hand movements, continuously reposition their viewpoint and
use pre-action visual fixation search strategies to locate relevant objects.
These behaviors create dynamic, task-driven head motions that static robot
sensing systems cannot replicate, leading to a significant distribution shift
that degrades policy performance. We present EgoMI (Egocentric Manipulation
Interface), a framework that captures synchronized end-effector and active head
trajectories during manipulation tasks, resulting in data that can be
retargeted to compatible semi-humanoid robot embodiments. To handle rapid and
wide-spanning head viewpoint changes, we introduce a memory-augmented policy
that selectively incorporates historical observations. We evaluate our approach
on a bimanual robot equipped with an actuated camera head and find that
policies with explicit head-motion modeling consistently outperform baseline
methods. Results suggest that coordinated hand-eye learning with EgoMI
effectively bridges the human-robot embodiment gap for robust imitation
learning on semi-humanoid embodiments. Project page:
https://egocentric-manipulation-interface.github.io

</details>


### [297] [Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach](https://arxiv.org/abs/2511.00193)
*Faranak Akbarifar,Nooshin Maghsoodi,Sean P Dukelow,Stephen Scott,Parvin Mousavi*

Main category: cs.RO

TL;DR: 使用时间序列基础模型预测未记录的Kinarm机器人VGR测试试验，仅需8-16个实际试验加上预测试验即可达到24-28个完整试验的可靠性，显著缩短评估时间。


<details>
  <summary>Details</summary>
Motivation: Kinarm机器人视觉引导伸手测试需要40-64次伸手，造成时间和疲劳负担，需要寻找方法减少实际试验次数同时保持运动学参数的可靠性。

Method: 分析461名中风和599名对照参与者的VGR速度信号，保留前8或16次试验，使用ARIMA、MOMENT和Chronos模型预测合成试验，重新计算运动学特征并与完整参考比较。

Result: Chronos模型预测仅需8个实际试验加预测试验即可恢复ICC≥0.90的可靠性，相当于24-28个完整试验的效果，显著缩短评估时间。

Conclusion: 基础模型预测可以大幅缩短Kinarm VGR评估时间，对最严重中风患者可将4-5分钟测试缩短至约1分钟，同时保持运动学精度，为中风后运动障碍评估提供高效机器人评估范式。

Abstract: Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive
kinematic biomarkers but requires 40-64 reaches, imposing time and fatigue
burdens. We evaluate whether time-series foundation models can replace
unrecorded trials from an early subset of reaches while preserving the
reliability of standard Kinarm parameters.
  Methods: We analyzed VGR speed signals from 461 stroke and 599 control
participants across 4- and 8-target reaching protocols. We withheld all but the
first 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,
fine-tuned on 70 percent of subjects, to forecast synthetic trials. We
recomputed four kinematic features of reaching (reaction time, movement time,
posture speed, maximum speed) on combined recorded plus forecasted trials and
compared them to full-length references using ICC(2,1).
  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only
8 recorded trials plus forecasts, matching the reliability of 24-28 recorded
reaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA
improvements were minimal. Across cohorts and protocols, synthetic trials
replaced reaches without materially compromising feature reliability.
  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR
assessment time. For the most impaired stroke survivors, sessions drop from 4-5
minutes to about 1 minute while preserving kinematic precision. This
forecast-augmented paradigm promises efficient robotic evaluations for
assessing motor impairments following stroke.

</details>


### [298] [Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial](https://arxiv.org/abs/2511.00259)
*Andria J. Farrens,Luis Garcia-Fernandez,Raymond Diaz Rojas,Jillian Obeso Estrada,Dylan Reinsdorf,Vicky Chan,Disha Gupta,Joel Perry,Eric Wolbrecht,An Do,Steven C. Cramer,David J. Reinkensmeyer*

Main category: cs.RO

TL;DR: 本研究测试了两种本体感觉定制的机器人训练方法（Propriopixel训练和虚拟辅助训练）对中风幸存者手部功能和神经处理的影响。结果显示，对于存在本体感觉缺陷的参与者，这两种训练方法比标准训练更能改善手部功能，且本体感觉改善与手部功能改善相关。


<details>
  <summary>Details</summary>
Motivation: 精准康复旨在通过定制化运动训练来改善康复效果。本研究旨在验证本体感觉定制的机器人训练是否能改善中风幸存者的手部功能和神经处理能力。

Method: 采用随机对照试验设计，46名慢性中风幸存者完成9次2小时的训练，分为标准训练、Propriopixel训练（使用机器人辅助的游戏化运动增强本体感觉处理）和虚拟辅助训练（减少机器人辅助以增加对自我生成反馈的依赖）。

Result: 在有本体感觉缺陷的参与者中，Propriopixel训练（盒子与积木测试：7±4.2，p=0.002）和虚拟辅助训练（4.5±4.4，p=0.068）比标准训练（0.8±2.3积木）产生更大的手部功能改善。本体感觉改善与手部功能改善相关。定制训练增强了神经对本体感觉线索的敏感性，通过新型EEG生物标志物——本体感觉相关负变电位得到证实。

Conclusion: 这些发现支持本体感觉定制训练作为精准神经康复的途径。

Abstract: Precision rehabilitation aims to tailor movement training to improve
outcomes. We tested whether proprioceptively-tailored robotic training improves
hand function and neural processing in stroke survivors. Using a robotic finger
exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel
Training, which uses robot-facilitated, gamified movements to enhance
proprioceptive processing, and Virtual Assistance Training, which reduces
robotic aid to increase reliance on self-generated feedback. In a randomized
controlled trial, forty-six chronic stroke survivors completed nine 2-hour
sessions of Standard, Propriopixel or Virtual training. Among participants with
proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)
and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand
function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with
improvements in hand function. Tailored training enhanced neural sensitivity to
proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive
Contingent Negative Variation. These findings support proprioceptively-tailored
training as a pathway to precision neurorehabilitation.

</details>


### [299] [FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications](https://arxiv.org/abs/2511.00306)
*Baoshan Song,Ruijie Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文揭示了滑动窗口因子图优化(SW-FGO)与卡尔曼滤波器变体(KFV)之间的理论联系，提出了递归FGO(Re-FGO)框架，在特定条件下Re-FGO可精确再生为EKF/IEKF/REKF/RIEKF，同时展示了SW-FGO在非线性、非高斯场景下的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然SW-FGO在导航研究中因对非高斯噪声和非线性测量模型的鲁棒近似而受到关注，但SW-FGO与EKF之间的理论关系仍不明确，需要建立两者之间的理论连接。

Method: 提出了连接SW-FGO和KFV的必要公平条件，基于这些条件构建了递归FGO(Re-FGO)框架，在显式条件下(马尔可夫假设、高斯噪声与L2损失、单状态窗口)验证Re-FGO可精确再生为各种KFV。

Result: 在特定条件下，Re-FGO可精确再生为EKF/IEKF/REKF/RIEKF，而SW-FGO在非线性、非高斯场景下表现出可衡量的优势，同时计算成本可预测。

Conclusion: 澄清了SW-FGO与KFV之间的联系，突出了SW-FGO在实际应用中的独特优势，特别是在数值估计和深度学习集成方面。

Abstract: Sliding window-factor graph optimization (SW-FGO) has gained more and more
attention in navigation research due to its robust approximation to
non-Gaussian noises and nonlinearity of measuring models. There are lots of
works focusing on its application performance compared to extended Kalman
filter (EKF) but there is still a myth at the theoretical relationship between
the SW-FGO and EKF. In this paper, we find the necessarily fair condition to
connect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF
(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the
conditions, we propose a recursive FGO (Re-FGO) framework to represent KFV
under SW-FGO formulation. Under explicit conditions (Markov assumption,
Gaussian noise with L2 loss, and a one-state window), Re-FGO regenerates
exactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in
nonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after
clarifying the connection between them, we highlight the unique advantages of
SW-FGO in practical phases, especially on numerical estimation and deep
learning integration. The code and data used in this work is open sourced at
https://github.com/Baoshan-Song/KFV-FGO-Comparison.

</details>


### [300] [SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping](https://arxiv.org/abs/2511.00392)
*Lingpeng Chen,Jiakun Tang,Apple Pui-Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: SonarSweep是一种端到端深度学习框架，通过改进平面扫描算法实现声纳和视觉数据的跨模态融合，在视觉退化的水下环境中生成密集准确的深度图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决视觉退化水下环境中3D重建的挑战，单模态方法（视觉方法因能见度差而失效，声纳方法因高程模糊和低分辨率而受限）不足，现有融合技术依赖启发式和有缺陷的几何假设，导致显著伪影和无法建模复杂场景。

Method: 提出SonarSweep框架，基于平面扫描算法原理，实现声纳和视觉数据的跨模态融合，采用端到端深度学习架构。

Result: 在高保真仿真和真实环境中的广泛实验表明，SonarSweep能持续生成密集准确的深度图，在挑战性条件下（特别是高浊度环境）显著优于最先进方法。

Conclusion: SonarSweep克服了现有方法的局限性，将公开发布代码和首个包含同步立体相机和声纳数据的新数据集，以促进进一步研究。

Abstract: Accurate 3D reconstruction in visually-degraded underwater environments
remains a formidable challenge. Single-modality approaches are insufficient:
vision-based methods fail due to poor visibility and geometric constraints,
while sonar is crippled by inherent elevation ambiguity and low resolution.
Consequently, prior fusion technique relies on heuristics and flawed geometric
assumptions, leading to significant artifacts and an inability to model complex
scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep
learning framework that overcomes these limitations by adapting the principled
plane sweep algorithm for cross-modal fusion between sonar and visual data.
Extensive experiments in both high-fidelity simulation and real-world
environments demonstrate that SonarSweep consistently generates dense and
accurate depth maps, significantly outperforming state-of-the-art methods
across challenging conditions, particularly in high turbidity. To foster
further research, we will publicly release our code and a novel dataset
featuring synchronized stereo-camera and sonar data, the first of its kind.

</details>


### [301] [Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory](https://arxiv.org/abs/2511.00412)
*John A. Christian,Michael R. Walker II,Wyatt Bridgman,Michael J. Sparapany*

Main category: cs.RO

TL;DR: 本文提出了一种基于经典龙格-库塔积分方法的新型圆锥补偿算法，用于陀螺仪测量积分中的旋转补偿。


<details>
  <summary>Details</summary>
Motivation: 现代导航系统中，陀螺仪积分需要圆锥补偿来考虑传感器在积分期间的旋转。现有算法虽多，但需要更系统的方法来生成高阶补偿算法。

Method: 直接从经典龙格-库塔积分方法构建圆锥校正算法，展示了如何生成高阶算法，并证明简单情况下可退化为最流行的圆锥算法之一。

Result: 成功开发了基于龙格-库塔方法的新型圆锥补偿算法类别，提供了生成高阶算法的清晰流程。

Conclusion: 该方法为圆锥补偿算法提供了系统化的生成框架，能够有效处理陀螺仪积分中的旋转效应，且与现有流行算法兼容。

Abstract: The integration of gyroscope measurements is an essential task for most
navigation systems. Modern vehicles typically use strapdown systems, such that
gyro integration requires coning compensation to account for the sensor's
rotation during the integration. Many coning compensation algorithms have been
developed and a few are reviewed. This work introduces a new class of coning
correction algorithm built directly from the classical Runge-Kutta integration
routines. A simple case is shown to collapse to one of the most popular coning
algorithms and a clear procedure for generating higher-order algorithms is
presented.

</details>


### [302] [Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU](https://arxiv.org/abs/2511.00492)
*Simon Giel,James Hurrell,Shreya Santra,Ashutosh Mishra,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 开发了一种用于月球资源利用的铲斗滚筒工具，作为日本Moonshot计划中模块化机器人系统MoonBot的一部分，通过3D打印原型测试验证了其挖掘效率。


<details>
  <summary>Details</summary>
Motivation: 月球原位资源利用是实现可持续月球探索的关键技术，挖掘月壤是获取月球资源的第一步。

Method: 使用PLA材料3D打印制造原型铲斗滚筒，通过一系列沙盒测试评估其效率。

Result: 工具重4.8kg，容积14.06L，连续挖掘速率为777.54kg/h，归一化能耗0.022Wh/kg；批量操作时挖掘速率172.02kg/h，归一化能耗0.86Wh/kg。

Conclusion: 成功实现了概念验证，该工具与模块化MoonBot机器人平台兼容，可实现灵活高效的任务规划，未来可集成传感器和自主控制系统以改进挖掘过程。

Abstract: In-Situ Resource Utilization (ISRU) is one of the key technologies for
enabling sustainable access to the Moon. The ability to excavate lunar regolith
is the first step in making lunar resources accessible and usable. This work
presents the development of a bucket drum for the modular robotic system
MoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made
of PLA was manufactured to evaluate its efficiency through a series of sandbox
tests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is
capable of continuous excavation at a rate of 777.54 kg/h with a normalized
energy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is
172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of
excavated material. The obtained results demonstrate the successful
implementation of the concept. A key advantage of the developed tool is its
compatibility with the modular MoonBot robotic platform, which enables flexible
and efficient mission planning. Further improvements may include the
integration of sensors and an autonomous control system to enhance the
excavation process.

</details>


### [303] [Descriptive Model-based Learning and Control for Bipedal Locomotion](https://arxiv.org/abs/2511.00512)
*Suraj Kumar,Andy Ruina*

Main category: cs.RO

TL;DR: 提出了一种新的双足平衡控制方法，避免将低维模型强加于完整模型，而是使用描述性模型来维持平衡，让剩余自由度在高维空间中自由演化，从而实现高效的人形行走步态和更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统双足机器人平衡控制方法依赖低维模型进行运动规划和反应控制，限制了完整机器人的行为，导致行走模式效率低下且膝盖弯曲。观察到双足平衡本质上是低维的，可以用简单的状态和动作描述符在低维状态空间中有效描述。

Method: 提出控制框架使用具有维持平衡所需最小自由度的描述性模型，允许剩余自由度在高维空间中自由演化，避免为完整模型规定低维模型。

Result: 实现了高效的人形行走步态和改善的鲁棒性。

Conclusion: 通过仅约束低维状态空间中的投影，让机器人的运动在其高维状态空间中自由演化，可以产生更自然和高效的双足行走模式。

Abstract: Bipedal balance is challenging due to its multi-phase, hybrid nature and
high-dimensional state space. Traditional balance control approaches for
bipedal robots rely on low-dimensional models for locomotion planning and
reactive control, constraining the full robot to behave like these simplified
models. This involves tracking preset reference paths for the Center of Mass
and upper body obtained through low-dimensional models, often resulting in
inefficient walking patterns with bent knees. However, we observe that bipedal
balance is inherently low-dimensional and can be effectively described with
simple state and action descriptors in a low-dimensional state space. This
allows the robot's motion to evolve freely in its high-dimensional state space,
only constraining its projection in the low-dimensional state space. In this
work, we propose a novel control approach that avoids prescribing a
low-dimensional model to the full model. Instead, our control framework uses a
descriptive model with the minimum degrees of freedom necessary to maintain
balance, allowing the remaining degrees of freedom to evolve freely in the
high-dimensional space. This results in an efficient human-like walking gait
and improved robustness.

</details>


### [304] [Adaptive and Multi-object Grasping via Deformable Origami Modules](https://arxiv.org/abs/2511.00516)
*Peiyi Wang,Paul A. M. Lefeuvre,Shangwei Zou,Zhenwei Ni,Daniela Rus,Cecilia Laschi*

Main category: cs.RO

TL;DR: 提出了一种基于折纸结构的混合软体机器人抓手，具有被动变形能力，可实现恒定力/扭矩输出和多物体同时抓取功能。


<details>
  <summary>Details</summary>
Motivation: 现有软体机器人抓手通常依赖笨重的执行器、复杂控制策略或先进触觉传感来实现稳定抓取，需要更简单高效的解决方案。

Method: 采用平行折纸模块构成的多指混合抓手，每个手指由1-DoF执行器驱动，具有被动形状适应性和稳定抓取力，无需主动传感或反馈控制。

Result: 实现了被动形状适应性、稳定抓取力，并展示了同时多物体抓取能力，可独立拾取、运输和放置不同形状大小的堆叠物体。

Conclusion: 折纸基柔性结构作为可扩展模块，在家庭和工业拾放场景中具有自适应、稳定和高效多物体操作的潜力。

Abstract: Soft robotics gripper have shown great promise in handling fragile and
geometrically complex objects. However, most existing solutions rely on bulky
actuators, complex control strategies, or advanced tactile sensing to achieve
stable and reliable grasping performance. In this work, we present a
multi-finger hybrid gripper featuring passively deformable origami modules that
generate constant force and torque output. Each finger composed of parallel
origami modules is driven by a 1-DoF actuator mechanism, enabling passive shape
adaptability and stable grasping force without active sensing or feedback
control. More importantly, we demonstrate an interesting capability in
simultaneous multi-object grasping, which allows stacked objects of varied
shape and size to be picked, transported and placed independently at different
states, significantly improving manipulation efficiency compared to
single-object grasping. These results highlight the potential of origami-based
compliant structures as scalable modules for adaptive, stable and efficient
multi-object manipulation in domestic and industrial pick-and-place scenarios.

</details>


### [305] [Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles](https://arxiv.org/abs/2511.00635)
*Hyungtae Lim,Daebeom Kim,Hyun Myung*

Main category: cs.RO

TL;DR: 提出了一种名为Multi-Mapcher的新型多会话SLAM框架，通过大规模地图到地图配准实现会话间初始对齐，替代传统依赖闭环检测的方法，显著提升了异构LiDAR传感器场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MSS方法过度依赖闭环检测，但由于不同会话使用的LiDAR传感器在点云密度和视场角方面存在差异，闭环检测性能可能下降。

Method: 采用大规模地图到地图配准进行会话间初始对齐，然后基于半径搜索找到会话间闭环，最后使用基于锚节点的鲁棒位姿图优化构建一致的全局地图。

Result: 实验表明该方法在各种LiDAR传感器场景下表现出更好的MSS性能，且比最先进方法更快。

Conclusion: 提出的Multi-Mapcher框架通过地图到地图配准成功解决了异构LiDAR传感器多会话SLAM的挑战，实现了更鲁棒和高效的性能。

Abstract: As various 3D light detection and ranging (LiDAR) sensors have been
introduced to the market, research on multi-session simultaneous localization
and mapping (MSS) using heterogeneous LiDAR sensors has been actively
conducted. Existing MSS methods mostly rely on loop closure detection for
inter-session alignment; however, the performance of loop closure detection can
be potentially degraded owing to the differences in the density and field of
view (FoV) of the sensors used in different sessions. In this study, we
challenge the existing paradigm that relies heavily on loop detection modules
and propose a novel MSS framework, called Multi-Mapcher, that employs
large-scale map-to-map registration to perform inter-session initial alignment,
which is commonly assumed to be infeasible, by leveraging outlier-robust 3D
point cloud registration. Next, after finding inter-session loops by radius
search based on the assumption that the inter-session initial alignment is
sufficiently precise, anchor node-based robust pose graph optimization is
employed to build a consistent global map. As demonstrated in our experiments,
our approach shows substantially better MSS performance for various LiDAR
sensors used to capture the sessions and is faster than state-of-the-art
approaches. Our code is available at
https://github.com/url-kaist/multi-mapcher.

</details>


### [306] [When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage](https://arxiv.org/abs/2511.00783)
*Jingzehua Xu,Weihang Zhang,Yangyang Li,Hongmiaoyi Zhang,Guanwen Xie,Jiwei Tang,Shuai Zhang,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于语义引导的模糊控制框架，将大语言模型与可解释控制和轻量级协调相结合，用于解决水下多机器人协同覆盖中的部分可观测性、有限通信和环境不确定性等问题。


<details>
  <summary>Details</summary>
Motivation: 水下多机器人协同覆盖面临部分可观测性、有限通信、环境不确定性以及缺乏全局定位等挑战，需要开发不依赖全局定位的鲁棒导航和协调方法。

Method: 使用LLM将原始多模态观测压缩为紧凑、可解释的语义标记，通过预定义隶属度函数的模糊推理系统生成平滑稳定的转向和步态命令，并引入语义通信实现多机器人协调。

Result: 在未知珊瑚礁环境中的大量仿真表明，该框架在有限感知和通信条件下实现了鲁棒的OOI导向导航和协同覆盖，提高了效率和适应性。

Conclusion: 该框架缩小了语义认知与分布式水下控制在GPS缺失、无地图条件下的差距，为水下多机器人系统提供了有效的解决方案。

Abstract: Underwater multi-robot cooperative coverage remains challenging due to
partial observability, limited communication, environmental uncertainty, and
the lack of access to global localization. To address these issues, this paper
presents a semantics-guided fuzzy control framework that couples Large Language
Models (LLMs) with interpretable control and lightweight coordination. Raw
multimodal observations are compressed by the LLM into compact,
human-interpretable semantic tokens that summarize obstacles, unexplored
regions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy
inference system with pre-defined membership functions then maps these tokens
into smooth and stable steering and gait commands, enabling reliable navigation
without relying on global positioning. Then, we further coordinate multiple
robots by introducing semantic communication that shares intent and local
context in linguistic form, enabling agreement on who explores where while
avoiding redundant revisits. Extensive simulations in unknown reef-like
environments show that, under limited sensing and communication, the proposed
framework achieves robust OOI-oriented navigation and cooperative coverage with
improved efficiency and adaptability, narrowing the gap between semantic
cognition and distributed underwater control in GPS-denied, map-free
conditions.

</details>


### [307] [Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning](https://arxiv.org/abs/2511.00814)
*Stella Kombo,Masih Haseli,Skylar Wei,Joel W. Burdick*

Main category: cs.RO

TL;DR: 提出了一种基于滑动窗口Hankel动态模态分解的在线框架，用于实时学习和预测其他智能体的非线性运动模型，通过噪声抑制和方差估计支持实时控制应用。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要从部分噪声数据中预测附近智能体的运动，但现有方法难以在实时条件下学习非线性预测模型。

Method: 使用改进的滑动窗口Hankel-DMD方法，通过Hankel矩阵嵌入部分噪声测量，利用Page矩阵进行奇异值硬阈值估计有效秩，采用Cadzow投影确保结构化低秩一致性，构建时变Hankel-DMD提升线性预测器进行多步预测。

Result: 在模拟和动态起重机实验平台上验证，方法在Gaussian和重尾噪声下均能实现稳定的方差感知去噪和短时域预测，适合集成到实时控制框架中。

Conclusion: 该方法能够实时学习非线性运动预测模型，提供方差跟踪信号支持下游估计器和风险感知规划，适用于实时自主系统控制。

Abstract: Autonomous systems often must predict the motions of nearby agents from
partial and noisy data. This paper asks and answers the question: "can we
learn, in real-time, a nonlinear predictive model of another agent's motions?"
Our online framework denoises and forecasts such dynamics using a modified
sliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy
measurements are embedded into a Hankel matrix, while an associated Page matrix
enables singular-value hard thresholding (SVHT) to estimate the effective rank.
A Cadzow projection enforces structured low-rank consistency, yielding a
denoised trajectory and local noise variance estimates. From this
representation, a time-varying Hankel-DMD lifted linear predictor is
constructed for multi-step forecasts. The residual analysis provides
variance-tracking signals that can support downstream estimators and risk-aware
planning. We validate the approach in simulation under Gaussian and
heavy-tailed noise, and experimentally on a dynamic crane testbed. Results show
that the method achieves stable variance-aware denoising and short-horizon
prediction suitable for integration into real-time control frameworks.

</details>


### [308] [Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches](https://arxiv.org/abs/2511.00840)
*William Suliman,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: 本文提出了一种基于学习的双足机器人行走框架，结合启发式步态规划策略和期望躯干速度跟踪，实现了在非结构化环境中的稳定行走，包括跨越间隙和精确接近目标等任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于完整或简化动力学的方法需要复杂的步态规划器和解析模型，限制了机器人在非结构化环境中的适应性和鲁棒性。本文旨在开发一种无需复杂模型组件即可实现稳定双足行走的方法。

Method: 采用启发式命令驱动步态规划，结合Raibert型控制器根据期望与实际躯干速度误差调节足部放置长度。与基于线性倒立摆模型(LIPM)的模型方法进行对比。

Result: 实验结果显示，该方法在维持目标速度方面达到可比或更优精度(最高80%)，在不平坦地形上的鲁棒性显著提高(超过50%改进)，并具有更好的能量效率。

Conclusion: 研究结果表明，在训练架构中引入复杂的解析模型组件对于实现稳定和鲁棒的双足行走可能是不必要的，即使在非结构化环境中也是如此。

Abstract: This work presents an extended framework for learning-based bipedal
locomotion that incorporates a heuristic step-planning strategy guided by
desired torso velocity tracking. The framework enables precise interaction
between a humanoid robot and its environment, supporting tasks such as crossing
gaps and accurately approaching target objects. Unlike approaches based on full
or simplified dynamics, the proposed method avoids complex step planners and
analytical models. Step planning is primarily driven by heuristic commands,
while a Raibert-type controller modulates the foot placement length based on
the error between desired and actual torso velocity. We compare our method with
a model-based step-planning approach -- the Linear Inverted Pendulum Model
(LIPM) controller. Experimental results demonstrate that our approach attains
comparable or superior accuracy in maintaining target velocity (up to 80%),
significantly greater robustness on uneven terrain (over 50% improvement), and
improved energy efficiency. These results suggest that incorporating complex
analytical, model-based components into the training architecture may be
unnecessary for achieving stable and robust bipedal walking, even in
unstructured environments.

</details>


### [309] [Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2511.00933)
*Xiangyu Shi,Zerui Li,Yanyuan Qiao,Qi Wu*

Main category: cs.RO

TL;DR: Fast-SmartWay是一个端到端的零样本视觉语言导航框架，仅使用三个前向RGB-D图像和自然语言指令，无需全景视图和路径点预测器，显著降低了延迟并提升了实际应用性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言导航方法依赖全景观测和两阶段流水线，导致显著延迟并限制实际应用。需要开发更高效的端到端框架。

Method: 提出Fast-SmartWay框架，仅使用三个前向RGB-D图像结合语言指令，让多模态大语言模型直接预测动作。引入不确定性感知推理模块，包括消歧模块和未来-过去双向推理机制。

Result: 在仿真和真实机器人环境中的实验表明，该方法显著降低了每步延迟，同时达到或优于基于全景视图的基线方法的性能。

Conclusion: Fast-SmartWay证明了在现实世界零样本具身导航中的实用性和有效性，为实时导航应用提供了可行解决方案。

Abstract: Recent advances in Vision-and-Language Navigation in Continuous Environments
(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve
zero-shot navigation. However, existing methods often rely on panoramic
observations and two-stage pipelines involving waypoint predictors, which
introduce significant latency and limit real-world applicability. In this work,
we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that
eliminates the need for panoramic views and waypoint predictors. Our approach
uses only three frontal RGB-D images combined with natural language
instructions, enabling MLLMs to directly predict actions. To enhance decision
robustness, we introduce an Uncertainty-Aware Reasoning module that integrates
(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past
Bidirectional Reasoning mechanism for globally coherent planning. Experiments
on both simulated and real-robot environments demonstrate that our method
significantly reduces per-step latency while achieving competitive or superior
performance compared to panoramic-view baselines. These results demonstrate the
practicality and effectiveness of Fast-SmartWay for real-world zero-shot
embodied navigation.

</details>


### [310] [URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model](https://arxiv.org/abs/2511.00940)
*Zhe Li,Xiang Bai,Jieyu Zhang,Zhuangzhe Wu,Che Xu,Ying Li,Chengkai Hou,Shanghang Zhang*

Main category: cs.RO

TL;DR: URDF-Anything是一个基于3D多模态大语言模型的端到端自动重建框架，能够从点云和文本输入联合优化几何分割和运动学参数预测，显著提升数字孪生构建的准确性和物理可执行性。


<details>
  <summary>Details</summary>
Motivation: 构建精确的关节物体数字孪生对机器人仿真训练和具身AI世界模型构建至关重要，但传统方法需要繁琐的手动建模或多阶段流水线。

Method: 采用基于点云和文本多模态输入的自回归预测框架，实现专门的[SEG]令牌机制与点云特征直接交互，在保持运动学参数预测一致性的同时实现细粒度部件级分割。

Result: 在模拟和真实数据集上的实验表明，该方法在几何分割（mIoU提升17%）、运动学参数预测（平均误差减少29%）和物理可执行性（超越基线50%）方面显著优于现有方法，且在训练集外物体上表现出色泛化能力。

Conclusion: 该工作为机器人仿真构建数字孪生提供了高效解决方案，显著增强了从仿真到现实的迁移能力。

Abstract: Constructing accurate digital twins of articulated objects is essential for
robotic simulation training and embodied AI world model building, yet
historically requires painstaking manual modeling or multi-stage pipelines. In
this work, we propose \textbf{URDF-Anything}, an end-to-end automatic
reconstruction framework based on a 3D multimodal large language model (MLLM).
URDF-Anything utilizes an autoregressive prediction framework based on
point-cloud and text multimodal input to jointly optimize geometric
segmentation and kinematic parameter prediction. It implements a specialized
$[SEG]$ token mechanism that interacts directly with point cloud features,
enabling fine-grained part-level segmentation while maintaining consistency
with the kinematic parameter predictions. Experiments on both simulated and
real-world datasets demonstrate that our method significantly outperforms
existing approaches regarding geometric segmentation (mIoU 17\% improvement),
kinematic parameter prediction (average error reduction of 29\%), and physical
executability (surpassing baselines by 50\%). Notably, our method exhibits
excellent generalization ability, performing well even on objects outside the
training set. This work provides an efficient solution for constructing digital
twins for robotic simulation, significantly enhancing the sim-to-real transfer
capability.

</details>


### [311] [GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies](https://arxiv.org/abs/2511.00998)
*Ziye Wang,Li Kang,Yiran Qin,Jiahua Ma,Zhanglin Peng,Lei Bai,Ruimao Zhang*

Main category: cs.RO

TL;DR: GauDP是一种新颖的高斯-图像协同表示方法，用于多智能体协作系统中的可扩展、感知感知的模仿学习，通过构建全局一致的3D高斯场并动态重新分配属性到每个智能体的局部视角，实现细粒度控制和全局一致行为。


<details>
  <summary>Details</summary>
Motivation: 解决具身多智能体系统中有效协调的基本挑战，特别是在智能体需要平衡个体视角与全局环境感知的场景中。现有方法难以平衡细粒度局部控制与全面场景理解，导致可扩展性有限和协作质量受损。

Method: 从分散的RGB观测构建全局一致的3D高斯场，然后动态地将3D高斯属性重新分配到每个智能体的局部视角，使所有智能体能够自适应地从共享场景表示中查询任务关键特征，同时保持各自的个体视角。

Result: 在RoboFactory基准测试中，GauDP在多样化多臂操作任务上实现了优于现有基于图像方法的性能，接近点云驱动方法的有效性，同时在智能体数量增加时保持强大的可扩展性。

Conclusion: GauDP通过高斯-图像协同表示实现了多智能体协作系统中细粒度控制和全局一致行为的平衡，无需额外的传感模式，在可扩展性和性能方面表现出色。

Abstract: Recently, effective coordination in embodied multi-agent systems has remained
a fundamental challenge, particularly in scenarios where agents must balance
individual perspectives with global environmental awareness. Existing
approaches often struggle to balance fine-grained local control with
comprehensive scene understanding, resulting in limited scalability and
compromised collaboration quality. In this paper, we present GauDP, a novel
Gaussian-image synergistic representation that facilitates scalable,
perception-aware imitation learning in multi-agent collaborative systems.
Specifically, GauDP constructs a globally consistent 3D Gaussian field from
decentralized RGB observations, then dynamically redistributes 3D Gaussian
attributes to each agent's local perspective. This enables all agents to
adaptively query task-critical features from the shared scene representation
while maintaining their individual viewpoints. This design facilitates both
fine-grained control and globally coherent behavior without requiring
additional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the
RoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our
method achieves superior performance over existing image-based methods and
approaches the effectiveness of point-cloud-driven methods, while maintaining
strong scalability as the number of agents increases.

</details>


### [312] [AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models](https://arxiv.org/abs/2511.01031)
*Mathieu Dubied,Paolo Tiso,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 提出了一种基于张量参数降阶模型的优化算法，用于高效解决软体机器人中的非线性约束优化问题，特别针对承受流体动力学的软体游泳机器人形状优化。


<details>
  <summary>Details</summary>
Motivation: 软体结构在复杂非线性力作用下的高效优化是机器人领域的关键挑战，传统有限元方法模拟非线性结构需要大量计算资源，尤其在优化过程中计算成本高昂。

Method: 利用张量参数降阶模型，结合维度缩减和解决方案近似技术，在特定选择的降阶基中使用解析梯度，通过无数据降阶基将内外非线性力纳入优化过程。

Result: 该方法显著提高了计算效率，降低了计算复杂度，能够快速准确地进行软体游泳机器人形状优化。

Conclusion: 该算法不仅减少了计算复杂性，还为软体机器人中复杂非线性系统的优化开辟了新机会，为更高效的设计和控制铺平了道路。

Abstract: The efficient optimization of actuated soft structures, particularly under
complex nonlinear forces, remains a critical challenge in advancing robotics.
Simulations of nonlinear structures, such as soft-bodied robots modeled using
the finite element method (FEM), often demand substantial computational
resources, especially during optimization. To address this challenge, we
propose a novel optimization algorithm based on a tensorial parametric reduced
order model (PROM). Our algorithm leverages dimensionality reduction and
solution approximation techniques to facilitate efficient solving of nonlinear
constrained optimization problems. The well-structured tensorial approach
enables the use of analytical gradients within a specifically chosen reduced
order basis (ROB), significantly enhancing computational efficiency. To
showcase the performance of our method, we apply it to optimizing soft robotic
swimmer shapes. These actuated soft robots experience hydrodynamic forces,
subjecting them to both internal and external nonlinear forces, which are
incorporated into our optimization process using a data-free ROB for fast and
accurate computations. This approach not only reduces computational complexity
but also unlocks new opportunities to optimize complex nonlinear systems in
soft robotics, paving the way for more efficient design and control.

</details>


### [313] [SLAP: Shortcut Learning for Abstract Planning](https://arxiv.org/abs/2511.01107)
*Y. Isabel Liu,Bowen Li,Benjamin Eysenbach,Tom Silver*

Main category: cs.RO

TL;DR: SLAP方法通过结合任务与运动规划(TAMP)和模型无关强化学习(RL)，自动发现新的抽象动作选项，解决了长时程决策中稀疏奖励和连续状态动作空间的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统TAMP方法依赖人工定义的抽象动作选项，限制了智能体只能执行人类工程师已知的行为。需要一种能自动发现新选项的方法来扩展智能体的行为范围。

Method: 利用现有TAMP选项构建抽象规划图，然后使用模型无关RL在该图中学习捷径，自动发现新的抽象动作选项。

Result: 在四个模拟机器人环境中，SLAP显著缩短了规划长度(减少50%以上)，提高了任务成功率，并发现了动态物理即兴行为(如拍打、摆动、擦拭)。

Conclusion: SLAP方法成功结合了规划和强化学习的优势，能够自动发现超越人工定义的新行为选项，在长时程决策任务中表现出色且具有良好的泛化能力。

Abstract: Long-horizon decision-making with sparse rewards and continuous states and
actions remains a fundamental challenge in AI and robotics. Task and motion
planning (TAMP) is a model-based framework that addresses this challenge by
planning hierarchically with abstract actions (options). These options are
manually defined, limiting the agent to behaviors that we as human engineers
know how to program (pick, place, move). In this work, we propose Shortcut
Learning for Abstract Planning (SLAP), a method that leverages existing TAMP
options to automatically discover new ones. Our key idea is to use model-free
reinforcement learning (RL) to learn shortcuts in the abstract planning graph
induced by the existing options in TAMP. Without any additional assumptions or
inputs, shortcut learning leads to shorter solutions than pure planning, and
higher task success rates than flat and hierarchical RL. Qualitatively, SLAP
discovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that
differ significantly from the manually-defined ones. In experiments in four
simulated robotic environments, we show that SLAP solves and generalizes to a
wide range of tasks, reducing overall plan lengths by over 50% and consistently
outperforming planning and RL baselines.

</details>


### [314] [An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs](https://arxiv.org/abs/2511.01165)
*Dong Heon Han,Mayank Mehta,Runze Zuo,Zachary Wanger,Daniel Bruder*

Main category: cs.RO

TL;DR: 提出一种改进的软机器人本体感知方法，仅使用现成传感器实现精确形状估计，具有成本效益和易用性。通过IMU和弯曲传感器融合，使用卡尔曼滤波器补偿IMU漂移，在45分钟连续运行中实现16.96mm均方根误差（总长度2.91%），比仅使用IMU的方法误差减少56%。


<details>
  <summary>Details</summary>
Motivation: 开发一种成本效益高且易于应用的软机器人形状估计方法，仅使用现成传感器，解决IMU漂移问题，实现可靠的长时本体感知。

Method: 集成惯性测量单元（IMU）和互补弯曲传感器，使用卡尔曼滤波器融合两种传感器的段端方向数据，采用分段恒定曲率模型从融合方向数据估计尖端位置并重建机器人变形。

Result: 在无负载、外力和被动障碍物交互的45分钟连续运行中，均方根误差为16.96mm（总长度2.91%），相比仅使用IMU的基准方法误差减少56%。

Conclusion: 该方法不仅能在软机器人中实现长时本体感知，而且在不同条件下保持高精度和鲁棒性。

Abstract: This study presents an enhanced proprioceptive method for accurate shape
estimation of soft robots using only off-the-shelf sensors, ensuring
cost-effectiveness and easy applicability. By integrating inertial measurement
units (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling
reliable long-term proprioception. A Kalman filter fuses segment tip
orientations from both sensors in a mutually compensatory manner, improving
shape estimation over single-sensor methods. A piecewise constant curvature
model estimates the tip location from the fused orientation data and
reconstructs the robot's deformation. Experiments under no loading, external
forces, and passive obstacle interactions during 45 minutes of continuous
operation showed a root mean square error of 16.96 mm (2.91% of total length),
a 56% reduction compared to IMU-only benchmarks. These results demonstrate that
our approach not only enables long-duration proprioception in soft robots but
also maintains high accuracy and robustness across these diverse conditions.

</details>


### [315] [Scaling Cross-Embodiment World Models for Dexterous Manipulation](https://arxiv.org/abs/2511.01177)
*Zihao He,Bo Ai,Tongzhou Mu,Yulin Liu,Weikang Wan,Jiawei Fu,Yilun Du,Henrik I. Christensen,Hao Su*

Main category: cs.RO

TL;DR: 本文提出了一种跨具身学习方法，通过将不同形态的机器人表示为3D粒子集合，并定义粒子位移作为动作，构建统一的图结构世界模型来捕捉环境动态，实现异构数据共享和策略迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机器人之间由于动作空间和运动学差异导致的数据共享和策略迁移困难问题，探索是否存在跨具身的动作迁移不变性。

Method: 将不同具身表示为3D粒子集合，定义粒子位移作为动作，训练基于图结构的世界模型，结合基于模型的规划方法部署到新硬件上。

Result: 实验表明：(1)增加训练具身数量能提高对未见具身的泛化能力；(2)同时使用仿真和真实数据训练优于单独使用；(3)学习到的模型能在不同自由度的机器人上实现有效控制。

Conclusion: 世界模型为跨具身灵巧操作提供了一个有前景的统一接口。

Abstract: Cross-embodiment learning seeks to build generalist robots that operate
across diverse morphologies, but differences in action spaces and kinematics
hinder data sharing and policy transfer. This raises a central question: Is
there any invariance that allows actions to transfer across embodiments? We
conjecture that environment dynamics are embodiment-invariant, and that world
models capturing these dynamics can provide a unified interface across
embodiments. To learn such a unified world model, the crucial step is to design
state and action representations that abstract away embodiment-specific details
while preserving control relevance. To this end, we represent different
embodiments (e.g., human hands and robot hands) as sets of 3D particles and
define actions as particle displacements, creating a shared representation for
heterogeneous data and control problems. A graph-based world model is then
trained on exploration data from diverse simulated robot hands and real human
hands, and integrated with model-based planning for deployment on novel
hardware. Experiments on rigid and deformable manipulation tasks reveal three
findings: (i) scaling to more training embodiments improves generalization to
unseen ones, (ii) co-training on both simulated and real data outperforms
training on either alone, and (iii) the learned models enable effective control
on robots with varied degrees of freedom. These results establish world models
as a promising interface for cross-embodiment dexterous manipulation.

</details>


### [316] [LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping](https://arxiv.org/abs/2511.01186)
*Lijie Wang,Lianjie Guo,Ziyi Xu,Qianhao Wang,Fei Gao,Xieyuanli Chen*

Main category: cs.RO

TL;DR: 提出LiDAR-VGGT框架，通过两阶段融合方法将LiDAR惯性里程计与VGGT模型紧密耦合，解决VGGT在大规模环境中可扩展性差和缺乏度量尺度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR惯性视觉里程计(LIVO)对外部标定高度敏感，而3D视觉基础模型VGGT在大规模环境中可扩展性有限且缺乏度量尺度。需要克服这些限制以实现大规模彩色点云重建。

Method: 采用两阶段粗到精融合管道：预融合模块通过鲁棒初始化细化估计VGGT位姿和点云；后融合模块增强跨模态3D相似变换，使用基于边界框的正则化减少LiDAR和相机FOV不一致引起的尺度失真。

Result: 在多个数据集上的广泛实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，优于VGGT方法和LIVO基线。

Conclusion: LiDAR-VGGT框架有效解决了VGGT模型的尺度问题和可扩展性限制，实现了高质量的大规模彩色点云重建，并开发了开源彩色点云评估工具包。

Abstract: Reconstructing large-scale colored point clouds is an important task in
robotics, supporting perception, navigation, and scene understanding. Despite
advances in LiDAR inertial visual odometry (LIVO), its performance remains
highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation
models, such as VGGT, suffer from limited scalability in large environments and
inherently lack metric scale. To overcome these limitations, we propose
LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with
the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion
pipeline: First, a pre-fusion module with robust initialization refinement
efficiently estimates VGGT poses and point clouds with coarse metric scale
within each session. Then, a post-fusion module enhances cross-modal 3D
similarity transformation, using bounding-box-based regularization to reduce
scale distortions caused by inconsistent FOVs between LiDAR and camera sensors.
Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT
achieves dense, globally consistent colored point clouds and outperforms both
VGGT-based methods and LIVO baselines. The implementation of our proposed novel
color point cloud evaluation toolkit will be released as open source.

</details>


### [317] [Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures](https://arxiv.org/abs/2511.01199)
*Max McCandless,Jonathan Hamid,Sammy Elmariah,Nathaniel Langer,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 本文提出了一种可操控球囊式心脏镜，通过单个输入（球囊充气压力）独立控制球囊直径和弯曲角度，用于心脏内可视化及器械输送。


<details>
  <summary>Details</summary>
Motivation: 为了从开胸手术转向更安全的经导管手术，需要改进成像技术和机器人解决方案以实现简单准确的手术器械导航。

Method: 设计可操控球囊式心脏镜，通过精心设计球囊壁厚度，使用单个充气压力输入独立控制球囊直径（对应视野直径）和弯曲角度（实现工作通道精确定位）。

Result: 开发了针对主动脉瓣叶撕裂任务的球囊设计，并演示了基于图像的闭环弯曲角度控制，可在器械插入和移除期间实现稳定的方向控制。

Conclusion: 这种球囊技术可调谐用于各种心脏内任务的心脏镜设计，为经导管手术提供了改进的成像和导航解决方案。

Abstract: To move away from open-heart surgery towards safer transcatheter procedures,
there is a growing need for improved imaging techniques and robotic solutions
to enable simple, accurate tool navigation. Common imaging modalities, such as
fluoroscopy and ultrasound, have limitations that can be overcome using
cardioscopy, i.e., direct optical visualization inside the beating heart. We
present a cardioscope designed as a steerable balloon. As a balloon, it can be
collapsed to pass through the vasculature and subsequently inflated inside the
heart for visualization and tool delivery through an integrated working
channel. Through careful design of balloon wall thickness, a single input,
balloon inflation pressure, is used to independently control two outputs,
balloon diameter (corresponding to field of view diameter) and balloon bending
angle (enabling precise working channel positioning). This balloon technology
can be tuned to produce cardioscopes designed for a range of intracardiac
tasks. To illustrate this approach, a balloon design is presented for the
specific task of aortic leaflet laceration. Image-based closed-loop control of
bending angle is also demonstrated as a means of enabling stable orientation
control during tool insertion and removal.

</details>


### [318] [Embodiment Transfer Learning for Vision-Language-Action Models](https://arxiv.org/abs/2511.01224)
*Chengmeng Li,Yaxin Peng*

Main category: cs.RO

TL;DR: 提出了ET-VLA框架，通过合成持续预训练和具身思维图技术，有效解决多机器人协作问题，在真实任务中性能提升超过53.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归VLA模型在多机器人协作方面表现不佳，需要一种高效的方法将预训练模型迁移到多机器人场景。

Method: 采用合成持续预训练(SCP)生成合成数据预热模型，结合具身思维图技术将子任务建模为节点，使模型能够区分不同具身的功能和角色。

Result: 在三个不同双手机器人具身上验证，在六个真实世界任务中比OpenVLA性能提升超过53.2%。

Conclusion: ET-VLA框架能够有效解决多机器人协作问题，显著提升性能，并将开源代码支持社区发展。

Abstract: Vision-language-action (VLA) models have significantly advanced robotic
learning, enabling training on large-scale, cross-embodiment data and
fine-tuning for specific robots. However, state-of-the-art autoregressive VLAs
struggle with multi-robot collaboration. We introduce embodiment transfer
learning, denoted as ET-VLA, a novel framework for efficient and effective
transfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic
Continued Pretraining (SCP), which uses synthetically generated data to warm up
the model for the new embodiment, bypassing the need for real human
demonstrations and reducing data collection costs. SCP enables the model to
learn correct actions and precise action token numbers. Following SCP, the
model is fine-tuned on target embodiment data. To further enhance the model
performance on multi-embodiment, we present the Embodied Graph-of-Thought
technique, a novel approach that formulates each sub-task as a node, that
allows the VLA model to distinguish the functionalities and roles of each
embodiment during task execution. Our work considers bimanual robots, a simple
version of multi-robot to verify our approaches. We validate the effectiveness
of our method on both simulation benchmarks and real robots covering three
different bimanual embodiments. In particular, our proposed ET-VLA \space can
outperform OpenVLA on six real-world tasks over 53.2%. We will open-source all
codes to support the community in advancing VLA models for robot learning.

</details>


### [319] [High-Precision Surgical Robotic System for Intraocular Procedures](https://arxiv.org/abs/2511.01232)
*Yu-Ting Lai,Jacob Rosen,Yasamin Foroutani,Ji Ma,Wen-Cheng Wu,Jean-Pierre Hubschman,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 开发了一种新型机器人系统，用于提高眼科手术中的工具尖端精度、跟踪性能和工具交换机制，通过OCT系统评估显示工具尖端定位精度达到0.053±0.031毫米。


<details>
  <summary>Details</summary>
Motivation: 现有白内障和玻璃体视网膜手术机器人系统在精度、自由度和工具交换方面存在不足，需要开发更精确、更灵活的机器人系统来改善手术效果。

Method: 设计和制造了新型机器人系统，通过机器人校准和精确坐标配准，使用光学相干断层扫描系统评估工具尖端精度、精度和机械能力。

Result: 工具尖端定位精度测量为0.053±0.031毫米，并在OCT引导的自动化白内障晶状体提取手术中展示了整体性能，结合了基于深度学习的术前解剖建模和实时监督。

Conclusion: 该机器人系统显著提高了眼科手术的工具尖端精度和跟踪性能，为自动化手术提供了可靠的技术支持。

Abstract: Despite the extensive demonstration of robotic systems for both cataract and
vitreoretinal procedures, existing technologies or mechanisms still possess
insufficient accuracy, precision, and degrees of freedom for instrument
manipulation or potentially automated tool exchange during surgical procedures.
A new robotic system that focuses on improving tooltip accuracy, tracking
performance, and smooth instrument exchange mechanism is therefore designed and
manufactured. Its tooltip accuracy, precision, and mechanical capability of
maintaining small incision through remote center of motion were externally
evaluated using an optical coherence tomography (OCT) system. Through robot
calibration and precise coordinate registration, the accuracy of tooltip
positioning was measured to be 0.053$\pm$0.031 mm, and the overall performance
was demonstrated on an OCT-guided automated cataract lens extraction procedure
with deep learning-based pre-operative anatomical modeling and real-time
supervision.

</details>


### [320] [Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments](https://arxiv.org/abs/2511.01236)
*Junwen Zhang,Changyue Liu,Pengqi Fu,Xiang Guo,Ye Shi,Xudong Liang,Zhijian Wang,Hanzhi Ma*

Main category: cs.RO

TL;DR: 本文提出了一种基于大语言模型的语义规划器SATPlanner，用于解决球形张拉整体机器人在未知环境中的路径规划问题，通过自适应观测窗口机制实现高效的语义推理规划。


<details>
  <summary>Details</summary>
Motivation: 球形张拉整体机器人具有优异的鲁棒性和适应性，但在未知环境中的路径规划面临挑战。传统基于几何网格的规划器缺乏语义理解，在复杂场景中容易失败且存在冗余搜索问题。

Method: 将路径规划重新定义为语义推理任务，提出SATPlanner框架，核心是受LLM快慢思维启发的自适应观测窗口机制，能够动态调整感知范围：在开阔空间缩小窗口快速穿越，在复杂障碍配置时扩大窗口进行推理。

Result: 在1000次仿真试验中达到100%成功率，相比A*算法减少37.2%的搜索空间，同时保持接近最优的路径长度，并在物理原型机器人上验证了可行性。

Conclusion: SATPlanner通过语义理解实现了高效的路径规划，搜索空间仅随路径长度线性增长(O(L))，为混合软刚性机器人在未知环境中的导航提供了有效解决方案。

Abstract: Endowed with inherent dynamical properties that grant them remarkable
ruggedness and adaptability, spherical tensegrity robots stand as prototypical
examples of hybrid softrigid designs and excellent mobile platforms. However,
path planning for these robots in unknown environments presents a significant
challenge, requiring a delicate balance between efficient exploration and
robust planning. Traditional path planners, which treat the environment as a
geometric grid, often suffer from redundant searches and are prone to failure
in complex scenarios due to their lack of semantic understanding. To overcome
these limitations, we reframe path planning in unknown environments as a
semantic reasoning task. We introduce a Semantic Agent for Tensegrity robots
(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages
high-level environmental comprehension to generate efficient and reliable
planning strategies.At the core of SATPlanner is an Adaptive Observation Window
mechanism, inspired by the "fast" and "slow" thinking paradigms of LLMs. This
mechanism dynamically adjusts the perceptual field of the agent: it narrows for
rapid traversal of open spaces and expands to reason about complex obstacle
configurations. This allows the agent to construct a semantic belief of the
environment, enabling the search space to grow only linearly with the path
length (O(L)) while maintaining path quality. We extensively evaluate
SATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,
outperforming other real-time planning algorithms. Critically, SATPlanner
reduces the search space by 37.2% compared to the A* algorithm while achieving
comparable, near-optimal path lengths. Finally, the practical feasibility of
SATPlanner is validated on a physical spherical tensegrity robot prototype.

</details>


### [321] [Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control](https://arxiv.org/abs/2511.01256)
*Yasamin Foroutani,Yasamin Mousavi-Motlagh,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 该论文提出了一种迭代学习控制策略，用于机器人手术中工具的精确旋转插入，相比直线插入提高了穿透效果和安全性。


<details>
  <summary>Details</summary>
Motivation: 机器人工具路径的精确控制面临系统不对准、未建模动力学和驱动不准确性等挑战，特别是在视网膜下注射等精细手术中。

Method: 使用4自由度机器人操纵器，通过校准前向运动学提高精度，然后基于光学相干断层扫描体积扫描的反馈，迭代调整关节命令。

Result: 在离体猪眼视网膜下注射任务中，优化轨迹相比直线插入在组织穿透和视网膜下注射方面获得了更高的成功率。

Conclusion: 迭代学习控制方法有效克服了对准挑战，为其他需要受控插入的高精度机器人任务提供了潜在应用。

Abstract: Achieving precise control of robotic tool paths is often challenged by
inherent system misalignments, unmodeled dynamics, and actuation inaccuracies.
This work introduces an Iterative Learning Control (ILC) strategy to enable
precise rotational insertion of a tool during robotic surgery, improving
penetration efficacy and safety compared to straight insertion tested in
subretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,
where misalignment of the fourth joint complicates the simple application of
needle rotation, motivating an ILC approach that iteratively adjusts joint
commands based on positional feedback. The process begins with calibrating the
forward kinematics for the chosen surgical tool to achieve higher accuracy,
followed by successive ILC iterations guided by Optical Coherence Tomography
(OCT) volume scans to measure the error and refine control inputs. Experimental
results, tested on subretinal injection tasks on ex vivo pig eyes, show that
the optimized trajectory resulted in higher success rates in tissue penetration
and subretinal injection compared to straight insertion, demonstrating the
effectiveness of ILC in overcoming misalignment challenges. This approach
offers potential applications for other high precision robot tasks requiring
controlled insertions as well.

</details>


### [322] [Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics](https://arxiv.org/abs/2511.01272)
*Sehui Jeong,Magaly C. Aviles,Athena X. Naylor,Cynthia Sung,Allison M. Okamura*

Main category: cs.RO

TL;DR: 本研究提出了一种结合折纸结构和针织面料的新型制造设计方法，通过编程针法和材料图案将折纸图案转化为针织设计，使用热熔纱创建刚性面板和柔性折痕，成功复制了复杂折纸图案并开发了可穿戴针织万花筒机器人。


<details>
  <summary>Details</summary>
Motivation: 软体机器人使用柔性材料和可变形结构在可穿戴设备领域具有巨大潜力，但如何同时实现结构完整性和佩戴舒适性仍是一个重大挑战。

Method: 提出通用设计方法，通过编程针法和材料图案将折纸图案转化为针织设计，选择性使用热熔纱在柔性折痕周围创建刚性面板，控制折叠方向并防止意外弯曲。

Result: 实验量化了折叠力矩，证明针法图案增强了折叠方向性，热熔纱通过减少边缘卷曲保持几何一致性，并通过硬化面板防止平面外变形。成功复制了Miura-ori、Yoshimura和Kresling等复杂折纸图案，开发了能够运动的可穿戴针织万花筒机器人。

Conclusion: 针织折纸结合了结构可重构性、材料可编程性和制造可扩展性潜力，是下一代可穿戴机器人的有前景平台。

Abstract: Soft robots employing compliant materials and deformable structures offer
great potential for wearable devices that are comfortable and safe for human
interaction. However, achieving both structural integrity and compliance for
comfort remains a significant challenge. In this study, we present a novel
fabrication and design method that combines the advantages of origami
structures with the material programmability and wearability of knitted
fabrics. We introduce a general design method that translates origami patterns
into knit designs by programming both stitch and material patterns. The method
creates folds in preferred directions while suppressing unintended buckling and
bending by selectively incorporating heat fusible yarn to create rigid panels
around compliant creases. We experimentally quantify folding moments and show
that stitch patterning enhances folding directionality while the heat fusible
yarn (1) keeps geometry consistent by reducing edge curl and (2) prevents
out-of-plane deformations by stiffening panels. We demonstrate the framework
through the successful reproduction of complex origami tessellations, including
Miura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted
Kaleidocycle robot capable of locomotion. The combination of structural
reconfigurability, material programmability, and potential for manufacturing
scalability highlights knitted origami as a promising platform for
next-generation wearable robotics.

</details>


### [323] [Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation](https://arxiv.org/abs/2511.01276)
*Yiyao Ma,Kai Chen,Kexin Zheng,Qi Dou*

Main category: cs.RO

TL;DR: 提出基于条件扩散模型的灵巧抓取生成框架，通过接触图、部件图和方向图的联合迁移，实现从形状模板到新物体的高质量抓取转移，平衡了抓取质量、生成效率和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 灵巧抓取生成面临稳定性与任务适应性之间的平衡挑战，分析方法稳定但效率低且缺乏任务适应性，生成方法效率高但泛化能力差。需要一种能有效迁移高质量抓取到新物体的方法。

Method: 使用条件扩散模型将抓取转移问题重新定义为物体接触图生成，引入双映射机制处理复杂形状变化，联合迁移接触图、部件图和方向图，并开发级联条件扩散模型框架和鲁棒的抓取恢复机制。

Result: 大量实验证明该方法在多种任务中有效平衡了抓取质量、生成效率和泛化性能，表现出优越性。

Conclusion: 提出的基于扩散模型的抓取转移框架能够有效生成稳定且适应任务的灵巧抓取，解决了现有方法在泛化能力和效率方面的局限性。

Abstract: Dexterous grasp generation is a fundamental challenge in robotics, requiring
both grasp stability and adaptability across diverse objects and tasks.
Analytical methods ensure stable grasps but are inefficient and lack task
adaptability, while generative approaches improve efficiency and task
integration but generalize poorly to unseen objects and tasks due to data
limitations. In this paper, we propose a transfer-based framework for dexterous
grasp generation, leveraging a conditional diffusion model to transfer
high-quality grasps from shape templates to novel objects within the same
category. Specifically, we reformulate the grasp transfer problem as the
generation of an object contact map, incorporating object shape similarity and
task specifications into the diffusion process. To handle complex shape
variations, we introduce a dual mapping mechanism, capturing intricate
geometric relationship between shape templates and novel objects. Beyond the
contact map, we derive two additional object-centric maps, the part map and
direction map, to encode finer contact details for more stable grasps. We then
develop a cascaded conditional diffusion model framework to jointly transfer
these three maps, ensuring their intra-consistency. Finally, we introduce a
robust grasp recovery mechanism, identifying reliable contact points and
optimizing grasp configurations efficiently. Extensive experiments demonstrate
the superiority of our proposed method. Our approach effectively balances grasp
quality, generation efficiency, and generalization performance across various
tasks. Project homepage: https://cmtdiffusion.github.io/

</details>


### [324] [A High-Speed Capable Spherical Robot](https://arxiv.org/abs/2511.01288)
*Bixuan Zhang,Fengqi Zhang,Haojie Chen,You Wang,Jie Hao,Zhiyuan Luo,Guang Li*

Main category: cs.RO

TL;DR: 设计了一种新型球形机器人结构，能够支持高达10 m/s的高速运动，通过结合动量轮和二次摆结构，实现了简单解耦控制下的稳定高速运动。


<details>
  <summary>Details</summary>
Motivation: 传统单摆驱动球形机器人无法实现稳定高速运动，需要设计新的结构来突破速度限制并提升性能。

Method: 在单摆驱动球形机器人基础上，增加与二次摆轴对齐的动量轮，创建新型球形机器人结构。

Result: 物理原型实验证明，新型球形机器人能够通过简单解耦控制实现稳定高速运动，同时显著提升了越障性能和地形鲁棒性。

Conclusion: 该新型球形机器人结构成功实现了高速运动能力，解决了原有结构的局限性，并在多方面提升了机器人性能。

Abstract: This paper designs a new spherical robot structure capable of supporting
high-speed motion at up to 10 m/s. Building upon a single-pendulum-driven
spherical robot, the design incorporates a momentum wheel with an axis aligned
with the secondary pendulum, creating a novel spherical robot structure.
Practical experiments with the physical prototype have demonstrated that this
new spherical robot can achieve stable high-speed motion through simple
decoupled control, which was unattainable with the original structure. The
spherical robot designed for high-speed motion not only increases speed but
also significantly enhances obstacle-crossing performance and terrain
robustness.

</details>


### [325] [Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects](https://arxiv.org/abs/2511.01294)
*Jiawei Wang,Dingyou Wang,Jiaming Hu,Qixuan Zhang,Jingyi Yu,Lan Xu*

Main category: cs.RO

TL;DR: Kinematify是一个从RGB图像或文本提示自动合成关节对象的框架，解决了高自由度对象的运动学拓扑推断和关节参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 关节对象对于机器人操作、物理仿真等任务至关重要，但现有方法依赖运动序列或手工数据集，难以扩展到复杂系统。

Method: 结合MCTS搜索进行结构推断和基于几何的优化进行关节推理，生成物理一致且功能有效的描述。

Result: 在合成和真实环境中的多样化输入上评估，在配准和运动学拓扑准确性方面优于先前工作。

Conclusion: Kinematify能够从静态几何自动生成关节对象模型，为机器人操作和仿真提供了可扩展的解决方案。

Abstract: A deep understanding of kinematic structures and movable components is
essential for enabling robots to manipulate objects and model their own
articulated forms. Such understanding is captured through articulated objects,
which are essential for tasks such as physical simulation, motion planning, and
policy learning. However, creating these models, particularly for complex
systems like robots or objects with high degrees of freedom (DoF), remains a
significant challenge. Existing methods typically rely on motion sequences or
strong assumptions from hand-curated datasets, which hinders scalability. In
this paper, we introduce Kinematify, an automated framework that synthesizes
articulated objects directly from arbitrary RGB images or text prompts. Our
method addresses two core challenges: (i) inferring kinematic topologies for
high-DoF objects and (ii) estimating joint parameters from static geometry. To
achieve this, we combine MCTS search for structural inference with
geometry-driven optimization for joint reasoning, producing physically
consistent and functionally valid descriptions. We evaluate Kinematify on
diverse inputs from both synthetic and real-world environments, demonstrating
improvements in registration and kinematic topology accuracy over prior work.

</details>


### [326] [RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models](https://arxiv.org/abs/2511.01331)
*Hongyin Zhang,Shuo Zhang,Junxi Jin,Qixin Zeng,Runze Li,Donglin Wang*

Main category: cs.RO

TL;DR: 本文提出了RobustVLA方法，通过在线强化学习后训练增强视觉-语言-动作模型的鲁棒性，解决其在分布外部署中对观测噪声和动作扰动的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在分布外部署时对观测噪声、传感器误差和动作扰动等环境不确定性缺乏鲁棒性，而现有的强化学习后训练方法主要关注奖励最大化，忽视了环境不确定性的影响。

Method: 提出RobustVLA轻量级在线强化学习后训练方法，通过雅可比正则化减少对观测噪声的敏感性，通过平滑正则化在动作扰动下稳定策略。

Result: 在多种机器人环境中的广泛实验表明，RobustVLA在鲁棒性和可靠性方面显著优于现有最先进方法。

Conclusion: 原则性的鲁棒性感知强化学习后训练是提高视觉-语言-动作模型可靠性和鲁棒性的关键步骤。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful
general-purpose policies for robotic manipulation, benefiting from large-scale
multi-modal pre-training. However, they often fail to generalize reliably in
out-of-distribution deployments, where unavoidable disturbances such as
observation noise, sensor errors, or actuation perturbations become prevalent.
While recent Reinforcement Learning (RL)-based post-training provides a
practical means to adapt pre-trained VLA models, existing methods mainly
emphasize reward maximization and overlook robustness to environmental
uncertainty. In this work, we introduce RobustVLA, a lightweight online RL
post-training method designed to explicitly enhance the resilience of VLA
models. Through a systematic robustness analysis, we identify two key
regularizations: Jacobian regularization, which mitigates sensitivity to
observation noise, and smoothness regularization, which stabilizes policies
under action perturbations. Extensive experiments across diverse robotic
environments demonstrate that RobustVLA significantly outperforms prior
state-of-the-art methods in robustness and reliability. Our results highlight
the importance of principled robustness-aware RL post-training as a key step
toward improving the reliability and robustness of VLA models.

</details>


### [327] [Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers](https://arxiv.org/abs/2511.01346)
*Shun Yoshida,Qingchuan Song,Bastian E. Rapp,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 本研究开发了一种能够自主闭合和重新打开的仿生捕蝇草软机器人，利用新型热响应形状记忆材料实现了在自然温度范围内的双向运动。


<details>
  <summary>Details</summary>
Motivation: 虽然捕蝇草的快速闭合运动已被广泛研究并应用于软机器人开发，但将其闭合和重新打开的双向运动转化为自主的植物启发软机器系统尚未实现。

Method: 采用新型热响应紫外固化形状记忆材料构建软机器人系统，使用形状记忆聚合物制作双曲瓣片实现闭合，利用形状记忆弹性体条作为拮抗驱动器促进瓣片重新打开。

Result: 开发出与真实捕蝇草尺寸相当的仿生系统，在38°C时闭合，在45°C左右开始重新打开，实现了对温度升高响应的程序化顺序运动。

Conclusion: 这是首次在仿生捕蝇草系统中展示热响应闭合和重新打开功能，标志着向自主双向移动软机器发展的下一步进展。

Abstract: Despite their often perceived static and slow nature, some plants can move
faster than the blink of an eye. The rapid snap closure motion of the Venus
flytrap (Dionaea muscipula) has long captivated the interest of researchers and
engineers alike, serving as a model for plant-inspired soft machines and
robots. The translation of the fast snapping closure has inspired the
development of various artificial Venus flytrap (AVF) systems. However,
translating both the closing and reopening motion of D. muscipula into an
autonomous plant inspired soft machine has yet to be achieved. In this study,
we present an AVF that autonomously closes and reopens, utilizing novel
thermo-responsive UV-curable shape memory materials for soft robotic systems.
The life-sized thermo-responsive AVF exhibits closing and reopening motions
triggered in a naturally occurring temperature range. The doubly curved trap
lobes, built from shape memory polymers, close at 38{\deg}C, while reopening
initiates around 45{\deg}C, employing shape memory elastomer strips as
antagonistic actuators to facilitate lobe reopening. This work represents the
first demonstration of thermo-responsive closing and reopening in an AVF with
programmed sequential motion in response to increasing temperature. This
approach marks the next step toward autonomously bidirectional moving soft
machines/robots.

</details>


### [328] [Design and development of an electronics-free earthworm robot](https://arxiv.org/abs/2511.01347)
*Riddhi Das,Joscha Teichmann,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 提出了一种基于改进气动逻辑门的无电子元件仿蚯蚓气动机器人，实现了无需外部电子控制的蠕动运动，降低了系统复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有仿蚯蚓机器人主要依赖气动驱动，但通常需要笨重、高功耗的电子控制单元，限制了实际应用。

Method: 通过将预配置的气动逻辑门单元与波纹管执行器集成，构建了即插即用式模块化系统，无需外部电子组件即可实现蠕动运动。

Result: 改进的气动逻辑门控制系统有效产生蠕动波传播，实现了自主运动且偏差最小。

Conclusion: 该研究为开发无电子元件的蠕动软体机器人提供了概念验证，在危险环境中具有应用潜力，未来将优化设计并探索使用机载压缩空气源的无缆操作。

Abstract: Soft robotic systems have gained widespread attention due to their inherent
flexibility, adaptability, and safety, making them well-suited for varied
applications. Among bioinspired designs, earthworm locomotion has been
extensively studied for its efficient peristaltic motion, enabling movement in
confined and unstructured environments. Existing earthworm-inspired robots
primarily utilize pneumatic actuation due to its high force-to-weight ratio and
ease of implementation. However, these systems often rely on bulky,
power-intensive electronic control units, limiting their practicality. In this
work, we present an electronics-free, earthworm-inspired pneumatic robot
utilizing a modified Pneumatic Logic Gate (PLG) design. By integrating
preconfigured PLG units with bellow actuators, we achieved a plug-and-play
style modular system capable of peristaltic locomotion without external
electronic components. The proposed design reduces system complexity while
maintaining efficient actuation. We characterize the bellow actuators under
different operating conditions and evaluate the robots locomotion performance.
Our findings demonstrate that the modified PLG-based control system effectively
generates peristaltic wave propagation, achieving autonomous motion with
minimal deviation. This study serves as a proof of concept for the development
of electronics-free, peristaltic soft robots. The proposed system has potential
for applications in hazardous environments, where untethered, adaptable
locomotion is critical. Future work will focus on further optimizing the robot
design and exploring untethered operation using onboard compressed air sources.

</details>


### [329] [Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator](https://arxiv.org/abs/2511.01350)
*Maartje H. M. Wermelink,Renate Sachse,Sebastian Kruppert,Thomas Speck,Falk J. Tauber*

Main category: cs.RO

TL;DR: 该研究分析了捕蝇草的快速闭合机制，并将其双稳态特性应用于设计人工双稳态叶片执行器，开发了两种3D打印模型来模拟捕蝇草的机械行为。


<details>
  <summary>Details</summary>
Motivation: 加深对捕蝇草运动力学的理解，并将其原理应用于人工双稳态执行器的设计，开发可用于软快速抓取器的人工捕蝇草。

Method: 识别捕蝇草叶片的几何特征（尺寸比例和厚度梯度），并将其转移到两种3D打印的双稳态执行器模型：一种模拟捕蝇草叶片几何形状，另一种使用CAD设计的叶片模型。

Result: 两种模型都表现出凹-凸双稳态特性，并能快速闭合，成功展示了双稳态行为。

Conclusion: 这些演示器是开发人工捕蝇草的第一步，能够模拟生物模型的机械行为，可用作软快速抓取器。

Abstract: The Venus flytrap (Dionaea muscipula) does not only serve as the textbook
model for a carnivorous plant, but also has long intrigued both botanists and
engineers with its rapidly closing leaf trap. The trap closure is triggered by
two consecutive touches of a potential prey, after which the lobes rapidly
switch from their concave open-state to their convex close-state and catch the
prey within 100-500 ms after being triggered. This transformation from concave
to convex is initiated by changes in turgor pressure and the release of stored
elastic energy from prestresses in the concave state, which accelerate this
movement, leading to inversion of the lobes bi-axial curvature. Possessing two
low-energy states, the leaves can be characterized as bistable systems. With
our research, we seek to deepen the understanding of Venus flytrap motion
mechanics and apply its principles to the design of an artificial bistable lobe
actuator. We identified geometrical characteristics, such as dimensional ratios
and the thickness gradient in the lobe, and transferred these to two 3D-printed
bistable actuator models. One actuator parallels the simulated geometry of a
Venus flytrap leaf, the other is a lobe model designed with CAD. Both models
display concave-convex bi-stability and snap close. These demonstrators are the
first step in the development of an artificial Venus flytrap that mimics the
mechanical behavior of the biological model and can be used as a soft fast
gripper.

</details>


### [330] [Lateral Velocity Model for Vehicle Parking Applications](https://arxiv.org/abs/2511.01369)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 提出了一种改进的侧向速度模型，用于解决消费级车辆在自动泊车场景中侧向速度估计的挑战，该模型仅需两个参数且能更准确地捕捉车辆在泊车过程中的侧向动力学特性。


<details>
  <summary>Details</summary>
Motivation: 自动泊车需要精确的定位，但消费级车辆缺乏专用传感器来测量侧向速度。现有方法依赖零滑移假设的简化车辆模型，但在低速驾驶时该假设不成立，导致估计不准确。

Method: 分析真实泊车场景数据，识别零滑移假设的系统性偏差，提出一个仅需两个参数的侧向速度模型来更好地捕捉车辆在泊车过程中的侧向动力学特性。

Result: 提出的侧向速度模型提高了估计精度，同时仅依赖两个参数，适合集成到消费级应用中。

Conclusion: 新提出的侧向速度模型能够更准确地估计车辆在泊车过程中的侧向速度，解决了现有零滑移假设在低速场景下的局限性，为消费级自动泊车系统提供了实用的解决方案。

Abstract: Automated parking requires accurate localization for quick and precise
maneuvering in tight spaces. While the longitudinal velocity can be measured
using wheel encoders, the estimation of the lateral velocity remains a key
challenge due to the absence of dedicated sensors in consumer-grade vehicles.
Existing approaches often rely on simplified vehicle models, such as the
zero-slip model, which assumes no lateral velocity at the rear axle. It is well
established that this assumption does not hold during low-speed driving and
researchers thus introduce additional heuristics to account for differences. In
this work, we analyze real-world data from parking scenarios and identify a
systematic deviation from the zero-slip assumption. We provide explanations for
the observed effects and then propose a lateral velocity model that better
captures the lateral dynamics of the vehicle during parking. The model improves
estimation accuracy, while relying on only two parameters, making it
well-suited for integration into consumer-grade applications.

</details>


### [331] [CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels](https://arxiv.org/abs/2511.01379)
*Kun Hu,Menggang Li,Zhiwen Jin,Chaoquan Tang,Eryi Hu,Gongbo Zhou*

Main category: cs.RO

TL;DR: 提出了一种用于地下煤矿环境的CM-LIUW-Odometry多模态SLAM框架，结合LiDAR、IMU、UWB和轮式里程计，通过IESKF实现高精度定位与建图。


<details>
  <summary>Details</summary>
Motivation: 解决地下煤矿环境中GPS不可用、地形恶劣、特征稀少等挑战，提高SLAM系统在复杂环境下的精度和鲁棒性。

Method: 基于IESKF的多模态融合框架：1）LiDAR-IMU里程计与UWB绝对定位约束紧耦合；2）轮式里程计通过非完整约束和车辆杠杆臂补偿集成；3）自适应运动模式切换机制。

Result: 实验验证在真实地下煤矿场景中取得了优于现有方法的精度和鲁棒性。

Conclusion: CM-LIUW-Odometry框架有效解决了地下煤矿环境中的SLAM挑战，代码已开源供机器人社区使用。

Abstract: Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and
GPS-denied underground coal mine environments presents significant challenges.
Sensors must contend with abnormal operating conditions: GPS unavailability
impedes scene reconstruction and absolute geographic referencing, uneven or
slippery terrain degrades wheel odometer accuracy, and long, feature-poor
tunnels reduce LiDAR effectiveness. To address these issues, we propose
CoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM
framework based on the Iterated Error-State Kalman Filter (IESKF). First,
LiDAR-inertial odometry is tightly fused with UWB absolute positioning
constraints to align the SLAM system with a global coordinate. Next, wheel
odometer is integrated through tight coupling, enhanced by nonholonomic
constraints (NHC) and vehicle lever arm compensation, to address performance
degradation in areas beyond UWB measurement range. Finally, an adaptive motion
mode switching mechanism dynamically adjusts the robot's motion mode based on
UWB measurement range and environmental degradation levels. Experimental
results validate that our method achieves superior accuracy and robustness in
real-world underground coal mine scenarios, outperforming state-of-the-art
approaches. We open source our code of this work on Github to benefit the
robotics community.

</details>


### [332] [CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation](https://arxiv.org/abs/2511.01383)
*Landson Guo,Andres M. Diaz Aguilar,William Talbot,Turcan Tuna,Marco Hutter,Cesar Cadena*

Main category: cs.RO

TL;DR: 本文提出了一种名为CaRLi-V的新型雷达、激光雷达和相机融合管道，用于点级3D速度估计，通过结合速度立方体、光流和激光雷达数据，能够为密集点阵生成3D速度估计。


<details>
  <summary>Details</summary>
Motivation: 在3D中准确估计点级速度对于机器人与非刚性动态智能体（如人类）的交互至关重要，能够在动态环境中实现稳健的路径规划、碰撞避免和物体操作性能。

Method: 利用原始雷达测量创建速度立方体表示径向速度，结合光流估计切向速度，通过激光雷达获取点级距离测量，采用闭式解方法融合这些数据源。

Result: CaRLi-V已作为开源ROS2包开发，在自定义数据集上进行了现场测试，相对于地面真实值产生了较低的速度误差指标。

Conclusion: 该方法能够为机器人应用实现点级速度估计，在动态环境中提供准确的速度信息。

Abstract: Accurate point-wise velocity estimation in 3D is crucial for robot
interaction with non-rigid, dynamic agents, such as humans, enabling robust
performance in path planning, collision avoidance, and object manipulation in
dynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,
and camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.
This pipeline leverages raw RADAR measurements to create a novel RADAR
representation, the velocity cube, which densely represents radial velocities
within the RADAR's field-of-view. By combining the velocity cube for radial
velocity extraction, optical flow for tangential velocity estimation, and LiDAR
for point-wise range measurements through a closed-form solution, our approach
can produce 3D velocity estimates for a dense array of points. Developed as an
open-source ROS2 package, CaRLi-V has been field-tested against a custom
dataset and proven to produce low velocity error metrics relative to ground
truth, enabling point-wise velocity estimation for robotic applications.

</details>


### [333] [FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths](https://arxiv.org/abs/2511.01407)
*Paolo Rabino,Gabriele Tiboni,Tatiana Tommasi*

Main category: cs.RO

TL;DR: FoldPath是一种基于神经场的端到端对象中心运动生成方法，通过将机器人运动建模为连续函数来生成平滑路径，无需后处理步骤，在工业环境中仅需少量样本即可实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 当前OCMG技术要么基于临时启发式方法，要么依赖学习但需要敏感的后处理步骤来生成可执行路径，需要更鲁棒的算法来生成复杂3D几何上的对象感知轨迹。

Method: 提出FoldPath方法，基于神经场学习机器人运动作为连续函数，隐式编码平滑输出路径，而非预测离散的末端执行器路径点序列。

Result: 相比最近提出的基于学习的方法，FoldPath展现出优越的预测性能，在真实工业环境中仅需70个专家样本即可实现泛化能力，通过真实仿真环境验证了其有效性。

Conclusion: FoldPath通过将机器人运动建模为连续函数，消除了脆弱的后处理步骤，推进了OCMG任务向实际应用的成熟发展，并引入了新的严格指标来全面评估长时程机器人路径。

Abstract: Object-Centric Motion Generation (OCMG) is instrumental in advancing
automated manufacturing processes, particularly in domains requiring
high-precision expert robotic motions, such as spray painting and welding. To
realize effective automation, robust algorithms are essential for generating
extended, object-aware trajectories across intricate 3D geometries. However,
contemporary OCMG techniques are either based on ad-hoc heuristics or employ
learning-based pipelines that are still reliant on sensitive post-processing
steps to generate executable paths. We introduce FoldPath, a novel, end-to-end,
neural field based method for OCMG. Unlike prior deep learning approaches that
predict discrete sequences of end-effector waypoints, FoldPath learns the robot
motion as a continuous function, thus implicitly encoding smooth output paths.
This paradigm shift eliminates the need for brittle post-processing steps that
concatenate and order the predicted discrete waypoints. Particularly, our
approach demonstrates superior predictive performance compared to recently
proposed learning-based methods, and attains generalization capabilities even
in real industrial settings, where only a limited amount of 70 expert samples
are provided. We validate FoldPath through comprehensive experiments in a
realistic simulation environment and introduce new, rigorous metrics designed
to comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG
task towards practical maturity.

</details>


### [334] [AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models](https://arxiv.org/abs/2511.01472)
*Sarthak Mishra,Rishabh Dev Yadav,Avirup Das,Saksham Gupta,Wei Pan,Spandan Roy*

Main category: cs.RO

TL;DR: AERMANI-VLM是一个将预训练视觉语言模型适配到空中机械臂控制的框架，通过分离高层推理和低层控制，无需任务特定微调即可实现安全可靠的操作。


<details>
  <summary>Details</summary>
Motivation: 直接部署视觉语言模型驱动的策略到空中机械臂上存在不安全和不可靠的问题，因为生成的动作往往不一致、容易产生幻觉，且对飞行来说动态不可行。

Method: 通过结构化提示编码自然语言指令、任务上下文和安全约束，引导模型生成逐步推理轨迹，然后从预定义的飞行安全技能库中选择离散技能。

Result: 在模拟和硬件上的多样化多步骤拾放任务中验证了该框架，展示了对未见过的指令、物体和环境的强泛化能力。

Conclusion: 通过解耦符号推理和物理动作，AERMANI-VLM减轻了幻觉命令并防止不安全行为，实现了稳健的任务完成。

Abstract: The rapid progress of vision--language models (VLMs) has sparked growing
interest in robotic control, where natural language can express the operation
goals while visual feedback links perception to action. However, directly
deploying VLM-driven policies on aerial manipulators remains unsafe and
unreliable since the generated actions are often inconsistent,
hallucination-prone, and dynamically infeasible for flight. In this work, we
present AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial
manipulation by separating high-level reasoning from low-level control, without
any task-specific fine-tuning. Our framework encodes natural language
instructions, task context, and safety constraints into a structured prompt
that guides the model to generate a step-by-step reasoning trace in natural
language. This reasoning output is used to select from a predefined library of
discrete, flight-safe skills, ensuring interpretable and temporally consistent
execution. By decoupling symbolic reasoning from physical action, AERMANI-VLM
mitigates hallucinated commands and prevents unsafe behavior, enabling robust
task completion. We validate the framework in both simulation and hardware on
diverse multi-step pick-and-place tasks, demonstrating strong generalization to
previously unseen commands, objects, and environments.

</details>


### [335] [MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments](https://arxiv.org/abs/2511.01476)
*Cankut Bora Tuncer,Marc Toussaint,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: MO-SeGMan是一个多目标顺序引导操作规划器，用于解决高度受限的重排问题。它通过最小化重规划和机器人移动距离，同时保持关键依赖结构，在非单调场景中高效重定位关键障碍物，并通过自适应子目标选择减少不必要的操作动作。


<details>
  <summary>Details</summary>
Motivation: 解决高度受限、非单调的重排规划问题，需要在保持关键依赖结构的同时优化多个目标（重规划次数和移动距离），并处理高度杂乱的场景。

Method: 采用多目标顺序引导操作规划，结合惰性评估方法保持依赖结构；提出选择性引导前向搜索（SGFS）高效重定位关键障碍物；采用自适应子目标选择细化方法减少不必要的拾放动作。

Result: 在9个基准重排任务上的广泛评估表明，MO-SeGMan在所有情况下都能生成可行的运动规划，相比基线方法获得更快的求解时间和更优的解决方案质量。

Conclusion: MO-SeGMan框架在复杂重排规划问题中表现出鲁棒性和可扩展性，能够有效处理高度受限的场景并生成高质量解决方案。

Abstract: In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided
Manipulation planner for highly constrained rearrangement problems. MO-SeGMan
generates object placement sequences that minimize both replanning per object
and robot travel distance while preserving critical dependency structures with
a lazy evaluation method. To address highly cluttered, non-monotone scenarios,
we propose a Selective Guided Forward Search (SGFS) that efficiently relocates
only critical obstacles and to feasible relocation points. Furthermore, we
adopt a refinement method for adaptive subgoal selection to eliminate
unnecessary pick-and-place actions, thereby improving overall solution quality.
Extensive evaluations on nine benchmark rearrangement tasks demonstrate that
MO-SeGMan generates feasible motion plans in all cases, consistently achieving
faster solution times and superior solution quality compared to the baselines.
These results highlight the robustness and scalability of the proposed
framework for complex rearrangement planning problems.

</details>


### [336] [Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues](https://arxiv.org/abs/2511.01493)
*Wei Huang,Jiaxin Li,Zang Wan,Huijun Di,Wei Liang,Zhu Yang*

Main category: cs.RO

TL;DR: 提出GlocDiff，一种基于扩散模型的导航策略，通过结合楼层平面图的全局路径规划和RGB观测的局部深度特征，解决室内导航中模态差异和定位精度问题。


<details>
  <summary>Details</summary>
Motivation: 解决室内导航中两个关键挑战：RGB观测与楼层平面图之间的模态差异阻碍了视觉和空间信息的融合，以及在未见环境中由于缺乏RGB输入与平面图的显式几何对齐而导致的定位精度问题。

Method: 提出GlocDiff扩散策略，集成来自楼层平面图的全局路径规划和来自RGB观测的局部深度感知特征。在训练中引入噪声扰动增强对姿态估计误差的鲁棒性，在推理时结合相对稳定的VO模块。

Result: 在FloNa基准测试上的广泛实验证明GlocDiff在实现优越导航性能方面的效率和有效性，真实世界部署的成功也突显其广泛实际应用的潜力。

Conclusion: GlocDiff通过融合全局平面图指导和局部深度特征，能够精确预测最优导航方向并实现鲁棒的障碍物避让，在室内导航任务中表现出色且具有实际应用价值。

Abstract: Guiding an agent to a specific target in indoor environments based solely on
RGB inputs and a floor plan is a promising yet challenging problem. Although
existing methods have made significant progress, two challenges remain
unresolved. First, the modality gap between egocentric RGB observations and the
floor plan hinders the integration of visual and spatial information for both
local obstacle avoidance and global planning. Second, accurate localization is
critical for navigation performance, but remains challenging at deployment in
unseen environments due to the lack of explicit geometric alignment between RGB
inputs and floor plans. We propose a novel diffusion-based policy, denoted as
GlocDiff, which integrates global path planning from the floor plan with local
depth-aware features derived from RGB observations. The floor plan offers
explicit global guidance, while the depth features provide implicit geometric
cues, collectively enabling precise prediction of optimal navigation directions
and robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation
during training to enhance robustness against pose estimation errors, and we
find that combining this with a relatively stable VO module during inference
results in significantly improved navigation performance. Extensive experiments
on the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in
achieving superior navigation performance, and the success of real-world
deployments also highlights its potential for widespread practical
applications.

</details>


### [337] [Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals](https://arxiv.org/abs/2511.01520)
*Shipeng Lyu,Lijie Sheng,Fangyuan Wang,Wenyao Zhang,Weiwei Lin,Zhenzhong Jia,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 提出Phy-Tac方法，通过物理条件触觉技术实现力最优稳定抓取，结合姿态选择、触觉预测和力调节，使机器人像人类一样用最小必要力稳定抓取物体。


<details>
  <summary>Details</summary>
Motivation: 解决机器人抓取时过度用力的问题，缩小与人类自然抓取（使用最小必要力）之间的差距。

Method: 1. 基于物理的姿态选择器识别最优力分布的接触区域；2. 物理条件潜在扩散模型预测目标触觉印记；3. 潜在空间LQR控制器驱动夹爪以最小驱动达到目标触觉。

Result: Phy-LDM在触觉预测精度上表现优异，Phy-Tac在抓取稳定性和力效率方面优于固定力和GraspNet基线方法。

Conclusion: 该方法实现了力高效和自适应操作，缩小了机器人与人类抓取之间的差距。

Abstract: Humans naturally grasp objects with minimal level required force for
stability, whereas robots often rely on rigid, over-squeezing control. To
narrow this gap, we propose a human-inspired physics-conditioned tactile method
(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,
tactile prediction, and force regulation. A physics-based pose selector first
identifies feasible contact regions with optimal force distribution based on
surface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)
predicts the tactile imprint under FOSG target. Last, a latent-space LQR
controller drives the gripper toward this tactile imprint with minimal
actuation, preventing unnecessary compression. Trained on a physics-conditioned
tactile dataset covering diverse objects and contact conditions, the proposed
Phy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac
outperforms fixed-force and GraspNet-based baselines in grasp stability and
force efficiency. Experiments on classical robotic platforms demonstrate
force-efficient and adaptive manipulation that bridges the gap between robotic
and human grasping.

</details>


### [338] [MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence](https://arxiv.org/abs/2511.01594)
*Renjun Gao,Peiyan Zhong*

Main category: cs.RO

TL;DR: MARS是一个基于多模态大语言模型的多智能体机器人系统，专为智能家居机器人设计，为残障人士提供辅助智能服务。该系统通过四个智能体的协同工作实现风险感知、个性化规划和可执行技能落地。


<details>
  <summary>Details</summary>
Motivation: 现有系统在风险感知规划、用户个性化和将语言计划转化为可执行技能方面存在困难，特别是在杂乱的家庭环境中。需要开发能够适应动态室内环境的自适应、风险感知和个性化辅助系统。

Method: 集成四个智能体：视觉感知智能体从环境图像中提取语义和空间特征，风险评估智能体识别和优先处理危险，规划智能体生成可执行动作序列，评估智能体进行迭代优化。通过多模态感知与分层多智能体决策相结合。

Result: 在多个数据集上的实验表明，该系统在风险感知规划和协调多智能体执行方面优于最先进的多模态模型，展现出卓越的整体性能。

Conclusion: 该方法展示了协作AI在实际辅助场景中的潜力，并为在真实环境中部署基于MLLM的多智能体系统提供了可推广的方法论。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities
in cross-modal understanding and reasoning, offering new opportunities for
intelligent assistive systems, yet existing systems still struggle with
risk-aware planning, user personalization, and grounding language plans into
executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic
System powered by MLLMs for assistive intelligence and designed for smart home
robots supporting people with disabilities. The system integrates four agents:
a visual perception agent for extracting semantic and spatial features from
environment images, a risk assessment agent for identifying and prioritizing
hazards, a planning agent for generating executable action sequences, and an
evaluation agent for iterative optimization. By combining multimodal perception
with hierarchical multi-agent decision-making, the framework enables adaptive,
risk-aware, and personalized assistance in dynamic indoor environments.
Experiments on multiple datasets demonstrate the superior overall performance
of the proposed system in risk-aware planning and coordinated multi-agent
execution compared with state-of-the-art multimodal models. The proposed
approach also highlights the potential of collaborative AI for practical
assistive scenarios and provides a generalizable methodology for deploying
MLLM-enabled multi-agent systems in real-world environments.

</details>


### [339] [Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping](https://arxiv.org/abs/2511.01770)
*Liudi Yang,Yang Bai,Yuhao Wang,Ibrahim Alsarraj,Gitta Kutyniok,Zhanchi Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了一种轻量级的驱动空间学习框架，利用流匹配模型从确定性演示中学习软体机器人抓取的控制表示，仅需30个演示即可在完整工作空间实现97.5%的抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 传统刚性机器人手在不确定接触下需要复杂模型和重反馈控制，而软体机器人具有机械智能特性，其欠驱动结构和全身柔顺性能够自然适应不确定接触。

Method: 使用流匹配模型从确定性演示中学习分布式的控制表示，无需密集传感或重控制循环，仅需少量演示数据。

Result: 仅用30个演示（不到可达工作空间的8%）即可在整个工作空间实现97.5%的抓取成功率，能够泛化到±33%的抓取物体尺寸变化，并在执行时间缩放20%-200%时保持稳定性能。

Conclusion: 驱动空间学习通过利用软体机器人的被动冗余自由度和柔顺性，将身体力学转化为功能控制智能，显著减轻了中央控制器在这种不确定任务中的负担。

Abstract: Robotic grasping under uncertainty remains a fundamental challenge due to its
uncertain and contact-rich nature. Traditional rigid robotic hands, with
limited degrees of freedom and compliance, rely on complex model-based and
heavy feedback controllers to manage such interactions. Soft robots, by
contrast, exhibit embodied mechanical intelligence: their underactuated
structures and passive flexibility of their whole body, naturally accommodate
uncertain contacts and enable adaptive behaviors. To harness this capability,
we propose a lightweight actuation-space learning framework that infers
distributional control representations for whole-body soft robotic grasping,
directly from deterministic demonstrations using a flow matching model
(Rectified Flow),without requiring dense sensing or heavy control loops. Using
only 30 demonstrations (less than 8% of the reachable workspace), the learned
policy achieves a 97.5% grasp success rate across the whole workspace,
generalizes to grasped-object size variations of +-33%, and maintains stable
performance when the robot's dynamic response is directly adjusted by scaling
the execution time from 20% to 200%. These results demonstrate that
actuation-space learning, by leveraging its passive redundant DOFs and
flexibility, converts the body's mechanics into functional control intelligence
and substantially reduces the burden on central controllers for this
uncertain-rich task.

</details>


### [340] [MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll](https://arxiv.org/abs/2511.01774)
*Alexander Schperberg,Yusuke Tanaka,Stefano Di Cairano,Dennis Hong*

Main category: cs.RO

TL;DR: MOBIUS是一个多模态双足智能城市侦察机器人，能够行走、爬行、攀爬和滚动，通过混合控制架构和高级规划器实现不同运动模式的平滑转换。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在多样化地形中无缝转换运动模式的机器人，扩展机器人的交互能力、工作空间和可穿越性。

Method: 采用四肢结构（两个6自由度手臂和两个4自由度腿部），结合强化学习运动控制与模型预测和导纳控制，使用MIQCP规划器自主选择运动模式。

Result: 硬件实验展示了稳健的步态转换、动态攀爬和通过夹持抓握实现的全身体重支撑。

Conclusion: MOBIUS证明了形态学、高级规划和控制之间的紧密集成对于实现移动定位操作和抓取的重要性，显著扩展了其交互能力。

Abstract: This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot
(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features
four limbs--two 6-DoF arms with two-finger grippers for manipulation and
climbing, and two 4-DoF legs for locomotion--enabling smooth transitions across
diverse terrains without reconfiguration. A hybrid control architecture
combines reinforcement learning-based locomotion with model-based predictive
and admittance control enhanced for safety by a Reference Governor toward
compliant contact interactions. A high-level MIQCP planner autonomously selects
locomotion modes to balance stability and energy efficiency. Hardware
experiments demonstrate robust gait transitions, dynamic climbing, and
full-body load support via pinch grasp. Overall, MOBIUS demonstrates the
importance of tight integration between morphology, high-level planning, and
control to enable mobile loco-manipulation and grasping, substantially
expanding its interaction capabilities, workspace, and traversability.

</details>
