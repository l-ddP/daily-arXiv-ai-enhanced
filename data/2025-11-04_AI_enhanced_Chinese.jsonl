{"id": "2511.00026", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00026", "abs": "https://arxiv.org/abs/2511.00026", "authors": ["Chaitanya Shinde", "Divya Garikapati"], "title": "Gen AI in Automotive: Applications, Challenges, and Opportunities with a Case study on In-Vehicle Experience", "comment": null, "summary": "Generative Artificial Intelligence is emerging as a transformative force in\nthe automotive industry, enabling novel applications across vehicle design,\nmanufacturing, autonomous driving, predictive maintenance, and in vehicle user\nexperience. This paper provides a comprehensive review of the current state of\nGenAI in automotive, highlighting enabling technologies such as Generative\nAdversarial Networks and Variational Autoencoders. Key opportunities include\naccelerating autonomous driving validation through synthetic data generation,\noptimizing component design, and enhancing human machine interaction via\npersonalized and adaptive interfaces. At the same time, the paper identifies\nsignificant technical, ethical, and safety challenges, including computational\ndemands, bias, intellectual property concerns, and adversarial robustness, that\nmust be addressed for responsible deployment. A case study on Mercedes Benzs\nMBUX Virtual Assistant illustrates how GenAI powered voice systems deliver more\nnatural, proactive, and personalized in car interactions compared to legacy\nrule based assistants. Through this review and case study, the paper outlines\nboth the promise and limitations of GenAI integration in the automotive sector\nand presents directions for future research and development aimed at achieving\nsafer, more efficient, and user centric mobility. Unlike prior reviews that\nfocus solely on perception or manufacturing, this paper emphasizes generative\nAI in voice based HMI, bridging safety and user experience perspectives.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u6c7d\u8f66\u884c\u4e1a\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u91cd\u70b9\u5206\u6790\u4e86GAN\u548cVAE\u7b49\u4f7f\u80fd\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u3001\u96f6\u90e8\u4ef6\u8bbe\u8ba1\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u65b9\u9762\u7684\u673a\u9047\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8ba1\u7b97\u9700\u6c42\u3001\u504f\u89c1\u3001\u77e5\u8bc6\u4ea7\u6743\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6b63\u5728\u6210\u4e3a\u6c7d\u8f66\u884c\u4e1a\u7684\u53d8\u9769\u529b\u91cf\uff0c\u4f46\u73b0\u6709\u7efc\u8ff0\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u6216\u5236\u9020\u9886\u57df\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u751f\u6210\u5f0fAI\u5728\u8bed\u97f3\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u8fde\u63a5\u5b89\u5168\u548c\u7528\u6237\u4f53\u9a8c\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6885\u8d5b\u5fb7\u65af-\u5954\u9a70MBUX\u865a\u62df\u52a9\u624b\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u751f\u6210\u5f0fAI\u5728\u6c7d\u8f66\u9886\u57df\u7684\u5e94\u7528\u73b0\u72b6\u3001\u673a\u9047\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u80fd\u591f\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u9a8c\u8bc1\u3001\u4f18\u5316\u96f6\u90e8\u4ef6\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u4e2a\u6027\u5316\u81ea\u9002\u5e94\u754c\u9762\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u8bed\u97f3\u7cfb\u7edf\u6bd4\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u52a9\u624b\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u4e3b\u52a8\u548c\u4e2a\u6027\u5316\u7684\u8f66\u5185\u4ea4\u4e92\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u6c7d\u8f66\u884c\u4e1a\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6280\u672f\u3001\u4f26\u7406\u548c\u5b89\u5168\u6311\u6218\u624d\u80fd\u5b9e\u73b0\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002\u672a\u6765\u7814\u7a76\u5e94\u81f4\u529b\u4e8e\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u79fb\u52a8\u51fa\u884c\u3002"}}
{"id": "2511.00033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00033", "abs": "https://arxiv.org/abs/2511.00033", "authors": ["Diqi He", "Xuehao Gao", "Hao Li", "Junwei Han", "Dingwen Zhang"], "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization", "comment": null, "summary": "The Zero-shot Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) task requires agents to navigate previously unseen 3D environments\nusing natural language instructions, without any scene-specific training. A\ncritical challenge in this setting lies in ensuring agents' actions align with\nboth spatial structure and task intent over long-horizon execution. Existing\nmethods often fail to achieve robust navigation due to a lack of structured\ndecision-making and insufficient integration of feedback from previous actions.\nTo address these challenges, we propose STRIDER (Instruction-Aligned Structural\nDecision Space Optimization), a novel framework that systematically optimizes\nthe agent's decision space by integrating spatial layout priors and dynamic\ntask feedback. Our approach introduces two key innovations: 1) a Structured\nWaypoint Generator that constrains the action space through spatial structure,\nand 2) a Task-Alignment Regulator that adjusts behavior based on task progress,\nensuring semantic alignment throughout navigation. Extensive experiments on the\nR2R-CE and RxR-CE benchmarks demonstrate that STRIDER significantly outperforms\nstrong SOTA across key metrics; in particular, it improves Success Rate (SR)\nfrom 29% to 35%, a relative gain of 20.7%. Such results highlight the\nimportance of spatially constrained decision-making and feedback-guided\nexecution in improving navigation fidelity for zero-shot VLN-CE.", "AI": {"tldr": "STRIDER\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u7a7a\u95f4\u5e03\u5c40\u5148\u9a8c\u548c\u52a8\u6001\u4efb\u52a1\u53cd\u9988\uff0c\u7cfb\u7edf\u4f18\u5316\u667a\u80fd\u4f53\u5728\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u5316\u51b3\u7b56\u548c\u5bf9\u5148\u524d\u52a8\u4f5c\u53cd\u9988\u7684\u5145\u5206\u6574\u5408\uff0c\u5bfc\u81f4\u5bfc\u822a\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSTRIDER\u6846\u67b6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u8def\u5f84\u70b9\u751f\u6210\u5668\uff08\u7ea6\u675f\u52a8\u4f5c\u7a7a\u95f4\uff09\u548c\u4efb\u52a1\u5bf9\u9f50\u8c03\u8282\u5668\uff08\u57fa\u4e8e\u4efb\u52a1\u8fdb\u5ea6\u8c03\u6574\u884c\u4e3a\uff09\uff0c\u786e\u4fdd\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTRIDER\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u4ece29%\u63d0\u5347\u81f335%\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe20.7%\u3002", "conclusion": "\u7a7a\u95f4\u7ea6\u675f\u51b3\u7b56\u548c\u53cd\u9988\u5f15\u5bfc\u6267\u884c\u5bf9\u4e8e\u63d0\u9ad8\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u4fdd\u771f\u5ea6\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.00041", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00041", "abs": "https://arxiv.org/abs/2511.00041", "authors": ["Yingzhao Jian", "Zhongan Wang", "Yi Yang", "Hehe Fan"], "title": "Endowing GPT-4 with a Humanoid Body: Building the Bridge Between Off-the-Shelf VLMs and the Physical World", "comment": null, "summary": "Humanoid agents often struggle to handle flexible and diverse interactions in\nopen environments. A common solution is to collect massive datasets to train a\nhighly capable model, but this approach can be prohibitively expensive. In this\npaper, we explore an alternative solution: empowering off-the-shelf\nVision-Language Models (VLMs, such as GPT-4) to control humanoid agents,\nthereby leveraging their strong open-world generalization to mitigate the need\nfor extensive data collection. To this end, we present \\textbf{BiBo}\n(\\textbf{B}uilding humano\\textbf{I}d agent \\textbf{B}y \\textbf{O}ff-the-shelf\nVLMs). It consists of two key components: (1) an \\textbf{embodied instruction\ncompiler}, which enables the VLM to perceive the environment and precisely\ntranslate high-level user instructions (e.g., {\\small\\itshape ``have a rest''})\ninto low-level primitive commands with control parameters (e.g.,\n{\\small\\itshape ``sit casually, location: (1, 2), facing: 90$^\\circ$''}); and\n(2) a diffusion-based \\textbf{motion executor}, which generates human-like\nmotions from these commands, while dynamically adapting to physical feedback\nfrom the environment. In this way, BiBo is capable of handling not only basic\ninteractions but also diverse and complex motions. Experiments demonstrate that\nBiBo achieves an interaction task success rate of 90.2\\% in open environments,\nand improves the precision of text-guided motion execution by 16.3\\% over prior\nmethods. The code will be made publicly available.", "AI": {"tldr": "BiBo\u7cfb\u7edf\u5229\u7528\u73b0\u6210\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u4eba\u5f62\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6307\u4ee4\u7f16\u8bd1\u5668\u548c\u8fd0\u52a8\u6267\u884c\u5668\u5b9e\u73b0\u5f00\u653e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u4ea4\u4e92\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5904\u7406\u7075\u6d3b\u591a\u6837\u4ea4\u4e92\u7684\u56f0\u96be\uff0c\u907f\u514d\u6602\u8d35\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u9700\u6c42\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u5177\u8eab\u6307\u4ee4\u7f16\u8bd1\u5668\uff0c\u5c06\u9ad8\u7ea7\u7528\u6237\u6307\u4ee4\u7ffb\u8bd1\u4e3a\u4f4e\u7ea7\u539f\u59cb\u547d\u4ee4\uff1b2) \u57fa\u4e8e\u6269\u6563\u7684\u8fd0\u52a8\u6267\u884c\u5668\uff0c\u4ece\u547d\u4ee4\u751f\u6210\u7c7b\u4eba\u8fd0\u52a8\u5e76\u9002\u5e94\u73af\u5883\u7269\u7406\u53cd\u9988\u3002", "result": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5b9e\u73b090.2%\u7684\u4ea4\u4e92\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6587\u672c\u5f15\u5bfc\u8fd0\u52a8\u6267\u884c\u7cbe\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad816.3%\u3002", "conclusion": "BiBo\u7cfb\u7edf\u6210\u529f\u5229\u7528\u73b0\u6210VLMs\u63a7\u5236\u4eba\u5f62\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u591a\u6837\u5316\u7684\u5f00\u653e\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2511.00088", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00088", "abs": "https://arxiv.org/abs/2511.00088", "authors": ["NVIDIA", ":", "Yan Wang", "Wenjie Luo", "Junjie Bai", "Yulong Cao", "Tong Che", "Ke Chen", "Yuxiao Chen", "Jenna Diamond", "Yifan Ding", "Wenhao Ding", "Liang Feng", "Greg Heinrich", "Jack Huang", "Peter Karkus", "Boyi Li", "Pinyi Li", "Tsung-Yi Lin", "Dongran Liu", "Ming-Yu Liu", "Langechuan Liu", "Zhijian Liu", "Jason Lu", "Yunxiang Mao", "Pavlo Molchanov", "Lindsey Pavao", "Zhenghao Peng", "Mike Ranzinger", "Ed Schmerling", "Shida Shen", "Yunfei Shi", "Sarah Tariq", "Ran Tian", "Tilman Wekel", "Xinshuo Weng", "Tianjun Xiao", "Eric Yang", "Xiaodong Yang", "Yurong You", "Xiaohui Zeng", "Wenyuan Zhang", "Boris Ivanovic", "Marco Pavone"], "title": "Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail", "comment": null, "summary": "End-to-end architectures trained via imitation learning have advanced\nautonomous driving by scaling model size and data, yet performance remains\nbrittle in safety-critical long-tail scenarios where supervision is sparse and\ncausal understanding is limited. To address this, we introduce Alpamayo-R1\n(AR1), a vision-language-action model (VLA) that integrates Chain of Causation\nreasoning with trajectory planning to enhance decision-making in complex\ndriving scenarios. Our approach features three key innovations: (1) the Chain\nof Causation (CoC) dataset, built through a hybrid auto-labeling and\nhuman-in-the-loop pipeline producing decision-grounded, causally linked\nreasoning traces aligned with driving behaviors; (2) a modular VLA architecture\ncombining Cosmos-Reason, a Vision-Language Model pre-trained for Physical AI\napplications, with a diffusion-based trajectory decoder that generates\ndynamically feasible plans in real time; (3) a multi-stage training strategy\nusing supervised fine-tuning to elicit reasoning and reinforcement learning\n(RL) to optimize reasoning quality via large reasoning model feedback and\nenforce reasoning-action consistency. Evaluation shows AR1 achieves up to a 12%\nimprovement in planning accuracy on challenging cases compared to a\ntrajectory-only baseline, with a 35% reduction in off-road rate and 25%\nreduction in close encounter rate in closed-loop simulation. RL post-training\nimproves reasoning quality by 45% as measured by a large reasoning model critic\nand reasoning-action consistency by 37%. Model scaling from 0.5B to 7B\nparameters shows consistent improvements. On-vehicle road tests confirm\nreal-time performance (99 ms latency) and successful urban deployment. By\nbridging interpretable reasoning with precise control, AR1 demonstrates a\npractical path towards Level 4 autonomous driving. We plan to release AR1\nmodels and a subset of the CoC in a future update.", "AI": {"tldr": "AR1\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u94fe\u63a8\u7406\u4e0e\u8f68\u8ff9\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5728\u95ed\u73af\u4eff\u771f\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u504f\u79bb\u9053\u8def\u548c\u8fd1\u8ddd\u79bb\u63a5\u89e6\u7387\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u7684\u957f\u5c3e\u573a\u666f\u4e2d\u6027\u80fd\u8106\u5f31\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u573a\u666f\u4e2d\u76d1\u7763\u7a00\u758f\u4e14\u56e0\u679c\u7406\u89e3\u6709\u9650\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u6784\u5efa\u56e0\u679c\u94fe\u6570\u636e\u96c6\uff1b2) \u6a21\u5757\u5316VLA\u67b6\u6784\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u8f68\u8ff9\u89e3\u7801\u5668\uff1b3) \u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6311\u6218\u6027\u6848\u4f8b\u4e2d\u89c4\u5212\u51c6\u786e\u7387\u63d0\u534712%\uff0c\u504f\u79bb\u9053\u8def\u7387\u51cf\u5c1135%\uff0c\u8fd1\u8ddd\u79bb\u63a5\u89e6\u7387\u51cf\u5c1125%\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u534745%\uff0c\u63a8\u7406-\u884c\u52a8\u4e00\u81f4\u6027\u63d0\u534737%\u3002", "conclusion": "AR1\u901a\u8fc7\u5c06\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u7cbe\u786e\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u5c55\u793a\u4e86\u5b9e\u73b0L4\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.00195", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00195", "abs": "https://arxiv.org/abs/2511.00195", "authors": ["Shengqian Wang", "Israt Jahan Jui", "Julie Thorpe"], "title": "Is Crowdsourcing a Puppet Show? Detecting a New Type of Fraud in Online Platforms", "comment": "20 pages, 2 figures", "summary": "Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) are important\ntools for researchers seeking to conduct studies with a broad, global\nparticipant base. Despite their popularity and demonstrated utility, we present\nevidence that suggests the integrity of data collected through Amazon MTurk is\nbeing threatened by the presence of puppeteers, apparently human workers\ncontrolling multiple puppet accounts that are capable of bypassing standard\nattention checks. If left undetected, puppeteers and their puppets can\nundermine the integrity of data collected on these platforms. This paper\ninvestigates data from two Amazon MTurk studies, finding that a substantial\nproportion of accounts (33% to 56.4%) are likely puppets. Our findings\nhighlight the importance of adopting multifaceted strategies to ensure data\nintegrity on crowdsourcing platforms. With the goal of detecting this type of\nfraud, we discuss a set of potential countermeasures for both puppets and bots\nwith varying degrees of sophistication (e.g., employing AI). The problem of\nsingle entities (or puppeteers) manually controlling multiple accounts could\nexist on other crowdsourcing platforms; as such, their detection may be of\nbroader application.\n  While our findings suggest the need to re-evaluate the quality of\ncrowdsourced data, many previous studies likely remain valid, particularly\nthose with robust experimental designs. However, the presence of puppets may\nhave contributed to false null results in some studies, suggesting that\nunpublished work may be worth revisiting with effective puppet detection\nstrategies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.00279", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DC", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00279", "abs": "https://arxiv.org/abs/2511.00279", "authors": ["Meituan LongCat Team", "Bairui Wang", "Bayan", "Bin Xiao", "Bo Zhang", "Bolin Rong", "Borun Chen", "Chang Wan", "Chao Zhang", "Chen Huang", "Chen Chen", "Chen Chen", "Chengxu Yang", "Chengzuo Yang", "Cong Han", "Dandan Peng", "Delian Ruan", "Detai Xin", "Disong Wang", "Dongchao Yang", "Fanfan Liu", "Fengjiao Chen", "Fengyu Yang", "Gan Dong", "Gang Huang", "Gang Xu", "Guanglu Wan", "Guoqiang Tan", "Guoqiao Yu", "Haibo Qiu", "Hao Lu", "Hongbo Liu", "Hongyu Xiang", "Jiaheng Wu", "Jian Yang", "Jiaxing Liu", "Jing Huang", "Jingang Wang", "Jinrui Ding", "Juchao Jiang", "Jun Kuang", "Jun Wang", "Junhui Mei", "Ke Ding", "Kefeng Zhang", "Lei Chen", "Liang Shi", "Limeng Qiao", "Liming Zheng", "Lin Ma", "Liuyang Guo", "Liya Ma", "Luying Sun", "Man Gao", "Mengshen Zhu", "Miao Cao", "Minliang Lin", "Nuo Xu", "Peng Shi", "Qi Zhang", "Qian Fang", "Qian Wang", "Qian Yang", "Quanxiu Wang", "Rongxiang Weng", "Rongxin Guo", "Ruoxuan Liang", "Senbin Yang", "Shanbo Xu", "Shanglin Lei", "Shengze Ye", "Shimin Chen", "Shuaiqi Chen", "Shujie Hu", "Shuo Li", "Siqi Yang", "Siyu Xu", "Siyu Ren", "Song Li", "Songxiang Liu", "Tianhao Bai", "Tianye Dai", "Wei Hong", "Wei Wang", "Weixiao Zhao", "Wengang Cao", "Wenlong Zhu", "Wenlong He", "Xi Su", "Xi Nan", "Xiaohan Zhao", "Xiaohao Wang", "Xiaoyu Zhao", "Xiaoyu Wang", "Xiaoyu Li", "Xin Pan", "Xin Chen", "Xiusong Sun", "Xu Xiang", "Xudong Xing", "Xuezhi Cao", "Xunliang Cai", "Yang Yang", "Yanli Tan", "Yao Yao", "Yerui Sun", "Yi Chen", "Yifan Lu", "Yin Gong", "Yining Zhang", "Yitian Chen", "Yiyang Gan", "Yuchen Tang", "Yuchen Xie", "Yueqian Wang", "Yuewen Zheng", "Yufei Zhang", "Yufeng Zhong", "Yulei Qian", "Yuqi Peng", "Yuwei Jiang", "Zeyang Hu", "Zheng Zhang", "Zhengkun Tian", "Zhiqing Hong", "Zhixiong Zeng", "Zhuqi Mi", "Ziran Li", "Ziwen Wang", "Ziyi Zhao", "Ziyuan Zhuang", "Zizhe Zhao"], "title": "LongCat-Flash-Omni Technical Report", "comment": null, "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community.", "AI": {"tldr": "LongCat-Flash-Omni\u662f\u4e00\u4e2a5600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u5168\u6a21\u6001\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u548c\u9ad8\u6548\u7684\u591a\u6a21\u6001\u611f\u77e5\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u5f3a\u5927\u5355\u6a21\u6001\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u97f3\u89c6\u9891\u4ea4\u4e92\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u79cd\u6a21\u6001\u4e14\u4fdd\u6301\u5b9e\u65f6\u4ea4\u4e92\u6027\u80fd\u7684\u5927\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u7684\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u7b80\u5355\u5230\u590d\u6742\u7684\u6a21\u6001\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\uff1b\u4f7f\u7528Shortcut\u8fde\u63a5\u7684MoE\u67b6\u6784\u548c\u96f6\u8ba1\u7b97\u4e13\u5bb6\uff1b\u5f00\u53d1\u6a21\u6001\u89e3\u8026\u5e76\u884c\u65b9\u6848\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u3002", "result": "\u6a21\u578b\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u5168\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u7406\u89e3\u548c\u97f3\u9891\u7406\u89e3\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc790%\u7684\u7eaf\u6587\u672c\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "conclusion": "LongCat-Flash-Omni\u6210\u529f\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u5b9e\u65f6\u4ea4\u4e92\u80fd\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2511.00010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark.", "AI": {"tldr": "PlotCraft\u662f\u4e00\u4e2a\u65b0\u7684\u53ef\u89c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1000\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u53ef\u89c6\u5316\u4efb\u52a1\uff0c\u6db5\u76d67\u4e2a\u9ad8\u7ea7\u53ef\u89c6\u5316\u4efb\u52a1\u548c48\u79cd\u56fe\u8868\u7c7b\u578b\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u5728\u5904\u7406\u590d\u6742\u53ef\u89c6\u5316\u4efb\u52a1\u65f6\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u4e3a\u6b64\u5f00\u53d1\u4e86SynthVis-30K\u6570\u636e\u96c6\u548cPlotCraftor\u6a21\u578b\uff0c\u5728\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e3a\u89c4\u6a21\u5316\u7ed3\u6784\u5316\u6570\u636e\u521b\u5efa\u590d\u6742\u53ef\u89c6\u5316\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u548c\u53d1\u5c55\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165PlotCraft\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f00\u53d1SynthVis-30K\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u590d\u6742\u53ef\u89c6\u5316\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efaPlotCraftor\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002", "result": "\u5bf923\u4e2a\u9886\u5148LLMs\u7684\u8bc4\u4f30\u663e\u793a\u5176\u5728\u5904\u7406\u590d\u6742\u53ef\u89c6\u5316\u4efb\u52a1\u65f6\u5b58\u5728\u660e\u663e\u6027\u80fd\u7f3a\u9677\u3002PlotCraftor\u6a21\u578b\u5728VisEval\u3001PandasPlotBench\u548cPlotCraft\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u9886\u5148\u4e13\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u5b9e\u73b0\u8d85\u8fc750%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PlotCraft\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86LLMs\u5728\u590d\u6742\u53ef\u89c6\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u800c\u57fa\u4e8eSynthVis-30K\u6570\u636e\u96c6\u5f00\u53d1\u7684PlotCraftor\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u6570\u636e\u53ef\u89c6\u5316\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.00011", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fd0\u52a8\u6355\u6349\u6570\u636e\u7684\u4ea4\u4e92\u6a21\u578b\uff0c\u901a\u8fc7\u90e8\u5206\u6a21\u4eff\u548c\u521b\u9020\u6027\u589e\u5f3a\u8f93\u5165\u7684\u8fd0\u52a8\u5e8f\u5217\u6765\u751f\u6210\u4eba\u5de5\u821e\u4f34\u3002\u8fd9\u662f\u9996\u4e2a\u5229\u7528\u5355\u4eba\u8fd0\u52a8\u6570\u636e\u548c\u9ad8\u5c42\u7279\u5f81\u5b9e\u73b0\u6b64\u76ee\u6807\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u3001\u8fd0\u52a8\u4fee\u590d\u548c\u8fd0\u52a8\u98ce\u683c\u8f6c\u6362\u6280\u672f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u652f\u6301\u7528\u6237\uff0c\u4f46\u7f3a\u4e4f\u5177\u8eab\u4ea4\u4e92\u7279\u6027\u3002\u821e\u8e48\u4f5c\u4e3a\u4eba\u7c7b\u8868\u8fbe\u7684\u539f\u59cb\u5f62\u5f0f\uff0c\u80fd\u591f\u8865\u5145\u8fd9\u79cd\u4f53\u9a8c\uff0c\u63a2\u7d22\u4ee5\u821e\u8e48\u4e3a\u4f8b\u7684\u521b\u9020\u6027\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u6269\u6563\u6a21\u578b\u3001\u8fd0\u52a8\u4fee\u590d\u548c\u8fd0\u52a8\u98ce\u683c\u8f6c\u6362\u7684\u601d\u60f3\uff0c\u5229\u7528\u5355\u4eba\u8fd0\u52a8\u6570\u636e\u548c\u9ad8\u5c42\u7279\u5f81\uff0c\u751f\u6210\u65e2\u5177\u6709\u65f6\u95f4\u8fde\u8d2f\u6027\u53c8\u80fd\u54cd\u5e94\u9009\u5b9a\u8fd0\u52a8\u53c2\u8003\u7684\u8fd0\u52a8\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u751f\u6210\u6837\u672c\u7279\u5f81\u5206\u5e03\u4e0e\u6d4b\u8bd5\u96c6\u7684\u6536\u655b\u6027\u6765\u8bc1\u660e\u6a21\u578b\u6210\u529f\uff0c\u751f\u6210\u7ed3\u679c\u65e2\u591a\u6837\u5316\uff08\u663e\u793a\u4e0e\u4eba\u7c7b\u821e\u4f34\u7684\u5404\u79cd\u504f\u5dee\uff09\u53c8\u663e\u5f97\u771f\u5b9e\u3002", "conclusion": "\u8be5\u6a21\u578b\u7684\u751f\u6210\u7ed3\u679c\u662f\u5b9e\u73b0\u4e0eAI\u521b\u9020\u6027\u821e\u8e48\u5171\u821e\u7684\u7b2c\u4e00\u6b65\uff0c\u4e3a\u5177\u8eab\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.00020", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6a21\u6001\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u5305\u542b21,142\u5f20\u7528\u6237\u4e0a\u4f20\u56fe\u7247\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e860.934\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u5546\u52a1\u4e2d\u865a\u5047\u8bc4\u8bba\u6cdb\u6ee5\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u6587\u672c\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u8de8\u6a21\u6001\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\uff0c\u5a01\u80c1\u6570\u5b57\u4fe1\u4efb\u751f\u6001\u3002", "method": "\u4f7f\u7528BERT\u7f16\u7801\u6587\u672c\u7279\u5f81\uff0cResNet-50\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u5206\u7c7b\u5934\u878d\u5408\u591a\u6a21\u6001\u8868\u793a\u6765\u8054\u5408\u9884\u6d4b\u8bc4\u8bba\u771f\u5b9e\u6027\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u5206\u6570\u8fbe\u52300.934\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u6587\u672c\u8d5e\u7f8e\u4e0e\u65e0\u5173/\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e4b\u95f4\u7684\u5fae\u5999\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u4fdd\u62a4\u6570\u5b57\u4fe1\u4efb\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5404\u5728\u7ebf\u5e73\u53f0\u7684\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00094", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00094", "abs": "https://arxiv.org/abs/2511.00094", "authors": ["Angelos Alexopoulos", "Agorakis Bompotas", "Nikitas Rigas Kalogeropoulos", "Panagiotis Kechagias", "Athanasios P. Kalogeras", "Christos Alexakos"], "title": "Digital Twin based Automatic Reconfiguration of Robotic Systems in Smart Environments", "comment": "Accepted for presentation to 11th IEEE International Smart Cities\n  Conference (ISC2 2025)", "summary": "Robotic systems have become integral to smart environments, enabling\napplications ranging from urban surveillance and automated agriculture to\nindustrial automation. However, their effective operation in dynamic settings -\nsuch as smart cities and precision farming - is challenged by continuously\nevolving topographies and environmental conditions. Traditional control systems\noften struggle to adapt quickly, leading to inefficiencies or operational\nfailures. To address this limitation, we propose a novel framework for\nautonomous and dynamic reconfiguration of robotic controllers using Digital\nTwin technology. Our approach leverages a virtual replica of the robot's\noperational environment to simulate and optimize movement trajectories in\nresponse to real-world changes. By recalculating paths and control parameters\nin the Digital Twin and deploying the updated code to the physical robot, our\nmethod ensures rapid and reliable adaptation without manual intervention. This\nwork advances the integration of Digital Twins in robotics, offering a scalable\nsolution for enhancing autonomy in smart, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u673a\u5668\u4eba\u63a7\u5236\u5668\u81ea\u4e3b\u52a8\u6001\u91cd\u6784\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u865a\u62df\u73af\u5883\u4e2d\u6a21\u62df\u548c\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\u6765\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u53d8\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u53ef\u9760\u7684\u9002\u5e94\u3002", "motivation": "\u4f20\u7edf\u63a7\u5236\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u667a\u6167\u57ce\u5e02\u548c\u7cbe\u51c6\u519c\u4e1a\uff09\u4e2d\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5730\u5f62\u548c\u73af\u5883\u6761\u4ef6\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u64cd\u4f5c\u5931\u8d25\u3002", "method": "\u5229\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u73af\u5883\u7684\u865a\u62df\u526f\u672c\uff0c\u5728\u6570\u5b57\u5b6a\u751f\u4e2d\u91cd\u65b0\u8ba1\u7b97\u8def\u5f84\u548c\u63a7\u5236\u53c2\u6570\uff0c\u5e76\u5c06\u66f4\u65b0\u540e\u7684\u4ee3\u7801\u90e8\u7f72\u5230\u7269\u7406\u673a\u5668\u4eba\u4e0a\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u786e\u4fdd\u5feb\u901f\u53ef\u9760\u7684\u9002\u5e94\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u673a\u5668\u4eba\u6280\u672f\u4e2d\u7684\u96c6\u6210\uff0c\u4e3a\u589e\u5f3a\u667a\u80fd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00207", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00207", "abs": "https://arxiv.org/abs/2511.00207", "authors": ["Di Hu", "Xi Lu", "Yunan Chen", "Michelle Keller", "An T. Nguyen", "Vu Le", "Tsung-Ting Kuo", "Lucila Ohno-Machado", "Kai Zheng"], "title": "Supporting Patients in Managing Electronic Health Records and Biospecimens Consent for Research: Insights from a Mixed-Methods Usability Evaluation of the iAGREE Portal", "comment": "Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA 2025)", "summary": "De-identified health data are frequently used in research. As AI advances\nheighten the risk of re-identification, it is important to respond to concerns\nabout transparency, data privacy, and patient preferences. However, few\npractical and user-friendly solutions exist. We developed iAGREE, a\npatient-centered electronic consent management portal that allows patients to\nset granular preferences for sharing electronic health records and biospecimens\nwith researchers. To refine the iAGREE portal, we conducted a mixed-methods\nusability evaluation with 40 participants from three U.S. health systems. Our\nresults show that the portal received highly positive usability feedback.\nMoreover, participants identified areas for improvement, suggested actionable\nenhancements, and proposed additional features to better support informed\ngranular consent while reducing patient burden. Insights from this study may\ninform further improvements to iAGREE and provide practical guidance for\ndesigning patient-centered consent management tools.", "AI": {"tldr": "\u5f00\u53d1\u4e86iAGREE\u60a3\u8005\u4e2d\u5fc3\u7535\u5b50\u540c\u610f\u7ba1\u7406\u95e8\u6237\uff0c\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u53ef\u7528\u6027\u8bc4\u4f30\u83b7\u5f97\u9ad8\u5ea6\u79ef\u6781\u53cd\u9988\uff0c\u5e76\u786e\u5b9a\u4e86\u6539\u8fdb\u9886\u57df\u4ee5\u652f\u6301\u77e5\u60c5\u7ec6\u7c92\u5ea6\u540c\u610f\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u53d1\u5c55\u589e\u52a0\u4e86\u91cd\u65b0\u8bc6\u522b\u98ce\u9669\uff0c\u9700\u8981\u89e3\u51b3\u900f\u660e\u5ea6\u3001\u6570\u636e\u9690\u79c1\u548c\u60a3\u8005\u504f\u597d\u7684\u62c5\u5fe7\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9e\u7528\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86iAGREE\u95e8\u6237\uff0c\u5141\u8bb8\u60a3\u8005\u8bbe\u7f6e\u5171\u4eab\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u751f\u7269\u6837\u672c\u7684\u7ec6\u7c92\u5ea6\u504f\u597d\uff0c\u5e76\u5bf9\u6765\u81ea\u4e09\u4e2a\u7f8e\u56fd\u536b\u751f\u7cfb\u7edf\u768440\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u6df7\u5408\u65b9\u6cd5\u53ef\u7528\u6027\u8bc4\u4f30\u3002", "result": "\u95e8\u6237\u83b7\u5f97\u4e86\u9ad8\u5ea6\u79ef\u6781\u7684\u53ef\u7528\u6027\u53cd\u9988\uff0c\u53c2\u4e0e\u8005\u786e\u5b9a\u4e86\u6539\u8fdb\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u53ef\u884c\u7684\u589e\u5f3a\u5efa\u8bae\u548c\u989d\u5916\u529f\u80fd\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u77e5\u60c5\u7ec6\u7c92\u5ea6\u540c\u610f\u540c\u65f6\u51cf\u5c11\u60a3\u8005\u8d1f\u62c5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aiAGREE\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u60a3\u8005\u4e2d\u5fc3\u7684\u540c\u610f\u7ba1\u7406\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.00707", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.00707", "abs": "https://arxiv.org/abs/2511.00707", "authors": ["Zoha Azimi", "Reza Farahani", "Vignesh V Menon", "Christian Timmerer"], "title": "Predicting Encoding Energy from Low-Pass Anchors for Green Video Streaming", "comment": "7 pages, 8 Figures, 4 tables, confernece paper", "summary": "Video streaming now represents the dominant share of Internet traffic, as\never-higher-resolution content is distributed across a growing range of\nheterogeneous devices to sustain user Quality of Experience (QoE). However,\nthis trend raises significant concerns about energy efficiency and carbon\nemissions, requiring methods to provide a trade-off between energy and QoE.\nThis paper proposes a lightweight energy prediction method that estimates the\nenergy consumption of high-resolution video encodings using reference encodings\ngenerated at lower resolutions (so-called anchors), eliminating the need for\nexhaustive per-segment energy measurements, a process that is infeasible at\nscale. We automatically select encoding parameters, such as resolution and\nquantization parameter (QP), to achieve substantial energy savings while\nmaintaining perceptual quality, as measured by the Video Multimethod Fusion\nAssessment (VMAF), within acceptable limits. We implement and evaluate our\napproach with the open-source VVenC encoder on 100 video sequences from the\nInter4K dataset across multiple encoding settings. Results show that, for an\naverage VMAF score reduction of only 1.68, which stays below the Just\nNoticeable Difference (JND) threshold, our method achieves 51.22% encoding\nenergy savings and 53.54% decoding energy savings compared to a scenario with\nno quality degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u80fd\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387\u53c2\u8003\u7f16\u7801\u9884\u6d4b\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7f16\u7801\u7684\u80fd\u8017\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u8282\u80fd\u3002", "motivation": "\u89c6\u9891\u6d41\u5a92\u4f53\u5360\u4e92\u8054\u7f51\u6d41\u91cf\u4e3b\u5bfc\u5730\u4f4d\uff0c\u9ad8\u5206\u8fa8\u7387\u5185\u5bb9\u5728\u5f02\u6784\u8bbe\u5907\u4e0a\u5206\u53d1\u9700\u8981\u5e73\u8861\u80fd\u8017\u4e0e\u7528\u6237\u4f53\u9a8c\u8d28\u91cf(QoE)\uff0c\u89e3\u51b3\u80fd\u6548\u548c\u78b3\u6392\u653e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u951a\u70b9\u7f16\u7801\u9884\u6d4b\u9ad8\u5206\u8fa8\u7387\u7f16\u7801\u80fd\u8017\uff0c\u81ea\u52a8\u9009\u62e9\u7f16\u7801\u53c2\u6570(\u5982\u5206\u8fa8\u7387\u548c\u91cf\u5316\u53c2\u6570)\uff0c\u5728\u53ef\u63a5\u53d7\u7684VMAF\u8d28\u91cf\u635f\u5931\u8303\u56f4\u5185\u4f18\u5316\u80fd\u8017\u3002", "result": "\u5728Inter4K\u6570\u636e\u96c6100\u4e2a\u89c6\u9891\u5e8f\u5217\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747VMAF\u4ec5\u964d\u4f4e1.68(\u4f4e\u4e8eJND\u9608\u503c)\uff0c\u7f16\u7801\u80fd\u8017\u8282\u770151.22%\uff0c\u89e3\u7801\u80fd\u8017\u8282\u770153.54%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u89c6\u9891\u7f16\u7801\u7684\u80fd\u8017\u4e0e\u8d28\u91cf\uff0c\u5728\u51e0\u4e4e\u4e0d\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u663e\u8457\u8282\u80fd\u6548\u679c\u3002"}}
{"id": "2511.00021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u73ca\u745a\u767d\u5316\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5305\u542b\u5065\u5eb7\u548c\u767d\u5316\u73ca\u745a\u7684\u5168\u7403\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86ResNet\u3001ViT\u548cCNN\u4e09\u79cd\u6a21\u578b\uff0c\u5176\u4e2dCNN\u6a21\u578b\u53d6\u5f97\u4e8688%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u73ca\u745a\u7901\u5bf9\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u548c\u6d77\u5cb8\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u6c61\u67d3\u3001\u6d77\u6d0b\u9178\u5316\u548c\u6d77\u6c34\u6e29\u5ea6\u5f02\u5e38\u7b49\u5a01\u80c1\uff0c\u9700\u8981\u9ad8\u6548\u7684\u76d1\u6d4b\u548c\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5305\u542b\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u5065\u5eb7\u548c\u767d\u5316\u73ca\u745a\u6837\u672c\u7684\u5168\u7403\u6570\u636e\u96c6\uff0c\u5bf9ResNet\u3001Vision Transformer\u548cCNN\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6bd4\u8f83\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u7ecf\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u540e\uff0cCNN\u6a21\u578b\u8fbe\u5230\u4e8688%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u81ea\u4e3b\u73ca\u745a\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u5bf9\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002"}}
{"id": "2511.00039", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00039", "abs": "https://arxiv.org/abs/2511.00039", "authors": ["Krishna Kumar Neelakanta Pillai Santha Kumari Amma"], "title": "Graph-Attentive MAPPO for Dynamic Retail Pricing", "comment": null, "summary": "Dynamic pricing in retail requires policies that adapt to shifting demand\nwhile coordinating decisions across related products. We present a systematic\nempirical study of multi-agent reinforcement learning for retail price\noptimization, comparing a strong MAPPO baseline with a\ngraph-attention-augmented variant (MAPPO+GAT) that leverages learned\ninteractions among products. Using a simulated pricing environment derived from\nreal transaction data, we evaluate profit, stability across random seeds,\nfairness across products, and training efficiency under a standardized\nevaluation protocol. The results indicate that MAPPO provides a robust and\nreproducible foundation for portfolio-level price control, and that MAPPO+GAT\nfurther enhances performance by sharing information over the product graph\nwithout inducing excessive price volatility. These results indicate that\ngraph-integrated MARL provides a more scalable and stable solution than\nindependent learners for dynamic retail pricing, offering practical advantages\nin multi-product decision-making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u96f6\u552e\u4ef7\u683c\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86MAPPO\u57fa\u51c6\u65b9\u6cd5\u548c\u56fe\u6ce8\u610f\u529b\u589e\u5f3a\u7684MAPPO+GAT\u65b9\u6cd5\uff0c\u53d1\u73b0\u540e\u8005\u901a\u8fc7\u4ea7\u54c1\u56fe\u4fe1\u606f\u5171\u4eab\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u96f6\u552e\u52a8\u6001\u5b9a\u4ef7\u9700\u8981\u80fd\u591f\u9002\u5e94\u9700\u6c42\u53d8\u5316\u5e76\u5728\u76f8\u5173\u4ea7\u54c1\u95f4\u534f\u8c03\u51b3\u7b56\u7684\u7b56\u7565\uff0c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e3a\u6b64\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u6784\u5efa\u7684\u6a21\u62df\u5b9a\u4ef7\u73af\u5883\uff0c\u6bd4\u8f83MAPPO\u57fa\u51c6\u65b9\u6cd5\u548c\u56fe\u6ce8\u610f\u529b\u589e\u5f3a\u7684MAPPO+GAT\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5229\u6da6\u3001\u7a33\u5b9a\u6027\u3001\u516c\u5e73\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "MAPPO\u4e3a\u6295\u8d44\u7ec4\u5408\u7ea7\u4ef7\u683c\u63a7\u5236\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u7840\uff0cMAPPO+GAT\u901a\u8fc7\u4ea7\u54c1\u56fe\u4fe1\u606f\u5171\u4eab\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e14\u672a\u5f15\u53d1\u8fc7\u5ea6\u4ef7\u683c\u6ce2\u52a8\u3002", "conclusion": "\u56fe\u96c6\u6210\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e3a\u52a8\u6001\u96f6\u552e\u5b9a\u4ef7\u63d0\u4f9b\u4e86\u6bd4\u72ec\u7acb\u5b66\u4e60\u66f4\u53ef\u6269\u5c55\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4ea7\u54c1\u51b3\u7b56\u4e2d\u5177\u6709\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2511.00112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00112", "abs": "https://arxiv.org/abs/2511.00112", "authors": ["Yanbing Mao", "Yihao Cai", "Lui Sha"], "title": "Real-DRL: Teach and Learn in Reality", "comment": "37 pages", "summary": "This paper introduces the Real-DRL framework for safety-critical autonomous\nsystems, enabling runtime learning of a deep reinforcement learning (DRL) agent\nto develop safe and high-performance action policies in real plants (i.e., real\nphysical systems to be controlled), while prioritizing safety! The Real-DRL\nconsists of three interactive components: a DRL-Student, a PHY-Teacher, and a\nTrigger. The DRL-Student is a DRL agent that innovates in the dual\nself-learning and teaching-to-learn paradigm and the real-time safety-informed\nbatch sampling. On the other hand, PHY-Teacher is a physics-model-based design\nof action policies that focuses solely on safety-critical functions.\nPHY-Teacher is novel in its real-time patch for two key missions: i) fostering\nthe teaching-to-learn paradigm for DRL-Student and ii) backing up the safety of\nreal plants. The Trigger manages the interaction between the DRL-Student and\nthe PHY-Teacher. Powered by the three interactive components, the Real-DRL can\neffectively address safety challenges that arise from the unknown unknowns and\nthe Sim2Real gap. Additionally, Real-DRL notably features i) assured safety,\nii) automatic hierarchy learning (i.e., safety-first learning and then\nhigh-performance learning), and iii) safety-informed batch sampling to address\nthe learning experience imbalance caused by corner cases. Experiments with a\nreal quadruped robot, a quadruped robot in NVIDIA Isaac Gym, and a cart-pole\nsystem, along with comparisons and ablation studies, demonstrate the Real-DRL's\neffectiveness and unique features.", "AI": {"tldr": "Real-DRL\u6846\u67b6\u901a\u8fc7DRL-Student\u3001PHY-Teacher\u548cTrigger\u4e09\u4e2a\u4ea4\u4e92\u7ec4\u4ef6\uff0c\u5728\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b89\u5168\u4f18\u5148\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u672a\u77e5\u98ce\u9669\u548cSim2Real\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5b89\u5168\u5173\u952e\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u672a\u77e5\u98ce\u9669\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u4e2d\u5b89\u5168\u5730\u5b66\u4e60\u9ad8\u6027\u80fd\u7b56\u7565\u3002", "method": "\u91c7\u7528\u4e09\u7ec4\u4ef6\u4ea4\u4e92\u6846\u67b6\uff1aDRL-Student\u5b9e\u73b0\u53cc\u81ea\u5b66\u4e60\u548c\u6559\u4e2d\u5b66\u8303\u5f0f\u53ca\u5b9e\u65f6\u5b89\u5168\u611f\u77e5\u6279\u91cf\u91c7\u6837\uff1bPHY-Teacher\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u4e13\u6ce8\u4e8e\u5b89\u5168\u5173\u952e\u529f\u80fd\uff1bTrigger\u7ba1\u7406\u4e24\u8005\u4ea4\u4e92\u3002", "result": "\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u3001NVIDIA Isaac Gym\u4e2d\u7684\u56db\u8db3\u673a\u5668\u4eba\u548c\u5012\u7acb\u6446\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4fdd\u8bc1\u5b89\u5168\u3001\u81ea\u52a8\u5206\u5c42\u5b66\u4e60\u548c\u89e3\u51b3\u5b66\u4e60\u7ecf\u9a8c\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "Real-DRL\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5b89\u5168\u5173\u952e\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u786e\u4fdd\u5b89\u5168\u4f18\u5148\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u5728\u591a\u79cd\u771f\u5b9e\u548c\u4eff\u771f\u7cfb\u7edf\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.00230", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00230", "abs": "https://arxiv.org/abs/2511.00230", "authors": ["Sheer Karny", "Anthony Baez", "Pat Pataranutaporn"], "title": "Neural Transparency: Mechanistic Interpretability Interfaces for Anticipating Model Behaviors for Personalized AI", "comment": "SK and AB are co-first authors", "summary": "Millions of users now design personalized LLM-based chatbots that shape their\ndaily interactions, yet they can only loosely anticipate how their design\nchoices will manifest as behaviors in deployment. This opacity is\nconsequential: seemingly innocuous prompts can trigger excessive sycophancy,\ntoxicity, or inconsistency, degrading utility and raising safety concerns. To\naddress this issue, we introduce an interface that enables neural transparency\nby exposing language model internals during chatbot design. Our approach\nextracts behavioral trait vectors (empathy, toxicity, sycophancy, etc.) by\ncomputing differences in neural activations between contrastive system prompts\nthat elicit opposing behaviors. We predict chatbot behaviors by projecting the\nsystem prompt's final token activations onto these trait vectors, normalizing\nfor cross-trait comparability, and visualizing results via an interactive\nsunburst diagram. To evaluate this approach, we conducted an online user study\nusing Prolific to compare our neural transparency interface against a baseline\nchatbot interface without any form of transparency. Our analyses suggest that\nusers systematically miscalibrated AI behavior: participants misjudged trait\nactivations for eleven of fifteen analyzable traits, motivating the need for\ntransparency tools in everyday human-AI interaction. While our interface did\nnot change design iteration patterns, it significantly increased user trust and\nwas enthusiastically received. Qualitative analysis indicated that users' had\nnuanced experiences with the visualization that may enrich future work\ndesigning neurally transparent interfaces. This work offers a path for how\nmechanistic interpretability can be operationalized for non-technical users,\nestablishing a foundation for safer, more aligned human-AI interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u900f\u660e\u5ea6\u754c\u9762\uff0c\u901a\u8fc7\u66b4\u9732\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u72b6\u6001\u6765\u5e2e\u52a9\u7528\u6237\u5728\u8bbe\u8ba1\u4e2a\u6027\u5316\u804a\u5929\u673a\u5668\u4eba\u65f6\u66f4\u597d\u5730\u9884\u6d4b\u5176\u884c\u4e3a\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u5728\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u4e2a\u6027\u5316\u804a\u5929\u673a\u5668\u4eba\u65f6\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u5176\u8bbe\u8ba1\u9009\u62e9\u4f1a\u5982\u4f55\u5f71\u54cd\u804a\u5929\u673a\u5668\u4eba\u5728\u90e8\u7f72\u540e\u7684\u884c\u4e3a\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u5949\u627f\u3001\u6bd2\u6027\u6216\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u5bf9\u6bd4\u6027\u7cfb\u7edf\u63d0\u793a\u4e4b\u95f4\u7684\u795e\u7ecf\u6fc0\u6d3b\u5dee\u5f02\u6765\u63d0\u53d6\u884c\u4e3a\u7279\u5f81\u5411\u91cf\uff08\u5982\u5171\u60c5\u3001\u6bd2\u6027\u3001\u5949\u627f\u7b49\uff09\uff0c\u7136\u540e\u5c06\u7cfb\u7edf\u63d0\u793a\u7684\u6700\u7ec8token\u6fc0\u6d3b\u6295\u5f71\u5230\u8fd9\u4e9b\u7279\u5f81\u5411\u91cf\u4e0a\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u65ed\u65e5\u56fe\u8fdb\u884c\u53ef\u89c6\u5316\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u5bf9AI\u884c\u4e3a\u7684\u5224\u65ad\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u572815\u4e2a\u53ef\u5206\u6790\u7279\u5f81\u4e2d\u670911\u4e2a\u88ab\u8bef\u5224\u3002\u795e\u7ecf\u900f\u660e\u5ea6\u754c\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u4fe1\u4efb\u5ea6\uff0c\u867d\u7136\u672a\u6539\u53d8\u8bbe\u8ba1\u8fed\u4ee3\u6a21\u5f0f\uff0c\u4f46\u53d7\u5230\u7528\u6237\u79ef\u6781\u8bc4\u4ef7\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u64cd\u4f5c\u5316\u4ee5\u670d\u52a1\u975e\u6280\u672f\u7528\u6237\uff0c\u4e3a\u66f4\u5b89\u5168\u3001\u66f4\u5bf9\u9f50\u7684\u4eba\u673a\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.00793", "categories": ["cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.00793", "abs": "https://arxiv.org/abs/2511.00793", "authors": ["Barathi Subramanian", "Rathinaraja Jeyaraj", "Anand Paul", "Kapilya Gangadharan"], "title": "Rhythm in the Air: Vision-based Real-Time Music Generation through Gestures", "comment": "8 pages, 7 figures", "summary": "Gesture recognition is an essential component of human-computer interaction\n(HCI), facilitating seamless interconnectivity between users and computer\nsystems without physical touch. This paper introduces an innovative application\nof vision-based dynamic gesture recognition (VDGR) for real-time music\ncomposition through gestures. To implement this application, we generate a\ncustom gesture dataset that encompasses over 15000 samples across 21 classes,\nincorporating 7 musical notes each manifesting at three distinct pitch levels.\nTo effectively deal with the modest volume of training data and to accurately\ndiscern and prioritize complex gesture sequences for music creation, we develop\na multi-layer attention-based gated recurrent unit (MLA-GRU) model, in which\ngated recurrent unit (GRU) is used to learn temporal patterns from the observed\nsequence and an attention layer is employed to focus on musically pertinent\ngesture segments. Our empirical studies demonstrate that MLA-GRU significantly\nsurpasses the classical GRU model, achieving a remarkable accuracy of 96.83%\ncompared to the baseline's 86.7%. Moreover, our approach exhibits superior\nefficiency and processing speed, which are crucial for interactive\napplications. Using our proposed system, we believe that people will interact\nwith music in a new and exciting way. It not only advances HCI experiences but\nalso highlights MLA-GRU's effectiveness in scenarios demanding swift and\nprecise gesture recognition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u52a8\u6001\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u97f3\u4e50\u521b\u4f5c\uff0c\u901a\u8fc7\u591a\u5c42\u6ce8\u610f\u529b\u95e8\u63a7\u5faa\u73af\u5355\u5143\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8696.83%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\uff0c\u5b9e\u73b0\u65e0\u9700\u7269\u7406\u63a5\u89e6\u7684\u5b9e\u65f6\u97f3\u4e50\u521b\u4f5c\uff0c\u8ba9\u7528\u6237\u901a\u8fc7\u624b\u52bf\u4e0e\u97f3\u4e50\u8fdb\u884c\u65b0\u9896\u4e92\u52a8\u3002", "method": "\u6784\u5efa\u5305\u542b15000\u4e2a\u6837\u672c\u300121\u4e2a\u7c7b\u522b\u7684\u81ea\u5b9a\u4e49\u624b\u52bf\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u591a\u5c42\u6ce8\u610f\u529b\u95e8\u63a7\u5faa\u73af\u5355\u5143\u6a21\u578b\uff0c\u5229\u7528GRU\u5b66\u4e60\u65f6\u95f4\u6a21\u5f0f\uff0c\u6ce8\u610f\u529b\u5c42\u805a\u7126\u97f3\u4e50\u76f8\u5173\u624b\u52bf\u7247\u6bb5\u3002", "result": "MLA-GRU\u6a21\u578b\u663e\u8457\u8d85\u8d8a\u4f20\u7edfGRU\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u523096.83%\uff0c\u76f8\u6bd4\u57fa\u7ebf86.7%\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5177\u5907\u66f4\u9ad8\u7684\u6548\u7387\u548c\u5904\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\uff0c\u8fd8\u8bc1\u660e\u4e86MLA-GRU\u5728\u9700\u8981\u5feb\u901f\u51c6\u786e\u624b\u52bf\u8bc6\u522b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u97f3\u4e50\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u65b9\u5f0f\u3002"}}
{"id": "2511.00180", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6b8b\u5dee\u6d41\u89e3\u7801\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u6d4b\u8bed\u8a00\u6a21\u578b\u4e2d\u6bb5\u843d\u548c\u6587\u6863\u7ea7\u522b\u7684\u89c4\u5212\u4fe1\u606f\uff0c\u53d1\u73b0\u5728\u5c0f\u6a21\u578b\u4e2d\u53ef\u89e3\u7801\u76f8\u5f53\u4e8e5\u4e2a\u4ee5\u4e0a\u672a\u6765token\u7684\u4fe1\u606f\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5904\u7406\u66f4\u957f\u65f6\u95f4\u8de8\u5ea6\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u7406\u89e3\u6fc0\u6d3b\u7684\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6d4b\u8bd5\u7279\u5b9a\u6982\u5ff5\u6216token\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u957f\u671f\u89c4\u5212\u4fe1\u606f\u3002", "method": "\u5f00\u53d1\u4e86\u6b8b\u5dee\u6d41\u89e3\u7801\u5668\u6846\u67b6\uff0c\u4f5c\u4e3a\u63a2\u6d4b\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6bb5\u843d\u7ea7\u548c\u6587\u6863\u7ea7\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u89e3\u7801\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5728\u5c0f\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u4ece\u6fc0\u6d3b\u4e2d\u89e3\u7801\u51fa\u76f8\u5f53\u4e8e5\u4e2a\u4ee5\u4e0a\u672a\u6765token\u4e0a\u4e0b\u6587\u7684\u4fe1\u606f\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u66f4\u597d\u5730\u76d1\u63a7\u8bed\u8a00\u6a21\u578b\u4ee5\u53ca\u7406\u89e3\u5b83\u4eec\u5982\u4f55\u7f16\u7801\u957f\u671f\u89c4\u5212\u4fe1\u606f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.00022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "\u8bc4\u4f30YOLOv8\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\u7528\u4e8e\u81ea\u52a8\u5316\u80af\u5c3c\u4e9a\u548c\u5766\u6851\u5c3c\u4e9a\u89c6\u9891\u6837\u5e26\u4e2d\u9c7c\u7c7b\u79d1\u7ea7\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u4e3a\u897f\u5370\u5ea6\u6d0b\u73ca\u745a\u7901\u76d1\u6d4b\u63d0\u4f9b\u9996\u4e2a\u533a\u57df\u7279\u5b9a\u57fa\u51c6\u3002", "motivation": "\u897f\u5370\u5ea6\u6d0b\u73ca\u745a\u7901\u76d1\u6d4b\u53d7\u5230\u6c34\u4e0b\u89c6\u89c9\u666e\u67e5\u52b3\u52a8\u9700\u6c42\u7684\u9650\u5236\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u76d1\u6d4b\u6548\u7387\u3002", "method": "\u4f7f\u7528YOLOv8\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u5728\u80af\u5c3c\u4e9a\u548c\u5766\u6851\u5c3c\u4e9a\u6536\u96c6\u7684\u89c6\u9891\u6837\u5e26\u4e0a\u6d4b\u8bd524\u4e2a\u9c7c\u7c7b\u79d1\u7684\u8bc6\u522b\uff0c\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6700\u4f73\u6a21\u578b\u8fbe\u5230mAP@0.5\u4e3a0.52\uff0c\u5bf9\u4e30\u5bcc\u9c7c\u7c7b\u79d1\u8bc6\u522b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5bf9\u7a00\u6709\u6216\u590d\u6742\u7c7b\u7fa4\u7684\u68c0\u6d4b\u8f83\u5f31\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6709\u6f5c\u529b\u4f5c\u4e3a\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2511.00139", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00139", "abs": "https://arxiv.org/abs/2511.00139", "authors": ["Yu Cui", "Yujian Zhang", "Lina Tao", "Yang Li", "Xinyu Yi", "Zhibin Li"], "title": "End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection", "comment": null, "summary": "Achieving human-like dexterous manipulation remains a major challenge for\ngeneral-purpose robots. While Vision-Language-Action (VLA) models show\npotential in learning skills from demonstrations, their scalability is limited\nby scarce high-quality training data. Existing data collection methods face\ninherent constraints: manual teleoperation overloads human operators, while\nautomated planning often produces unnatural motions. We propose a Shared\nAutonomy framework that divides control between macro and micro motions. A\nhuman operator guides the robot's arm pose through intuitive VR teleoperation,\nwhile an autonomous DexGrasp-VLA policy handles fine-grained hand control using\nreal-time tactile and visual feedback. This division significantly reduces\ncognitive load and enables efficient collection of high-quality coordinated\narm-hand demonstrations. Using this data, we train an end-to-end VLA policy\nenhanced with our novel Arm-Hand Feature Enhancement module, which captures\nboth distinct and shared representations of macro and micro movements for more\nnatural coordination. Our Corrective Teleoperation system enables continuous\npolicy improvement through human-in-the-loop failure recovery. Experiments\ndemonstrate that our framework generates high-quality data with minimal\nmanpower and achieves a 90% success rate across diverse objects, including\nunseen instances. Comprehensive evaluations validate the system's effectiveness\nin developing dexterous manipulation capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff0c\u901a\u8fc7VR\u9065\u64cd\u4f5c\u4e0e\u81ea\u4e3b\u624b\u90e8\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u9ad8\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u534f\u8c03\u81c2\u624b\u6f14\u793a\u6570\u636e\uff0c\u8bad\u7ec3\u589e\u5f3a\u7684VLA\u7b56\u7565\uff0c\u5728\u591a\u6837\u5316\u7269\u4f53\u4e0a\u8fbe\u523090%\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4e2d\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4eba\u5de5\u9065\u64cd\u4f5c\u8d1f\u62c5\u91cd\u6216\u81ea\u52a8\u89c4\u5212\u52a8\u4f5c\u4e0d\u81ea\u7136\u7684\u9650\u5236\u3002", "method": "\u5171\u4eab\u81ea\u4e3b\u6846\u67b6\uff1a\u4eba\u7c7b\u901a\u8fc7VR\u63a7\u5236\u81c2\u90e8\u5b8f\u89c2\u8fd0\u52a8\uff0c\u81ea\u4e3bDexGrasp-VLA\u7b56\u7565\u57fa\u4e8e\u89e6\u89c9\u548c\u89c6\u89c9\u53cd\u9988\u63a7\u5236\u624b\u90e8\u7cbe\u7ec6\u52a8\u4f5c\uff1b\u8bad\u7ec3\u5e26\u81c2\u624b\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u7684\u7aef\u5230\u7aefVLA\u7b56\u7565\uff1b\u901a\u8fc7\u7ea0\u6b63\u9065\u64cd\u4f5c\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u4ee5\u6700\u5c11\u4eba\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5728\u591a\u6837\u5316\u7269\u4f53\uff08\u5305\u62ec\u672a\u89c1\u5b9e\u4f8b\uff09\u4e0a\u8fbe\u523090%\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5f00\u53d1\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9ad8\u8d28\u91cf\u534f\u8c03\u81c2\u624b\u6f14\u793a\u6570\u636e\u6536\u96c6\uff0c\u4e3a\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00273", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00273", "abs": "https://arxiv.org/abs/2511.00273", "authors": ["Sander de Jong", "Jane Hsieh", "Tzu-Sheng Kuo", "Rune M\u00f8berg Jacobsen", "Niels van Berkel", "Haiyi Zhu"], "title": "Understanding, Demystifying and Challenging Perceptions of Gig Worker Vulnerabilities", "comment": null, "summary": "Gig workers face several vulnerabilities, which are rarely discussed among\npeers due to the absence of infrastructure for mutual support. To understand\nhow individual gig workers perceive such vulnerabilities and why they continue\nto pursue such labor, we conducted a scalable two-phase study to probe their\nrationales. In Phase I, participants (N = 236) rated their agreement with five\ncommonly misconstrued vulnerabilities. In Phase II, we challenged participants\nwho held one or more myth(s) (N = 204) to defend their views, after which we\npresented an expert- or LLM-generated counterargument to their rationale. Our\nfindings show how workers are underexposed to the personal and shared\nvulnerabilities of gig work, revealing a knowledge gap where persuasive\ninterventions may help workers recognize such hidden conditions. We discuss the\nimplications of our results to support collective bargaining of workers' rights\nand reflect on the effectiveness of different persuasion strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u9636\u6bb5\u8c03\u67e5\u63ed\u793a\u4e86\u96f6\u5de5\u5de5\u4eba\u5bf9\u81ea\u8eab\u8106\u5f31\u6027\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u53d1\u73b0\u5de5\u4eba\u666e\u904d\u4f4e\u4f30\u4e86\u96f6\u5de5\u5de5\u4f5c\u7684\u4e2a\u4eba\u548c\u5171\u540c\u8106\u5f31\u6027\uff0c\u5b58\u5728\u77e5\u8bc6\u9e3f\u6c9f\u3002\u7814\u7a76\u6d4b\u8bd5\u4e86\u4e13\u5bb6\u548cLLM\u751f\u6210\u7684\u53cd\u9a73\u8bba\u70b9\u7684\u8bf4\u670d\u6548\u679c\uff0c\u4e3a\u652f\u6301\u5de5\u4eba\u96c6\u4f53\u8c08\u5224\u6743\u5229\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u4e86\u89e3\u96f6\u5de5\u5de5\u4eba\u5982\u4f55\u8ba4\u77e5\u81ea\u8eab\u8106\u5f31\u6027\u4ee5\u53ca\u4e3a\u4f55\u7ee7\u7eed\u4ece\u4e8b\u6b64\u7c7b\u52b3\u52a8\uff0c\u56e0\u4e3a\u96f6\u5de5\u5de5\u4eba\u5f88\u5c11\u4e0e\u540c\u884c\u8ba8\u8bba\u8106\u5f31\u6027\u95ee\u9898\uff0c\u7f3a\u4e4f\u76f8\u4e92\u652f\u6301\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u7684\u4e24\u9636\u6bb5\u7814\u7a76\uff1a\u7b2c\u4e00\u9636\u6bb5236\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u5bf95\u79cd\u5e38\u89c1\u8bef\u89e3\u8106\u5f31\u6027\u7684\u8ba4\u540c\u5ea6\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u6301\u6709\u4e00\u79cd\u6216\u591a\u79cd\u8bef\u89e3\u7684204\u540d\u53c2\u4e0e\u8005\uff0c\u5148\u8ba9\u4ed6\u4eec\u4e3a\u81ea\u5df1\u7684\u89c2\u70b9\u8fa9\u62a4\uff0c\u7136\u540e\u5448\u73b0\u4e13\u5bb6\u6216LLM\u751f\u6210\u7684\u53cd\u9a73\u8bba\u70b9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5de5\u4eba\u5bf9\u96f6\u5de5\u5de5\u4f5c\u7684\u4e2a\u4eba\u548c\u5171\u4eab\u8106\u5f31\u6027\u4e86\u89e3\u4e0d\u8db3\uff0c\u63ed\u793a\u4e86\u77e5\u8bc6\u9e3f\u6c9f\uff0c\u8bf4\u670d\u6027\u5e72\u9884\u53ef\u80fd\u5e2e\u52a9\u5de5\u4eba\u8ba4\u8bc6\u8fd9\u4e9b\u9690\u85cf\u72b6\u51b5\u3002\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u652f\u6301\u5de5\u4eba\u96c6\u4f53\u8c08\u5224\u6743\u5229\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u9700\u8981\u53cd\u601d\u4e0d\u540c\u8bf4\u670d\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u4ee5\u5e2e\u52a9\u96f6\u5de5\u5de5\u4eba\u66f4\u597d\u5730\u8ba4\u8bc6\u5de5\u4f5c\u8106\u5f31\u6027\u3002"}}
{"id": "2511.01590", "categories": ["cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01590", "abs": "https://arxiv.org/abs/2511.01590", "authors": ["Yongcun Hu", "Yingzhen Zhai", "Jixiang Luo", "Wenrui Dai", "Dell Zhang", "Hongkai Xiong", "Xuelong Li"], "title": "EV-NVC: Efficient Variable bitrate Neural Video Compression", "comment": null, "summary": "Training neural video codec (NVC) with variable rate is a highly challenging\ntask due to its complex training strategies and model structure. In this paper,\nwe train an efficient variable bitrate neural video codec (EV-NVC) with the\npiecewise linear sampler (PLS) to improve the rate-distortion performance in\nhigh bitrate range, and the long-short-term feature fusion module (LSTFFM) to\nenhance the context modeling. Besides, we introduce mixed-precision training\nand discuss the different training strategies for each stage in detail to fully\nevaluate its effectiveness. Experimental results show that our approach reduces\nthe BD-rate by 30.56% compared to HM-16.25 within low-delay mode.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u53d8\u6bd4\u7279\u7387\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u5668\uff08EV-NVC\uff09\uff0c\u901a\u8fc7\u5206\u6bb5\u7ebf\u6027\u91c7\u6837\u5668\u63d0\u9ad8\u9ad8\u6bd4\u7279\u7387\u8303\u56f4\u7684\u7387\u5931\u771f\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u957f\u77ed\u65f6\u7279\u5f81\u878d\u5408\u6a21\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u540c\u65f6\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u8bad\u7ec3\u53ef\u53d8\u6bd4\u7279\u7387\u7684\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u5668\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u4e3b\u8981\u7531\u4e8e\u590d\u6742\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u63d0\u9ad8\u9ad8\u6bd4\u7279\u7387\u8303\u56f4\u7684\u7387\u5931\u771f\u6027\u80fd\u5e76\u589e\u5f3a\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5206\u6bb5\u7ebf\u6027\u91c7\u6837\u5668\uff08PLS\uff09\u6765\u6539\u5584\u9ad8\u6bd4\u7279\u7387\u8303\u56f4\u7684\u6027\u80fd\uff0c\u91c7\u7528\u957f\u77ed\u65f6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08LSTFFM\uff09\u6765\u589e\u5f3a\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7b56\u7565\uff0c\u8be6\u7ec6\u8ba8\u8bba\u4e86\u6bcf\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u4e0b\uff0c\u4e0eHM-16.25\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06BD-rate\u964d\u4f4e\u4e8630.56%\u3002", "conclusion": "\u63d0\u51fa\u7684EV-NVC\u65b9\u6cd5\u5728\u795e\u7ecf\u89c6\u9891\u7f16\u7801\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6bd4\u7279\u7387\u8303\u56f4\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.00198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u4f7f\u7528\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u51fa\u901a\u8fc7\u9884\u6d4b\u4fe1\u606f\u4e30\u5bcc\u7684\u8bcd\u5143\u6765\u66f4\u6709\u6548\u5730\u8bad\u7ec3LLM\uff0c\u5e76\u5728\u7b97\u672f\u3001\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u3002\u4f20\u7edf\u4f7f\u7528\u4e0b\u4e00\u8bcd\u9884\u6d4b\u7684\u65b9\u6cd5\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u4e0b\u4e00\u8bcd\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u76ee\u6807\u8bcd\u5143\u8fdb\u884c\u9884\u6d4b\u6765\u8bad\u7ec3LLM\u3002\u5728\u7b97\u672f\u3001\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e09\u79cd\u4efb\u52a1\u7c7b\u578b\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9884\u6d4b\u4fe1\u606f\u4e30\u5bcc\u7684\u8bcd\u5143\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u8bad\u7ec3LLM\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u7c7b\u578b\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u4e0b\u4e00\u8bcd\u9884\u6d4b\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f18\u5316LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u6df1\u5316\u4e86\u5bf9\u76ee\u6807\u8bcd\u5143\u9009\u62e9\u7b56\u7565\u7684\u7406\u8bba\u7406\u89e3\u3002"}}
{"id": "2511.00028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5728\u81ea\u7136\u6270\u52a8\u4e0b\u5177\u6709\u9ad8\u4e92\u4fe1\u606f\u7684\u573a\u666f\u8865\u4e01\u4f5c\u4e3a\u6b63\u6837\u672c\uff0c\u7528\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684InfoNCE\u635f\u5931\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u4f46\u6570\u636e\u9009\u62e9\u548c\u589e\u5f3a\u4ecd\u4f9d\u8d56\u4eba\u5de5\u5047\u8bbe\u6216\u5de5\u7a0b\uff0c\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u4e92\u4fe1\u606f\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u4e0b\u7684\u4e92\u4fe1\u606f\uff0c\u9009\u62e9\u5728\u81ea\u7136\u6270\u52a8\uff08\u5982\u989c\u8272\u53d8\u5316\u548c\u8fd0\u52a8\uff09\u4e0b\u5177\u6709\u9ad8\u4e92\u4fe1\u606f\u7684\u573a\u666f\u8865\u4e01\u4f5c\u4e3a\u6b63\u6837\u672c\uff0c\u7528\u4e8e\u5bf9\u6bd4\u635f\u5931\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6700\u5148\u8fdb\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u80fd\u591f\u63d0\u5347\u5b66\u4e60\u7279\u5f81\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.00092", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86QuantumBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u91cf\u5b50\u79d1\u5b66\u9886\u57df\u7684LLM\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6800\u4e2a\u9009\u62e9\u9898\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u91cf\u5b50\u9886\u57df\u7684\u7406\u89e3\u548c\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLM\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u80fd\u51c6\u786e\u638c\u63e1\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u7b26\u53f7\u8868\u793a\u3002\u91cf\u5b50\u79d1\u5b66\u5177\u6709\u975e\u76f4\u89c2\u73b0\u8c61\u548c\u9ad8\u7ea7\u6570\u5b66\u8981\u6c42\uff0c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u96be\u4ee5\u53cd\u6620\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u5229\u7528\u516c\u5f00\u6750\u6599\u7f16\u5236\u7ea6800\u4e2a\u95ee\u9898\u53ca\u5176\u7b54\u6848\uff0c\u6db5\u76d6\u91cf\u5b50\u79d1\u5b66\u4e5d\u4e2a\u9886\u57df\uff0c\u7ec4\u7ec7\u6210\u516b\u9009\u9879\u9009\u62e9\u9898\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u4e2a\u73b0\u6709LLM\u5728\u91cf\u5b50\u9886\u57df\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5bf9\u95ee\u9898\u683c\u5f0f\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "result": "QuantumBench\u662f\u9996\u4e2a\u4e3a\u91cf\u5b50\u9886\u57df\u6784\u5efa\u7684LLM\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86LLM\u5728\u91cf\u5b50\u79d1\u5b66\u4e2d\u7684\u8868\u73b0\u7279\u70b9\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u65e8\u5728\u6307\u5bfcLLM\u5728\u91cf\u5b50\u7814\u7a76\u4e2d\u7684\u6709\u6548\u5e94\u7528\uff0c\u586b\u8865\u4e86\u91cf\u5b50\u9886\u57df\u4e13\u4e1a\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.00153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00153", "abs": "https://arxiv.org/abs/2511.00153", "authors": ["Justin Yu", "Yide Shentu", "Di Wu", "Pieter Abbeel", "Ken Goldberg", "Philipp Wu"], "title": "EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations", "comment": null, "summary": "Imitation learning from human demonstrations offers a promising approach for\nrobot skill acquisition, but egocentric human data introduces fundamental\nchallenges due to the embodiment gap. During manipulation, humans actively\ncoordinate head and hand movements, continuously reposition their viewpoint and\nuse pre-action visual fixation search strategies to locate relevant objects.\nThese behaviors create dynamic, task-driven head motions that static robot\nsensing systems cannot replicate, leading to a significant distribution shift\nthat degrades policy performance. We present EgoMI (Egocentric Manipulation\nInterface), a framework that captures synchronized end-effector and active head\ntrajectories during manipulation tasks, resulting in data that can be\nretargeted to compatible semi-humanoid robot embodiments. To handle rapid and\nwide-spanning head viewpoint changes, we introduce a memory-augmented policy\nthat selectively incorporates historical observations. We evaluate our approach\non a bimanual robot equipped with an actuated camera head and find that\npolicies with explicit head-motion modeling consistently outperform baseline\nmethods. Results suggest that coordinated hand-eye learning with EgoMI\neffectively bridges the human-robot embodiment gap for robust imitation\nlearning on semi-humanoid embodiments. Project page:\nhttps://egocentric-manipulation-interface.github.io", "AI": {"tldr": "EgoMI\u6846\u67b6\u901a\u8fc7\u6355\u83b7\u540c\u6b65\u7684\u672b\u7aef\u6267\u884c\u5668\u548c\u4e3b\u52a8\u5934\u90e8\u8f68\u8ff9\u6765\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4eba\u673a\u4f53\u73b0\u5dee\u8ddd\u95ee\u9898\uff0c\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u7b56\u7565\u5904\u7406\u5feb\u901f\u89c6\u89d2\u53d8\u5316\uff0c\u5728\u53cc\u624b\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u534f\u8c03\u624b\u773c\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u56e0\u4eba\u7c7b\u4e3b\u52a8\u534f\u8c03\u5934\u90e8\u548c\u624b\u90e8\u8fd0\u52a8\u3001\u52a8\u6001\u8c03\u6574\u89c6\u89d2\u800c\u4ea7\u751f\u7684\u4f53\u73b0\u5dee\u8ddd\u95ee\u9898\uff0c\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\u4f1a\u964d\u4f4e\u7b56\u7565\u6027\u80fd\u3002", "method": "\u63d0\u51faEgoMI\u6846\u67b6\u6355\u83b7\u540c\u6b65\u7684\u672b\u7aef\u6267\u884c\u5668\u548c\u4e3b\u52a8\u5934\u90e8\u8f68\u8ff9\u6570\u636e\uff0c\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u7b56\u7565\u9009\u62e9\u6027\u6574\u5408\u5386\u53f2\u89c2\u5bdf\u6765\u5904\u7406\u5feb\u901f\u89c6\u89d2\u53d8\u5316\u3002", "result": "\u5728\u914d\u5907\u9a71\u52a8\u76f8\u673a\u5934\u7684\u53cc\u624b\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\uff0c\u663e\u793a\u5177\u6709\u663e\u5f0f\u5934\u90e8\u8fd0\u52a8\u5efa\u6a21\u7684\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EgoMI\u901a\u8fc7\u534f\u8c03\u624b\u773c\u5b66\u4e60\u6709\u6548\u5f25\u5408\u4e86\u4eba\u673a\u4f53\u73b0\u5dee\u8ddd\uff0c\u4e3a\u534a\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6a21\u4eff\u5b66\u4e60\u3002"}}
{"id": "2511.00289", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2511.00289", "abs": "https://arxiv.org/abs/2511.00289", "authors": ["You-Jin Kim", "Radha Kumaran", "Ehsan Sayyad", "Anne Milner", "Tom Bullock", "Barry Giesbrecht", "Tobias H\u00f6llerer"], "title": "Investigating Search Among Physical and Virtual Objects Under Different Lighting Conditions", "comment": "Journal Article, 11 pages. Published in IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) in 2022", "summary": "By situating computer-generated content in the physical world, mobile\naugmented reality (AR) can support many tasks that involve effective search and\ninspection of physical environments. Currently, there is limited information\nregarding the viability of using AR in realistic wide-area outdoor environments\nand how AR experiences affect human behavior in these environments. Here, we\nconducted a wide-area outdoor AR user study (n = 48) using a commercially\navailable AR headset (Microsoft Hololens 2) to compare (1) user interactions\nwith physical and virtual objects in the environment (2) the effects of\ndifferent lighting conditions on user behavior and AR experience and (3) the\nimpact of varying cognitive load on AR task performance. Participants engaged\nin a treasure hunt task where they searched for and classified virtual target\nitems (green ``gems\") in an augmented outdoor courtyard scene populated with\nphysical and virtual objects. Cognitive load was manipulated so that in half\nthe search trials users were required to monitor an audio stream and respond to\nspecific target sounds. Walking paths, head orientation and eye gaze\ninformation were measured, and users were queried about their memory of\nencountered objects and provided feedback on the experience. Key findings\nincluded (1) Participants self-reported significantly lower comfort in the\nambient natural light condition, with virtual objects more visible and\nparticipants more likely to walk into physical objects at night; (2) recall for\nphysical objects was worse than for virtual objects, (3) participants\ndiscovered more gems hidden behind virtual objects than physical objects,\nimplying higher attention on virtual objects and (4) dual-tasking modified\nsearch behavior. These results suggest there are important technical,\nperceptual and cognitive factors that must be considered.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6237\u5916AR\u7528\u6237\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u7269\u7406\u4e0e\u865a\u62df\u5bf9\u8c61\u4ea4\u4e92\u3001\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u5bf9AR\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8ba4\u77e5\u8d1f\u8377\u5bf9AR\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u5149\u7167\u6761\u4ef6\u5f71\u54cd\u7528\u6237\u8212\u9002\u5ea6\uff0c\u865a\u62df\u5bf9\u8c61\u6bd4\u7269\u7406\u5bf9\u8c61\u66f4\u53d7\u5173\u6ce8\uff0c\u8ba4\u77e5\u8d1f\u8377\u4f1a\u6539\u53d8\u641c\u7d22\u884c\u4e3a\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u5728\u73b0\u5b9e\u6237\u5916\u73af\u5883\u4e2d\u4f7f\u7528AR\u7684\u53ef\u884c\u6027\u4fe1\u606f\uff0c\u4ee5\u53caAR\u4f53\u9a8c\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u7684\u7814\u7a76\u3002\u9700\u8981\u4e86\u89e3AR\u5728\u5bbd\u533a\u57df\u6237\u5916\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u4f7f\u7528Microsoft Hololens 2 AR\u5934\u663e\u8fdb\u884c\u6237\u5916\u7528\u6237\u7814\u7a76\uff08n=48\uff09\uff0c\u53c2\u4e0e\u8005\u6267\u884c\u5bfb\u5b9d\u4efb\u52a1\uff0c\u5728\u589e\u5f3a\u7684\u6237\u5916\u5ead\u9662\u573a\u666f\u4e2d\u641c\u7d22\u548c\u5206\u7c7b\u865a\u62df\u76ee\u6807\u7269\u54c1\u3002\u901a\u8fc7\u64cd\u7eb5\u8ba4\u77e5\u8d1f\u8377\uff08\u97f3\u9891\u76d1\u63a7\u4efb\u52a1\uff09\u3001\u6d4b\u91cf\u884c\u8d70\u8def\u5f84\u3001\u5934\u90e8\u65b9\u5411\u548c\u773c\u52a8\u6570\u636e\u6765\u6536\u96c6\u6570\u636e\u3002", "result": "1\uff09\u81ea\u7136\u5149\u7167\u6761\u4ef6\u4e0b\u7528\u6237\u8212\u9002\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u591c\u95f4\u865a\u62df\u5bf9\u8c61\u66f4\u53ef\u89c1\u4f46\u66f4\u5bb9\u6613\u649e\u5230\u7269\u7406\u5bf9\u8c61\uff1b2\uff09\u5bf9\u7269\u7406\u5bf9\u8c61\u7684\u8bb0\u5fc6\u6bd4\u865a\u62df\u5bf9\u8c61\u5dee\uff1b3\uff09\u9690\u85cf\u5728\u865a\u62df\u5bf9\u8c61\u540e\u9762\u7684\u5b9d\u77f3\u53d1\u73b0\u7387\u66f4\u9ad8\uff1b4\uff09\u53cc\u4efb\u52a1\u5904\u7406\u6539\u53d8\u4e86\u641c\u7d22\u884c\u4e3a\u3002", "conclusion": "AR\u5e94\u7528\u4e2d\u5b58\u5728\u91cd\u8981\u7684\u6280\u672f\u3001\u611f\u77e5\u548c\u8ba4\u77e5\u56e0\u7d20\u9700\u8981\u8003\u8651\uff0c\u5305\u62ec\u5149\u7167\u6761\u4ef6\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5f71\u54cd\u3001\u865a\u62df\u4e0e\u7269\u7406\u5bf9\u8c61\u6ce8\u610f\u529b\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u8ba4\u77e5\u8d1f\u8377\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u3002"}}
{"id": "2511.00801", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.00801", "abs": "https://arxiv.org/abs/2511.00801", "authors": ["Zhihui Chen", "Mengling Feng"], "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing", "comment": null, "summary": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k].", "AI": {"tldr": "\u63d0\u51fa\u4e86Med-Banana-50K\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b5\u4e07\u5f20\u56fe\u50cf\u7684\u533b\u7597\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u79cd\u6a21\u6001\u548c23\u79cd\u75be\u75c5\u7c7b\u578b\uff0c\u901a\u8fc7Gemini-2.5-Flash-Image\u751f\u6210\u53cc\u5411\u7f16\u8f91\uff0c\u5e76\u91c7\u7528\u533b\u5b66\u8d28\u91cf\u63a7\u5236\u548c\u8fed\u4ee3\u4f18\u5316\u786e\u4fdd\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u793e\u533a\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u5f00\u653e\u53ef\u8bbf\u95ee\u7684\u4e13\u95e8\u9488\u5bf9\u533b\u7597\u56fe\u50cf\u7f16\u8f91\u7684\u6570\u636e\u96c6\uff0c\u4e14\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u89e3\u5256\u5b66\u548c\u4e34\u5e8a\u7ea6\u675f\u3002", "method": "\u5229\u7528Gemini-2.5-Flash-Image\u4ece\u771f\u5b9e\u533b\u7597\u56fe\u50cf\u751f\u6210\u53cc\u5411\u7f16\u8f91\uff08\u75c5\u7076\u6dfb\u52a0\u548c\u79fb\u9664\uff09\uff0c\u91c7\u7528LLM-as-Judge\u533b\u5b66\u8bc4\u4f30\u6807\u51c6\u548c\u5386\u53f2\u611f\u77e5\u8fed\u4ee3\u4f18\u5316\uff08\u6700\u591a5\u8f6e\uff09\uff0c\u786e\u4fdd\u7f16\u8f91\u8d28\u91cf\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b5\u4e07\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u80f8\u90e8X\u5149\u3001\u8111\u90e8MRI\u548c\u773c\u5e95\u6444\u5f71\u4e09\u79cd\u6a21\u6001\uff0c23\u79cd\u75be\u75c5\u7c7b\u578b\uff0c\u8fd8\u5305\u62ec3.7\u4e07\u6b21\u5931\u8d25\u5c1d\u8bd5\u7684\u5b8c\u6574\u5bf9\u8bdd\u8bb0\u5f55\u3002", "conclusion": "Med-Banana-50K\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e0b\u4e00\u4ee3\u533b\u7597\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u533b\u5b66\u9a8c\u8bc1\u4e14\u5b8c\u5168\u6587\u6863\u5316\u7684\u8d44\u6e90\u3002"}}
{"id": "2511.00037", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.00037", "abs": "https://arxiv.org/abs/2511.00037", "authors": ["Riya Gupta", "Alexander Chowdhury", "Sahil Nalawade"], "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in medical\nAI, enabling collaborative model training across institutions without direct\ndata sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,\nFlower, and Owkin Substra to evaluate their suitability for medical imaging\napplications in real-world settings. Using the PathMNIST dataset, we assess\nmodel performance, convergence efficiency, communication overhead, scalability,\nand developer experience. Results indicate that NVIDIA FLARE offers superior\nproduction scalability, Flower provides flexibility for prototyping and\nacademic research, and Owkin Substra demonstrates exceptional privacy and\ncompliance features. Each framework exhibits strengths optimized for distinct\nuse cases, emphasizing their relevance to practical deployment in healthcare\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u4e09\u4e2a\u4e3b\u6d41\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff08NVIDIA FLARE\u3001Flower\u548cOwkin Substra\uff09\u5728\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u6027\u80fd\u3001\u6536\u655b\u6548\u7387\u3001\u901a\u4fe1\u5f00\u9500\u3001\u53ef\u6269\u5c55\u6027\u548c\u5f00\u53d1\u8005\u4f53\u9a8c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5df2\u6210\u4e3a\u533b\u5b66AI\u4e2d\u7684\u53d8\u9769\u6027\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u673a\u6784\u95f4\u8fdb\u884c\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u800c\u65e0\u9700\u76f4\u63a5\u5171\u4eab\u6570\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cFL\u6846\u67b6\u5728\u771f\u5b9e\u533b\u7597\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528PathMNIST\u6570\u636e\u96c6\uff0c\u5bf9\u4e09\u4e2aFL\u6846\u67b6\uff08NVIDIA FLARE\u3001Flower\u548cOwkin Substra\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u591a\u4e2a\u5173\u952e\u6307\u6807\u3002", "result": "NVIDIA FLARE\u5728\u751f\u4ea7\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cFlower\u5728\u539f\u578b\u8bbe\u8ba1\u548c\u5b66\u672f\u7814\u7a76\u65b9\u9762\u63d0\u4f9b\u7075\u6d3b\u6027\uff0cOwkin Substra\u5728\u9690\u79c1\u548c\u5408\u89c4\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6bcf\u4e2a\u6846\u67b6\u90fd\u9488\u5bf9\u4e0d\u540c\u7684\u4f7f\u7528\u573a\u666f\u4f18\u5316\u4e86\u5404\u81ea\u7684\u4f18\u52bf\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u533b\u7597\u73af\u5883\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u76f8\u5173\u6027\u3002"}}
{"id": "2511.00122", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00122", "abs": "https://arxiv.org/abs/2511.00122", "authors": ["Ran Xu", "Yupeng Qi", "Jingsen Feng", "Xu Chu"], "title": "Engineering.ai: A Platform for Teams of AI Engineers in Computational Design", "comment": null, "summary": "In modern engineering practice, human engineers collaborate in specialized\nteams to design complex products, with each expert completing their respective\ntasks while communicating and exchanging results and data with one another.\nWhile this division of expertise is essential for managing multidisciplinary\ncomplexity, it demands substantial development time and cost. Recently, we\nintroduced OpenFOAMGPT (1.0, 2.0), which functions as an autonomous AI engineer\nfor computational fluid dynamics, and turbulence.ai, which can conduct\nend-to-end research in fluid mechanics draft publications and PhD theses.\nBuilding upon these foundations, we present Engineering.ai, a platform for\nteams of AI engineers in computational design. The framework employs a\nhierarchical multi-agent architecture where a Chief Engineer coordinates\nspecialized agents consisting of Aerodynamics, Structural, Acoustic, and\nOptimization Engineers, each powered by LLM with domain-specific knowledge.\nAgent-agent collaboration is achieved through file-mediated communication for\ndata provenance and reproducibility, while a comprehensive memory system\nmaintains project context, execution history, and retrieval-augmented domain\nknowledge to ensure reliable decision-making across the workflow. The system\nintegrates FreeCAD, Gmsh, OpenFOAM, CalculiX, and BPM acoustic analysis,\nenabling parallel multidisciplinary simulations while maintaining computational\naccuracy. The framework is validated through UAV wing optimization. This work\ndemonstrates that agentic-AI-enabled AI engineers has the potential to perform\ncomplex engineering tasks autonomously. Remarkably, the automated workflow\nachieved a 100% success rate across over 400 parametric configurations, with\nzero mesh generation failures, solver convergence issues, or manual\ninterventions required, validating that the framework is trustworthy.", "AI": {"tldr": "\u63d0\u51fa\u4e86Engineering.ai\u5e73\u53f0\uff0c\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7531\u9996\u5e2d\u5de5\u7a0b\u5e08\u534f\u8c03\u7a7a\u6c14\u52a8\u529b\u5b66\u3001\u7ed3\u6784\u3001\u58f0\u5b66\u548c\u4f18\u5316\u7b49\u4e13\u4e1aAI\u5de5\u7a0b\u5e08\uff0c\u901a\u8fc7\u6587\u4ef6\u901a\u4fe1\u5b9e\u73b0\u534f\u4f5c\uff0c\u5728\u65e0\u4eba\u673a\u7ffc\u4f18\u5316\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u5b9e\u8df5\u4e2d\uff0c\u4e13\u5bb6\u56e2\u961f\u534f\u4f5c\u8bbe\u8ba1\u590d\u6742\u4ea7\u54c1\u9700\u8981\u5927\u91cf\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u7684AI\u5de5\u7a0b\u5e08\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u9996\u5e2d\u5de5\u7a0b\u5e08\u534f\u8c03\u4e13\u4e1aAI\u5de5\u7a0b\u5e08\uff0c\u901a\u8fc7\u6587\u4ef6\u901a\u4fe1\u5b9e\u73b0\u6570\u636e\u6eaf\u6e90\u548c\u53ef\u91cd\u590d\u6027\uff0c\u96c6\u6210FreeCAD\u3001Gmsh\u3001OpenFOAM\u3001CalculiX\u548cBPM\u58f0\u5b66\u5206\u6790\u7b49\u5de5\u5177\u8fdb\u884c\u5e76\u884c\u591a\u5b66\u79d1\u4eff\u771f\u3002", "result": "\u5728\u65e0\u4eba\u673a\u7ffc\u4f18\u5316\u4e2d\uff0c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u5728400\u591a\u4e2a\u53c2\u6570\u914d\u7f6e\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff0c\u96f6\u7f51\u683c\u751f\u6210\u5931\u8d25\u3001\u6c42\u89e3\u5668\u6536\u655b\u95ee\u9898\u6216\u9700\u8981\u624b\u52a8\u5e72\u9884\u3002", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u4f53AI\u7684AI\u5de5\u7a0b\u5e08\u5177\u6709\u81ea\u4e3b\u6267\u884c\u590d\u6742\u5de5\u7a0b\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00193", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00193", "abs": "https://arxiv.org/abs/2511.00193", "authors": ["Faranak Akbarifar", "Nooshin Maghsoodi", "Sean P Dukelow", "Stephen Scott", "Parvin Mousavi"], "title": "Reducing Robotic Upper-Limb Assessment Time While Maintaining Precision: A Time Series Foundation Model Approach", "comment": null, "summary": "Purpose: Visually Guided Reaching (VGR) on the Kinarm robot yields sensitive\nkinematic biomarkers but requires 40-64 reaches, imposing time and fatigue\nburdens. We evaluate whether time-series foundation models can replace\nunrecorded trials from an early subset of reaches while preserving the\nreliability of standard Kinarm parameters.\n  Methods: We analyzed VGR speed signals from 461 stroke and 599 control\nparticipants across 4- and 8-target reaching protocols. We withheld all but the\nfirst 8 or 16 reaching trials and used ARIMA, MOMENT, and Chronos models,\nfine-tuned on 70 percent of subjects, to forecast synthetic trials. We\nrecomputed four kinematic features of reaching (reaction time, movement time,\nposture speed, maximum speed) on combined recorded plus forecasted trials and\ncompared them to full-length references using ICC(2,1).\n  Results: Chronos forecasts restored ICC >= 0.90 for all parameters with only\n8 recorded trials plus forecasts, matching the reliability of 24-28 recorded\nreaches (Delta ICC <= 0.07). MOMENT yielded intermediate gains, while ARIMA\nimprovements were minimal. Across cohorts and protocols, synthetic trials\nreplaced reaches without materially compromising feature reliability.\n  Conclusion: Foundation-model forecasting can greatly shorten Kinarm VGR\nassessment time. For the most impaired stroke survivors, sessions drop from 4-5\nminutes to about 1 minute while preserving kinematic precision. This\nforecast-augmented paradigm promises efficient robotic evaluations for\nassessing motor impairments following stroke.", "AI": {"tldr": "\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u672a\u8bb0\u5f55\u7684Kinarm\u673a\u5668\u4ebaVGR\u6d4b\u8bd5\u8bd5\u9a8c\uff0c\u4ec5\u97008-16\u4e2a\u5b9e\u9645\u8bd5\u9a8c\u52a0\u4e0a\u9884\u6d4b\u8bd5\u9a8c\u5373\u53ef\u8fbe\u523024-28\u4e2a\u5b8c\u6574\u8bd5\u9a8c\u7684\u53ef\u9760\u6027\uff0c\u663e\u8457\u7f29\u77ed\u8bc4\u4f30\u65f6\u95f4\u3002", "motivation": "Kinarm\u673a\u5668\u4eba\u89c6\u89c9\u5f15\u5bfc\u4f38\u624b\u6d4b\u8bd5\u9700\u898140-64\u6b21\u4f38\u624b\uff0c\u9020\u6210\u65f6\u95f4\u548c\u75b2\u52b3\u8d1f\u62c5\uff0c\u9700\u8981\u5bfb\u627e\u65b9\u6cd5\u51cf\u5c11\u5b9e\u9645\u8bd5\u9a8c\u6b21\u6570\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u5b66\u53c2\u6570\u7684\u53ef\u9760\u6027\u3002", "method": "\u5206\u6790461\u540d\u4e2d\u98ce\u548c599\u540d\u5bf9\u7167\u53c2\u4e0e\u8005\u7684VGR\u901f\u5ea6\u4fe1\u53f7\uff0c\u4fdd\u7559\u524d8\u621616\u6b21\u8bd5\u9a8c\uff0c\u4f7f\u7528ARIMA\u3001MOMENT\u548cChronos\u6a21\u578b\u9884\u6d4b\u5408\u6210\u8bd5\u9a8c\uff0c\u91cd\u65b0\u8ba1\u7b97\u8fd0\u52a8\u5b66\u7279\u5f81\u5e76\u4e0e\u5b8c\u6574\u53c2\u8003\u6bd4\u8f83\u3002", "result": "Chronos\u6a21\u578b\u9884\u6d4b\u4ec5\u97008\u4e2a\u5b9e\u9645\u8bd5\u9a8c\u52a0\u9884\u6d4b\u8bd5\u9a8c\u5373\u53ef\u6062\u590dICC\u22650.90\u7684\u53ef\u9760\u6027\uff0c\u76f8\u5f53\u4e8e24-28\u4e2a\u5b8c\u6574\u8bd5\u9a8c\u7684\u6548\u679c\uff0c\u663e\u8457\u7f29\u77ed\u8bc4\u4f30\u65f6\u95f4\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u53ef\u4ee5\u5927\u5e45\u7f29\u77edKinarm VGR\u8bc4\u4f30\u65f6\u95f4\uff0c\u5bf9\u6700\u4e25\u91cd\u4e2d\u98ce\u60a3\u8005\u53ef\u5c064-5\u5206\u949f\u6d4b\u8bd5\u7f29\u77ed\u81f3\u7ea61\u5206\u949f\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u5b66\u7cbe\u5ea6\uff0c\u4e3a\u4e2d\u98ce\u540e\u8fd0\u52a8\u969c\u788d\u8bc4\u4f30\u63d0\u4f9b\u9ad8\u6548\u673a\u5668\u4eba\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2511.00407", "categories": ["cs.HC", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00407", "abs": "https://arxiv.org/abs/2511.00407", "authors": ["\u0141ukasz Sikorski", "Jacek Matulewski"], "title": "Reducing students' misconceptions about video game development. A mixed-method study", "comment": null, "summary": "This study examines students' na\\\"ive mindset (misconceptions) about video\ngame development, idealized and inaccurate beliefs that shape an unrealistic\nunderstanding of the field. The research evaluated the effectiveness of a\nfifteen-hour-long lecture series delivered by industry professionals, designed\nto challenge this mindset and expose students to the complexities and realities\nof game production. A mixed-methods approach was employed, combining\nqualitative analysis with a prototype quantitative tool developed to measure\nlevels of misconception. Participants included students (n = 91) from diverse\nacademic backgrounds interested in game creation and professionals (n = 94)\nworking in the video game industry. Findings show that the intervention\nsignificantly reduced students' na\\\"ive beliefs while enhancing their\nmotivation to pursue careers in the industry. Exposure to professional\nperspectives fostered a more realistic and informed mindset, taking into\naccount the understanding of the technical, collaborative, and business aspects\nof game development. The results suggest that incorporating similar expert-led\ninterventions early in game development education can improve learning\noutcomes, support informed career choices, and mitigate future professional\ndisappointment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5b66\u751f\u5bf9\u6e38\u620f\u5f00\u53d1\u7684\u9519\u8bef\u8ba4\u77e5\uff0c\u901a\u8fc7\u884c\u4e1a\u4e13\u5bb6\u8bb2\u5ea7\u5e72\u9884\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5b66\u751f\u7684\u9519\u8bef\u89c2\u5ff5\u5e76\u63d0\u5347\u4e86\u4ed6\u4eec\u8fdb\u5165\u6e38\u620f\u884c\u4e1a\u7684\u52a8\u673a\u3002", "motivation": "\u5b66\u751f\u666e\u904d\u5bf9\u6e38\u620f\u5f00\u53d1\u5b58\u5728\u7406\u60f3\u5316\u548c\u4e0d\u51c6\u786e\u7684\u8ba4\u77e5\uff0c\u8fd9\u79cd\u9519\u8bef\u89c2\u5ff5\u4f1a\u5f71\u54cd\u4ed6\u4eec\u7684\u804c\u4e1a\u9009\u62e9\u548c\u672a\u6765\u53d1\u5c55\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e13\u4e1a\u5e72\u9884\u6765\u7ea0\u6b63\u8fd9\u4e9b\u8bef\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9a\u6027\u5206\u6790\u548c\u5b9a\u91cf\u6d4b\u91cf\u5de5\u5177\uff0c\u5bf991\u540d\u5bf9\u6e38\u620f\u5f00\u53d1\u611f\u5174\u8da3\u7684\u5b66\u751f\u548c94\u540d\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "15\u5c0f\u65f6\u7684\u884c\u4e1a\u4e13\u5bb6\u8bb2\u5ea7\u663e\u8457\u964d\u4f4e\u4e86\u5b66\u751f\u7684\u9519\u8bef\u8ba4\u77e5\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u4ed6\u4eec\u8ffd\u6c42\u6e38\u620f\u884c\u4e1a\u804c\u4e1a\u7684\u52a8\u673a\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5efa\u7acb\u4e86\u66f4\u73b0\u5b9e\u548c\u5168\u9762\u7684\u6e38\u620f\u5f00\u53d1\u8ba4\u77e5\u3002", "conclusion": "\u5728\u6e38\u620f\u5f00\u53d1\u6559\u80b2\u65e9\u671f\u5f15\u5165\u4e13\u5bb6\u4e3b\u5bfc\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u53ef\u4ee5\u6539\u5584\u5b66\u4e60\u6210\u679c\uff0c\u652f\u6301\u660e\u667a\u7684\u804c\u4e1a\u9009\u62e9\uff0c\u5e76\u51cf\u5c11\u672a\u6765\u7684\u804c\u4e1a\u5931\u671b\u3002"}}
{"id": "2511.01390", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01390", "abs": "https://arxiv.org/abs/2511.01390", "authors": ["Xinyu Mao", "Junsi Li", "Haoji Zhang", "Yu Liang", "Ming Sun"], "title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "comment": null, "summary": "Fine-grained cross-modal alignment aims to establish precise local\ncorrespondences between vision and language, forming a cornerstone for visual\nquestion answering and related multimodal applications. Current approaches face\nchallenges in addressing patch redundancy and ambiguity, which arise from the\ninherent information density disparities across modalities. Recently,\nMultimodal Large Language Models (MLLMs) have emerged as promising solutions to\nbridge this gap through their robust semantic generation capabilities. However,\nthe dense textual outputs from MLLMs may introduce conflicts with the original\nsparse captions. Furthermore, accurately quantifying semantic relevance between\nrich visual patches and concise textual descriptions remains a core challenge.\nTo overcome these limitations, we introduce the Semantic-Enhanced Patch\nSlimming (SEPS) framework, which systematically addresses patch redundancy and\nambiguity. Our approach employs a two-stage mechanism to integrate unified\nsemantics from both dense and sparse texts, enabling the identification of\nsalient visual patches. Additionally, it leverages relevance-aware selection\nwith mean value computation to highlight crucial patch-word correspondences,\nthereby improving cross-modal similarity assessment. Comprehensive experiments\non Flickr30K and MS-COCO datasets validate that SEPS achieves superior\nperformance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse\nmodel architectures, with notable enhancements in text-to-image retrieval\nscenarios. Our implementation is available at\nhttps://github.com/Sweet4tars/seps.git.", "AI": {"tldr": "SEPS\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5bc6\u96c6\u548c\u7a00\u758f\u6587\u672c\u7684\u7edf\u4e00\u8bed\u4e49\u6765\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4e2d\u7684\u8865\u4e01\u5197\u4f59\u548c\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\u8bc6\u522b\u663e\u8457\u89c6\u89c9\u8865\u4e01\uff0c\u5e76\u901a\u8fc7\u76f8\u5173\u6027\u611f\u77e5\u9009\u62e9\u63d0\u5347\u8de8\u6a21\u6001\u76f8\u4f3c\u6027\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u5904\u7406\u8de8\u6a21\u6001\u4fe1\u606f\u5bc6\u5ea6\u5dee\u5f02\u5bfc\u81f4\u7684\u8865\u4e01\u5197\u4f59\u548c\u6a21\u7cca\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cMLLMs\u7684\u5bc6\u96c6\u6587\u672c\u8f93\u51fa\u53ef\u80fd\u4e0e\u539f\u59cb\u7a00\u758f\u63cf\u8ff0\u4ea7\u751f\u51b2\u7a81\uff0c\u51c6\u786e\u91cf\u5316\u89c6\u89c9\u8865\u4e01\u4e0e\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\u662f\u6838\u5fc3\u96be\u9898\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u589e\u5f3a\u8865\u4e01\u7cbe\u7b80\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\u6574\u5408\u5bc6\u96c6\u548c\u7a00\u758f\u6587\u672c\u7684\u7edf\u4e00\u8bed\u4e49\u6765\u8bc6\u522b\u663e\u8457\u89c6\u89c9\u8865\u4e01\uff0c\u5229\u7528\u76f8\u5173\u6027\u611f\u77e5\u9009\u62e9\u548c\u5747\u503c\u8ba1\u7b97\u7a81\u51fa\u5173\u952e\u8865\u4e01-\u5355\u8bcd\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728Flickr30K\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1SEPS\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e0brSum\u6307\u6807\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd523%-86%\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SEPS\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u89e3\u51b3\u8865\u4e01\u5197\u4f59\u548c\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5c40\u90e8\u5bf9\u5e94\u5173\u7cfb\u3002"}}
{"id": "2511.00046", "categories": ["cs.CV", "68U10, 94A08", "I.4.3; I.4.4; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.00046", "abs": "https://arxiv.org/abs/2511.00046", "authors": ["Rupjyoti Chutia", "Dibya Jyoti Bora"], "title": "Enhancing rice leaf images: An overview of image denoising techniques", "comment": "18 pages, 6 figures. Research Article published in the International\n  Journal of Agricultural and Natural Sciences (IJANS), Vol. 18, Issue 2, 2025.\n  This paper presents a comparative study of image denoising and CLAHE\n  techniques for enhancing rice leaf images corrupted by Gaussian,\n  Salt-and-pepper, Speckle, and Random noise for agricultural analysis", "summary": "Digital image processing involves the systematic handling of images using\nadvanced computer algorithms, and has gained significant attention in both\nacademic and practical fields. Image enhancement is a crucial preprocessing\nstage in the image-processing chain, improving image quality and emphasizing\nfeatures. This makes subsequent tasks (segmentation, feature extraction,\nclassification) more reliable. Image enhancement is essential for rice leaf\nanalysis, aiding in disease detection, nutrient deficiency evaluation, and\ngrowth analysis. Denoising followed by contrast enhancement are the primary\nsteps. Image filters, generally employed for denoising, transform or enhance\nvisual characteristics like brightness, contrast, and sharpness, playing a\ncrucial role in improving overall image quality and enabling the extraction of\nuseful information. This work provides an extensive comparative study of\nwell-known image-denoising methods combined with CLAHE (Contrast Limited\nAdaptive Histogram Equalization) for efficient denoising of rice leaf images.\nThe experiments were performed on a rice leaf image dataset to ensure the data\nis relevant and representative. Results were examined using various metrics to\ncomprehensively test enhancement methods. This approach provides a strong basis\nfor assessing the effectiveness of methodologies in digital image processing\nand reveals insights useful for future adaptation in agricultural research and\nother domains.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6c34\u7a3b\u53f6\u7247\u56fe\u50cf\u8fdb\u884c\u4e86\u56fe\u50cf\u53bb\u566a\u4e0e\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u6cd5\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u7ed3\u5408CLAHE\u6280\u672f\u8bc4\u4f30\u4e0d\u540c\u53bb\u566a\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u56fe\u50cf\u589e\u5f3a\u662f\u56fe\u50cf\u5904\u7406\u94fe\u4e2d\u7684\u5173\u952e\u9884\u5904\u7406\u9636\u6bb5\uff0c\u5bf9\u4e8e\u6c34\u7a3b\u53f6\u7247\u5206\u6790\u5c24\u4e3a\u91cd\u8981\uff0c\u6709\u52a9\u4e8e\u75be\u75c5\u68c0\u6d4b\u3001\u8425\u517b\u7f3a\u4e4f\u8bc4\u4f30\u548c\u751f\u957f\u5206\u6790\u3002", "method": "\u91c7\u7528\u77e5\u540d\u7684\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u4e0eCLAHE\uff08\u5bf9\u6bd4\u5ea6\u53d7\u9650\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff09\u76f8\u7ed3\u5408\uff0c\u5728\u6c34\u7a3b\u53f6\u7247\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u591a\u79cd\u6307\u6807\u5168\u9762\u6d4b\u8bd5\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u53bb\u566a\u65b9\u6cd5\u7ed3\u5408CLAHE\u5728\u6c34\u7a3b\u53f6\u7247\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u6548\u679c\uff0c\u4e3a\u8bc4\u4f30\u6570\u5b57\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u5b57\u56fe\u50cf\u5904\u7406\u65b9\u6cd5\u6709\u6548\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u5e76\u4e3a\u519c\u4e1a\u7814\u7a76\u548c\u5176\u4ed6\u9886\u57df\u7684\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.00162", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00162", "abs": "https://arxiv.org/abs/2511.00162", "authors": ["Michael D. Moffitt"], "title": "ARC-GEN: A Mimetic Procedural Benchmark Generator for the Abstraction and Reasoning Corpus", "comment": null, "summary": "The Abstraction and Reasoning Corpus remains one of the most compelling and\nchallenging benchmarks for tracking progress toward achieving Artificial\nGeneral Intelligence. In contrast to other evaluation datasets designed to\nassess an agent's task-specific skills or accumulated knowledge, the ARC-AGI\nsuite is specifically targeted at measuring skill acquisition efficiency, a\ntrait that has (so far) been lacking in even the most sophisticated machine\nlearning systems. For algorithms that require extensive intra-task exemplars, a\nsignificant constraint imposed by ARC-AGI is the modest cardinality of its\ndemonstration set, comprising a small number of $\\langle$ input, output\n$\\rangle$ grids per task specifying the corresponding transformation. To\nembellish the space of viable sample pairs, this paper introduces ARC-GEN, an\nopen-source procedural generator aimed at extending the original ARC-AGI\ntraining dataset as faithfully as possible. Unlike prior efforts, our generator\nis both exhaustive (covering all four-hundred tasks) and mimetic (more closely\nhonoring the distributional properties and characteristics embodied in the\ninitial ARC-AGI-1 release). We also discuss the use of this generator in\nestablishing a static benchmark suite to verify the correctness of programs\nsubmitted to the 2025 Google Code Golf Championship.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ARC-GEN\uff0c\u4e00\u4e2a\u5f00\u6e90\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u65e8\u5728\u5fe0\u5b9e\u6269\u5c55ARC-AGI\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u539f\u59cb\u6570\u636e\u96c6\u6837\u672c\u6570\u91cf\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u667a\u80fd\u4f53\u6280\u80fd\u83b7\u53d6\u6548\u7387\uff0c\u4f46\u6bcf\u4e2a\u4efb\u52a1\u53ea\u63d0\u4f9b\u5c11\u91cf\u8f93\u5165\u8f93\u51fa\u7f51\u683c\u5bf9\uff0c\u9650\u5236\u4e86\u9700\u8981\u5927\u91cf\u6837\u672c\u7684\u7b97\u6cd5\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86ARC-GEN\u7a0b\u5e8f\u751f\u6210\u5668\uff0c\u8986\u76d6\u5168\u90e8400\u4e2a\u4efb\u52a1\uff0c\u5e76\u66f4\u51c6\u786e\u5730\u6a21\u62df\u539f\u59cbARC-AGI-1\u7248\u672c\u7684\u6570\u636e\u5206\u5e03\u7279\u6027\u548c\u7279\u5f81\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u65e2\u80fd\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u53c8\u80fd\u4fdd\u6301\u539f\u59cb\u6570\u636e\u5206\u5e03\u7279\u6027\u7684\u751f\u6210\u5668\u3002", "conclusion": "ARC-GEN\u4e3aARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6837\u672c\u6269\u5c55\u5de5\u5177\uff0c\u5e76\u5df2\u5e94\u7528\u4e8e2025\u5e74Google Code Golf\u9526\u6807\u8d5b\u7684\u7a0b\u5e8f\u6b63\u786e\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2511.00259", "categories": ["cs.RO", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00259", "abs": "https://arxiv.org/abs/2511.00259", "authors": ["Andria J. Farrens", "Luis Garcia-Fernandez", "Raymond Diaz Rojas", "Jillian Obeso Estrada", "Dylan Reinsdorf", "Vicky Chan", "Disha Gupta", "Joel Perry", "Eric Wolbrecht", "An Do", "Steven C. Cramer", "David J. Reinkensmeyer"], "title": "Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial", "comment": "Main manuscript: 38 pages (double spaced, with references), 6\n  figures, 2 tables and collated supplemental materials (17 pages, double\n  spaced)", "summary": "Precision rehabilitation aims to tailor movement training to improve\noutcomes. We tested whether proprioceptively-tailored robotic training improves\nhand function and neural processing in stroke survivors. Using a robotic finger\nexoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel\nTraining, which uses robot-facilitated, gamified movements to enhance\nproprioceptive processing, and Virtual Assistance Training, which reduces\nrobotic aid to increase reliance on self-generated feedback. In a randomized\ncontrolled trial, forty-six chronic stroke survivors completed nine 2-hour\nsessions of Standard, Propriopixel or Virtual training. Among participants with\nproprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002)\nand Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand\nfunction (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with\nimprovements in hand function. Tailored training enhanced neural sensitivity to\nproprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive\nContingent Negative Variation. These findings support proprioceptively-tailored\ntraining as a pathway to precision neurorehabilitation.", "AI": {"tldr": "\u672c\u7814\u7a76\u6d4b\u8bd5\u4e86\u4e24\u79cd\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u65b9\u6cd5\uff08Propriopixel\u8bad\u7ec3\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff09\u5bf9\u4e2d\u98ce\u5e78\u5b58\u8005\u624b\u90e8\u529f\u80fd\u548c\u795e\u7ecf\u5904\u7406\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u5b58\u5728\u672c\u4f53\u611f\u89c9\u7f3a\u9677\u7684\u53c2\u4e0e\u8005\uff0c\u8fd9\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u6bd4\u6807\u51c6\u8bad\u7ec3\u66f4\u80fd\u6539\u5584\u624b\u90e8\u529f\u80fd\uff0c\u4e14\u672c\u4f53\u611f\u89c9\u6539\u5584\u4e0e\u624b\u90e8\u529f\u80fd\u6539\u5584\u76f8\u5173\u3002", "motivation": "\u7cbe\u51c6\u5eb7\u590d\u65e8\u5728\u901a\u8fc7\u5b9a\u5236\u5316\u8fd0\u52a8\u8bad\u7ec3\u6765\u6539\u5584\u5eb7\u590d\u6548\u679c\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u662f\u5426\u80fd\u6539\u5584\u4e2d\u98ce\u5e78\u5b58\u8005\u7684\u624b\u90e8\u529f\u80fd\u548c\u795e\u7ecf\u5904\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u8bbe\u8ba1\uff0c46\u540d\u6162\u6027\u4e2d\u98ce\u5e78\u5b58\u8005\u5b8c\u62109\u6b212\u5c0f\u65f6\u7684\u8bad\u7ec3\uff0c\u5206\u4e3a\u6807\u51c6\u8bad\u7ec3\u3001Propriopixel\u8bad\u7ec3\uff08\u4f7f\u7528\u673a\u5668\u4eba\u8f85\u52a9\u7684\u6e38\u620f\u5316\u8fd0\u52a8\u589e\u5f3a\u672c\u4f53\u611f\u89c9\u5904\u7406\uff09\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff08\u51cf\u5c11\u673a\u5668\u4eba\u8f85\u52a9\u4ee5\u589e\u52a0\u5bf9\u81ea\u6211\u751f\u6210\u53cd\u9988\u7684\u4f9d\u8d56\uff09\u3002", "result": "\u5728\u6709\u672c\u4f53\u611f\u89c9\u7f3a\u9677\u7684\u53c2\u4e0e\u8005\u4e2d\uff0cPropriopixel\u8bad\u7ec3\uff08\u76d2\u5b50\u4e0e\u79ef\u6728\u6d4b\u8bd5\uff1a7\u00b14.2\uff0cp=0.002\uff09\u548c\u865a\u62df\u8f85\u52a9\u8bad\u7ec3\uff084.5\u00b14.4\uff0cp=0.068\uff09\u6bd4\u6807\u51c6\u8bad\u7ec3\uff080.8\u00b12.3\u79ef\u6728\uff09\u4ea7\u751f\u66f4\u5927\u7684\u624b\u90e8\u529f\u80fd\u6539\u5584\u3002\u672c\u4f53\u611f\u89c9\u6539\u5584\u4e0e\u624b\u90e8\u529f\u80fd\u6539\u5584\u76f8\u5173\u3002\u5b9a\u5236\u8bad\u7ec3\u589e\u5f3a\u4e86\u795e\u7ecf\u5bf9\u672c\u4f53\u611f\u89c9\u7ebf\u7d22\u7684\u654f\u611f\u6027\uff0c\u901a\u8fc7\u65b0\u578bEEG\u751f\u7269\u6807\u5fd7\u7269\u2014\u2014\u672c\u4f53\u611f\u89c9\u76f8\u5173\u8d1f\u53d8\u7535\u4f4d\u5f97\u5230\u8bc1\u5b9e\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u672c\u4f53\u611f\u89c9\u5b9a\u5236\u8bad\u7ec3\u4f5c\u4e3a\u7cbe\u51c6\u795e\u7ecf\u5eb7\u590d\u7684\u9014\u5f84\u3002"}}
{"id": "2511.00529", "categories": ["cs.HC", "cs.AI", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00529", "abs": "https://arxiv.org/abs/2511.00529", "authors": ["Botao 'Amber' Hu"], "title": "On Improvisation and Open-Endedness: Insights for Experiential AI", "comment": "Submitted to AAAI 2026 Creative AI for Live Interactive Performances\n  Workshop (CLIP) as a work-in-progress paper", "summary": "Improvisation-the art of spontaneous creation that unfolds moment-to-moment\nwithout a scripted outcome-requires practitioners to continuously sense, adapt,\nand create anew. It is a fundamental mode of human creativity spanning music,\ndance, and everyday life. The open-ended nature of improvisation produces a\nstream of novel, unrepeatable moments-an aspect highly valued in artistic\ncreativity. In parallel, open-endedness (OE)-a system's capacity for unbounded\nnovelty and endless \"interestingness\"-is exemplified in natural or cultural\nevolution and has been considered \"the last grand challenge\" in artificial life\n(ALife). The rise of generative AI now raises the question in computational\ncreativity (CC) research: What makes a \"good\" improvisation for AI? Can AI\nlearn to improvise in a genuinely open-ended way? In this work-in-progress\npaper, we report insights from in-depth interviews with 6 experts in\nimprovisation across dance, music, and contact improvisation. We draw systemic\nconnections between human improvisational arts and the design of future\nexperiential AI agents that could improvise alone or alongside humans-or even\nwith other AI agents-embodying qualities of improvisation drawn from practice:\nactive listening (umwelt and awareness), being in the time (mindfulness and\nephemerality), embracing the unknown (source of randomness and serendipity),\nnon-judgmental flow (acceptance and dynamical stability, balancing structure\nand surprise (unpredictable criticality at edge of chaos), imaginative metaphor\n(synaesthesia and planning), empathy, trust, boundary, and care (mutual theory\nof mind), and playfulness and intrinsic motivation (maintaining\ninterestingness).", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.01775", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01775", "abs": "https://arxiv.org/abs/2511.01775", "authors": ["Zhen Chen", "Qing Xu", "Jinlin Wu", "Biao Yang", "Yuhao Zhai", "Geng Guo", "Jing Zhang", "Yinlu Ding", "Nassir Navab", "Jiebo Luo"], "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment", "comment": null, "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.", "AI": {"tldr": "SurgVeo\u662f\u9996\u4e2a\u7528\u4e8e\u624b\u672f\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u7684\u4e13\u5bb6\u7b56\u5212\u57fa\u51c6\uff0c\u7ed3\u5408\u624b\u672f\u5408\u7406\u6027\u91d1\u5b57\u5854\u6846\u67b6\uff0c\u63ed\u793a\u4e86Veo-3\u6a21\u578b\u5728\u624b\u672fAI\u4e2d\u89c6\u89c9\u6a21\u4eff\u4e0e\u56e0\u679c\u7406\u89e3\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\u5728\u6a21\u62df\u7269\u7406\u4e16\u754c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u6df1\u5ea6\u4e13\u4e1a\u56e0\u679c\u77e5\u8bc6\u7684\u9ad8\u98ce\u9669\u9886\u57df\u5982\u624b\u672f\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faSurgVeo\u57fa\u51c6\u548c\u624b\u672f\u5408\u7406\u6027\u91d1\u5b57\u5854\u6846\u67b6\uff0c\u4f7f\u7528Veo-3\u6a21\u578b\u5bf9\u8179\u8154\u955c\u548c\u795e\u7ecf\u5916\u79d1\u624b\u672f\u7247\u6bb5\u8fdb\u884c\u96f6\u6837\u672c\u9884\u6d4b\u4efb\u52a1\uff0c\u7531\u56db\u4f4d\u8ba4\u8bc1\u5916\u79d1\u533b\u751f\u6309\u7167SPP\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u3002", "result": "\u53d1\u73b0\u660e\u663e\u7684\"\u5408\u7406\u6027\u5dee\u8ddd\"\uff1aVeo-3\u5728\u89c6\u89c9\u611f\u77e5\u5408\u7406\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u66f4\u9ad8\u5c42\u6b21\u7684\u5668\u68b0\u64cd\u4f5c\u5408\u7406\u6027\u3001\u73af\u5883\u53cd\u9988\u5408\u7406\u6027\u548c\u624b\u672f\u610f\u56fe\u5408\u7406\u6027\u65b9\u9762\u4e25\u91cd\u5931\u8d25\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u91cf\u5316\u8bc1\u660e\u4e86\u624b\u672fAI\u4e2d\u89c6\u89c9\u53ef\u4fe1\u6a21\u4eff\u4e0e\u56e0\u679c\u7406\u89e3\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u4e13\u4e1a\u533b\u7597\u9886\u57df\u590d\u6742\u6027\u7684\u672a\u6765\u6a21\u578b\u5960\u5b9a\u4e86\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2511.00268", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86IL-PCR\u8bed\u6599\u5e93\uff0c\u4e3a\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u548c\u6cd5\u89c4\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u7edf\u4e00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7LLM\u91cd\u6392\u5e8f\u65b9\u6cd5\u5229\u7528\u4e24\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u6cd5\u5f8b\u6848\u4f8b\u68c0\u7d22\u548c\u6cd5\u89c4\u68c0\u7d22\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\u5904\u7406\uff0c\u4f46\u8fd9\u4e24\u4e2a\u4efb\u52a1\u672c\u8d28\u76f8\u5173\uff08\u76f8\u4f3c\u6848\u4f8b\u5f80\u5f80\u5f15\u7528\u76f8\u4f3c\u6cd5\u89c4\uff09\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5229\u7528\u8fd9\u79cd\u4f9d\u8d56\u5173\u7cfb\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u6784\u5efaIL-PCR\u8bed\u6599\u5e93\u4f5c\u4e3a\u7edf\u4e00\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5b9e\u9a8c\u4e86\u8bcd\u6cd5\u6a21\u578b\u3001\u8bed\u4e49\u6a21\u578b\u548c\u57fa\u4e8eGNN\u7684\u96c6\u6210\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u6765\u5229\u7528\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u5728\u4e24\u4e2a\u68c0\u7d22\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u6709\u6548\u6027\u3002", "conclusion": "IL-PCR\u8bed\u6599\u5e93\u4e3a\u6cd5\u5f8b\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u51c6\uff0cLLM\u91cd\u6392\u5e8f\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6848\u4f8b\u68c0\u7d22\u548c\u6cd5\u89c4\u68c0\u7d22\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.00060", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00060", "abs": "https://arxiv.org/abs/2511.00060", "authors": ["Zhiqi Qi", "Runxin Zhao", "Hanyang Zhuang", "Chunxiang Wang", "Ming Yang"], "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "comment": null, "summary": "LiDAR-based roadside perception is a cornerstone of advanced Intelligent\nTransportation Systems (ITS). While considerable research has addressed optimal\nLiDAR placement for infrastructure, the profound impact of differing LiDAR\nscanning patterns on perceptual performance remains comparatively\nunder-investigated. The inherent nature of various scanning modes - such as\ntraditional repetitive (mechanical/solid-state) versus emerging non-repetitive\n(e.g. prism-based) systems - leads to distinct point cloud distributions at\nvarying distances, critically dictating the efficacy of object detection and\noverall environmental understanding. To systematically investigate these\ndifferences in infrastructure-based contexts, we introduce the \"InfraLiDARs'\nBenchmark,\" a novel dataset meticulously collected in the CARLA simulation\nenvironment using concurrently operating infrastructure-based LiDARs exhibiting\nboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensive\nstatistical analysis of the respective LiDAR scanning abilities and evaluate\nthe impact of these distinct patterns on the performance of various leading 3D\nobject detection algorithms. Our findings reveal that non-repetitive scanning\nLiDAR and the 128-line repetitive LiDAR were found to exhibit comparable\ndetection performance across various scenarios. Despite non-repetitive LiDAR's\nlimited perception range, it's a cost-effective option considering its low\nprice. Ultimately, this study provides insights for setting up roadside\nperception system with optimal LiDAR scanning patterns and compatible\nalgorithms for diverse roadside applications, and publicly releases the\n\"InfraLiDARs' Benchmark\" dataset to foster further research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540cLiDAR\u626b\u63cf\u6a21\u5f0f\u5bf9\u8def\u8fb9\u611f\u77e5\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7CARLA\u4eff\u771f\u73af\u5883\u521b\u5efa\u4e86InfraLiDARs' Benchmark\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u975e\u91cd\u590d\u626b\u63cfLiDAR\u4e0e128\u7ebf\u91cd\u590d\u626b\u63cfLiDAR\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u76f8\u5f53\uff0c\u4e3a\u975e\u91cd\u590d\u626b\u63cfLiDAR\u4f5c\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u5927\u91cf\u7814\u7a76\u5173\u6ce8\u57fa\u7840\u8bbe\u65bdLiDAR\u7684\u6700\u4f18\u90e8\u7f72\uff0c\u4f46\u4e0d\u540cLiDAR\u626b\u63cf\u6a21\u5f0f\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u6df1\u8fdc\u5f71\u54cd\u4ecd\u76f8\u5bf9\u7f3a\u4e4f\u7814\u7a76\u3002\u4f20\u7edf\u91cd\u590d\u626b\u63cf\uff08\u673a\u68b0/\u56fa\u6001\uff09\u4e0e\u65b0\u5174\u975e\u91cd\u590d\u626b\u63cf\uff08\u5982\u68f1\u955c\u5f0f\uff09\u7cfb\u7edf\u5728\u4e0d\u540c\u8ddd\u79bb\u4e0b\u4ea7\u751f\u4e0d\u540c\u7684\u70b9\u4e91\u5206\u5e03\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u7269\u4f53\u68c0\u6d4b\u6548\u679c\u548c\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u521b\u5efa\u4e86InfraLiDARs' Benchmark\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u540c\u65f6\u8fd0\u884c\u7684\u57fa\u7840\u8bbe\u65bdLiDAR\u91c7\u96c6\u6570\u636e\uff0c\u6db5\u76d6\u4e24\u79cd\u626b\u63cf\u6a21\u5f0f\u3002\u57fa\u4e8e\u6b64\u57fa\u51c6\uff0c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u626b\u63cf\u6a21\u5f0f\u5bf9\u591a\u79cd\u9886\u51483D\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u975e\u91cd\u590d\u626b\u63cfLiDAR\u548c128\u7ebf\u91cd\u590d\u626b\u63cfLiDAR\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u76f8\u5f53\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u5c3d\u7ba1\u975e\u91cd\u590d\u626b\u63cfLiDAR\u7684\u611f\u77e5\u8303\u56f4\u6709\u9650\uff0c\u4f46\u8003\u8651\u5230\u5176\u8f83\u4f4e\u7684\u4ef7\u683c\uff0c\u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u9009\u62e9\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8bbe\u7f6e\u5177\u6709\u6700\u4f18LiDAR\u626b\u63cf\u6a21\u5f0f\u548c\u517c\u5bb9\u7b97\u6cd5\u7684\u8def\u8fb9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86InfraLiDARs' Benchmark\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.00194", "categories": ["cs.AI", "F.2.2, F.4.1"], "pdf": "https://arxiv.org/pdf/2511.00194", "abs": "https://arxiv.org/abs/2511.00194", "authors": ["Jovial Cheukam Ngouonou", "Ramiz Gindullin", "Claude-Guy Quimper", "Nicolas Beldiceanu", "Remi Douence"], "title": "Incremental Selection of Most-Filtering Conjectures and Proofs of the Selected Conjectures", "comment": null, "summary": "We present an improved incremental selection algorithm of the selection\nalgorithm presented in [1] and prove all the selected conjectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u589e\u91cf\u9009\u62e9\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u6709\u9009\u5b9a\u7684\u731c\u60f3", "motivation": "\u6539\u8fdb\u73b0\u6709\u7684\u9009\u62e9\u7b97\u6cd5\uff0c\u63d0\u9ad8\u7b97\u6cd5\u6548\u7387\u548c\u6027\u80fd", "method": "\u57fa\u4e8e\u6587\u732e[1]\u4e2d\u7684\u9009\u62e9\u7b97\u6cd5\u8fdb\u884c\u6539\u8fdb\uff0c\u5f00\u53d1\u589e\u91cf\u5f0f\u9009\u62e9\u65b9\u6cd5", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u6240\u6709\u9009\u5b9a\u7684\u731c\u60f3", "conclusion": "\u6539\u8fdb\u7684\u589e\u91cf\u9009\u62e9\u7b97\u6cd5\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5b8c\u6210\u6240\u6709\u9009\u5b9a\u731c\u60f3\u7684\u8bc1\u660e"}}
{"id": "2511.00306", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00306", "abs": "https://arxiv.org/abs/2511.00306", "authors": ["Baoshan Song", "Ruijie Xu", "Li-Ta Hsu"], "title": "FGO MythBusters: Explaining how Kalman Filter variants achieve the same performance as FGO in navigation applications", "comment": null, "summary": "Sliding window-factor graph optimization (SW-FGO) has gained more and more\nattention in navigation research due to its robust approximation to\nnon-Gaussian noises and nonlinearity of measuring models. There are lots of\nworks focusing on its application performance compared to extended Kalman\nfilter (EKF) but there is still a myth at the theoretical relationship between\nthe SW-FGO and EKF. In this paper, we find the necessarily fair condition to\nconnect SW-FGO and Kalman filter variants (KFV) (e.g., EKF, iterative EKF\n(IEKF), robust EKF (REKF) and robust iterative EKF (RIEKF)). Based on the\nconditions, we propose a recursive FGO (Re-FGO) framework to represent KFV\nunder SW-FGO formulation. Under explicit conditions (Markov assumption,\nGaussian noise with L2 loss, and a one-state window), Re-FGO regenerates\nexactly to EKF/IEKF/REKF/RIEKF, while SW-FGO shows measurable benefits in\nnonlinear, non-Gaussian regimes at a predictable compute cost. Finally, after\nclarifying the connection between them, we highlight the unique advantages of\nSW-FGO in practical phases, especially on numerical estimation and deep\nlearning integration. The code and data used in this work is open sourced at\nhttps://github.com/Baoshan-Song/KFV-FGO-Comparison.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6ed1\u52a8\u7a97\u53e3\u56e0\u5b50\u56fe\u4f18\u5316(SW-FGO)\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u53d8\u4f53(KFV)\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u9012\u5f52FGO(Re-FGO)\u6846\u67b6\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0bRe-FGO\u53ef\u7cbe\u786e\u518d\u751f\u4e3aEKF/IEKF/REKF/RIEKF\uff0c\u540c\u65f6\u5c55\u793a\u4e86SW-FGO\u5728\u975e\u7ebf\u6027\u3001\u975e\u9ad8\u65af\u573a\u666f\u4e0b\u7684\u4f18\u52bf\u3002", "motivation": "\u867d\u7136SW-FGO\u5728\u5bfc\u822a\u7814\u7a76\u4e2d\u56e0\u5bf9\u975e\u9ad8\u65af\u566a\u58f0\u548c\u975e\u7ebf\u6027\u6d4b\u91cf\u6a21\u578b\u7684\u9c81\u68d2\u8fd1\u4f3c\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46SW-FGO\u4e0eEKF\u4e4b\u95f4\u7684\u7406\u8bba\u5173\u7cfb\u4ecd\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5efa\u7acb\u4e24\u8005\u4e4b\u95f4\u7684\u7406\u8bba\u8fde\u63a5\u3002", "method": "\u63d0\u51fa\u4e86\u8fde\u63a5SW-FGO\u548cKFV\u7684\u5fc5\u8981\u516c\u5e73\u6761\u4ef6\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6761\u4ef6\u6784\u5efa\u4e86\u9012\u5f52FGO(Re-FGO)\u6846\u67b6\uff0c\u5728\u663e\u5f0f\u6761\u4ef6\u4e0b(\u9a6c\u5c14\u53ef\u592b\u5047\u8bbe\u3001\u9ad8\u65af\u566a\u58f0\u4e0eL2\u635f\u5931\u3001\u5355\u72b6\u6001\u7a97\u53e3)\u9a8c\u8bc1Re-FGO\u53ef\u7cbe\u786e\u518d\u751f\u4e3a\u5404\u79cdKFV\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cRe-FGO\u53ef\u7cbe\u786e\u518d\u751f\u4e3aEKF/IEKF/REKF/RIEKF\uff0c\u800cSW-FGO\u5728\u975e\u7ebf\u6027\u3001\u975e\u9ad8\u65af\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u53ef\u8861\u91cf\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u53ef\u9884\u6d4b\u3002", "conclusion": "\u6f84\u6e05\u4e86SW-FGO\u4e0eKFV\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u7a81\u51fa\u4e86SW-FGO\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u6570\u503c\u4f30\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u96c6\u6210\u65b9\u9762\u3002"}}
{"id": "2511.00654", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00654", "abs": "https://arxiv.org/abs/2511.00654", "authors": ["Jaime Banks"], "title": "Measuring Machine Companionship: Scale Development and Validation for AI Companions", "comment": null, "summary": "The mainstreaming of companionable machines--customizable artificial agents\ndesigned to participate in ongoing, idiosyncratic, socioemotional\nrelationships--is met with relative theoretical and empirical disarray,\naccording to recent systematic reviews. In particular, the conceptualization\nand measurement of machine companionship (MC) is inconsistent or sometimes\naltogether missing. This study starts to bridge that gap by developing and\ninitially validating a novel measurement to capture MC experiences--the\nunfolding, autotelic, positively experienced, coordinated connection between\nhuman and machine--with AI companions (AICs). After systematic generation and\nexpert review of an item pool (including items pertaining to dyadism,\ncoordination, autotelicity, temporality, and positive valence), N = 467 people\ninteracting with AICs responded to the item pool and to construct validation\nmeasures. Through exploratory factor analysis, two factors were induced:\nEudaimonic Exchange and Connective Coordination. Construct validation analyses\n(confirmed in a second sample; N = 249) indicate the factors function largely\nas expected. Post-hoc analyses of deviations suggest two different templates\nfor MC with AICs: One socioinstrumental and one autotelic.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u966a\u4f34(MC)\u6d4b\u91cf\u5de5\u5177\uff0c\u7528\u4e8e\u6355\u6349\u4eba\u7c7b\u4e0eAI\u4f34\u4fa3\u4e4b\u95f4\u5c55\u5f00\u7684\u3001\u81ea\u4e3a\u76ee\u7684\u7684\u3001\u79ef\u6781\u4f53\u9a8c\u7684\u534f\u8c03\u8fde\u63a5\u4f53\u9a8c\u3002", "motivation": "\u4e3b\u6d41\u5316\u7684\u966a\u4f34\u673a\u5668\u9762\u4e34\u7740\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\u7684\u6df7\u4e71\uff0c\u7279\u522b\u662f\u673a\u5668\u966a\u4f34\u7684\u6982\u5ff5\u5316\u548c\u6d4b\u91cf\u4e0d\u4e00\u81f4\u6216\u7f3a\u5931\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u751f\u6210\u548c\u4e13\u5bb6\u8bc4\u5ba1\u9879\u76ee\u6c60\uff0c\u8ba9467\u540d\u4e0eAI\u4f34\u4fa3\u4e92\u52a8\u7684\u4eba\u56de\u7b54\u9879\u76ee\u6c60\u548c\u6784\u5ff5\u9a8c\u8bc1\u6d4b\u91cf\uff0c\u4f7f\u7528\u63a2\u7d22\u6027\u56e0\u5b50\u5206\u6790\u8bf1\u5bfc\u51fa\u4e24\u4e2a\u56e0\u5b50\u3002", "result": "\u8bf1\u5bfc\u51fa\u4e24\u4e2a\u56e0\u5b50\uff1a\u5e78\u798f\u4ea4\u6362\u548c\u8fde\u63a5\u534f\u8c03\uff0c\u6784\u5ff5\u9a8c\u8bc1\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u56e0\u5b50\u57fa\u672c\u6309\u9884\u671f\u8fd0\u4f5c\uff0c\u4e8b\u540e\u5206\u6790\u63ed\u793a\u4e86\u4e24\u79cd\u4e0d\u540c\u7684MC\u6a21\u677f\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u673a\u5668\u966a\u4f34\u7684\u6d4b\u91cf\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eAI\u4f34\u4fa3\u4e92\u52a8\u7684\u4e24\u79cd\u4e0d\u540c\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u8fd9\u79cd\u65b0\u5174\u5173\u7cfb\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2511.00270", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings.", "AI": {"tldr": "\u63d0\u51faPOSESTITCH-SLT\u9884\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u8a00\u6a21\u677f\u7684\u53e5\u5b50\u751f\u6210\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u624b\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u624b\u8bed\u7ffb\u8bd1\u9762\u4e34\u5927\u89c4\u6a21\u53e5\u5b50\u5bf9\u9f50\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bed\u8a00\u6a21\u677f\u7684\u53e5\u5b50\u751f\u6210\u6280\u672f\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u7b80\u5355\u7684transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u8bad\u7ec3\u4e2d\u8003\u8651\u6a21\u677f\u751f\u6210\u7684\u53e5\u5b50\u5bf9\u3002", "result": "\u5728How2Sign\u6570\u636e\u96c6\u4e0aBLEU-4\u5206\u6570\u4ece1.97\u63d0\u5347\u52304.56\uff0c\u5728iSign\u6570\u636e\u96c6\u4e0a\u4ece0.55\u63d0\u5347\u52303.43\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e\u59ff\u6001\u7684\u65e0\u8bcd\u6c47\u8868\u7ffb\u8bd1\u7684\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u677f\u9a71\u52a8\u7684\u5408\u6210\u76d1\u7763\u5728\u4f4e\u8d44\u6e90\u624b\u8bed\u8bbe\u7f6e\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00062", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00062", "abs": "https://arxiv.org/abs/2511.00062", "authors": ["NVIDIA", ":", "Arslan Ali", "Junjie Bai", "Maciej Bala", "Yogesh Balaji", "Aaron Blakeman", "Tiffany Cai", "Jiaxin Cao", "Tianshi Cao", "Elizabeth Cha", "Yu-Wei Chao", "Prithvijit Chattopadhyay", "Mike Chen", "Yongxin Chen", "Yu Chen", "Shuai Cheng", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Jiaojiao Fan", "Linxi Fan", "Liang Feng", "Francesco Ferroni", "Sanja Fidler", "Xiao Fu", "Ruiyuan Gao", "Yunhao Ge", "Jinwei Gu", "Aryaman Gupta", "Siddharth Gururani", "Imad El Hanafi", "Ali Hassani", "Zekun Hao", "Jacob Huffman", "Joel Jang", "Pooya Jannaty", "Jan Kautz", "Grace Lam", "Xuan Li", "Zhaoshuo Li", "Maosheng Liao", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yen-Chen Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Seungjun Nah", "Yashraj Narang", "Abhijeet Panaskar", "Lindsey Pavao", "Trung Pham", "Morteza Ramezanali", "Fitsum Reda", "Scott Reed", "Xuanchi Ren", "Haonan Shao", "Yue Shen", "Stella Shi", "Shuran Song", "Bartosz Stefaniak", "Shangkun Sun", "Shitao Tang", "Sameena Tasmeen", "Lyne Tchapmi", "Wei-Cheng Tseng", "Jibin Varghese", "Andrew Z. Wang", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Jiashu Xu", "Dinghao Yang", "Xiaodong Yang", "Haotian Ye", "Seonghyeon Ye", "Xiaohui Zeng", "Jing Zhang", "Qinsheng Zhang", "Kaiwen Zheng", "Andrew Zhu", "Yuke Zhu"], "title": "World Simulation with Video Foundation Models for Physical AI", "comment": null, "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5$\\times$ smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.", "AI": {"tldr": "Cosmos-Predict2.5\u662fCosmos\u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u6700\u65b0\u7248\u672c\uff0c\u57fa\u4e8e\u6d41\u5f0f\u67b6\u6784\u7edf\u4e00\u4e86\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u5230\u4e16\u754c\u7684\u751f\u6210\uff0c\u5e76\u901a\u8fc7Cosmos-Reason1\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u6587\u672c\u57fa\u7840\u548c\u66f4\u7cbe\u7ec6\u7684\u4e16\u754c\u6a21\u62df\u63a7\u5236\u3002\u8be5\u6a21\u578b\u5728200M\u7cbe\u9009\u89c6\u9891\u526a\u8f91\u4e0a\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4f18\u5316\uff0c\u5728\u89c6\u9891\u8d28\u91cf\u548c\u6307\u4ee4\u5bf9\u9f50\u65b9\u9762\u76f8\u6bd4\u524d\u4ee3\u6709\u663e\u8457\u63d0\u5347\uff0c\u63d0\u4f9b2B\u548c14B\u4e24\u79cd\u89c4\u6a21\u3002", "motivation": "\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u7b56\u7565\u8bc4\u4f30\u4ee5\u53ca\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u95ed\u73af\u4eff\u771f\u5de5\u5177\uff0c\u63a8\u52a8\u7269\u7406AI\u548c\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6d41\u5f0f\u7684\u67b6\u6784\uff0c\u7edf\u4e00\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u5230\u4e16\u754c\u7684\u751f\u6210\uff1b\u5229\u7528Cosmos-Reason1\u7269\u7406AI\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6587\u672c\u57fa\u7840\u548c\u63a7\u5236\uff1b\u5728200M\u7cbe\u9009\u89c6\u9891\u526a\u8f91\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u540e\u8bad\u7ec3\u4f18\u5316\u3002", "result": "\u76f8\u6bd4Cosmos-Predict1\uff0c\u5728\u89c6\u9891\u8d28\u91cf\u548c\u6307\u4ee4\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1bCosmos-Transfer2.5\u867d\u7136\u6bd4\u524d\u4ee3\u5c0f3.5\u500d\uff0c\u4f46\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u7684\u957f\u65f6\u7a0b\u89c6\u9891\u751f\u6210\u80fd\u529b\u3002", "conclusion": "Cosmos-Predict2.5\u548cCosmos-Transfer2.5\u6210\u4e3a\u6269\u5c55\u5177\u8eab\u667a\u80fd\u7684\u591a\u529f\u80fd\u5de5\u5177\uff0c\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u3001\u9884\u8bad\u7ec3\u68c0\u67e5\u70b9\u548c\u7cbe\u9009\u57fa\u51c6\u6d4b\u8bd5\uff0c\u964d\u4f4e\u91c7\u7528\u95e8\u69db\u5e76\u4fc3\u8fdb\u7269\u7406AI\u9886\u57df\u7684\u521b\u65b0\u3002"}}
{"id": "2511.00206", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5e2e\u52a9\u89e3\u51b3\u8ba4\u77e5\u79d1\u5b66\u9762\u4e34\u7684\u8de8\u5b66\u79d1\u6574\u5408\u3001\u7406\u8bba\u5f62\u5f0f\u5316\u7b49\u6311\u6218\uff0c\u6307\u51faLLMs\u53ef\u4ee5\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u4fc3\u8fdb\u66f4\u6574\u5408\u548c\u7d2f\u79ef\u7684\u8ba4\u77e5\u79d1\u5b66\u53d1\u5c55\u3002", "motivation": "\u8ba4\u77e5\u79d1\u5b66\u56e0\u5176\u591a\u9762\u6027\u548c\u8de8\u5b66\u79d1\u6027\u8d28\uff0c\u5728\u77e5\u8bc6\u6574\u5408\u548c\u6982\u5ff5\u6e05\u6670\u5ea6\u65b9\u9762\u9762\u4e34\u6301\u7eed\u6311\u6218\u3002\u4eba\u5de5\u667a\u80fd\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u5206\u6790\uff0c\u8003\u5bdfLLMs\u5728\u5efa\u7acb\u8de8\u5b66\u79d1\u8054\u7cfb\u3001\u5f62\u5f0f\u5316\u7406\u8bba\u3001\u53d1\u5c55\u6e05\u6670\u6d4b\u91cf\u5206\u7c7b\u3001\u901a\u8fc7\u96c6\u6210\u5efa\u6a21\u6846\u67b6\u5b9e\u73b0\u6cdb\u5316\u3001\u6355\u6349\u60c5\u5883\u548c\u4e2a\u4f53\u5dee\u5f02\u7b49\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "LLMs\u5728\u5f53\u524d\u8fd9\u4e9b\u9886\u57df\u5177\u6709\u7279\u5b9a\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u5305\u62ec\u6f5c\u5728\u7684\u7f3a\u9677\u3002\u5f53\u5ba1\u614e\u4f7f\u7528\u65f6\uff0c\u5b83\u4eec\u53ef\u4ee5\u8865\u5145\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "LLMs\u53ef\u4ee5\u4f5c\u4e3a\u5de5\u5177\uff0c\u5728\u8c28\u614e\u4f7f\u7528\u7684\u524d\u63d0\u4e0b\uff0c\u4fc3\u8fdb\u8ba4\u77e5\u79d1\u5b66\u5411\u66f4\u6574\u5408\u548c\u7d2f\u79ef\u7684\u65b9\u5411\u53d1\u5c55\uff0c\u4f46\u9700\u8981\u4e0e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u76f8\u7ed3\u5408\u3002"}}
{"id": "2511.00392", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00392", "abs": "https://arxiv.org/abs/2511.00392", "authors": ["Lingpeng Chen", "Jiakun Tang", "Apple Pui-Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping", "comment": "8 pages, 9 figures, conference", "summary": "Accurate 3D reconstruction in visually-degraded underwater environments\nremains a formidable challenge. Single-modality approaches are insufficient:\nvision-based methods fail due to poor visibility and geometric constraints,\nwhile sonar is crippled by inherent elevation ambiguity and low resolution.\nConsequently, prior fusion technique relies on heuristics and flawed geometric\nassumptions, leading to significant artifacts and an inability to model complex\nscenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep\nlearning framework that overcomes these limitations by adapting the principled\nplane sweep algorithm for cross-modal fusion between sonar and visual data.\nExtensive experiments in both high-fidelity simulation and real-world\nenvironments demonstrate that SonarSweep consistently generates dense and\naccurate depth maps, significantly outperforming state-of-the-art methods\nacross challenging conditions, particularly in high turbidity. To foster\nfurther research, we will publicly release our code and a novel dataset\nfeaturing synchronized stereo-camera and sonar data, the first of its kind.", "AI": {"tldr": "SonarSweep\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u5e73\u9762\u626b\u63cf\u7b97\u6cd5\u5b9e\u73b0\u58f0\u7eb3\u548c\u89c6\u89c9\u6570\u636e\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u5728\u89c6\u89c9\u9000\u5316\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u751f\u6210\u5bc6\u96c6\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u9000\u5316\u6c34\u4e0b\u73af\u5883\u4e2d3D\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5355\u6a21\u6001\u65b9\u6cd5\uff08\u89c6\u89c9\u65b9\u6cd5\u56e0\u80fd\u89c1\u5ea6\u5dee\u800c\u5931\u6548\uff0c\u58f0\u7eb3\u65b9\u6cd5\u56e0\u9ad8\u7a0b\u6a21\u7cca\u548c\u4f4e\u5206\u8fa8\u7387\u800c\u53d7\u9650\uff09\u4e0d\u8db3\uff0c\u73b0\u6709\u878d\u5408\u6280\u672f\u4f9d\u8d56\u542f\u53d1\u5f0f\u548c\u6709\u7f3a\u9677\u7684\u51e0\u4f55\u5047\u8bbe\uff0c\u5bfc\u81f4\u663e\u8457\u4f2a\u5f71\u548c\u65e0\u6cd5\u5efa\u6a21\u590d\u6742\u573a\u666f\u3002", "method": "\u63d0\u51faSonarSweep\u6846\u67b6\uff0c\u57fa\u4e8e\u5e73\u9762\u626b\u63cf\u7b97\u6cd5\u539f\u7406\uff0c\u5b9e\u73b0\u58f0\u7eb3\u548c\u89c6\u89c9\u6570\u636e\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002", "result": "\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSonarSweep\u80fd\u6301\u7eed\u751f\u6210\u5bc6\u96c6\u51c6\u786e\u7684\u6df1\u5ea6\u56fe\uff0c\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\uff08\u7279\u522b\u662f\u9ad8\u6d4a\u5ea6\u73af\u5883\uff09\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SonarSweep\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5c06\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u9996\u4e2a\u5305\u542b\u540c\u6b65\u7acb\u4f53\u76f8\u673a\u548c\u58f0\u7eb3\u6570\u636e\u7684\u65b0\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.00709", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00709", "abs": "https://arxiv.org/abs/2511.00709", "authors": ["Veronica Bossio Botero", "Vijay Yadav", "Jacob Ouyang", "Anzar Abbas", "Michelle Worthington"], "title": "A Voice-Enabled Virtual Patient System for Interactive Training in Standardized Clinical Assessment", "comment": null, "summary": "Training mental health clinicians to conduct standardized clinical\nassessments is challenging due to a lack of scalable, realistic practice\nopportunities, which can impact data quality in clinical trials. To address\nthis gap, we introduce a voice-enabled virtual patient simulation system\npowered by a large language model (LLM). This study describes the system's\ndevelopment and validates its ability to generate virtual patients who\naccurately adhere to pre-defined clinical profiles, maintain coherent\nnarratives, and produce realistic dialogue. We implemented a system using a LLM\nto simulate patients with specified symptom profiles, demographics, and\ncommunication styles. The system was evaluated by 5 experienced clinical raters\nwho conducted 20 simulated structured MADRS interviews across 4 virtual patient\npersonas. The virtual patients demonstrated strong adherence to their clinical\nprofiles, with a mean item difference between rater-assigned MADRS scores and\nconfigured scores of 0.52 (SD=0.75). Inter-rater reliability across items was\n0.90 (95% CI=0.68-0.99). Expert raters consistently rated the qualitative\nrealism and cohesiveness of the virtual patients favorably, giving average\nratings between \"Agree\" and \"Strongly Agree.\" Our findings suggest that\nLLM-powered virtual patient simulations are a viable and scalable tool for\ntraining clinicians, capable of producing high-fidelity, clinically relevant\npractice scenarios.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u865a\u62df\u60a3\u8005\u6a21\u62df\u7cfb\u7edf\uff0c\u7528\u4e8e\u57f9\u8bad\u5fc3\u7406\u5065\u5eb7\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u6807\u51c6\u5316\u4e34\u5e8a\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u7f3a\u4e4f\u53ef\u6269\u5c55\u5b9e\u8df5\u673a\u4f1a\u7684\u95ee\u9898\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u4e34\u5e8a\u533b\u751f\u57f9\u8bad\u9762\u4e34\u7f3a\u4e4f\u53ef\u6269\u5c55\u3001\u73b0\u5b9e\u7684\u5b9e\u8df5\u673a\u4f1a\u7684\u6311\u6218\uff0c\u8fd9\u5f71\u54cd\u4e86\u4e34\u5e8a\u8bd5\u9a8c\u7684\u6570\u636e\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u865a\u62df\u60a3\u8005\u6a21\u62df\u7cfb\u7edf\uff0c\u6a21\u62df\u5177\u6709\u7279\u5b9a\u75c7\u72b6\u7279\u5f81\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u548c\u6c9f\u901a\u98ce\u683c\u7684\u60a3\u8005\u3002\u75315\u540d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u4e34\u5e8a\u8bc4\u4f30\u5458\u5bf94\u4e2a\u865a\u62df\u60a3\u8005\u89d2\u8272\u8fdb\u884c20\u6b21\u6a21\u62dfMADRS\u8bbf\u8c08\u8bc4\u4f30\u3002", "result": "\u865a\u62df\u60a3\u8005\u8868\u73b0\u51fa\u5bf9\u4e34\u5e8a\u7279\u5f81\u7684\u5f3a\u4f9d\u4ece\u6027\uff0c\u8bc4\u5206\u8005\u5206\u914d\u7684MADRS\u5206\u6570\u4e0e\u914d\u7f6e\u5206\u6570\u4e4b\u95f4\u7684\u5e73\u5747\u9879\u76ee\u5dee\u5f02\u4e3a0.52\u3002\u9879\u76ee\u95f4\u8bc4\u5206\u8005\u53ef\u9760\u6027\u4e3a0.90\u3002\u4e13\u5bb6\u8bc4\u4f30\u5458\u5bf9\u865a\u62df\u60a3\u8005\u7684\u5b9a\u6027\u771f\u5b9e\u6027\u548c\u8fde\u8d2f\u6027\u7ed9\u4e88\u79ef\u6781\u8bc4\u4ef7\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u865a\u62df\u60a3\u8005\u6a21\u62df\u662f\u57f9\u8bad\u4e34\u5e8a\u533b\u751f\u7684\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u4fdd\u771f\u5ea6\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u5b9e\u8df5\u573a\u666f\u3002"}}
{"id": "2511.00315", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures.", "AI": {"tldr": "\u63d0\u51faFactorization Memory\uff0c\u4e00\u79cd\u9ad8\u6548\u7684RNN\u67b6\u6784\uff0c\u5728\u77ed\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0eTransformer\u76f8\u5f53\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u57fa\u4e8eMamba-2\u6784\u5efa\uff0c\u652f\u6301\u8bad\u7ec3\u65f6\u5e76\u884c\u8ba1\u7b97\uff0c\u63a8\u7406\u65f6\u4fdd\u6301\u6052\u5b9a\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u3002\u8fd8\u5f00\u53d1\u4e86\u7a00\u758f\u7248\u672c\uff0c\u4ec5\u66f4\u65b0\u90e8\u5206\u5faa\u73af\u72b6\u6001\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e2\u80fd\u8fbe\u5230Transformer\u6027\u80fd\u6c34\u5e73\uff0c\u53c8\u80fd\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u66f4\u597d\u6cdb\u5316\u7684\u9ad8\u6548RNN\u67b6\u6784\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8eMamba-2\u6784\u5efaFactorization Memory\uff0c\u652f\u6301\u8bad\u7ec3\u5e76\u884c\u5316\u548c\u63a8\u7406\u6052\u5b9a\u590d\u6742\u5ea6\u3002\u5f00\u53d1\u7a00\u758f\u7248\u672c\uff0c\u4ec5\u66f4\u65b0\u90e8\u5206\u5faa\u73af\u72b6\u6001\u4ee5\u4f18\u5316\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u77ed\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0eTransformer\u76f8\u5f53\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7a00\u758f\u7248\u672c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "Factorization Memory\u662f\u9996\u4e2a\u6210\u529f\u5c06\u7a00\u758f\u5185\u5b58\u6fc0\u6d3b\u4e0e\u5728\u77ed\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\u7ade\u4e89\u6027\u80fd\u76f8\u7ed3\u5408\u7684RNN\u67b6\u6784\uff0c\u4e3a\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.00073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00073", "abs": "https://arxiv.org/abs/2511.00073", "authors": ["Harald Kristen", "Daniel Kulmer", "Manuela Hirschmugl"], "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures", "comment": null, "summary": "Rapid climate change and other disturbances in alpine ecosystems demand\nfrequent habitat monitoring, yet manual mapping remains prohibitively expensive\nfor the required temporal resolution. We employ deep learning for change\ndetection using long-term alpine habitat data from Gesaeuse National Park,\nAustria, addressing a major gap in applying geospatial foundation models (GFMs)\nto complex natural environments with fuzzy class boundaries and highly\nimbalanced classes. We compare two paradigms: post-classification change\ndetection (CD) versus direct CD. For post-classification CD, we evaluate GFMs\nPrithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the\ntransformer ChangeViT against U-Net baselines. Using high-resolution multimodal\ndata (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes\nover 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus\nU-Net's 41% for multi-class habitat change, while both reach 67% for binary\nchange detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but\nonly 28% accuracy for multi-class detection. Cross-temporal evaluation reveals\nGFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's\n23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.\nAlthough overall accuracies are lower than in more homogeneous landscapes, they\nreflect realistic performance for complex alpine habitats. Future work will\nintegrate object-based post-processing and physical constraints to enhance\napplicability.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u9ad8\u5c71\u6816\u606f\u5730\u53d8\u5316\u68c0\u6d4b\uff0c\u6bd4\u8f83\u4e86\u540e\u5206\u7c7b\u53d8\u5316\u68c0\u6d4b\u548c\u76f4\u63a5\u53d8\u5316\u68c0\u6d4b\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0Clay v1.0\u6a21\u578b\u5728\u590d\u6742\u9ad8\u5c71\u73af\u5883\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523051%\uff0cLiDAR\u6570\u636e\u96c6\u6210\u53ef\u5c06\u51c6\u786e\u7387\u4ece30%\u63d0\u5347\u81f350%\u3002", "motivation": "\u9ad8\u5c71\u751f\u6001\u7cfb\u7edf\u9762\u4e34\u5feb\u901f\u6c14\u5019\u53d8\u5316\u548c\u5176\u4ed6\u5e72\u6270\uff0c\u9700\u8981\u9891\u7e41\u7684\u6816\u606f\u5730\u76d1\u6d4b\uff0c\u4f46\u4eba\u5de5\u6d4b\u7ed8\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u81ea\u7136\u73af\u5883\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u7c7b\u522b\u8fb9\u754c\u6a21\u7cca\u548c\u7c7b\u522b\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u5728\u5965\u5730\u5229Gesaeuse\u56fd\u5bb6\u516c\u56ed\u4f7f\u7528\u957f\u671f\u9ad8\u5c71\u6816\u606f\u5730\u6570\u636e\uff0c\u6bd4\u8f83\u4e24\u79cd\u53d8\u5316\u68c0\u6d4b\u8303\u5f0f\uff1a\u540e\u5206\u7c7b\u53d8\u5316\u68c0\u6d4b\uff08\u8bc4\u4f30Prithvi-EO-2.0\u3001Clay v1.0\u548cU-Net CNN\uff09\u548c\u76f4\u63a5\u53d8\u5316\u68c0\u6d4b\uff08\u6d4b\u8bd5ChangeViT\u548cU-Net\u57fa\u7ebf\uff09\u3002\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u6570\u636e\uff08RGB\u3001NIR\u3001LiDAR\u3001\u5730\u5f62\u5c5e\u6027\uff09\uff0c\u8986\u76d615.3\u5e73\u65b9\u516c\u91cc\u5185\u76844,480\u4e2a\u8bb0\u5f55\u53d8\u5316\u3002", "result": "Clay v1.0\u5728\u591a\u7c7b\u6816\u606f\u5730\u53d8\u5316\u68c0\u6d4b\u4e2d\u8fbe\u523051%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8eU-Net\u768441%\uff1b\u5728\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u4e2d\u4e24\u8005\u5747\u8fbe\u523067%\u3002\u76f4\u63a5\u53d8\u5316\u68c0\u6d4b\u5728\u4e8c\u5143\u68c0\u6d4b\u4e2dIoU\u66f4\u9ad8\uff080.53 vs 0.35\uff09\uff0c\u4f46\u5728\u591a\u7c7b\u68c0\u6d4b\u4e2d\u51c6\u786e\u7387\u4ec5\u4e3a28%\u3002\u8de8\u65f6\u95f4\u8bc4\u4f30\u663e\u793aGFM\u66f4\u5177\u9c81\u68d2\u6027\uff0cClay\u57282020\u5e74\u6570\u636e\u4e0a\u4fdd\u630133%\u51c6\u786e\u7387\uff0c\u800cU-Net\u4e3a23%\u3002\u96c6\u6210LiDAR\u53ef\u5c06\u8bed\u4e49\u5206\u5272\u51c6\u786e\u7387\u4ece30%\u63d0\u5347\u81f350%\u3002", "conclusion": "\u867d\u7136\u603b\u4f53\u51c6\u786e\u7387\u4f4e\u4e8e\u66f4\u5747\u8d28\u666f\u89c2\uff0c\u4f46\u53cd\u6620\u4e86\u590d\u6742\u9ad8\u5c71\u6816\u606f\u5730\u7684\u771f\u5b9e\u6027\u80fd\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u6210\u57fa\u4e8e\u5bf9\u8c61\u7684\u540e\u5904\u7406\u548c\u7269\u7406\u7ea6\u675f\u4ee5\u589e\u5f3a\u9002\u7528\u6027\u3002"}}
{"id": "2511.00267", "categories": ["cs.AI", "cs.CY", "cs.GL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00267", "abs": "https://arxiv.org/abs/2511.00267", "authors": ["Christian Prothmann", "Vijay Gadepally", "Jeremy Kepner", "Koley Borchard", "Luca Carlone", "Zachary Folcik", "J. Daniel Grith", "Michael Houle", "Jonathan P. How", "Nathan Hughes", "Ifueko Igbinedion", "Hayden Jananthan", "Tejas Jayashankar", "Michael Jones", "Sertac Karaman", "Binoy G. Kurien", "Alejandro Lancho", "Giovanni Lavezzi", "Gary C. F. Lee", "Charles E. Leiserson", "Richard Linares", "Lindsey McEvoy", "Peter Michaleas", "Chasen Milner", "Alex Pentland", "Yury Polyanskiy", "Jovan Popovich", "Jeffrey Price", "Tim W. Reid", "Stephanie Riley", "Siddharth Samsi", "Peter Saunders", "Olga Simek", "Mark S. Veillette", "Amir Weiss", "Gregory W. Wornell", "Daniela Rus", "Scott T. Ruppel"], "title": "Advancing AI Challenges for the United States Department of the Air Force", "comment": "8 pages, 8 figures, 59 references. To appear in IEEE HPEC 2025", "summary": "The DAF-MIT AI Accelerator is a collaboration between the United States\nDepartment of the Air Force (DAF) and the Massachusetts Institute of Technology\n(MIT). This program pioneers fundamental advances in artificial intelligence\n(AI) to expand the competitive advantage of the United States in the defense\nand civilian sectors. In recent years, AI Accelerator projects have developed\nand launched public challenge problems aimed at advancing AI research in\npriority areas. Hallmarks of AI Accelerator challenges include large, publicly\navailable, and AI-ready datasets to stimulate open-source solutions and engage\nthe wider academic and private sector AI ecosystem. This article supplements\nour previous publication, which introduced AI Accelerator challenges. We\nprovide an update on how ongoing and new challenges have successfully\ncontributed to AI research and applications of AI technologies.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DAF-MIT AI\u52a0\u901f\u5668\u9879\u76ee\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u8be5\u9879\u76ee\u901a\u8fc7\u516c\u5f00\u6311\u6218\u95ee\u9898\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u63d0\u4f9b\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u53d1\u5c55\u3002", "motivation": "\u901a\u8fc7\u516c\u5f00\u6311\u6218\u95ee\u9898\u523a\u6fc0\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u6269\u5927\u7f8e\u56fd\u5728\u56fd\u9632\u548c\u6c11\u7528\u9886\u57df\u7684\u7ade\u4e89\u4f18\u52bf\uff0c\u5e76\u5438\u5f15\u66f4\u5e7f\u6cdb\u7684\u5b66\u672f\u548c\u79c1\u8425\u90e8\u95e8\u53c2\u4e0e\u3002", "method": "\u5f00\u53d1\u548c\u53d1\u5e03\u516c\u5f00\u6311\u6218\u95ee\u9898\uff0c\u63d0\u4f9b\u5927\u578b\u3001\u516c\u5f00\u53ef\u7528\u7684AI\u5c31\u7eea\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u3002", "result": "\u6301\u7eed\u548c\u65b0\u6311\u6218\u5df2\u6210\u529f\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u548c\u6280\u672f\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u6d3b\u8dc3\u7684AI\u751f\u6001\u7cfb\u7edf\u3002", "conclusion": "DAF-MIT AI\u52a0\u901f\u5668\u901a\u8fc7\u516c\u5f00\u6311\u6218\u95ee\u9898\u6709\u6548\u63a8\u52a8\u4e86\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\uff0c\u4e3a\u7f8e\u56fd\u5728AI\u9886\u57df\u7684\u7ade\u4e89\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2511.00412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00412", "abs": "https://arxiv.org/abs/2511.00412", "authors": ["John A. Christian", "Michael R. Walker II", "Wyatt Bridgman", "Michael J. Sparapany"], "title": "Runge-Kutta Approximations for Direct Coning Compensation Applying Lie Theory", "comment": null, "summary": "The integration of gyroscope measurements is an essential task for most\nnavigation systems. Modern vehicles typically use strapdown systems, such that\ngyro integration requires coning compensation to account for the sensor's\nrotation during the integration. Many coning compensation algorithms have been\ndeveloped and a few are reviewed. This work introduces a new class of coning\ncorrection algorithm built directly from the classical Runge-Kutta integration\nroutines. A simple case is shown to collapse to one of the most popular coning\nalgorithms and a clear procedure for generating higher-order algorithms is\npresented.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u5178\u9f99\u683c-\u5e93\u5854\u79ef\u5206\u65b9\u6cd5\u7684\u65b0\u578b\u5706\u9525\u8865\u507f\u7b97\u6cd5\uff0c\u7528\u4e8e\u9640\u87ba\u4eea\u6d4b\u91cf\u79ef\u5206\u4e2d\u7684\u65cb\u8f6c\u8865\u507f\u3002", "motivation": "\u73b0\u4ee3\u5bfc\u822a\u7cfb\u7edf\u4e2d\uff0c\u9640\u87ba\u4eea\u79ef\u5206\u9700\u8981\u5706\u9525\u8865\u507f\u6765\u8003\u8651\u4f20\u611f\u5668\u5728\u79ef\u5206\u671f\u95f4\u7684\u65cb\u8f6c\u3002\u73b0\u6709\u7b97\u6cd5\u867d\u591a\uff0c\u4f46\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u9636\u8865\u507f\u7b97\u6cd5\u3002", "method": "\u76f4\u63a5\u4ece\u7ecf\u5178\u9f99\u683c-\u5e93\u5854\u79ef\u5206\u65b9\u6cd5\u6784\u5efa\u5706\u9525\u6821\u6b63\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u751f\u6210\u9ad8\u9636\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u7b80\u5355\u60c5\u51b5\u4e0b\u53ef\u9000\u5316\u4e3a\u6700\u6d41\u884c\u7684\u5706\u9525\u7b97\u6cd5\u4e4b\u4e00\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u57fa\u4e8e\u9f99\u683c-\u5e93\u5854\u65b9\u6cd5\u7684\u65b0\u578b\u5706\u9525\u8865\u507f\u7b97\u6cd5\u7c7b\u522b\uff0c\u63d0\u4f9b\u4e86\u751f\u6210\u9ad8\u9636\u7b97\u6cd5\u7684\u6e05\u6670\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5706\u9525\u8865\u507f\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9640\u87ba\u4eea\u79ef\u5206\u4e2d\u7684\u65cb\u8f6c\u6548\u5e94\uff0c\u4e14\u4e0e\u73b0\u6709\u6d41\u884c\u7b97\u6cd5\u517c\u5bb9\u3002"}}
{"id": "2511.00730", "categories": ["cs.HC", "I.2"], "pdf": "https://arxiv.org/pdf/2511.00730", "abs": "https://arxiv.org/abs/2511.00730", "authors": ["Mahya Qorbani", "Kamran Paynabar", "Mohsen Moghaddam"], "title": "Teaching LLMs to See and Guide: Context-Aware Real-Time Assistance in Augmented Reality", "comment": "This work is intended for submission to the IEEE Transactions on\n  Systems, Man, and Cybernetics: Systems for possible publication", "summary": "The growing adoption of augmented and virtual reality (AR and VR)\ntechnologies in industrial training and on-the-job assistance has created new\nopportunities for intelligent, context-aware support systems. As workers\nperform complex tasks guided by AR and VR, these devices capture rich streams\nof multimodal data, including gaze, hand actions, and task progression, that\ncan reveal user intent and task state in real time. Leveraging this information\neffectively remains a major challenge. In this work, we present a context-aware\nlarge language model (LLM) assistant that integrates diverse data modalities,\nsuch as hand actions, task steps, and dialogue history, into a unified\nframework for real-time question answering. To systematically study how context\ninfluences performance, we introduce an incremental prompting framework, where\neach model version receives progressively richer contextual inputs. Using the\nHoloAssist dataset, which records AR-guided task executions, we evaluate how\neach modality contributes to the assistant's effectiveness. Our experiments\nshow that incorporating multimodal context significantly improves the accuracy\nand relevance of responses. These findings highlight the potential of\nLLM-driven multimodal integration to enable adaptive, intuitive assistance for\nAR and VR-based industrial training and assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a9\u624b\uff0c\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u5de5\u4e1a\u57f9\u8bad\u4e2d\u7684\u5b9e\u65f6\u95ee\u7b54\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u63d0\u793a\u6846\u67b6\u8bc4\u4f30\u4e0d\u540c\u6a21\u6001\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AR/VR\u6280\u672f\u5728\u5de5\u4e1a\u57f9\u8bad\u548c\u73b0\u573a\u8f85\u52a9\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8fd9\u4e9b\u8bbe\u5907\u6355\u83b7\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u6ce8\u89c6\u3001\u624b\u90e8\u52a8\u4f5c\u3001\u4efb\u52a1\u8fdb\u5ea6\uff09\u4e3a\u667a\u80fd\u4e0a\u4e0b\u6587\u611f\u77e5\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a9\u624b\uff0c\u6574\u5408\u624b\u90e8\u52a8\u4f5c\u3001\u4efb\u52a1\u6b65\u9aa4\u548c\u5bf9\u8bdd\u5386\u53f2\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u5f15\u5165\u6e10\u8fdb\u5f0f\u63d0\u793a\u6846\u67b6\uff0c\u9010\u6b65\u589e\u52a0\u4e0a\u4e0b\u6587\u8f93\u5165\u6765\u7cfb\u7edf\u7814\u7a76\u4e0a\u4e0b\u6587\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u4f7f\u7528HoloAssist\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6574\u5408\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u663e\u8457\u63d0\u9ad8\u4e86\u54cd\u5e94\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u6574\u5408\u5177\u6709\u4e3aAR/VR\u5de5\u4e1a\u57f9\u8bad\u548c\u8f85\u52a9\u63d0\u4f9b\u81ea\u9002\u5e94\u3001\u76f4\u89c2\u652f\u6301\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.00341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity.", "AI": {"tldr": "\u8bba\u6587\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u7684\u7ed3\u6784\u7279\u6027\uff1a\u53cd\u8f6c\u4e0d\u53d8\u6027\uff0c\u5373\u6807\u51c6CLM\u9884\u8bad\u7ec3\u5bf9\u6587\u672c\u53ca\u5176\u53cd\u8f6c\u7248\u672c\u5206\u914d\u76f8\u540c\u4f3c\u7136\u5ea6\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8bed\u8a00\u7684\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u76ee\u6807\u5b58\u5728\u65b9\u5411\u76f2\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4eba\u7c7b\u8bed\u8a00\u548c\u63a8\u7406\u4e2d\u56fa\u6709\u7684\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u8bed\u8a00\u4e2d\u65b9\u5411\u6027\u4f9d\u8d56\u5173\u7cfb\u7684\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5f62\u5f0f\u5316\u5206\u6790\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u7684\u53cd\u8f6c\u4e0d\u53d8\u6027\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4ece\u65f6\u95f4\u4e0d\u5bf9\u79f0\u6027\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u9884\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0\u6807\u51c6CLM\u9884\u8bad\u7ec3\u5bf9\u6b63\u5411\u548c\u53cd\u5411\u6587\u672c\u8d4b\u4e88\u76f8\u540c\u4f3c\u7136\u5ea6\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5728\u53cd\u5411\u6587\u672c\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u8fbe\u5230\u4e0e\u6b63\u5411\u6587\u672c\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u53cd\u8f6c\u4e0d\u53d8\u6027\u662f\u5f53\u524d\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u9650\u5236\u800c\u975e\u826f\u6027\u7279\u5f81\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5f00\u53d1\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u8bed\u8a00\u65b9\u5411\u6027\u7684\u635f\u5931\u51fd\u6570\u548c\u67b6\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2511.00090", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00090", "abs": "https://arxiv.org/abs/2511.00090", "authors": ["Huanlin Gao", "Ping Chen", "Fuyuan Shi", "Chao Tan", "Zhaoxiang Liu", "Fang Zhao", "Kai Wang", "Shiguo Lian"], "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation", "comment": "NeurIPS 2025", "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa", "AI": {"tldr": "LeMiCa\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6269\u6563\u89c6\u9891\u751f\u6210\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8bcd\u5178\u6700\u5c0f\u6700\u5927\u8def\u5f84\u4f18\u5316\u7b56\u7565\u9650\u5236\u6700\u574f\u8def\u5f84\u8bef\u5dee\uff0c\u5728\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u7684\u540c\u65f6\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7f13\u5b58\u7b56\u7565\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u5c40\u90e8\u542f\u53d1\u5f0f\u8bef\u5dee\uff0c\u4f46\u5ffd\u7565\u4e86\u5168\u5c40\u8bef\u5dee\u7d2f\u79ef\uff0c\u5bfc\u81f4\u52a0\u901f\u89c6\u9891\u4e0e\u539f\u59cb\u89c6\u9891\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5185\u5bb9\u9000\u5316\u3002", "method": "\u5c06\u7f13\u5b58\u8c03\u5ea6\u5efa\u6a21\u4e3a\u5e26\u8bef\u5dee\u6743\u91cd\u8fb9\u7684\u6709\u5411\u56fe\uff0c\u5f15\u5165\u8bcd\u5178\u6700\u5c0f\u6700\u5927\u8def\u5f84\u4f18\u5316\u7b56\u7565\u660e\u786e\u9650\u5236\u6700\u574f\u8def\u5f84\u8bef\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u5230\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLeMiCa\u5728\u63a8\u7406\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5b9e\u73b0\u53cc\u91cd\u63d0\u5347\uff1a\u5728Latte\u6a21\u578b\u4e0a\u8fbe\u52302.9\u500d\u52a0\u901f\uff0c\u5728Open-Sora\u4e0aLPIPS\u5f97\u5206\u4e3a0.05\uff0c\u4f18\u4e8e\u73b0\u6709\u7f13\u5b58\u6280\u672f\u3002", "conclusion": "LeMiCa\u4e3a\u52a0\u901f\u6269\u6563\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u8303\u5f0f\uff0c\u5728\u6700\u5c0f\u611f\u77e5\u8d28\u91cf\u635f\u5931\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u53ef\u4f5c\u4e3a\u672a\u6765\u9ad8\u6548\u53ef\u9760\u89c6\u9891\u5408\u6210\u7814\u7a76\u7684\u57fa\u7840\u3002"}}
{"id": "2511.00340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00340", "abs": "https://arxiv.org/abs/2511.00340", "authors": ["Manan Roy Choudhury", "Adithya Chandramouli", "Mannan Anand", "Vivek Gupta"], "title": "Better Call CLAUSE: A Discrepancy Benchmark for Auditing LLMs Legal Reasoning Capabilities", "comment": "41 pages, 4 images", "summary": "The rapid integration of large language models (LLMs) into high-stakes legal\nwork has exposed a critical gap: no benchmark exists to systematically\nstress-test their reliability against the nuanced, adversarial, and often\nsubtle flaws present in real-world contracts. To address this, we introduce\nCLAUSE, a first-of-its-kind benchmark designed to evaluate the fragility of an\nLLM's legal reasoning. We study the capabilities of LLMs to detect and reason\nabout fine-grained discrepancies by producing over 7500 real-world perturbed\ncontracts from foundational datasets like CUAD and ContractNLI. Our novel,\npersona-driven pipeline generates 10 distinct anomaly categories, which are\nthen validated against official statutes using a Retrieval-Augmented Generation\n(RAG) system to ensure legal fidelity. We use CLAUSE to evaluate leading LLMs'\nability to detect embedded legal flaws and explain their significance. Our\nanalysis shows a key weakness: these models often miss subtle errors and\nstruggle even more to justify them legally. Our work outlines a path to\nidentify and correct such reasoning failures in legal AI.", "AI": {"tldr": "CLAUSE\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30LLM\u6cd5\u5f8b\u63a8\u7406\u8106\u5f31\u6027\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u751f\u62107500+\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u5408\u540c\u6765\u6d4b\u8bd5\u6a21\u578b\u68c0\u6d4b\u7ec6\u5fae\u6cd5\u5f8b\u5dee\u5f02\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8bc6\u522b\u548c\u89e3\u91ca\u6cd5\u5f8b\u9519\u8bef\u65b9\u9762\u5b58\u5728\u660e\u663e\u5f31\u70b9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9ad8\u98ce\u9669\u5de5\u4f5c\u4e2d\u5feb\u901f\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u771f\u5b9e\u5408\u540c\u590d\u6742\u3001\u5bf9\u6297\u6027\u548c\u7ec6\u5fae\u7f3a\u9677\u65b9\u9762\u53ef\u9760\u6027\u7684\u57fa\u51c6\u3002", "method": "\u4eceCUAD\u548cContractNLI\u6570\u636e\u96c6\u751f\u62107500+\u6270\u52a8\u5408\u540c\uff0c\u4f7f\u7528\u89d2\u8272\u9a71\u52a8\u7ba1\u9053\u521b\u5efa10\u79cd\u5f02\u5e38\u7c7b\u522b\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u9a8c\u8bc1\u6cd5\u5f8b\u51c6\u786e\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\u4e3b\u8981LLMs\u5728\u68c0\u6d4b\u5d4c\u5165\u6cd5\u5f8b\u7f3a\u9677\u65b9\u9762\u5b58\u5728\u5173\u952e\u5f31\u70b9\uff0c\u7ecf\u5e38\u9057\u6f0f\u7ec6\u5fae\u9519\u8bef\u4e14\u5728\u6cd5\u5f8b\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bc6\u522b\u548c\u7ea0\u6b63\u6cd5\u5f8bAI\u4e2d\u7684\u63a8\u7406\u5931\u8d25\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdb\u6a21\u578b\u5728\u590d\u6742\u6cd5\u5f8b\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00492", "abs": "https://arxiv.org/abs/2511.00492", "authors": ["Simon Giel", "James Hurrell", "Shreya Santra", "Ashutosh Mishra", "Kentaro Uno", "Kazuya Yoshida"], "title": "Design and Development of a Modular Bucket Drum Excavator for Lunar ISRU", "comment": "6 pages, 4 figures. Accepted at IEEE iSpaRo 2025", "summary": "In-Situ Resource Utilization (ISRU) is one of the key technologies for\nenabling sustainable access to the Moon. The ability to excavate lunar regolith\nis the first step in making lunar resources accessible and usable. This work\npresents the development of a bucket drum for the modular robotic system\nMoonBot, as part of the Japanese Moonshot program. A 3D-printed prototype made\nof PLA was manufactured to evaluate its efficiency through a series of sandbox\ntests. The resulting tool weighs 4.8 kg and has a volume of 14.06 L. It is\ncapable of continuous excavation at a rate of 777.54 kg/h with a normalized\nenergy consumption of 0.022 Wh/kg. In batch operation, the excavation rate is\n172.02 kg/h with a normalized energy consumption of 0.86 Wh per kilogram of\nexcavated material. The obtained results demonstrate the successful\nimplementation of the concept. A key advantage of the developed tool is its\ncompatibility with the modular MoonBot robotic platform, which enables flexible\nand efficient mission planning. Further improvements may include the\nintegration of sensors and an autonomous control system to enhance the\nexcavation process.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u6708\u7403\u8d44\u6e90\u5229\u7528\u7684\u94f2\u6597\u6eda\u7b52\u5de5\u5177\uff0c\u4f5c\u4e3a\u65e5\u672cMoonshot\u8ba1\u5212\u4e2d\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edfMoonBot\u7684\u4e00\u90e8\u5206\uff0c\u901a\u8fc73D\u6253\u5370\u539f\u578b\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u6316\u6398\u6548\u7387\u3002", "motivation": "\u6708\u7403\u539f\u4f4d\u8d44\u6e90\u5229\u7528\u662f\u5b9e\u73b0\u53ef\u6301\u7eed\u6708\u7403\u63a2\u7d22\u7684\u5173\u952e\u6280\u672f\uff0c\u6316\u6398\u6708\u58e4\u662f\u83b7\u53d6\u6708\u7403\u8d44\u6e90\u7684\u7b2c\u4e00\u6b65\u3002", "method": "\u4f7f\u7528PLA\u6750\u65993D\u6253\u5370\u5236\u9020\u539f\u578b\u94f2\u6597\u6eda\u7b52\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6c99\u76d2\u6d4b\u8bd5\u8bc4\u4f30\u5176\u6548\u7387\u3002", "result": "\u5de5\u5177\u91cd4.8kg\uff0c\u5bb9\u79ef14.06L\uff0c\u8fde\u7eed\u6316\u6398\u901f\u7387\u4e3a777.54kg/h\uff0c\u5f52\u4e00\u5316\u80fd\u80170.022Wh/kg\uff1b\u6279\u91cf\u64cd\u4f5c\u65f6\u6316\u6398\u901f\u7387172.02kg/h\uff0c\u5f52\u4e00\u5316\u80fd\u80170.86Wh/kg\u3002", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u8be5\u5de5\u5177\u4e0e\u6a21\u5757\u5316MoonBot\u673a\u5668\u4eba\u5e73\u53f0\u517c\u5bb9\uff0c\u53ef\u5b9e\u73b0\u7075\u6d3b\u9ad8\u6548\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u672a\u6765\u53ef\u96c6\u6210\u4f20\u611f\u5668\u548c\u81ea\u4e3b\u63a7\u5236\u7cfb\u7edf\u4ee5\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002"}}
{"id": "2511.00343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation.", "AI": {"tldr": "LingGym\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u5143\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u4f7f\u7528\u8de8\u8bed\u8a00\u6ce8\u91ca\u6587\u672c\u548c\u8bed\u6cd5\u63cf\u8ff0\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u7ed3\u6784\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30LLMs\u662f\u5426\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u672a\u89c1\u8fc7\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u7ed3\u6784\u4e0a\u8fdb\u884c\u8bed\u8a00\u63a8\u7406\u7684\u6cdb\u5316\uff0c\u8d85\u8d8a\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u7684\u5c40\u9650\u3002", "method": "\u521b\u5efaWord-Gloss Inference\u4efb\u52a1\uff0c\u6a21\u578b\u5fc5\u987b\u4ece\u4e0a\u4e0b\u6587\u4e2d\u63a8\u65ad\u7f3a\u5931\u7684\u5355\u8bcd\u548c\u6ce8\u91ca\uff0c\u4f7f\u7528\u4e0d\u540c\u5c42\u6b21\u7684\u8bed\u8a00\u4fe1\u606f\uff08\u6ce8\u91ca\u3001\u8bed\u6cd5\u89e3\u91ca\u3001\u7ffb\u8bd1\uff09\u3002", "result": "\u6574\u5408\u7ed3\u6784\u5316\u8bed\u8a00\u7ebf\u7d22\u5728\u6240\u6709\u6a21\u578b\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u51f8\u663e\u4e86\u4f7f\u7528LLMs\u8fdb\u884c\u7c7b\u578b\u5b66\u8bed\u8a00\u5206\u6790\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6587\u6863\u5316\u7684\u524d\u666f\u548c\u5f53\u524d\u5c40\u9650\u6027\u3002"}}
{"id": "2511.00091", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00091", "abs": "https://arxiv.org/abs/2511.00091", "authors": ["Wenli Xiao", "Haotian Lin", "Andy Peng", "Haoru Xue", "Tairan He", "Yuqi Xie", "Fengyuan Hu", "Jimmy Wu", "Zhengyi Luo", "Linxi \"Jim\" Fan", "Guanya Shi", "Yuke Zhu"], "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL", "comment": "26 pages", "summary": "Supervised fine-tuning (SFT) has become the de facto post-training strategy\nfor large vision-language-action (VLA) models, but its reliance on costly human\ndemonstrations limits scalability and generalization. We propose Probe, Learn,\nDistill (PLD), a three-stage plug-and-play framework that improves VLAs through\nresidual reinforcement learning (RL) and distribution-aware data collection. In\nStage 1, we train lightweight residual actors to probe failure regions of the\nVLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns\ncollected trajectories with the generalist's deployment distribution while\ncapturing recovery behaviors. In Stage 3, we distill the curated trajectories\nback into the generalist with standard SFT. PLD achieves near-saturated 99%\ntask success on LIBERO, over 50% gains in SimplerEnv, and 100% success on\nreal-world Franka and YAM arm manipulation tasks. Ablations show that residual\nprobing and distribution-aware replay are key to collecting deployment-aligned\ndata that improves both seen and unseen tasks, offering a scalable path toward\nself-improving VLA models.", "AI": {"tldr": "PLD\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u5e03\u611f\u77e5\u6570\u636e\u6536\u96c6\u6765\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6f14\u793a\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u9971\u548c\u7684\u6210\u529f\u7387\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6f14\u793a\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u6267\u884c\u5668\u63a2\u6d4bVLA\u901a\u7528\u6a21\u578b\u7684\u5931\u8d25\u533a\u57df\uff1b2)\u4f7f\u7528\u6df7\u5408rollout\u65b9\u6848\u6536\u96c6\u4e0e\u901a\u7528\u6a21\u578b\u90e8\u7f72\u5206\u5e03\u5bf9\u9f50\u7684\u8f68\u8ff9\uff1b3)\u901a\u8fc7\u6807\u51c6SFT\u5c06\u7b5b\u9009\u7684\u8f68\u8ff9\u84b8\u998f\u56de\u901a\u7528\u6a21\u578b\u3002", "result": "\u5728LIBERO\u4e0a\u8fbe\u5230\u63a5\u8fd1\u9971\u548c\u768499%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5728SimplerEnv\u4e0a\u63d0\u5347\u8d85\u8fc750%\uff0c\u5728\u771f\u5b9e\u4e16\u754cFranka\u548cYAM\u673a\u68b0\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0100%\u6210\u529f\u7387\u3002", "conclusion": "\u6b8b\u5dee\u63a2\u6d4b\u548c\u5206\u5e03\u611f\u77e5\u56de\u653e\u662f\u6536\u96c6\u90e8\u7f72\u5bf9\u9f50\u6570\u636e\u7684\u5173\u952e\uff0c\u4e3a\u81ea\u6539\u8fdbVLA\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2511.00379", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f26\u7406\u51b3\u7b56\u6a21\u578b\u7684LLM\u4f26\u7406\u63a8\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u4e94\u6b65\u7ed3\u6784\u5316\u6d41\u7a0b\u589e\u5f3aLLM\u5bf9\u4eba\u7c7b\u591a\u5143\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u533a\u57df\u4ef7\u503c\u89c2\u5bf9\u9f50\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u5bf9\u9f50\u65b9\u6cd5\u5f80\u5f80\u53ea\u4ea7\u751f\u8868\u9762\u4e00\u81f4\u6027\u800c\u975e\u771f\u6b63\u7684\u4f26\u7406\u7406\u89e3\uff0c\u65e0\u6cd5\u5904\u7406\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u590d\u6742\u6027\u548c\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u5730\u533a\u3001\u8de8\u6587\u5316\u7684\u591a\u5143\u4ef7\u503c\u89c2\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6210\u719f\u4f26\u7406\u51b3\u7b56\u6a21\u578b\u7684\u4e94\u6b65\u63a8\u7406\u6846\u67b6\uff1a\u60c5\u5883\u4e8b\u5b9e\u6536\u96c6\u3001\u5206\u5c42\u793e\u4f1a\u89c4\u8303\u8bc6\u522b\u3001\u9009\u9879\u751f\u6210\u3001\u591a\u89c6\u89d2\u4f26\u7406\u5f71\u54cd\u5206\u6790\u3001\u53cd\u601d\uff0c\u53ef\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u3002", "result": "\u5728\u4e13\u95e8\u8bbe\u8ba1\u7684\u533a\u57df\u4ef7\u503c\u89c2\u5bf9\u9f50\u57fa\u51c6SafeWorld\u4e0a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u4e0e\u591a\u5143\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u793e\u4f1a\u89c4\u8303\u8bc6\u522b\u548c\u66f4\u6587\u5316\u9002\u5b9c\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u901a\u8fc7\u8de8\u5b66\u79d1\u7814\u7a76\u5f00\u53d1\u66f4\u6709\u6548\u5bf9\u9f50\u5168\u7403\u793e\u4f1a\u591a\u5143\u4ef7\u503c\u89c2\u7684LLM\u63d0\u4f9b\u4e86\u5177\u4f53\u8def\u5f84\uff0c\u589e\u5f3a\u4e86LLM\u7684\u533a\u57df\u7279\u5f02\u6027\u548c\u7ec6\u81f4\u4f26\u7406\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2511.00512", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00512", "abs": "https://arxiv.org/abs/2511.00512", "authors": ["Suraj Kumar", "Andy Ruina"], "title": "Descriptive Model-based Learning and Control for Bipedal Locomotion", "comment": "8 pages, 15 figures", "summary": "Bipedal balance is challenging due to its multi-phase, hybrid nature and\nhigh-dimensional state space. Traditional balance control approaches for\nbipedal robots rely on low-dimensional models for locomotion planning and\nreactive control, constraining the full robot to behave like these simplified\nmodels. This involves tracking preset reference paths for the Center of Mass\nand upper body obtained through low-dimensional models, often resulting in\ninefficient walking patterns with bent knees. However, we observe that bipedal\nbalance is inherently low-dimensional and can be effectively described with\nsimple state and action descriptors in a low-dimensional state space. This\nallows the robot's motion to evolve freely in its high-dimensional state space,\nonly constraining its projection in the low-dimensional state space. In this\nwork, we propose a novel control approach that avoids prescribing a\nlow-dimensional model to the full model. Instead, our control framework uses a\ndescriptive model with the minimum degrees of freedom necessary to maintain\nbalance, allowing the remaining degrees of freedom to evolve freely in the\nhigh-dimensional space. This results in an efficient human-like walking gait\nand improved robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u8db3\u5e73\u8861\u63a7\u5236\u65b9\u6cd5\uff0c\u907f\u514d\u5c06\u4f4e\u7ef4\u6a21\u578b\u5f3a\u52a0\u4e8e\u5b8c\u6574\u6a21\u578b\uff0c\u800c\u662f\u4f7f\u7528\u63cf\u8ff0\u6027\u6a21\u578b\u6765\u7ef4\u6301\u5e73\u8861\uff0c\u8ba9\u5269\u4f59\u81ea\u7531\u5ea6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u4eba\u5f62\u884c\u8d70\u6b65\u6001\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8db3\u673a\u5668\u4eba\u5e73\u8861\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u7ef4\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u548c\u53cd\u5e94\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5b8c\u6574\u673a\u5668\u4eba\u7684\u884c\u4e3a\uff0c\u5bfc\u81f4\u884c\u8d70\u6a21\u5f0f\u6548\u7387\u4f4e\u4e0b\u4e14\u819d\u76d6\u5f2f\u66f2\u3002\u89c2\u5bdf\u5230\u53cc\u8db3\u5e73\u8861\u672c\u8d28\u4e0a\u662f\u4f4e\u7ef4\u7684\uff0c\u53ef\u4ee5\u7528\u7b80\u5355\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u63cf\u8ff0\u7b26\u5728\u4f4e\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u6709\u6548\u63cf\u8ff0\u3002", "method": "\u63d0\u51fa\u63a7\u5236\u6846\u67b6\u4f7f\u7528\u5177\u6709\u7ef4\u6301\u5e73\u8861\u6240\u9700\u6700\u5c0f\u81ea\u7531\u5ea6\u7684\u63cf\u8ff0\u6027\u6a21\u578b\uff0c\u5141\u8bb8\u5269\u4f59\u81ea\u7531\u5ea6\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u907f\u514d\u4e3a\u5b8c\u6574\u6a21\u578b\u89c4\u5b9a\u4f4e\u7ef4\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4eba\u5f62\u884c\u8d70\u6b65\u6001\u548c\u6539\u5584\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ec5\u7ea6\u675f\u4f4e\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u6295\u5f71\uff0c\u8ba9\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u5728\u5176\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u4e2d\u81ea\u7531\u6f14\u5316\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u81ea\u7136\u548c\u9ad8\u6548\u7684\u53cc\u8db3\u884c\u8d70\u6a21\u5f0f\u3002"}}
{"id": "2511.00371", "categories": ["cs.CL", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u82cf\u683c\u62c9\u5e95\u8c03\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u63a8\u7406\u8f68\u8ff9\u5e2e\u52a9\u5b66\u751f\u8bc6\u522b\u548c\u4fee\u6b63\u7f16\u7a0b\u8bef\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u524d\u6cbf\u6a21\u578b\u80fd\u751f\u621091%\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\u548c98.7%\u6709\u6548\u7684\u5bf9\u8bdd\u8f6e\u6b21\u3002", "motivation": "\u5927\u591a\u6570\u65b0\u624b\u7a0b\u5e8f\u5458\u7684\u9519\u8bef\u6e90\u4e8e\u7f16\u7a0b\u8bef\u89e3\uff0c\u4f20\u7edf\u8c03\u8bd5\u65b9\u6cd5\u76f4\u63a5\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\uff0c\u800c\u82cf\u683c\u62c9\u5e95\u8c03\u8bd5\u80fd\u5f15\u5bfc\u5b66\u751f\u81ea\u4e3b\u53d1\u73b0\u548c\u4fee\u6b63\u9519\u8bef\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u4efb\u52a1\uff0c\u6784\u5efa\u624b\u52a8\u6807\u6ce8\u7684\u8c03\u8bd5\u95ee\u9898\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u548c\u57fa\u4e8e\u8f68\u8ff9\u7684\u82cf\u683c\u62c9\u5e95\u5bf9\u8bdd\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5927\u89c4\u6a21LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u524d\u6cbf\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u8fbe91%\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\u548c98.7%\u6709\u6548\u7684\u5bf9\u8bdd\u8f6e\u6b21\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u751f\u6210\u82cf\u683c\u62c9\u5e95\u8c03\u8bd5\u6240\u9700\u7684\u63a8\u7406\u8f68\u8ff9\u548c\u5bf9\u8bdd\uff0c\u4e3a\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u81ea\u4e3b\u9519\u8bef\u4fee\u6b63\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2511.00095", "categories": ["cs.CV", "cs.AI", "92C55", "I.2.10"], "pdf": "https://arxiv.org/pdf/2511.00095", "abs": "https://arxiv.org/abs/2511.00095", "authors": ["Jiaming Liu", "Dingwei Fan", "Junyong Zhao", "Chunlin Li", "Haipeng Si", "Liang Sun"], "title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation", "comment": "2 Tables,5 Figures,16 Equations", "summary": "The anatomical structure segmentation of the spine and adjacent structures\nfrom computed tomography (CT) images is a key step for spinal disease diagnosis\nand treatment. However, the segmentation of CT images is impeded by low\ncontrast and complex vertebral boundaries. Although advanced models such as the\nSegment Anything Model (SAM) have shown promise in various segmentation tasks,\ntheir performance in spinal CT imaging is limited by high annotation\nrequirements and poor domain adaptability. To address these limitations, we\npropose SpinalSAM-R1, a multimodal vision-language interactive system that\nintegrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.\nSpecifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism\nto improve spine segmentation performance, and a semantics-driven interaction\nprotocol powered by DeepSeek-R1, enabling natural language-guided refinement.\nThe SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient\nadaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with\nCT images. Experimental results suggest that our method achieves superior\nsegmentation performance. Meanwhile, we develop a PyQt5-based interactive\nsoftware, which supports point, box, and text-based prompts. The system\nsupports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms\nresponse times. The software is released on\nhttps://github.com/6jm233333/spinalsam-r1.", "AI": {"tldr": "\u63d0\u51faSpinalSAM-R1\u7cfb\u7edf\uff0c\u7ed3\u5408\u5fae\u8c03SAM\u548cDeepSeek-R1\uff0c\u7528\u4e8e\u810a\u67f1CT\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u89e3\u5256\u5b66\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u810a\u67f1CT\u56fe\u50cf\u5206\u5272\u9762\u4e34\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u690e\u4f53\u8fb9\u754c\u590d\u6742\u7b49\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u5982SAM\u5728\u810a\u67f1CT\u9886\u57df\u8868\u73b0\u53d7\u9650\uff0c\u9700\u8981\u9ad8\u6807\u6ce8\u6210\u672c\u548c\u9886\u57df\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSpinalSAM-R1\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u4ea4\u4e92\u7cfb\u7edf\uff0c\u96c6\u6210\u5fae\u8c03SAM\u548cDeepSeek-R1\uff0c\u5f15\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u548c\u8bed\u4e49\u9a71\u52a8\u4ea4\u4e92\u534f\u8bae\uff0c\u4f7f\u7528LoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5728\u810a\u67f1\u89e3\u5256\u7ed3\u6784CT\u56fe\u50cf\u4e0a\u9a8c\u8bc1\uff0c\u83b7\u5f97\u4f18\u8d8a\u5206\u5272\u6027\u80fd\uff0c\u5f00\u53d1\u57fa\u4e8ePyQt5\u7684\u4ea4\u4e92\u8f6f\u4ef6\uff0c\u652f\u6301\u70b9\u3001\u6846\u548c\u6587\u672c\u63d0\u793a\uff0c\u5b9e\u73b094.3%\u89e3\u6790\u51c6\u786e\u7387\u548c\u4f4e\u4e8e800ms\u54cd\u5e94\u65f6\u95f4\u3002", "conclusion": "SpinalSAM-R1\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u810a\u67f1CT\u56fe\u50cf\u5206\u5272\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u4ea4\u4e92\u8f6f\u4ef6\u5de5\u5177\u3002"}}
{"id": "2511.00382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00382", "abs": "https://arxiv.org/abs/2511.00382", "authors": ["Mina Taraghi", "Yann Pequignot", "Amin Nikanjam", "Mohamed Amine Merzouk", "Foutse Khomh"], "title": "Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs", "comment": null, "summary": "Organizations are increasingly adopting and adapting Large Language Models\n(LLMs) hosted on public repositories such as HuggingFace. Although these\nadaptations often improve performance on specialized downstream tasks, recent\nevidence indicates that they can also degrade a model's safety or fairness.\nSince different fine-tuning techniques may exert distinct effects on these\ncritical dimensions, this study undertakes a systematic assessment of their\ntrade-offs. Four widely used Parameter-Efficient Fine-Tuning methods, LoRA,\nIA3, Prompt-Tuning, and P-Tuning, are applied to four instruction-tuned model\nfamilies (Meta-Llama-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B). In total, 235\nfine-tuned variants are evaluated across eleven safety hazard categories and\nnine demographic fairness dimensions. The results show that adapter-based\napproaches (LoRA, IA3) tend to improve safety scores and are the least\ndisruptive to fairness, retaining higher accuracy and lower bias scores. In\ncontrast, prompt-based methods (Prompt-Tuning and P-Tuning) generally reduce\nsafety and cause larger fairness regressions, with decreased accuracy and\nincreased bias. Alignment shifts are strongly moderated by base model type:\nLLaMA remains stable, Qwen records modest gains, Gemma experiences the steepest\nsafety decline, and Mistral, which is released without an internal moderation\nlayer, displays the greatest variance. Improvements in safety do not\nnecessarily translate into improvements in fairness, and no single\nconfiguration optimizes all fairness metrics simultaneously, indicating an\ninherent trade-off between these objectives. These findings suggest a practical\nguideline for safety-critical deployments: begin with a well-aligned base\nmodel, favour adapter-based PEFT, and conduct category-specific audits of both\nsafety and fairness.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08LoRA\u3001IA3\u3001Prompt-Tuning\u3001P-Tuning\uff09\u5728\u56db\u4e2a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0a\u7684\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\uff08LoRA\u3001IA3\uff09\u901a\u5e38\u80fd\u63d0\u9ad8\u5b89\u5168\u6027\u4e14\u5bf9\u516c\u5e73\u6027\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u4f1a\u964d\u4f4e\u5b89\u5168\u6027\u5e76\u5bfc\u81f4\u66f4\u5927\u7684\u516c\u5e73\u6027\u9000\u5316\u3002", "motivation": "\u968f\u7740\u7ec4\u7ec7\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u548c\u8c03\u6574\u6258\u7ba1\u5728\u516c\u5171\u5b58\u50a8\u5e93\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u867d\u7136\u8fd9\u4e9b\u8c03\u6574\u901a\u5e38\u80fd\u63d0\u9ad8\u4e13\u4e1a\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u6700\u8fd1\u8bc1\u636e\u8868\u660e\u5b83\u4eec\u4e5f\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u7684\u5b89\u5168\u6027\u6216\u516c\u5e73\u6027\u3002\u7531\u4e8e\u4e0d\u540c\u7684\u5fae\u8c03\u6280\u672f\u53ef\u80fd\u5bf9\u8fd9\u4e9b\u5173\u952e\u7ef4\u5ea6\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u6743\u8861\u5173\u7cfb\u3002", "method": "\u7814\u7a76\u5e94\u7528\u56db\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08LoRA\u3001IA3\u3001Prompt-Tuning\u3001P-Tuning\uff09\u5230\u56db\u4e2a\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5bb6\u65cf\uff08Meta-Llama-3-8B\u3001Qwen2.5-7B\u3001Mistral-7B\u548cGemma-7B\uff09\uff0c\u5171\u8bc4\u4f30\u4e86235\u4e2a\u5fae\u8c03\u53d8\u4f53\uff0c\u6db5\u76d611\u4e2a\u5b89\u5168\u5371\u5bb3\u7c7b\u522b\u548c9\u4e2a\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u7ef4\u5ea6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\uff08LoRA\u3001IA3\uff09\u503e\u5411\u4e8e\u63d0\u9ad8\u5b89\u5168\u5206\u6570\uff0c\u5bf9\u516c\u5e73\u6027\u7834\u574f\u6700\u5c0f\uff0c\u4fdd\u6301\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u504f\u89c1\u5206\u6570\uff1b\u800c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff08Prompt-Tuning\u548cP-Tuning\uff09\u901a\u5e38\u964d\u4f4e\u5b89\u5168\u6027\u5e76\u5bfc\u81f4\u66f4\u5927\u7684\u516c\u5e73\u6027\u9000\u5316\uff0c\u51c6\u786e\u6027\u4e0b\u964d\u4e14\u504f\u89c1\u589e\u52a0\u3002\u5bf9\u9f50\u53d8\u5316\u53d7\u57fa\u7840\u6a21\u578b\u7c7b\u578b\u5f3a\u70c8\u8c03\u8282\uff1aLLaMA\u4fdd\u6301\u7a33\u5b9a\uff0cQwen\u6709\u9002\u5ea6\u589e\u76ca\uff0cGemma\u5b89\u5168\u6027\u4e0b\u964d\u6700\u4e25\u91cd\uff0cMistral\uff08\u6ca1\u6709\u5185\u90e8\u8c03\u8282\u5c42\uff09\u663e\u793a\u51fa\u6700\u5927\u65b9\u5dee\u3002", "conclusion": "\u5b89\u5168\u6027\u7684\u6539\u8fdb\u4e0d\u4e00\u5b9a\u8f6c\u5316\u4e3a\u516c\u5e73\u6027\u7684\u6539\u8fdb\uff0c\u6ca1\u6709\u5355\u4e00\u914d\u7f6e\u80fd\u540c\u65f6\u4f18\u5316\u6240\u6709\u516c\u5e73\u6027\u6307\u6807\uff0c\u8868\u660e\u8fd9\u4e9b\u76ee\u6807\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u7684\u6743\u8861\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5b89\u5168\u5173\u952e\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff1a\u4ece\u826f\u597d\u5bf9\u9f50\u7684\u57fa\u7840\u6a21\u578b\u5f00\u59cb\uff0c\u4f18\u5148\u9009\u62e9\u57fa\u4e8e\u9002\u914d\u5668\u7684PEFT\u65b9\u6cd5\uff0c\u5e76\u5bf9\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u8fdb\u884c\u7c7b\u522b\u7279\u5b9a\u7684\u5ba1\u8ba1\u3002"}}
{"id": "2511.00516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00516", "abs": "https://arxiv.org/abs/2511.00516", "authors": ["Peiyi Wang", "Paul A. M. Lefeuvre", "Shangwei Zou", "Zhenwei Ni", "Daniela Rus", "Cecilia Laschi"], "title": "Adaptive and Multi-object Grasping via Deformable Origami Modules", "comment": null, "summary": "Soft robotics gripper have shown great promise in handling fragile and\ngeometrically complex objects. However, most existing solutions rely on bulky\nactuators, complex control strategies, or advanced tactile sensing to achieve\nstable and reliable grasping performance. In this work, we present a\nmulti-finger hybrid gripper featuring passively deformable origami modules that\ngenerate constant force and torque output. Each finger composed of parallel\norigami modules is driven by a 1-DoF actuator mechanism, enabling passive shape\nadaptability and stable grasping force without active sensing or feedback\ncontrol. More importantly, we demonstrate an interesting capability in\nsimultaneous multi-object grasping, which allows stacked objects of varied\nshape and size to be picked, transported and placed independently at different\nstates, significantly improving manipulation efficiency compared to\nsingle-object grasping. These results highlight the potential of origami-based\ncompliant structures as scalable modules for adaptive, stable and efficient\nmulti-object manipulation in domestic and industrial pick-and-place scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6298\u7eb8\u7ed3\u6784\u7684\u6df7\u5408\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u624b\uff0c\u5177\u6709\u88ab\u52a8\u53d8\u5f62\u80fd\u529b\uff0c\u53ef\u5b9e\u73b0\u6052\u5b9a\u529b/\u626d\u77e9\u8f93\u51fa\u548c\u591a\u7269\u4f53\u540c\u65f6\u6293\u53d6\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u624b\u901a\u5e38\u4f9d\u8d56\u7b28\u91cd\u7684\u6267\u884c\u5668\u3001\u590d\u6742\u63a7\u5236\u7b56\u7565\u6216\u5148\u8fdb\u89e6\u89c9\u4f20\u611f\u6765\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\uff0c\u9700\u8981\u66f4\u7b80\u5355\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5e73\u884c\u6298\u7eb8\u6a21\u5757\u6784\u6210\u7684\u591a\u6307\u6df7\u5408\u6293\u624b\uff0c\u6bcf\u4e2a\u624b\u6307\u75311-DoF\u6267\u884c\u5668\u9a71\u52a8\uff0c\u5177\u6709\u88ab\u52a8\u5f62\u72b6\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6293\u53d6\u529b\uff0c\u65e0\u9700\u4e3b\u52a8\u4f20\u611f\u6216\u53cd\u9988\u63a7\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u88ab\u52a8\u5f62\u72b6\u9002\u5e94\u6027\u3001\u7a33\u5b9a\u6293\u53d6\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u540c\u65f6\u591a\u7269\u4f53\u6293\u53d6\u80fd\u529b\uff0c\u53ef\u72ec\u7acb\u62fe\u53d6\u3001\u8fd0\u8f93\u548c\u653e\u7f6e\u4e0d\u540c\u5f62\u72b6\u5927\u5c0f\u7684\u5806\u53e0\u7269\u4f53\u3002", "conclusion": "\u6298\u7eb8\u57fa\u67d4\u6027\u7ed3\u6784\u4f5c\u4e3a\u53ef\u6269\u5c55\u6a21\u5757\uff0c\u5728\u5bb6\u5ead\u548c\u5de5\u4e1a\u62fe\u653e\u573a\u666f\u4e2d\u5177\u6709\u81ea\u9002\u5e94\u3001\u7a33\u5b9a\u548c\u9ad8\u6548\u591a\u7269\u4f53\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.00927", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00927", "abs": "https://arxiv.org/abs/2511.00927", "authors": ["Pavithren V S Pakianathan", "Hannah McGowan", "Isabel H\u00f6ppchen", "Daniela Wurhofer", "Gunnar Treff", "Mahdi Sareban", "Josef Niebauer", "Albrecht Schmidt", "Jan David Smeddinck"], "title": "Towards Data-Enabled Physical Activity Planning: An Exploratory Study of HCP Perspectives On The Integration Of Patient-Generated Health Data", "comment": "Published at 19th EAI International Conference on Pervasive Computing\n  Technologies for Healthcare", "summary": "Physical activity planning is an essential part of cardiovascular\nrehabilitation. Through a two-part formative design exploration, we\ninvestigated integrating patient-generated health data (PGHD) into clinical\nworkflows supporting shared decision-making (SDM) in physical activity\nplanning. In part one, during a two-week situated study, to reduce risk of\nworking with cardiovascular disease patients, we recruited healthy participants\nwho self-tracked health and physical activity data and attended a physical\nactivity planning session with a healthcare professional (HCP). Subsequently\nboth HCPs and participants were interviewed. In part two, findings from part\none were presented to HCPs in a card-sorting workshop to corroborate findings\nand identify information needs of HCPs alongside patient journeys and clinical\nworkflows. Our outcomes highlight HCP information needs around patient risk\nfactors, vital signs, and adherence to physical activity. Enablers for PGHD\nintegration include adaptive data sense-making, standardization and\norganizational support for integration. Barriers include lack of time, data\nquality, trust and liability concerns. Our research highlights implications for\ndesigning digital health technologies that support PGHD in physical activity\nplanning during cardiac rehabilitation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e24\u90e8\u5206\u7684\u8bbe\u8ba1\u63a2\u7d22\uff0c\u8c03\u67e5\u4e86\u5c06\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u6574\u5408\u5230\u5fc3\u810f\u5eb7\u590d\u7269\u7406\u6d3b\u52a8\u89c4\u5212\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u652f\u6301\u5171\u4eab\u51b3\u7b56\u3002\u7814\u7a76\u53d1\u73b0\u533b\u62a4\u4eba\u5458\u9700\u8981\u60a3\u8005\u98ce\u9669\u56e0\u7d20\u3001\u751f\u547d\u4f53\u5f81\u548c\u6d3b\u52a8\u4f9d\u4ece\u6027\u7b49\u4fe1\u606f\uff0c\u5e76\u8bc6\u522b\u4e86\u6570\u636e\u6574\u5408\u7684\u4fc3\u8fdb\u56e0\u7d20\u548c\u969c\u788d\u3002", "motivation": "\u7269\u7406\u6d3b\u52a8\u89c4\u5212\u662f\u5fc3\u8840\u7ba1\u5eb7\u590d\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u6574\u5408\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u533b\u62a4\u4eba\u5458\u548c\u60a3\u8005\u5728\u7269\u7406\u6d3b\u52a8\u89c4\u5212\u4e2d\u7684\u5171\u4eab\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u4e24\u90e8\u5206\u5f62\u6210\u6027\u8bbe\u8ba1\u63a2\u7d22\uff1a\u7b2c\u4e00\u90e8\u5206\u4e3a\u4e24\u5468\u7684\u60c5\u5883\u7814\u7a76\uff0c\u62db\u52df\u5065\u5eb7\u53c2\u4e0e\u8005\u81ea\u6211\u8ffd\u8e2a\u5065\u5eb7\u6570\u636e\u5e76\u4e0e\u533b\u62a4\u4eba\u5458\u8fdb\u884c\u7269\u7406\u6d3b\u52a8\u89c4\u5212\u4f1a\u8bae\uff1b\u7b2c\u4e8c\u90e8\u5206\u901a\u8fc7\u5361\u7247\u5206\u7c7b\u5de5\u4f5c\u574a\u5411\u533b\u62a4\u4eba\u5458\u5448\u73b0\u7814\u7a76\u53d1\u73b0\uff0c\u9a8c\u8bc1\u7ed3\u679c\u5e76\u8bc6\u522b\u4fe1\u606f\u9700\u6c42\u3002", "result": "\u7814\u7a76\u660e\u786e\u4e86\u533b\u62a4\u4eba\u5458\u5bf9\u60a3\u8005\u98ce\u9669\u56e0\u7d20\u3001\u751f\u547d\u4f53\u5f81\u548c\u7269\u7406\u6d3b\u52a8\u4f9d\u4ece\u6027\u7684\u4fe1\u606f\u9700\u6c42\u3002\u4fc3\u8fdb\u6570\u636e\u6574\u5408\u7684\u56e0\u7d20\u5305\u62ec\u9002\u5e94\u6027\u6570\u636e\u7406\u89e3\u3001\u6807\u51c6\u5316\u548c\u7ec4\u7ec7\u652f\u6301\uff1b\u969c\u788d\u5305\u62ec\u65f6\u95f4\u4e0d\u8db3\u3001\u6570\u636e\u8d28\u91cf\u3001\u4fe1\u4efb\u548c\u8d23\u4efb\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u652f\u6301\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u5728\u5fc3\u810f\u5eb7\u590d\u7269\u7406\u6d3b\u52a8\u89c4\u5212\u4e2d\u5e94\u7528\u7684\u6570\u5b57\u5065\u5eb7\u6280\u672f\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2511.00416", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8fed\u4ee3\u6539\u5199\u7684AI\u751f\u6210\u6587\u672c\u80fd\u6709\u6548\u9003\u907f\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u68c0\u6d4b\u5668\u5bf9\u76f4\u63a5LLM\u8f93\u51fa\u51c6\u786e\u7387\u8d8590%\uff0c\u4f46\u5bf9\u8fed\u4ee3\u6539\u5199\u5185\u5bb9\u68c0\u6d4b\u5931\u8d25\u3002\u8bba\u6587\u63d0\u51fa\u4e86PADBen\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u68c0\u6d4b\u5668\u5728\u527d\u7a83\u9003\u907f\u68c0\u6d4b\u6210\u529f\u4f46\u5728\u4f5c\u8005\u8eab\u4efd\u6df7\u6dc6\u68c0\u6d4b\u5931\u8d25\u7684\u4e25\u91cd\u4e0d\u5bf9\u79f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5bf9\u76f4\u63a5LLM\u8f93\u51fa\u68c0\u6d4b\u6548\u679c\u826f\u597d\uff0c\u4f46\u5bf9\u8fed\u4ee3\u6539\u5199\u5185\u5bb9\u68c0\u6d4b\u6548\u679c\u6781\u5dee\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u68c0\u6d4b\u5931\u8d25\u7684\u539f\u56e0\u5e76\u8bc4\u4f30\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5185\u5728\u673a\u5236\u5206\u6790\u63ed\u793a\u8fed\u4ee3\u6539\u5199\u521b\u5efa\u4e86\u8bed\u4e49\u4f4d\u79fb\u4f46\u4fdd\u7559\u751f\u6210\u6a21\u5f0f\u7684\u4e2d\u95f4\u6e05\u6d17\u533a\u57df\uff0c\u5f15\u5165PADBen\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u8bc4\u4f30\u68c0\u6d4b\u5668\u5bf9\u4e24\u79cd\u6539\u5199\u653b\u51fb\u573a\u666f\u7684\u9c81\u68d2\u6027\uff0c\u5305\u542b\u4e94\u79cd\u6587\u672c\u7c7b\u578b\u5206\u7c7b\u548c\u4e94\u4e2a\u6e10\u8fdb\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u8bc4\u4f30\u4e8611\u4e2a\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\uff0c\u53d1\u73b0\u5173\u952e\u4e0d\u5bf9\u79f0\u6027\uff1a\u68c0\u6d4b\u5668\u80fd\u6210\u529f\u8bc6\u522b\u527d\u7a83\u9003\u907f\u95ee\u9898\uff0c\u4f46\u5728\u4f5c\u8005\u8eab\u4efd\u6df7\u6dc6\u60c5\u51b5\u4e0b\u5931\u8d25\u3002\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e2d\u95f4\u6e05\u6d17\u533a\u57df\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fed\u4ee3\u6539\u5199\u521b\u5efa\u7684\u4e2d\u95f4\u6e05\u6d17\u533a\u57df\uff0c\u9700\u8981\u5728\u68c0\u6d4b\u67b6\u6784\u4e0a\u8fdb\u884c\u6839\u672c\u6027\u6539\u8fdb\uff0c\u8d85\u8d8a\u73b0\u6709\u7684\u8bed\u4e49\u548c\u98ce\u683c\u533a\u5206\u65b9\u6cd5\u3002"}}
{"id": "2511.00936", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00936", "abs": "https://arxiv.org/abs/2511.00936", "authors": ["Pavithren V S Pakianathan", "Rania Islambouli", "Hannah McGowan", "Diogo Branco", "Tiago Guerreiro", "Jan David Smeddinck"], "title": "Exploring Human-AI Interaction with Patient-Generated Health Data Sensemaking for Cardiac Risk Reduction", "comment": "Presented as demonstration at the workshop on visual analytics in\n  healthcare (VAHC) (in conjunction with IEEE VIS 2025)", "summary": "Patient-generated health data (PGHD) allows healthcare professionals to have\na holistic and objective view of their patients. However, its integration in\ncardiac risk reduction remains unexplored. Through co-design with experienced\nhealthcare professionals (n=5) in cardiac rehabilitation, we designed a\ndashboard, INSIGHT (INvestigating the potentialS of PatIent Generated Health\ndata for CVD Prevention and ReHabiliTation), integrating multi-modal PGHD to\nsupport healthcare professionals in physical activity planning in cardiac risk\nreduction. To further augment healthcare professionals' (HCPs') data\nsensemaking and exploration capabilities, we integrate large language models\n(LLMs) for generating summaries and insights and for using natural language\ninteraction to perform personalized data analysis. The aim of this integration\nis to explore the potential of AI in augmenting HCPs' data sensemaking and\nanalysis capabilities.", "AI": {"tldr": "INSIGHT\u662f\u4e00\u4e2a\u96c6\u6210\u591a\u6a21\u6001\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u7684\u4eea\u8868\u677f\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u6570\u636e\u7406\u89e3\u548c\u5206\u6790\u80fd\u529b\uff0c\u652f\u6301\u5fc3\u810f\u5eb7\u590d\u4e2d\u7684\u4f53\u529b\u6d3b\u52a8\u89c4\u5212\u3002", "motivation": "\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\u5728\u5fc3\u810f\u98ce\u9669\u964d\u4f4e\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u8bbe\u8ba1\u5de5\u5177\u6765\u5e2e\u52a9\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u66f4\u597d\u5730\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u62a4\u7406\u3002", "method": "\u901a\u8fc7\u4e0e\u5fc3\u810f\u5eb7\u590d\u9886\u57df\u7684\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5171\u540c\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86INSIGHT\u4eea\u8868\u677f\uff0c\u96c6\u6210\u591a\u6a21\u6001\u60a3\u8005\u751f\u6210\u5065\u5eb7\u6570\u636e\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6458\u8981\u3001\u6d1e\u5bdf\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u5728\u5fc3\u810f\u98ce\u9669\u964d\u4f4e\u4e2d\u8fdb\u884c\u4f53\u529b\u6d3b\u52a8\u89c4\u5212\u7684\u4eea\u8868\u677f\uff0c\u589e\u5f3a\u4e86\u6570\u636e\u7406\u89e3\u548c\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "AI\u6280\u672f\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u589e\u5f3a\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u7684\u6570\u636e\u7406\u89e3\u548c\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u5fc3\u810f\u5eb7\u590d\u63d0\u4f9b\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2511.00421", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages.", "AI": {"tldr": "MedRECT\u662f\u9996\u4e2a\u8de8\u8bed\u8a00\uff08\u65e5\u8bed/\u82f1\u8bed\uff09\u533b\u7597\u9519\u8bef\u5904\u7406\u57fa\u51c6\uff0c\u5305\u542b\u9519\u8bef\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u4fee\u6b63\u4e09\u4e2a\u5b50\u4efb\u52a1\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u82f1\u8bed\u5230\u65e5\u8bed\u5b58\u57285-10%\u6027\u80fd\u5dee\u8ddd\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u5728\u7ed3\u6784\u5316\u533b\u7597\u9519\u8bef\u4fee\u6b63\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u68c0\u6d4b\u548c\u4fee\u6b63\u4e34\u5e8a\u6587\u672c\u9519\u8bef\u7684\u80fd\u529b\uff08\u5b89\u5168\u90e8\u7f72\u7684\u524d\u63d0\u6761\u4ef6\uff09\u4ecd\u672a\u88ab\u5145\u5206\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u82f1\u8bed\u4ee5\u5916\u7684\u8bed\u8a00\u4e2d\u3002", "method": "\u4ece\u65e5\u672c\u533b\u5e08\u8d44\u683c\u8003\u8bd5\u548c\u7cbe\u5fc3\u7b56\u5212\u7684\u82f1\u8bed\u5bf9\u5e94\u5185\u5bb9\u4e2d\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efaMedRECT\u57fa\u51c6\uff0c\u5305\u542bMedRECT-ja\uff08663\u4e2a\u6587\u672c\uff09\u548cMedRECT-en\uff08458\u4e2a\u6587\u672c\uff09\u3002\u8bc4\u4f30\u4e869\u4e2a\u5f53\u4ee3LLM\uff0c\u6db5\u76d6\u4e13\u6709\u3001\u5f00\u6e90\u548c\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u5e76\u8fdb\u884c\u4e86\u9488\u5bf9\u6027\u7684LoRA\u5fae\u8c03\u3002", "result": "\u63a8\u7406\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u67b6\u6784\uff0c\u9519\u8bef\u68c0\u6d4b\u76f8\u5bf9\u63d0\u534713.5%\uff0c\u53e5\u5b50\u63d0\u53d6\u63d0\u534751.0%\uff1b\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u82f1\u8bed\u5230\u65e5\u8bed\u5b58\u57285-10%\u6027\u80fd\u5dee\u8ddd\uff1b\u5fae\u8c03\u5728\u9519\u8bef\u4fee\u6b63\u6027\u80fd\u4e0a\u4ea7\u751f\u4e0d\u5bf9\u79f0\u6539\u8fdb\uff08\u65e5\u8bed\uff1a+0.078\uff0c\u82f1\u8bed\uff1a+0.168\uff09\uff1b\u5fae\u8c03\u540e\u6a21\u578b\u5728\u7ed3\u6784\u5316\u533b\u7597\u9519\u8bef\u4fee\u6b63\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\u3002", "conclusion": "MedRECT\u662f\u9996\u4e2a\u5168\u9762\u7684\u8de8\u8bed\u8a00\u533b\u7597\u9519\u8bef\u4fee\u6b63\u57fa\u51c6\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u8de8\u8bed\u8a00\u533b\u7597LLM\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u6846\u67b6\u548c\u8d44\u6e90\u3002"}}
{"id": "2511.00103", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00103", "abs": "https://arxiv.org/abs/2511.00103", "authors": ["Rotem Ezra", "Hedi Zisling", "Nimrod Berman", "Ilan Naiman", "Alexey Gorkor", "Liran Nochumsohn", "Eliya Nachmani", "Omri Azencot"], "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video", "comment": null, "summary": "Diffusion models have become state-of-the-art generative models for images,\naudio, and video, yet enabling fine-grained controllable generation, i.e.,\ncontinuously steering specific concepts without disturbing unrelated content,\nremains challenging. Concept Sliders (CS) offer a promising direction by\ndiscovering semantic directions through textual contrasts, but they require\nper-concept training and architecture-specific fine-tuning (e.g., LoRA),\nlimiting scalability to new modalities. In this work we introduce FreeSliders,\na simple yet effective approach that is fully training-free and\nmodality-agnostic, achieved by partially estimating the CS formula during\ninference. To support modality-agnostic evaluation, we extend the CS benchmark\nto include both video and audio, establishing the first suite for fine-grained\nconcept generation control with multiple modalities. We further propose three\nevaluation properties along with new metrics to improve evaluation quality.\nFinally, we identify an open problem of scale selection and non-linear\ntraversals and introduce a two-stage procedure that automatically detects\nsaturation points and reparameterizes traversal for perceptually uniform,\nsemantically meaningful edits. Extensive experiments demonstrate that our\nmethod enables plug-and-play, training-free concept control across modalities,\nimproves over existing baselines, and establishes new tools for principled\ncontrollable generation. An interactive presentation of our benchmark and\nmethod is available at: https://azencot-group.github.io/FreeSliders/", "AI": {"tldr": "FreeSliders\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u6001\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u90e8\u5206\u4f30\u8ba1\u6982\u5ff5\u6ed1\u5757\u516c\u5f0f\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u63a7\u5236\u751f\u6210\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u5df2\u6210\u4e3a\u6700\u5148\u8fdb\u6280\u672f\uff0c\u4f46\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\uff08\u5373\u5728\u4e0d\u5e72\u6270\u65e0\u5173\u5185\u5bb9\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u63a7\u5236\u7279\u5b9a\u6982\u5ff5\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u6982\u5ff5\u6ed1\u5757\u65b9\u6cd5\u9700\u8981\u6bcf\u4e2a\u6982\u5ff5\u7684\u8bad\u7ec3\u548c\u7279\u5b9a\u67b6\u6784\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u65b0\u6a21\u6001\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faFreeSliders\u65b9\u6cd5\uff0c\u5b8c\u5168\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u6001\u65e0\u5173\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u90e8\u5206\u4f30\u8ba1\u6982\u5ff5\u6ed1\u5757\u516c\u5f0f\u6765\u5b9e\u73b0\u3002\u8fd8\u5f15\u5165\u4e24\u9636\u6bb5\u7a0b\u5e8f\u81ea\u52a8\u68c0\u6d4b\u9971\u548c\u70b9\u5e76\u91cd\u65b0\u53c2\u6570\u5316\u904d\u5386\uff0c\u5b9e\u73b0\u611f\u77e5\u5747\u5300\u3001\u8bed\u4e49\u6709\u610f\u4e49\u7684\u7f16\u8f91\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u652f\u6301\u5373\u63d2\u5373\u7528\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u63a7\u5236\uff0c\u6539\u8fdb\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e3a\u539f\u5219\u6027\u53ef\u63a7\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u5de5\u5177\u3002\u5c06\u6982\u5ff5\u6ed1\u5757\u57fa\u51c6\u6269\u5c55\u5230\u89c6\u9891\u548c\u97f3\u9891\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u591a\u6a21\u6001\u7ec6\u7c92\u5ea6\u6982\u5ff5\u751f\u6210\u63a7\u5236\u5957\u4ef6\u3002", "conclusion": "FreeSliders\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u5373\u63d2\u5373\u7528\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u63a7\u5236\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u4e3a\u539f\u5219\u6027\u53ef\u63a7\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u5de5\u5177\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u8bc4\u4f30\u5c5e\u6027\u548c\u65b0\u6307\u6807\u6765\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3002"}}
{"id": "2511.00457", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00457", "abs": "https://arxiv.org/abs/2511.00457", "authors": ["Chunyu Wei", "Wenji Hu", "Xingjia Hao", "Xin Wang", "Yifan Yang", "Yueguo Chen", "Yang Tian", "Yunhai Wang"], "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining", "comment": null, "summary": "Large Language Models (LLMs) face significant limitations when applied to\nlarge-scale graphs, struggling with context constraints and inflexible\nreasoning. We present GraphChain, a framework that enables LLMs to analyze\ncomplex graphs through dynamic sequences of specialized tools, mimicking human\nexploratory intelligence. Our approach introduces two key innovations: (1)\nProgressive Graph Distillation, a reinforcement learning mechanism that\ngenerates optimized tool sequences balancing task relevance with information\ncompression, and (2) Structure-aware Test-Time Adaptation, which efficiently\ntailors tool selection strategies to diverse graph topologies using spectral\nproperties and lightweight adapters without costly retraining. Experiments show\nGraphChain significantly outperforms prior methods, enabling scalable and\nadaptive LLM-driven graph analysis.", "AI": {"tldr": "GraphChain\u662f\u4e00\u4e2a\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5206\u6790\u590d\u6742\u56fe\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u5e8f\u5217\u6a21\u62df\u4eba\u7c7b\u63a2\u7d22\u667a\u80fd\uff0c\u89e3\u51b3\u4e86LLM\u5728\u5927\u89c4\u6a21\u56fe\u5206\u6790\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u63a8\u7406\u4e0d\u7075\u6d3b\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u56fe\u5206\u6790\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u7ea6\u675f\u548c\u63a8\u7406\u4e0d\u7075\u6d3b\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u56fe\u5206\u6790\u3002", "method": "\u63d0\u51faGraphChain\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u6e10\u8fdb\u56fe\u84b8\u998f\uff08\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u751f\u6210\u4f18\u5316\u7684\u5de5\u5177\u5e8f\u5217\uff09\u548c\u7ed3\u6784\u611f\u77e5\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08\u5229\u7528\u8c31\u5c5e\u6027\u548c\u8f7b\u91cf\u9002\u914d\u5668\u9002\u5e94\u4e0d\u540c\u56fe\u62d3\u6251\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGraphChain\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684LLM\u9a71\u52a8\u56fe\u5206\u6790\u3002", "conclusion": "GraphChain\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u5e8f\u5217\u548c\u7ed3\u6784\u611f\u77e5\u9002\u5e94\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u5927\u89c4\u6a21\u56fe\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00635", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00635", "abs": "https://arxiv.org/abs/2511.00635", "authors": ["Hyungtae Lim", "Daebeom Kim", "Hyun Myung"], "title": "Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles", "comment": "13 pages, 12 figures", "summary": "As various 3D light detection and ranging (LiDAR) sensors have been\nintroduced to the market, research on multi-session simultaneous localization\nand mapping (MSS) using heterogeneous LiDAR sensors has been actively\nconducted. Existing MSS methods mostly rely on loop closure detection for\ninter-session alignment; however, the performance of loop closure detection can\nbe potentially degraded owing to the differences in the density and field of\nview (FoV) of the sensors used in different sessions. In this study, we\nchallenge the existing paradigm that relies heavily on loop detection modules\nand propose a novel MSS framework, called Multi-Mapcher, that employs\nlarge-scale map-to-map registration to perform inter-session initial alignment,\nwhich is commonly assumed to be infeasible, by leveraging outlier-robust 3D\npoint cloud registration. Next, after finding inter-session loops by radius\nsearch based on the assumption that the inter-session initial alignment is\nsufficiently precise, anchor node-based robust pose graph optimization is\nemployed to build a consistent global map. As demonstrated in our experiments,\nour approach shows substantially better MSS performance for various LiDAR\nsensors used to capture the sessions and is faster than state-of-the-art\napproaches. Our code is available at\nhttps://github.com/url-kaist/multi-mapcher.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMulti-Mapcher\u7684\u65b0\u578b\u591a\u4f1a\u8bddSLAM\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5730\u56fe\u5230\u5730\u56fe\u914d\u51c6\u5b9e\u73b0\u4f1a\u8bdd\u95f4\u521d\u59cb\u5bf9\u9f50\uff0c\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u95ed\u73af\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u6784LiDAR\u4f20\u611f\u5668\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MSS\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u95ed\u73af\u68c0\u6d4b\uff0c\u4f46\u7531\u4e8e\u4e0d\u540c\u4f1a\u8bdd\u4f7f\u7528\u7684LiDAR\u4f20\u611f\u5668\u5728\u70b9\u4e91\u5bc6\u5ea6\u548c\u89c6\u573a\u89d2\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\u53ef\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u5730\u56fe\u5230\u5730\u56fe\u914d\u51c6\u8fdb\u884c\u4f1a\u8bdd\u95f4\u521d\u59cb\u5bf9\u9f50\uff0c\u7136\u540e\u57fa\u4e8e\u534a\u5f84\u641c\u7d22\u627e\u5230\u4f1a\u8bdd\u95f4\u95ed\u73af\uff0c\u6700\u540e\u4f7f\u7528\u57fa\u4e8e\u951a\u8282\u70b9\u7684\u9c81\u68d2\u4f4d\u59ff\u56fe\u4f18\u5316\u6784\u5efa\u4e00\u81f4\u7684\u5168\u5c40\u5730\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5404\u79cdLiDAR\u4f20\u611f\u5668\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684MSS\u6027\u80fd\uff0c\u4e14\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u5feb\u3002", "conclusion": "\u63d0\u51fa\u7684Multi-Mapcher\u6846\u67b6\u901a\u8fc7\u5730\u56fe\u5230\u5730\u56fe\u914d\u51c6\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784LiDAR\u4f20\u611f\u5668\u591a\u4f1a\u8bddSLAM\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u6027\u80fd\u3002"}}
{"id": "2511.00945", "categories": ["cs.HC", "H.5"], "pdf": "https://arxiv.org/pdf/2511.00945", "abs": "https://arxiv.org/abs/2511.00945", "authors": ["Yi Zhao", "Siqi Wang", "Qiqun Geng", "Erxin Yu", "Jing Li"], "title": "\"Less is More\": Reducing Cognitive Load and Task Drift in Real-Time Multimodal Assistive Agents for the Visually Impaired", "comment": "20 pages", "summary": "Vision-Language Models (VLMs) enable on-demand visual assistance, yet current\napplications for people with visual impairments (PVI) impose high cognitive\nload and exhibit task drift, limiting real-world utility. We first conducted a\nformative study with 15 PVI and identified three requirements for visually\nimpaired assistance (VIA): low latency for real-time use, minimal cognitive\nload, and hallucination-resistant responses to sustain trust. Informed by the\nformative study, we present VIA-Agent, a prototype that co-optimizes its\ncognitive 'brain' and interactive 'body'. The brain implements a\ngoal-persistent design with calibrated conciseness to produce brief, actionable\nguidance; the body adopts a real-time communication (RTC) embodiment-evolving\nfrom a request-response model Context Protocol (MCP) pipeline-to-support fluid\ninteraction. We evaluated VIA-Agent with 9 PVI across navigation and object\nretrieval in the wild against BeMyAI and Doubao. VIA-Agent significantly\noutperformed BeMyAI both quantitatively and qualitatively. While achieving\nsuccess rates comparable to Doubao, it reduced mean task time by 39.9% (70.1 s\nvs. 110.7 s), required fewer conversational turns (4.3 vs. 5.0), and lowered\nperceived cognitive load and task drift. System Usability Scale (SUS) results\naligned with these findings, with VIA-Agent achieving the highest usability. We\nhope this work inspires the development of more human-centered VIA systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VIA-Agent\uff0c\u4e00\u79cd\u4e3a\u89c6\u969c\u4eba\u58eb\u8bbe\u8ba1\u7684\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u8ba4\u77e5\u5927\u8111\u548c\u4ea4\u4e92\u8eab\u4f53\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\u548c\u4efb\u52a1\u65f6\u95f4\uff0c\u5728\u5bfc\u822a\u548c\u7269\u54c1\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u969c\u8f85\u52a9\u7cfb\u7edf\u5b58\u5728\u9ad8\u8ba4\u77e5\u8d1f\u8377\u548c\u4efb\u52a1\u6f02\u79fb\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002\u901a\u8fc715\u540d\u89c6\u969c\u4eba\u58eb\u7684\u521d\u6b65\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u4e09\u4e2a\u5173\u952e\u9700\u6c42\uff1a\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u8ba4\u77e5\u8d1f\u8377\u548c\u6297\u5e7b\u89c9\u54cd\u5e94\u3002", "method": "VIA-Agent\u91c7\u7528\u76ee\u6807\u6301\u4e45\u6027\u8bbe\u8ba1\u548c\u6821\u51c6\u7b80\u6d01\u6027\u6765\u751f\u6210\u7b80\u77ed\u3001\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u540c\u65f6\u91c7\u7528\u5b9e\u65f6\u901a\u4fe1\u673a\u5236\uff0c\u4ece\u8bf7\u6c42-\u54cd\u5e94\u6a21\u578b\u6f14\u53d8\u4e3a\u652f\u6301\u6d41\u7545\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u534f\u8bae\u7ba1\u9053\u3002", "result": "\u4e0eBeMyAI\u548cDoubao\u76f8\u6bd4\uff0cVIA-Agent\u57289\u540d\u89c6\u969c\u4eba\u58eb\u7684\u5b9e\u5730\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u4efb\u52a1\u65f6\u95f4\u51cf\u5c1139.9%\uff0870.1\u79d2 vs 110.7\u79d2\uff09\uff0c\u5bf9\u8bdd\u8f6e\u6b21\u51cf\u5c11\uff084.3 vs 5.0\uff09\uff0c\u8ba4\u77e5\u8d1f\u8377\u548c\u4efb\u52a1\u6f02\u79fb\u663e\u8457\u964d\u4f4e\uff0c\u7cfb\u7edf\u53ef\u7528\u6027\u8bc4\u5206\u6700\u9ad8\u3002", "conclusion": "VIA-Agent\u901a\u8fc7\u534f\u540c\u4f18\u5316\u8ba4\u77e5\u5927\u8111\u548c\u4ea4\u4e92\u8eab\u4f53\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u4f4e\u8ba4\u77e5\u8d1f\u8377\u7684\u89c6\u89c9\u8f85\u52a9\uff0c\u4e3a\u5f00\u53d1\u66f4\u4eba\u6027\u5316\u7684\u89c6\u969c\u8f85\u52a9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2511.00432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality.", "AI": {"tldr": "\u63d0\u51faG2\u65b9\u6cd5\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u6280\u672f\uff0c\u901a\u8fc7\u53cc\u5f15\u5bfc\u673a\u5236\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5e72\u9884\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u51fa\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u591a\u6b21\u5c1d\u8bd5\u751f\u6210\u9ad8\u5ea6\u76f8\u4f3c\u5185\u5bb9\uff0c\u5f71\u54cd\u9700\u8981\u591a\u6837\u5316\u8f93\u51fa\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u6e29\u5ea6\u8c03\u8282\u867d\u80fd\u589e\u5f3a\u591a\u6837\u6027\u4f46\u4f1a\u727a\u7272\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u751f\u6210\u5668\u914d\u5408\u53cc\u5f15\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u89e3\u7801\u7684\u5e72\u9884\u6765\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5728\u539f\u59cb\u67e5\u8be2\u6761\u4ef6\u4e0b\u9f13\u52b1\u66f4\u591a\u6837\u5316\u7684\u8f93\u51fa\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cG2\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u8f93\u51fa\u591a\u6837\u6027\uff0c\u540c\u65f6\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e4b\u95f4\u4fdd\u6301\u4e86\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "G2\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u591a\u6837\u6027\u3002"}}
{"id": "2511.00107", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00107", "abs": "https://arxiv.org/abs/2511.00107", "authors": ["Piyushkumar Patel"], "title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency", "comment": null, "summary": "Text to video generation has emerged as a critical frontier in generative\nartificial intelligence, yet existing approaches struggle with maintaining\ntemporal consistency, compositional understanding, and fine grained control\nover visual narratives. We present MOVAI (Multimodal Original Video AI), a\nnovel hierarchical framework that integrates compositional scene understanding\nwith temporal aware diffusion models for high fidelity text to video synthesis.\nOur approach introduces three key innovations: (1) a Compositional Scene Parser\n(CSP) that decomposes textual descriptions into hierarchical scene graphs with\ntemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that\nensures coherent motion dynamics across frames while preserving spatial\ndetails, and (3) a Progressive Video Refinement (PVR) module that iteratively\nenhances video quality through multi-scale temporal reasoning. Extensive\nexperiments on standard benchmarks demonstrate that MOVAI achieves\nstate-of-the-art performance, improving video quality metrics by 15.3% in\nLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing\nmethods. Our framework shows particular strength in generating complex\nmulti-object scenes with realistic temporal dynamics and fine-grained semantic\ncontrol.", "AI": {"tldr": "MOVAI\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u573a\u666f\u89e3\u6790\u3001\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u548c\u6e10\u8fdb\u5f0f\u89c6\u9891\u7ec6\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7ec4\u5408\u7406\u89e3\u548c\u89c6\u89c9\u53d9\u4e8b\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u7ec4\u5408\u573a\u666f\u89e3\u6790\u5668\u5c06\u6587\u672c\u63cf\u8ff0\u5206\u89e3\u4e3a\u5e26\u65f6\u95f4\u6807\u6ce8\u7684\u5206\u5c42\u573a\u666f\u56fe\uff1b\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u786e\u4fdd\u5e27\u95f4\u8fde\u8d2f\u8fd0\u52a8\u52a8\u6001\u540c\u65f6\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff1b\u6e10\u8fdb\u5f0f\u89c6\u9891\u7ec6\u5316\u6a21\u5757\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a8\u7406\u8fed\u4ee3\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOVAI\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6027\u80fd\uff0cLPIPS\u6307\u6807\u63d0\u534715.3%\uff0cFVD\u6307\u6807\u63d0\u534712.7%\uff0c\u7528\u6237\u504f\u597d\u7814\u7a76\u63d0\u534718.9%\u3002", "conclusion": "MOVAI\u6846\u67b6\u5728\u751f\u6210\u590d\u6742\u591a\u5bf9\u8c61\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5b9e\u73b0\u771f\u5b9e\u7684\u65f6\u95f4\u52a8\u6001\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63a7\u5236\u3002"}}
{"id": "2511.00509", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00509", "abs": "https://arxiv.org/abs/2511.00509", "authors": ["Yifan Xia", "Guorui Chen", "Wenqian Yu", "Zhijiang Li", "Philip Torr", "Jindong Gu"], "title": "Reimagining Safety Alignment with An Image", "comment": null, "summary": "Large language models (LLMs) excel in diverse applications but face dual\nchallenges: generating harmful content under jailbreak attacks and over-refusal\nof benign queries due to rigid safety mechanisms. These issues are further\ncomplicated by the need to accommodate different value systems and precisely\nalign with given safety preferences. Moreover, traditional methods like SFT and\nRLHF lack this capability due to their costly parameter tuning requirements and\ninability to support multiple value systems within a single model. These\nproblems are more obvious in multimodal large language models (MLLMs),\nespecially in terms of heightened over-refusal in cross-modal tasks and new\nsecurity risks arising from expanded attack surfaces. We propose Magic Image,\nan optimization-driven visual prompt framework that enhances security while\nreducing over-refusal. By optimizing image prompts using harmful/benign\nsamples, our method enables a single model to adapt to different value systems\nand better align with given safety preferences without parameter updates.\nExperiments demonstrate improved safety-effectiveness balance across diverse\ndatasets while preserving model performance, offering a practical solution for\ndeployable MLLM safety alignment.", "AI": {"tldr": "Magic Image\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u7684\u89c6\u89c9\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u63d0\u793a\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u4ef7\u503c\u7cfb\u7edf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u53cc\u91cd\u6311\u6218\uff1a\u5728\u8d8a\u72f1\u653b\u51fb\u4e0b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u4ee5\u53ca\u7531\u4e8e\u50f5\u5316\u7684\u5b89\u5168\u673a\u5236\u800c\u8fc7\u5ea6\u62d2\u7edd\u826f\u6027\u67e5\u8be2\u3002\u8fd9\u4e9b\u95ee\u9898\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u66f4\u52a0\u660e\u663e\uff0c\u7279\u522b\u662f\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8fc7\u5ea6\u62d2\u7edd\u548c\u6269\u5c55\u653b\u51fb\u9762\u5e26\u6765\u7684\u65b0\u5b89\u5168\u98ce\u9669\u3002", "method": "\u63d0\u51faMagic Image\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u6709\u5bb3/\u826f\u6027\u6837\u672c\u4f18\u5316\u56fe\u50cf\u63d0\u793a\uff0c\u4f7f\u5355\u4e2a\u6a21\u578b\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u4ef7\u503c\u7cfb\u7edf\u5e76\u66f4\u597d\u5730\u4e0e\u7ed9\u5b9a\u5b89\u5168\u504f\u597d\u5bf9\u9f50\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u6539\u5584\u4e86\u5b89\u5168\u6027\u4e0e\u6709\u6548\u6027\u7684\u5e73\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Magic Image\u4e3a\u53ef\u90e8\u7f72\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u65e0\u9700\u53c2\u6570\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u5b89\u5168\u63a7\u5236\u3002"}}
{"id": "2511.00783", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.00783", "abs": "https://arxiv.org/abs/2511.00783", "authors": ["Jingzehua Xu", "Weihang Zhang", "Yangyang Li", "Hongmiaoyi Zhang", "Guanwen Xie", "Jiwei Tang", "Shuai Zhang", "Yi Li"], "title": "When Semantics Connect the Swarm: LLM-Driven Fuzzy Control for Cooperative Multi-Robot Underwater Coverage", "comment": "This paper has been submitted to IEEE Transactions on Mobile\n  Computing", "summary": "Underwater multi-robot cooperative coverage remains challenging due to\npartial observability, limited communication, environmental uncertainty, and\nthe lack of access to global localization. To address these issues, this paper\npresents a semantics-guided fuzzy control framework that couples Large Language\nModels (LLMs) with interpretable control and lightweight coordination. Raw\nmultimodal observations are compressed by the LLM into compact,\nhuman-interpretable semantic tokens that summarize obstacles, unexplored\nregions, and Objects Of Interest (OOIs) under uncertain perception. A fuzzy\ninference system with pre-defined membership functions then maps these tokens\ninto smooth and stable steering and gait commands, enabling reliable navigation\nwithout relying on global positioning. Then, we further coordinate multiple\nrobots by introducing semantic communication that shares intent and local\ncontext in linguistic form, enabling agreement on who explores where while\navoiding redundant revisits. Extensive simulations in unknown reef-like\nenvironments show that, under limited sensing and communication, the proposed\nframework achieves robust OOI-oriented navigation and cooperative coverage with\nimproved efficiency and adaptability, narrowing the gap between semantic\ncognition and distributed underwater control in GPS-denied, map-free\nconditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u6a21\u7cca\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u53ef\u89e3\u91ca\u63a7\u5236\u548c\u8f7b\u91cf\u7ea7\u534f\u8c03\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u89e3\u51b3\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u534f\u540c\u8986\u76d6\u4e2d\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u6709\u9650\u901a\u4fe1\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u534f\u540c\u8986\u76d6\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u6709\u9650\u901a\u4fe1\u3001\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u7f3a\u4e4f\u5168\u5c40\u5b9a\u4f4d\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u5168\u5c40\u5b9a\u4f4d\u7684\u9c81\u68d2\u5bfc\u822a\u548c\u534f\u8c03\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u5c06\u539f\u59cb\u591a\u6a21\u6001\u89c2\u6d4b\u538b\u7f29\u4e3a\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u6807\u8bb0\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u96b6\u5c5e\u5ea6\u51fd\u6570\u7684\u6a21\u7cca\u63a8\u7406\u7cfb\u7edf\u751f\u6210\u5e73\u6ed1\u7a33\u5b9a\u7684\u8f6c\u5411\u548c\u6b65\u6001\u547d\u4ee4\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u901a\u4fe1\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u534f\u8c03\u3002", "result": "\u5728\u672a\u77e5\u73ca\u745a\u7901\u73af\u5883\u4e2d\u7684\u5927\u91cf\u4eff\u771f\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6709\u9650\u611f\u77e5\u548c\u901a\u4fe1\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684OOI\u5bfc\u5411\u5bfc\u822a\u548c\u534f\u540c\u8986\u76d6\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u7f29\u5c0f\u4e86\u8bed\u4e49\u8ba4\u77e5\u4e0e\u5206\u5e03\u5f0f\u6c34\u4e0b\u63a7\u5236\u5728GPS\u7f3a\u5931\u3001\u65e0\u5730\u56fe\u6761\u4ef6\u4e0b\u7684\u5dee\u8ddd\uff0c\u4e3a\u6c34\u4e0b\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00961", "categories": ["cs.HC", "H.5.1; I.3.7; H.5.2; J.5"], "pdf": "https://arxiv.org/pdf/2511.00961", "abs": "https://arxiv.org/abs/2511.00961", "authors": ["You-Jin Kim", "Joshua Lu", "Tobias H\u00f6llerer"], "title": "Dynamic Theater: Location-Based Immersive Dance Theater, Investigating User Guidance and Experience", "comment": "Conference Paper, 11 pages. Published at the 2023 ACM Symposium on\n  Virtual Reality Software and Technology (VRST)", "summary": "Dynamic Theater explores the use of augmented reality (AR) in immersive\ntheater as a platform for digital dance performances. The project presents a\nlocomotion-based experience that allows for full spatial exploration. A large\nindoor AR theater space was designed to allow users to freely explore the\naugmented environment. The curated wide-area experience employs various\nguidance mechanisms to direct users to the main content zones. Results from our\n20-person user study show how users experience the performance piece while\nusing a guidance system. The importance of stage layout, guidance system, and\ndancer placement in immersive theater experiences are highlighted as they cater\nto user preferences while enhancing the overall reception of digital content in\nwide-area AR. Observations after working with dancers and choreographers, as\nwell as their experience and feedback are also discussed.", "AI": {"tldr": "\u52a8\u6001\u5267\u573a\u9879\u76ee\u63a2\u7d22\u4e86\u5728\u6c89\u6d78\u5f0f\u5267\u573a\u4e2d\u4f7f\u7528\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u4f5c\u4e3a\u6570\u5b57\u821e\u8e48\u8868\u6f14\u5e73\u53f0\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u652f\u6301\u81ea\u7531\u7a7a\u95f4\u63a2\u7d22\u7684\u5927\u578b\u5ba4\u5185AR\u5267\u573a\uff0c\u901a\u8fc7\u5f15\u5bfc\u7cfb\u7edf\u5e2e\u52a9\u7528\u6237\u4f53\u9a8c\u8868\u6f14\u5185\u5bb9\u3002", "motivation": "\u63a2\u7d22\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u5728\u6c89\u6d78\u5f0f\u5267\u573a\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u6570\u5b57\u821e\u8e48\u8868\u6f14\u521b\u9020\u65b0\u7684\u5e73\u53f0\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u901a\u8fc7\u81ea\u7531\u7a7a\u95f4\u63a2\u7d22\u6765\u4f53\u9a8c\u8868\u6f14\u5185\u5bb9\u3002", "method": "\u8bbe\u8ba1\u5927\u578b\u5ba4\u5185AR\u5267\u573a\u7a7a\u95f4\uff0c\u91c7\u7528\u57fa\u4e8e\u8fd0\u52a8\u7684\u4f53\u9a8c\u65b9\u5f0f\uff0c\u4f7f\u7528\u591a\u79cd\u5f15\u5bfc\u673a\u5236\u5c06\u7528\u6237\u5f15\u5bfc\u81f3\u4e3b\u8981\u5185\u5bb9\u533a\u57df\uff0c\u5e76\u4e0e\u821e\u8005\u548c\u7f16\u821e\u5e08\u5408\u4f5c\u5f00\u53d1\u8868\u6f14\u5185\u5bb9\u3002", "result": "20\u4eba\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7528\u6237\u5728\u4f7f\u7528\u5f15\u5bfc\u7cfb\u7edf\u65f6\u80fd\u591f\u6709\u6548\u4f53\u9a8c\u8868\u6f14\u4f5c\u54c1\uff0c\u821e\u53f0\u5e03\u5c40\u3001\u5f15\u5bfc\u7cfb\u7edf\u548c\u821e\u8005\u4f4d\u7f6e\u5bf9\u6c89\u6d78\u5f0f\u5267\u573a\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u821e\u53f0\u5e03\u5c40\u3001\u5f15\u5bfc\u7cfb\u7edf\u548c\u821e\u8005\u4f4d\u7f6e\u5728\u589e\u5f3a\u73b0\u5b9e\u6c89\u6d78\u5f0f\u5267\u573a\u4f53\u9a8c\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u80fd\u591f\u6ee1\u8db3\u7528\u6237\u504f\u597d\u5e76\u63d0\u5347\u6570\u5b57\u5185\u5bb9\u5728\u5e7f\u57dfAR\u4e2d\u7684\u6574\u4f53\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2511.00110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00110", "abs": "https://arxiv.org/abs/2511.00110", "authors": ["YingQiao Wang", "Eric Bigelow", "Boyi Li", "Tomer Ullman"], "title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "comment": null, "summary": "We propose a novel cognitively-inspired method to improve and interpret\nphysical simulation in vision-language models. Our ``Chain of Time\" method\ninvolves generating a series of intermediate images during a simulation, and it\nis motivated by in-context reasoning in machine learning, as well as mental\nsimulation in humans. Chain of Time is used at inference time, and requires no\nadditional fine-tuning. We apply the Chain-of-Time method to synthetic and\nreal-world domains, including 2-D graphics simulations and natural 3-D videos.\nThese domains test a variety of particular physical properties, including\nvelocity, acceleration, fluid dynamics, and conservation of momentum. We found\nthat using Chain-of-Time simulation substantially improves the performance of a\nstate-of-the-art image generation model. Beyond examining performance, we also\nanalyzed the specific states of the world simulated by an image model at each\ntime step, which sheds light on the dynamics underlying these simulations. This\nanalysis reveals insights that are hidden from traditional evaluations of\nphysical reasoning, including cases where an image generation model is able to\nsimulate physical properties that unfold over time, such as velocity, gravity,\nand collisions. Our analysis also highlights particular cases where the image\ngeneration model struggles to infer particular physical parameters from input\nimages, despite being capable of simulating relevant physical processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"Chain of Time\"\u7684\u8ba4\u77e5\u542f\u53d1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u62df\u8fc7\u7a0b\u4e2d\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u56fe\u50cf\u6765\u6539\u8fdb\u548c\u89e3\u91ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u7406\u6a21\u62df\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "motivation": "\u53d7\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u4eba\u7c7b\u5fc3\u7406\u6a21\u62df\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u6a21\u62df\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u5176\u5185\u90e8\u6a21\u62df\u52a8\u6001\u3002", "method": "\u5728\u63a8\u7406\u65f6\u751f\u6210\u6a21\u62df\u8fc7\u7a0b\u4e2d\u7684\u4e2d\u95f4\u56fe\u50cf\u5e8f\u5217\uff0c\u5e94\u7528\u4e8e2D\u56fe\u5f62\u6a21\u62df\u548c\u771f\u5b9e3D\u89c6\u9891\uff0c\u6d4b\u8bd5\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u52a8\u91cf\u5b88\u6052\u7b49\u7269\u7406\u5c5e\u6027\u3002", "result": "Chain-of-Time\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u591f\u6a21\u62df\u968f\u65f6\u95f4\u5c55\u5f00\u7684\u7269\u7406\u5c5e\u6027\uff08\u5982\u901f\u5ea6\u3001\u91cd\u529b\u548c\u78b0\u649e\uff09\uff0c\u4f46\u4e5f\u53d1\u73b0\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u96be\u4ee5\u4ece\u8f93\u5165\u56fe\u50cf\u63a8\u65ad\u7279\u5b9a\u7269\u7406\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7269\u7406\u6a21\u62df\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5185\u90e8\u7269\u7406\u63a8\u7406\u8fc7\u7a0b\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u6a21\u62df\u80fd\u529b\u3002"}}
{"id": "2511.00547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00547", "abs": "https://arxiv.org/abs/2511.00547", "authors": ["Alain Riou"], "title": "Efficient Generation of Binary Magic Squares", "comment": null, "summary": "We propose a simple algorithm for generating Binary Magic Squares (BMS),\ni.e., square binary matrices where the sum of all rows and all columns are\nequal. We show by induction that our algorithm always returns valid BMS with\noptimal theoretical complexity. We then extend our study to non-square Binary\nMagic Squares, formalize conditions on the sum of rows and columns for these\nBMS to exist, and show that a slight variant of our first algorithm can\ngenerate provably generate them. Finally, we publicly release two\nimplementations of our algorithm as Python packages, including one that can\ngenerate several BMS in parallel using GPU acceleration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u4e8c\u8fdb\u5236\u5e7b\u65b9(BMS)\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u751f\u6210\u884c\u5217\u548c\u76f8\u7b49\u7684\u4e8c\u8fdb\u5236\u77e9\u9635\uff0c\u5e76\u6269\u5c55\u5230\u975e\u65b9\u5f62BMS\uff0c\u540c\u65f6\u53d1\u5e03\u4e86Python\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u4e8c\u8fdb\u5236\u5e7b\u65b9\u7684\u751f\u6210\u95ee\u9898\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u751f\u6210\u884c\u5217\u548c\u76f8\u7b49\u7684\u4e8c\u8fdb\u5236\u77e9\u9635\uff0c\u5e76\u6269\u5c55\u5230\u975e\u65b9\u5f62\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5f52\u7eb3\u6cd5\u8bc1\u660e\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u6709\u6548\u7684BMS\uff0c\u5e76\u5f00\u53d1\u4e86\u53d8\u4f53\u7b97\u6cd5\u6765\u5904\u7406\u975e\u65b9\u5f62BMS\u3002", "result": "\u7b97\u6cd5\u5177\u6709\u6700\u4f18\u7406\u8bba\u590d\u6742\u5ea6\uff0c\u80fd\u591f\u751f\u6210\u6709\u6548\u7684\u4e8c\u8fdb\u5236\u5e7b\u65b9\uff0c\u5e76\u63d0\u4f9b\u4e86\u652f\u6301GPU\u52a0\u901f\u7684\u5e76\u884c\u5b9e\u73b0\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u751f\u6210\u4e8c\u8fdb\u5236\u5e7b\u65b9\u7684\u6709\u6548\u7b97\u6cd5\uff0c\u5305\u62ec\u65b9\u5f62\u548c\u975e\u65b9\u5f62\u60c5\u51b5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684Python\u5b9e\u73b0\u3002"}}
{"id": "2511.00814", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY", "93C41, 93E11, 37M10", "I.2.9; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2511.00814", "abs": "https://arxiv.org/abs/2511.00814", "authors": ["Stella Kombo", "Masih Haseli", "Skylar Wei", "Joel W. Burdick"], "title": "Real-Time Learning of Predictive Dynamic Obstacle Models for Robotic Motion Planning", "comment": "10 pages, 6 figures, submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2025", "summary": "Autonomous systems often must predict the motions of nearby agents from\npartial and noisy data. This paper asks and answers the question: \"can we\nlearn, in real-time, a nonlinear predictive model of another agent's motions?\"\nOur online framework denoises and forecasts such dynamics using a modified\nsliding-window Hankel Dynamic Mode Decomposition (Hankel-DMD). Partial noisy\nmeasurements are embedded into a Hankel matrix, while an associated Page matrix\nenables singular-value hard thresholding (SVHT) to estimate the effective rank.\nA Cadzow projection enforces structured low-rank consistency, yielding a\ndenoised trajectory and local noise variance estimates. From this\nrepresentation, a time-varying Hankel-DMD lifted linear predictor is\nconstructed for multi-step forecasts. The residual analysis provides\nvariance-tracking signals that can support downstream estimators and risk-aware\nplanning. We validate the approach in simulation under Gaussian and\nheavy-tailed noise, and experimentally on a dynamic crane testbed. Results show\nthat the method achieves stable variance-aware denoising and short-horizon\nprediction suitable for integration into real-time control frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3Hankel\u52a8\u6001\u6a21\u6001\u5206\u89e3\u7684\u5728\u7ebf\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u5b66\u4e60\u548c\u9884\u6d4b\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u578b\uff0c\u901a\u8fc7\u566a\u58f0\u6291\u5236\u548c\u65b9\u5dee\u4f30\u8ba1\u652f\u6301\u5b9e\u65f6\u63a7\u5236\u5e94\u7528\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u9700\u8981\u4ece\u90e8\u5206\u566a\u58f0\u6570\u636e\u4e2d\u9884\u6d4b\u9644\u8fd1\u667a\u80fd\u4f53\u7684\u8fd0\u52a8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5b9e\u65f6\u6761\u4ef6\u4e0b\u5b66\u4e60\u975e\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u6ed1\u52a8\u7a97\u53e3Hankel-DMD\u65b9\u6cd5\uff0c\u901a\u8fc7Hankel\u77e9\u9635\u5d4c\u5165\u90e8\u5206\u566a\u58f0\u6d4b\u91cf\uff0c\u5229\u7528Page\u77e9\u9635\u8fdb\u884c\u5947\u5f02\u503c\u786c\u9608\u503c\u4f30\u8ba1\u6709\u6548\u79e9\uff0c\u91c7\u7528Cadzow\u6295\u5f71\u786e\u4fdd\u7ed3\u6784\u5316\u4f4e\u79e9\u4e00\u81f4\u6027\uff0c\u6784\u5efa\u65f6\u53d8Hankel-DMD\u63d0\u5347\u7ebf\u6027\u9884\u6d4b\u5668\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u548c\u52a8\u6001\u8d77\u91cd\u673a\u5b9e\u9a8c\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u5728Gaussian\u548c\u91cd\u5c3e\u566a\u58f0\u4e0b\u5747\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684\u65b9\u5dee\u611f\u77e5\u53bb\u566a\u548c\u77ed\u65f6\u57df\u9884\u6d4b\uff0c\u9002\u5408\u96c6\u6210\u5230\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u65f6\u5b66\u4e60\u975e\u7ebf\u6027\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u4f9b\u65b9\u5dee\u8ddf\u8e2a\u4fe1\u53f7\u652f\u6301\u4e0b\u6e38\u4f30\u8ba1\u5668\u548c\u98ce\u9669\u611f\u77e5\u89c4\u5212\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u81ea\u4e3b\u7cfb\u7edf\u63a7\u5236\u3002"}}
{"id": "2511.01106", "categories": ["cs.HC", "H.5.2"], "pdf": "https://arxiv.org/pdf/2511.01106", "abs": "https://arxiv.org/abs/2511.01106", "authors": ["Guillaume Rivi\u00e8re"], "title": "Defining a Role-Centered Terminology for Physical Representations and Controls", "comment": "16 pages. Approximately 7000 words. 4 figures and 7 tables", "summary": "Previous classifications advanced research through a better understanding of\nthe field and the variety of tangible user interfaces and related physical user\ninterfaces, especially by discretizing a degree of tangibility based on the\nspecimens produced by the community over the years, since the conceptualization\nof Tangible User Interface initiated a research effort to deepen the\nexploration of the concept. However, no taxonomy enables the classification of\ntangible user interfaces at the application level. This article proposes to\nrefine the description of tangible user interfaces' interactional components\nthrough a terminological approach. The resulting terms are blended words, built\nfrom known words, that self-contain what digital role is represented or\ncontrolled and how it becomes physical. This holistic terminology then enables\nthe definition of applications' hallmarks and four classes of tangibility for\napplications, which surpass the description of physical user interface\nspecimens' morphology by abstracting and discriminating specimens at the\napplicative level. The descriptiveness and holisticness of the new terminology,\nas well as the clustering and discriminative power of the limited number of\nfour classes, are showed on a corpus of applicative tangible user interfaces'\nspecimens from the literature. Promising future work will benefit from the\nholistic terminology, the applications' hallmarks, and the tangibility classes,\nto describe applicative tangible user interfaces and related physical user\ninterfaces to better understand the dozens of specimens that were produced by\nthe field over three decades. Indeed, describing and classifying this whole set\nwould deepen our understanding to provide tools for future developers and\ndesigners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672f\u8bed\u65b9\u6cd5\u6765\u63cf\u8ff0\u6709\u5f62\u7528\u6237\u754c\u9762\u7684\u4ea4\u4e92\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6784\u5efa\u6df7\u5408\u8bcd\u6765\u5b9a\u4e49\u6570\u5b57\u89d2\u8272\u5982\u4f55\u7269\u7406\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9a\u4e49\u4e86\u5e94\u7528\u7684\u6807\u5fd7\u7279\u5f81\u548c\u56db\u79cd\u6709\u5f62\u6027\u7c7b\u522b\uff0c\u7528\u4e8e\u5728\u5e94\u7528\u5c42\u9762\u5206\u7c7b\u6709\u5f62\u7528\u6237\u754c\u9762\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6709\u5f62\u7528\u6237\u754c\u9762\u7269\u7406\u5f62\u6001\u7684\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u5728\u5e94\u7528\u5c42\u9762\u8fdb\u884c\u5206\u7c7b\u7684\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u672f\u8bed\u5b66\u65b9\u6cd5\u6539\u8fdb\u6709\u5f62\u7528\u6237\u754c\u9762\u4ea4\u4e92\u7ec4\u4ef6\u7684\u63cf\u8ff0\uff0c\u5b9e\u73b0\u5e94\u7528\u7ea7\u522b\u7684\u5206\u7c7b\u3002", "method": "\u91c7\u7528\u672f\u8bed\u5b66\u65b9\u6cd5\uff0c\u6784\u5efa\u6df7\u5408\u8bcd\u6765\u63cf\u8ff0\u6570\u5b57\u89d2\u8272\u5982\u4f55\u7269\u7406\u5316\uff0c\u8fd9\u4e9b\u6df7\u5408\u8bcd\u81ea\u6211\u5305\u542b\u4e86\u6240\u8868\u793a\u6216\u63a7\u5236\u7684\u6570\u5b57\u89d2\u8272\u53ca\u5176\u7269\u7406\u5316\u65b9\u5f0f\u3002\u57fa\u4e8e\u8fd9\u79cd\u6574\u4f53\u672f\u8bed\u5b9a\u4e49\u4e86\u5e94\u7528\u7684\u6807\u5fd7\u7279\u5f81\u548c\u56db\u79cd\u6709\u5f62\u6027\u7c7b\u522b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u5957\u6574\u4f53\u672f\u8bed\u7cfb\u7edf\uff0c\u80fd\u591f\u63cf\u8ff0\u5e94\u7528\u7ea7\u6709\u5f62\u7528\u6237\u754c\u9762\uff0c\u5e76\u5b9a\u4e49\u4e86\u56db\u79cd\u6709\u5f62\u6027\u7c7b\u522b\u3002\u901a\u8fc7\u5bf9\u6587\u732e\u4e2d\u6709\u5f62\u7528\u6237\u754c\u9762\u6837\u672c\u7684\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u65b0\u672f\u8bed\u7684\u63cf\u8ff0\u6027\u548c\u6574\u4f53\u6027\uff0c\u4ee5\u53ca\u56db\u79cd\u7c7b\u522b\u7684\u805a\u7c7b\u548c\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6574\u4f53\u672f\u8bed\u3001\u5e94\u7528\u6807\u5fd7\u7279\u5f81\u548c\u56db\u79cd\u6709\u5f62\u6027\u7c7b\u522b\u4e3a\u63cf\u8ff0\u548c\u5206\u7c7b\u5e94\u7528\u7ea7\u6709\u5f62\u7528\u6237\u754c\u9762\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u8be5\u9886\u57df\u4e09\u5341\u5e74\u6765\u4ea7\u751f\u7684\u6570\u5341\u4e2a\u6837\u672c\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u8005\u548c\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2511.00486", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u4e5f\u662f\u6700\u5927\u7684Bhili-Hindi-English\u5e73\u884c\u8bed\u6599\u5e93(BHEPC)\uff0c\u5305\u542b11\u4e07\u53e5\u7cbe\u5fc3\u6574\u7406\u7684\u53e5\u5b50\uff0c\u7528\u4e8e\u89e3\u51b3\u5370\u5ea6\u90e8\u843d\u8bed\u8a00Bhili\u7684\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\u3002\u901a\u8fc7\u8bc4\u4f30\u591a\u79cd\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u53d1\u73b0\u5fae\u8c03\u7684NLLB-200\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5370\u5ea6\u8bed\u8a00\u591a\u6837\u6027\u5e26\u6765\u673a\u5668\u7ffb\u8bd1\u6311\u6218\uff0c\u7279\u522b\u662f\u50cfBhili\u8fd9\u6837\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u90e8\u843d\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bed\u8a00\u8d44\u6e90\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u8d44\u6e90\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5305\u5bb9\u6027\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b11\u4e07\u53e5\u7684Bhili-Hindi-English\u5e73\u884c\u8bed\u6599\u5e93(BHEPC)\uff0c\u6db5\u76d6\u6559\u80b2\u3001\u884c\u653f\u548c\u65b0\u95fb\u7b49\u5173\u952e\u9886\u57df\u3002\u8bc4\u4f30\u4e86\u4e13\u6709\u548c\u5f00\u6e90\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u82f1\u8bed/\u5370\u5730\u8bed\u4e0eBhili\u4e4b\u95f4\u7684\u53cc\u5411\u7ffb\u8bd1\u4efb\u52a1\uff0c\u5e76\u7814\u7a76\u4e86\u591a\u8bed\u8a00LLM\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u751f\u6210\u7ffb\u8bd1\u80fd\u529b\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u5fae\u8c03\u7684NLLB-200\u84b8\u998f600M\u53d8\u4f53\u6a21\u578b\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u7a81\u663e\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u6027\u80fd\u5e76\u91cf\u5316\u4e86\u5206\u5e03\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86\u5173\u952e\u8d44\u6e90\u7a7a\u767d\uff0c\u4e3a\u5168\u7403\u4f4e\u8d44\u6e90\u548c\u8fb9\u7f18\u5316\u8bed\u8a00\u4fc3\u8fdb\u4e86\u5305\u5bb9\u6027\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4e3a\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u5efa\u7acb\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00114", "abs": "https://arxiv.org/abs/2511.00114", "authors": ["Hanae Elmekki", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Ahmed Alagha", "Hani Sami", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Azzam Mourad", "Sami Muhaidat"], "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning", "comment": null, "summary": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in\ncardiology for assessing heart health, but its effectiveness is limited by\noperator dependence, time constraints, and human error. The shortage of trained\nprofessionals, especially in remote areas, further restricts access. These\nissues underscore the need for automated solutions that can ensure consistent,\nand accessible cardiac imaging regardless of operator skill or location. Recent\nprogress in artificial intelligence (AI), especially in deep reinforcement\nlearning (DRL), has gained attention for enabling autonomous decision-making.\nHowever, existing DRL-based approaches to cardiac US scanning lack\nreproducibility, rely on proprietary data, and use simplified models. Motivated\nby these gaps, we present the first end-to-end framework that integrates\ngenerative AI and DRL to enable autonomous and reproducible cardiac US\nscanning. The framework comprises two components: (i) a conditional generative\nsimulator combining Generative Adversarial Networks (GANs) with Variational\nAutoencoders (VAEs), that models the cardiac US environment producing realistic\naction-conditioned images; and (ii) a DRL module that leverages this simulator\nto learn autonomous, accurate scanning policies. The proposed framework\ndelivers AI-driven guidance through expert-validated models that classify image\ntype and assess quality, supports conditional generation of realistic US\nimages, and establishes a reproducible foundation extendable to other organs.\nTo ensure reproducibility, a publicly available dataset of real cardiac US\nscans is released. The solution is validated through several experiments. The\nVAE-GAN is benchmarked against existing GAN variants, with performance assessed\nusing qualitative and quantitative approaches, while the DRL-based scanning\nsystem is evaluated under varying configurations to demonstrate effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u96c6\u6210\u751f\u6210AI\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u81ea\u4e3b\u3001\u53ef\u91cd\u590d\u7684\u5fc3\u810f\u8d85\u58f0\u626b\u63cf\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u64cd\u4f5c\u8005\u3001\u7f3a\u4e4f\u53ef\u91cd\u590d\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u5fc3\u810f\u8d85\u58f0\u8bca\u65ad\u5b58\u5728\u64cd\u4f5c\u8005\u4f9d\u8d56\u6027\u5f3a\u3001\u65f6\u95f4\u9650\u5236\u3001\u4eba\u4e3a\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u4e14\u504f\u8fdc\u5730\u533a\u7f3a\u4e4f\u4e13\u4e1a\u533b\u751f\u3002\u73b0\u6709AI\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u91cd\u590d\u6027\u3001\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u3001\u4f7f\u7528\u7b80\u5316\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u4e3b\u3001\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a(1) \u6761\u4ef6\u751f\u6210\u6a21\u62df\u5668\uff0c\u7ed3\u5408GAN\u548cVAE\u751f\u6210\u903c\u771f\u7684\u52a8\u4f5c\u6761\u4ef6\u56fe\u50cf\uff1b(2) \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\uff0c\u5229\u7528\u6a21\u62df\u5668\u5b66\u4e60\u81ea\u4e3b\u3001\u51c6\u786e\u7684\u626b\u63cf\u7b56\u7565\u3002", "result": "VAE-GAN\u5728\u6027\u80fd\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709GAN\u53d8\u4f53\uff0cDRL\u626b\u63cf\u7cfb\u7edf\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u516c\u5f00\u53ef\u7528\u7684\u771f\u5b9e\u5fc3\u810f\u8d85\u58f0\u6570\u636e\u96c6\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6a21\u578b\u63d0\u4f9bAI\u9a71\u52a8\u6307\u5bfc\uff0c\u652f\u6301\u751f\u6210\u903c\u771f\u8d85\u58f0\u56fe\u50cf\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u5668\u5b98\u7684\u53ef\u91cd\u590d\u57fa\u7840\uff0c\u4e3a\u89e3\u51b3\u5fc3\u810f\u8d85\u58f0\u626b\u63cf\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.00551", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00551", "abs": "https://arxiv.org/abs/2511.00551", "authors": ["Qiang Li", "Ningjing Zeng", "Lina Yu"], "title": "Single-agent Reinforcement Learning Model for Regional Adaptive Traffic Signal Control", "comment": null, "summary": "Several studies have employed reinforcement learning (RL) to address the\nchallenges of regional adaptive traffic signal control (ATSC) and achieved\npromising results. In this field, existing research predominantly adopts\nmulti-agent frameworks. However, the adoption of multi-agent frameworks\npresents challenges for scalability. Instead, the Traffic signal control (TSC)\nproblem necessitates a single-agent framework. TSC inherently relies on\ncentralized management by a single control center, which can monitor traffic\nconditions across all roads in the study area and coordinate the control of all\nintersections. This work proposes a single-agent RL-based regional ATSC model\ncompatible with probe vehicle technology. Key components of the RL design\ninclude state, action, and reward function definitions. To facilitate learning\nand manage congestion, both state and reward functions are defined based on\nqueue length, with action designed to regulate queue dynamics. The queue length\ndefinition used in this study differs slightly from conventional definitions\nbut is closely correlated with congestion states. More importantly, it allows\nfor reliable estimation using link travel time data from probe vehicles. With\nprobe vehicle data already covering most urban roads, this feature enhances the\nproposed method's potential for widespread deployment. The method was\ncomprehensively evaluated using the SUMO simulation platform. Experimental\nresults demonstrate that the proposed model effectively mitigates large-scale\nregional congestion levels via coordinated multi-intersection control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u533a\u57df\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u6a21\u578b\uff0c\u517c\u5bb9\u6d6e\u52a8\u8f66\u6280\u672f\uff0c\u901a\u8fc7\u961f\u5217\u957f\u5ea6\u5b9a\u4e49\u72b6\u6001\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5728SUMO\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u7f13\u89e3\u5927\u89c4\u6a21\u533a\u57df\u62e5\u5835\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5904\u7406\u533a\u57df\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u4f46\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u672c\u8d28\u4e0a\u9700\u8981\u7531\u5355\u4e00\u63a7\u5236\u4e2d\u5fc3\u8fdb\u884c\u96c6\u4e2d\u7ba1\u7406\uff0c\u80fd\u591f\u76d1\u63a7\u6240\u6709\u9053\u8def\u7684\u4ea4\u901a\u72b6\u51b5\u5e76\u534f\u8c03\u6240\u6709\u4ea4\u53c9\u53e3\u7684\u63a7\u5236\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5355\u667a\u80fd\u4f53\u533a\u57dfATSC\u6a21\u578b\uff0c\u72b6\u6001\u548c\u5956\u52b1\u51fd\u6570\u57fa\u4e8e\u961f\u5217\u957f\u5ea6\u5b9a\u4e49\uff0c\u52a8\u4f5c\u8bbe\u8ba1\u7528\u4e8e\u8c03\u8282\u961f\u5217\u52a8\u6001\u3002\u961f\u5217\u957f\u5ea6\u5b9a\u4e49\u4e0e\u4f20\u7edf\u7565\u6709\u4e0d\u540c\u4f46\u4e0e\u62e5\u5835\u72b6\u6001\u5bc6\u5207\u76f8\u5173\uff0c\u4e14\u53ef\u5229\u7528\u6d6e\u52a8\u8f66\u7684\u8def\u6bb5\u884c\u7a0b\u65f6\u95f4\u6570\u636e\u8fdb\u884c\u53ef\u9760\u4f30\u8ba1\u3002", "result": "\u5728SUMO\u4eff\u771f\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u534f\u8c03\u591a\u4ea4\u53c9\u53e3\u63a7\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u89c4\u6a21\u533a\u57df\u62e5\u5835\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u533a\u57df\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u95ee\u9898\uff0c\u4e14\u7531\u4e8e\u517c\u5bb9\u6d6e\u52a8\u8f66\u6280\u672f\uff0c\u5177\u6709\u5e7f\u6cdb\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.00840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00840", "abs": "https://arxiv.org/abs/2511.00840", "authors": ["William Suliman", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches", "comment": null, "summary": "This work presents an extended framework for learning-based bipedal\nlocomotion that incorporates a heuristic step-planning strategy guided by\ndesired torso velocity tracking. The framework enables precise interaction\nbetween a humanoid robot and its environment, supporting tasks such as crossing\ngaps and accurately approaching target objects. Unlike approaches based on full\nor simplified dynamics, the proposed method avoids complex step planners and\nanalytical models. Step planning is primarily driven by heuristic commands,\nwhile a Raibert-type controller modulates the foot placement length based on\nthe error between desired and actual torso velocity. We compare our method with\na model-based step-planning approach -- the Linear Inverted Pendulum Model\n(LIPM) controller. Experimental results demonstrate that our approach attains\ncomparable or superior accuracy in maintaining target velocity (up to 80%),\nsignificantly greater robustness on uneven terrain (over 50% improvement), and\nimproved energy efficiency. These results suggest that incorporating complex\nanalytical, model-based components into the training architecture may be\nunnecessary for achieving stable and robust bipedal walking, even in\nunstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u53cc\u8db3\u673a\u5668\u4eba\u884c\u8d70\u6846\u67b6\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u6b65\u6001\u89c4\u5212\u7b56\u7565\u548c\u671f\u671b\u8eaf\u5e72\u901f\u5ea6\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u884c\u8d70\uff0c\u5305\u62ec\u8de8\u8d8a\u95f4\u9699\u548c\u7cbe\u786e\u63a5\u8fd1\u76ee\u6807\u7b49\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5b8c\u6574\u6216\u7b80\u5316\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u6b65\u6001\u89c4\u5212\u5668\u548c\u89e3\u6790\u6a21\u578b\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u590d\u6742\u6a21\u578b\u7ec4\u4ef6\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u53cc\u8db3\u884c\u8d70\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u547d\u4ee4\u9a71\u52a8\u6b65\u6001\u89c4\u5212\uff0c\u7ed3\u5408Raibert\u578b\u63a7\u5236\u5668\u6839\u636e\u671f\u671b\u4e0e\u5b9e\u9645\u8eaf\u5e72\u901f\u5ea6\u8bef\u5dee\u8c03\u8282\u8db3\u90e8\u653e\u7f6e\u957f\u5ea6\u3002\u4e0e\u57fa\u4e8e\u7ebf\u6027\u5012\u7acb\u6446\u6a21\u578b(LIPM)\u7684\u6a21\u578b\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7ef4\u6301\u76ee\u6807\u901f\u5ea6\u65b9\u9762\u8fbe\u5230\u53ef\u6bd4\u6216\u66f4\u4f18\u7cbe\u5ea6(\u6700\u9ad880%)\uff0c\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u9c81\u68d2\u6027\u663e\u8457\u63d0\u9ad8(\u8d85\u8fc750%\u6539\u8fdb)\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u67b6\u6784\u4e2d\u5f15\u5165\u590d\u6742\u7684\u89e3\u6790\u6a21\u578b\u7ec4\u4ef6\u5bf9\u4e8e\u5b9e\u73b0\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u53cc\u8db3\u884c\u8d70\u53ef\u80fd\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u5373\u4f7f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2511.00487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6570\u636e\u96c6\u5927\u5c0f\u56e0\u7d20\u5f15\u5165\u5dee\u5206\u9690\u79c1\u6587\u672c\u9690\u79c1\u5316\u8bc4\u4f30\u4e2d\uff0c\u901a\u8fc7\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u52a8\u6001\u5206\u5272\u7684\u6548\u7528\u548c\u9690\u79c1\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u9690\u79c1-\u6548\u7528\u6743\u8861\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u5728\u8bc4\u4f30\u6587\u672c\u91cd\u5199\u673a\u5236\u65f6\u5f80\u5f80\u5ffd\u7565\u6570\u636e\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u673a\u5236\u6548\u7528\u548c\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u6548\u7528\u548c\u9690\u79c1\u6d4b\u8bd5\uff0c\u4f7f\u7528\u52a8\u6001\u5206\u5272\u5927\u5c0f\uff0c\u5728\u5305\u542b\u591a\u8fbe100\u4e07\u4e2a\u6587\u672c\u7684\u4e0d\u540c\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fd0\u884c\u6d4b\u8bd5\uff0c\u91cf\u5316\u6570\u636e\u96c6\u5927\u5c0f\u589e\u52a0\u5bf9\u9690\u79c1-\u6548\u7528\u6743\u8861\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u5728\u8bc4\u4f30\u5dee\u5206\u9690\u79c1\u6587\u672c\u91cd\u5199\u673a\u5236\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u7684\u53d8\u5316\u4f1a\u663e\u8457\u5f71\u54cd\u9690\u79c1\u4fdd\u62a4\u4e0e\u6570\u636e\u6548\u7528\u4e4b\u95f4\u7684\u5e73\u8861\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8981\u6c42\u5dee\u5206\u9690\u79c1\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u91c7\u7528\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u5e76\u4e3a\u5dee\u5206\u9690\u79c1\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u5b9e\u8df5\u548c\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2511.00120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00120", "abs": "https://arxiv.org/abs/2511.00120", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images", "comment": "This paper has been accepted to IEIE( The Institute Of Electronics\n  and Information Engineering, South Korea) Fall,2025 Conference", "summary": "The primary challenge in computer vision is precisely calculating the pose of\n6D objects, however many current approaches are still fragile and have trouble\ngeneralizing from synthetic data to real-world situations with fluctuating\nlighting, textureless objects, and significant occlusions. To address these\nlimitations, VLM6D, a novel dual-stream architecture that leverages the\ndistinct strengths of visual and geometric data from RGB-D input for robust and\nprecise pose estimation. Our framework uniquely integrates two specialized\nencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the\nRGB modality, harnessing its rich, pre-trained understanding of visual grammar\nto achieve remarkable resilience against texture and lighting variations.\nConcurrently, a PointNet++ encoder processes the 3D point cloud derived from\ndepth data, enabling robust geometric reasoning that excels even with the\nsparse, fragmented data typical of severe occlusion. These complementary\nfeature streams are effectively fused to inform a multi task prediction head.\nWe demonstrate through comprehensive experiments that VLM6D obtained new SOTA\nperformance on the challenging Occluded-LineMOD, validating its superior\nrobustness and accuracy.", "AI": {"tldr": "VLM6D\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6d41\u67b6\u6784\uff0c\u5229\u7528RGB-D\u8f93\u5165\u7684\u89c6\u89c9\u548c\u51e0\u4f55\u6570\u636e\u4f18\u52bf\uff0c\u901a\u8fc7\u96c6\u6210\u81ea\u76d1\u7763\u89c6\u89c9\u53d8\u6362\u5668\u548cPointNet++\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u9c81\u68d2\u4e14\u7cbe\u786e\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d6D\u7269\u4f53\u59ff\u6001\u7cbe\u786e\u8ba1\u7b97\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u524d\u65b9\u6cd5\u5728\u4ece\u5408\u6210\u6570\u636e\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65f6\uff0c\u9762\u5bf9\u5149\u7167\u53d8\u5316\u3001\u65e0\u7eb9\u7406\u7269\u4f53\u548c\u4e25\u91cd\u906e\u6321\u65f6\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u67b6\u6784\uff1a\u4f7f\u7528\u81ea\u76d1\u7763\u89c6\u89c9\u53d8\u6362\u5668(DINOv2)\u5904\u7406RGB\u6a21\u6001\uff0c\u5229\u7528\u5176\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff1b\u540c\u65f6\u4f7f\u7528PointNet++\u7f16\u7801\u5668\u5904\u7406\u6df1\u5ea6\u6570\u636e\u751f\u6210\u76843D\u70b9\u4e91\u3002\u4e24\u79cd\u4e92\u8865\u7279\u5f81\u6d41\u6709\u6548\u878d\u5408\u540e\u8f93\u5165\u591a\u4efb\u52a1\u9884\u6d4b\u5934\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684Occluded-LineMOD\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "VLM6D\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u548c\u51e0\u4f55\u6570\u636e\u7684\u4f18\u52bf\uff0c\u6210\u529f\u89e3\u51b3\u4e866D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\u3002"}}
{"id": "2511.00609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00609", "abs": "https://arxiv.org/abs/2511.00609", "authors": ["Shengqi Xu", "Xinpeng Zhou", "Yabo Zhang", "Ming Liu", "Tao Liang", "Tianyu Zhang", "Yalong Bai", "Zuxuan Wu", "Wangmeng Zuo"], "title": "PreferThinker: Reasoning-based Personalized Image Preference Assessment", "comment": null, "summary": "Personalized image preference assessment aims to evaluate an individual\nuser's image preferences by relying only on a small set of reference images as\nprior information. Existing methods mainly focus on general preference\nassessment, training models with large-scale data to tackle well-defined tasks\nsuch as text-image alignment. However, these approaches struggle to handle\npersonalized preference because user-specific data are scarce and not easily\nscalable, and individual tastes are often diverse and complex. To overcome\nthese challenges, we introduce a common preference profile that serves as a\nbridge across users, allowing large-scale user data to be leveraged for\ntraining profile prediction and capturing complex personalized preferences.\nBuilding on this idea, we propose a reasoning-based personalized image\npreference assessment framework that follows a \\textit{predict-then-assess}\nparadigm: it first predicts a user's preference profile from reference images,\nand then provides interpretable, multi-dimensional scores and assessments of\ncandidate images based on the predicted profile. To support this, we first\nconstruct a large-scale Chain-of-Thought (CoT)-style personalized assessment\ndataset annotated with diverse user preference profiles and high-quality\nCoT-style reasoning, enabling explicit supervision of structured reasoning.\nNext, we adopt a two-stage training strategy: a cold-start supervised\nfine-tuning phase to empower the model with structured reasoning capabilities,\nfollowed by reinforcement learning to incentivize the model to explore more\nreasonable assessment paths and enhance generalization. Furthermore, we propose\na similarity-aware prediction reward to encourage better prediction of the\nuser's preference profile, which facilitates more reasonable assessments\nexploration. Extensive experiments demonstrate the superiority of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a8\u7406\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u504f\u597d\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u504f\u597d\u6863\u6848\u5e76\u57fa\u4e8e\u8be5\u6863\u6848\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u5206\uff0c\u89e3\u51b3\u4e2a\u6027\u5316\u504f\u597d\u8bc4\u4f30\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u7528\u6237\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u504f\u597d\u8bc4\u4f30\uff0c\u96be\u4ee5\u5904\u7406\u4e2a\u6027\u5316\u504f\u597d\uff0c\u56e0\u4e3a\u7528\u6237\u7279\u5b9a\u6570\u636e\u7a00\u7f3a\u4e14\u4e2a\u4f53\u54c1\u5473\u590d\u6742\u591a\u6837\u3002", "method": "\u91c7\u7528\u9884\u6d4b-\u8bc4\u4f30\u8303\u5f0f\uff1a\u9996\u5148\u4ece\u53c2\u8003\u56fe\u50cf\u9884\u6d4b\u7528\u6237\u504f\u597d\u6863\u6848\uff0c\u7136\u540e\u57fa\u4e8e\u9884\u6d4b\u6863\u6848\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u7ef4\u5ea6\u8bc4\u5206\uff1b\u6784\u5efa\u5927\u89c4\u6a21CoT\u98ce\u683c\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u63d0\u51fa\u76f8\u4f3c\u6027\u611f\u77e5\u9884\u6d4b\u5956\u52b1\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u901a\u7528\u504f\u597d\u6863\u6848\u4f5c\u4e3a\u7528\u6237\u95f4\u7684\u6865\u6881\uff0c\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u7528\u6237\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u4e2a\u6027\u5316\u504f\u597d\u5e76\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002"}}
{"id": "2511.01248", "categories": ["cs.HC", "H.5.2; K.3.1"], "pdf": "https://arxiv.org/pdf/2511.01248", "abs": "https://arxiv.org/abs/2511.01248", "authors": ["Ziqi Liu", "Yuankun Wang", "Hui-Ru Ho", "Yuheng Wu", "Yuhang Zhao", "Bilge Mutlu"], "title": "AskNow: An LLM-powered Interactive System for Real-Time Question Answering in Large-Scale Classrooms", "comment": "18 pages, 9 figures", "summary": "In large-scale classrooms, students often struggle to ask questions due to\nlimited instructor attention and social pressure. Based on findings from a\nformative study with 24 students and 12 instructors, we designed AskNow, an\nLLM-powered system that enables students to ask questions and receive\nreal-time, context-aware responses grounded in the ongoing lecture and that\nallows instructors to view students' questions collectively. We deployed AskNow\nin three university computer science courses and tested with 117 students. To\nevaluate AskNow's responses, each instructor rated the perceived correctness\nand satisfaction of 100 randomly sampled AskNow-generated responses. In\naddition, we conducted interviews with 24 students and the three instructors to\nunderstand their experience with AskNow. We found that AskNow significantly\nreduced students' perceived time to resolve confusion. Instructors rated\nAskNow's responses as highly accurate and satisfactory. Instructor and student\nfeedback provided insights into supporting real-time learning in large lecture\nsettings.", "AI": {"tldr": "AskNow\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u8bfe\u5802\u4e2d\u5b66\u751f\u56e0\u6559\u5e08\u5173\u6ce8\u6709\u9650\u548c\u793e\u4ea4\u538b\u529b\u800c\u96be\u4ee5\u63d0\u95ee\u7684\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u5141\u8bb8\u5b66\u751f\u63d0\u95ee\u5e76\u83b7\u5f97\u57fa\u4e8e\u8bb2\u5ea7\u5185\u5bb9\u7684\u5b9e\u65f6\u54cd\u5e94\uff0c\u540c\u65f6\u8ba9\u6559\u5e08\u67e5\u770b\u5b66\u751f\u95ee\u9898\u6c47\u603b\u3002\u5728\u4e09\u4e2a\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e2d\u90e8\u7f72\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793aAskNow\u663e\u8457\u51cf\u5c11\u4e86\u5b66\u751f\u89e3\u51b3\u56f0\u60d1\u7684\u65f6\u95f4\uff0c\u6559\u5e08\u5bf9\u5176\u54cd\u5e94\u51c6\u786e\u6027\u548c\u6ee1\u610f\u5ea6\u8bc4\u4ef7\u5f88\u9ad8\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u8bfe\u5802\u4e2d\uff0c\u5b66\u751f\u5e38\u5e38\u56e0\u4e3a\u6559\u5e08\u5173\u6ce8\u6709\u9650\u548c\u793e\u4ea4\u538b\u529b\u800c\u96be\u4ee5\u63d0\u95ee\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b66\u4e60\u6548\u679c\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5e2e\u52a9\u5b66\u751f\u53ca\u65f6\u89e3\u51b3\u7591\u60d1\u3002", "method": "\u57fa\u4e8e\u5bf924\u540d\u5b66\u751f\u548c12\u540d\u6559\u5e08\u7684\u5f62\u6210\u6027\u7814\u7a76\uff0c\u8bbe\u8ba1\u4e86AskNow\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5b66\u751f\u63d0\u4f9b\u57fa\u4e8e\u8bb2\u5ea7\u5185\u5bb9\u7684\u5b9e\u65f6\u54cd\u5e94\uff0c\u5e76\u5141\u8bb8\u6559\u5e08\u67e5\u770b\u95ee\u9898\u6c47\u603b\u3002\u5728\u4e09\u4e2a\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e2d\u90e8\u7f72\u6d4b\u8bd5\uff0c\u6d89\u53ca117\u540d\u5b66\u751f\u3002\u901a\u8fc7\u6559\u5e08\u5bf9100\u4e2a\u968f\u673a\u6837\u672c\u54cd\u5e94\u7684\u8bc4\u4f30\uff0c\u4ee5\u53ca24\u540d\u5b66\u751f\u548c3\u540d\u6559\u5e08\u7684\u8bbf\u8c08\u6765\u8bc4\u4f30\u7cfb\u7edf\u6548\u679c\u3002", "result": "AskNow\u663e\u8457\u51cf\u5c11\u4e86\u5b66\u751f\u89e3\u51b3\u56f0\u60d1\u7684\u65f6\u95f4\u3002\u6559\u5e08\u5bf9AskNow\u751f\u6210\u7684\u54cd\u5e94\u51c6\u786e\u6027\u548c\u6ee1\u610f\u5ea6\u8bc4\u4ef7\u5f88\u9ad8\u3002\u5b66\u751f\u548c\u6559\u5e08\u7684\u53cd\u9988\u4e3a\u652f\u6301\u5927\u8bfe\u5802\u5b9e\u65f6\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "conclusion": "AskNow\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bfe\u5802\u4e2d\u5b66\u751f\u63d0\u95ee\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u4f9b\u5b9e\u65f6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u54cd\u5e94\u663e\u8457\u6539\u5584\u4e86\u5b66\u4e60\u4f53\u9a8c\u3002\u8be5\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u6559\u80b2\u73af\u5883\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u652f\u6301\u5b9e\u65f6\u5b66\u4e60\u548c\u5e08\u751f\u4e92\u52a8\u3002"}}
{"id": "2511.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM .", "AI": {"tldr": "ToM\u662f\u4e00\u4e2a\u9762\u5411\u6811\u7ed3\u6784\u7684MapReduce\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6587\u6863\u7684\u5c42\u6b21\u7ed3\u6784\u8fdb\u884c\u9012\u5f52\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5206\u6cbb\u6846\u67b6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u6392\u540d\uff0c\u727a\u7272\u4e86\u903b\u8f91\u8fde\u8d2f\u6027\uff1b\u5206\u6cbb\u6846\u67b6\u867d\u7136\u80fd\u8fdb\u884c\u5c40\u90e8\u63a8\u7406\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e14\u7531\u4e8e\u5b64\u7acb\u5904\u7406\u7247\u6bb5\u800c\u53ef\u80fd\u5f15\u53d1\u51b2\u7a81\u3002", "method": "ToM\u901a\u8fc7\u5c42\u6b21\u8bed\u4e49\u89e3\u6790\u6784\u5efa\u6587\u6863\u6811\uff08DocTree\uff09\uff0c\u91c7\u7528\u6811\u5f62MapReduce\u65b9\u6cd5\u8fdb\u884c\u9012\u5f52\u63a8\u7406\uff1a\u5728Map\u6b65\u9aa4\u4e2d\uff0c\u5728\u5b50\u8282\u70b9\u751f\u6210\u63a8\u7406\u4f9d\u636e\uff1b\u5728Reduce\u6b65\u9aa4\u4e2d\uff0c\u8de8\u5144\u5f1f\u8282\u70b9\u805a\u5408\u8fd9\u4e9b\u63a8\u7406\u4f9d\u636e\uff0c\u4ee5\u5728\u7236\u8282\u70b9\u89e3\u51b3\u51b2\u7a81\u6216\u8fbe\u6210\u5171\u8bc6\u3002", "result": "\u572870B+\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cToM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5206\u6cbb\u6846\u67b6\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ToM\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6587\u6863\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u9012\u5f52\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6587\u6863\u5904\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00123", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00123", "abs": "https://arxiv.org/abs/2511.00123", "authors": ["Gaby Maroun", "Salah Eddine Bekhouche", "Fadi Dornaika"], "title": "Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation", "comment": null, "summary": "Age estimation from facial images is a complex and multifaceted challenge in\ncomputer vision. In this study, we present a novel hybrid architecture that\ncombines ConvNeXt, a state-of-the-art advancement of convolutional neural\nnetworks (CNNs), with Vision Transformers (ViT). While each model independently\ndelivers excellent performance on a variety of tasks, their integration\nleverages the complementary strengths of the CNNs localized feature extraction\ncapabilities and the Transformers global attention mechanisms. Our proposed\nConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age\nestimation datasets, including MORPH II, CACD, and AFAD, and achieved superior\nperformance in terms of mean absolute error (MAE). To address computational\nconstraints, we leverage pre-trained models and systematically explore\ndifferent configurations, using linear layers and advanced regularization\ntechniques to optimize the architecture. Comprehensive ablation studies\nhighlight the critical role of individual components and training strategies,\nand in particular emphasize the importance of adapted attention mechanisms\nwithin the CNN framework to improve the model focus on age-relevant facial\nfeatures. The results show that the ConvNeXt-ViT hybrid not only outperforms\ntraditional methods, but also provides a robust foundation for future advances\nin age estimation and related visual tasks. This work underscores the\ntransformative potential of hybrid architectures and represents a promising\ndirection for the seamless integration of CNNs and transformers to address\ncomplex computer vision challenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408ConvNeXt\u548cVision Transformers\u7684\u6df7\u5408\u67b6\u6784\u7528\u4e8e\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5e74\u9f84\u4f30\u8ba1\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u590d\u6742\u6311\u6218\uff0c\u9700\u8981\u7ed3\u5408CNN\u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548cTransformer\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528ConvNeXt-ViT\u6df7\u5408\u67b6\u6784\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u7ebf\u6027\u5c42\u548c\u9ad8\u7ea7\u6b63\u5219\u5316\u6280\u672f\u4f18\u5316\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u9002\u5e94\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5173\u6ce8\u5e74\u9f84\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728MORPH II\u3001CACD\u548cAFAD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684MAE\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "ConvNeXt-ViT\u6df7\u5408\u67b6\u6784\u4e0d\u4ec5\u6027\u80fd\u4f18\u5f02\uff0c\u8fd8\u4e3a\u5e74\u9f84\u4f30\u8ba1\u548c\u76f8\u5173\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5065\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u67b6\u6784\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2511.00640", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning.", "AI": {"tldr": "DTS\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9ad8\u71b5token\u5904\u9009\u62e9\u6027\u5206\u652f\u5e76\u5e94\u7528\u65e9\u671f\u505c\u6b62\u6765\u9009\u62e9\u6700\u77ed\u7684\u5b8c\u6574\u63a8\u7406\u8def\u5f84\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5e38\u5e38\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u4ea7\u751f\u8fc7\u957f\u7684\u601d\u7ef4\u94fe\u75d5\u8ff9\uff0c\u8fd9\u4f1a\u589e\u52a0\u63a8\u7406\u6210\u672c\u5e76\u53ef\u80fd\u964d\u4f4e\u51c6\u786e\u6027\u3002\u5206\u6790\u53d1\u73b0\u63a8\u7406\u957f\u5ea6\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\u3002", "method": "\u63d0\u51faDTS\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9ad8\u71b5token\u5904\u9009\u62e9\u6027\u5206\u652f\u6765\u52fe\u52d2\u63a8\u7406\u7a7a\u95f4\uff0c\u5e76\u5e94\u7528\u65e9\u671f\u505c\u6b62\u6765\u9009\u62e9\u6700\u77ed\u7684\u5df2\u5b8c\u6210\u63a8\u7406\u8def\u5f84\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u76d1\u7763\u3002", "result": "\u5728AIME2024\u548cAIME2025\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDTS\u5c06\u51c6\u786e\u6027\u63d0\u9ad8\u4e868%\uff0c\u5e73\u5747\u63a8\u7406\u957f\u5ea6\u51cf\u5c11\u4e8623%\uff0c\u91cd\u590d\u9891\u7387\u964d\u4f4e\u4e8612%\u3002", "conclusion": "DTS\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\uff0c\u8fd1\u4f3c\u6700\u4f18\u89e3\uff0c\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.00933", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00933", "abs": "https://arxiv.org/abs/2511.00933", "authors": ["Xiangyu Shi", "Zerui Li", "Yanyuan Qiao", "Qi Wu"], "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation", "comment": null, "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.", "AI": {"tldr": "Fast-SmartWay\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4e09\u4e2a\u524d\u5411RGB-D\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u65e0\u9700\u5168\u666f\u89c6\u56fe\u548c\u8def\u5f84\u70b9\u9884\u6d4b\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u5168\u666f\u89c2\u6d4b\u548c\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u5bfc\u81f4\u663e\u8457\u5ef6\u8fdf\u5e76\u9650\u5236\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002", "method": "\u63d0\u51faFast-SmartWay\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4e09\u4e2a\u524d\u5411RGB-D\u56fe\u50cf\u7ed3\u5408\u8bed\u8a00\u6307\u4ee4\uff0c\u8ba9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u9884\u6d4b\u52a8\u4f5c\u3002\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\u6a21\u5757\uff0c\u5305\u62ec\u6d88\u6b67\u6a21\u5757\u548c\u672a\u6765-\u8fc7\u53bb\u53cc\u5411\u63a8\u7406\u673a\u5236\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6bcf\u6b65\u5ef6\u8fdf\uff0c\u540c\u65f6\u8fbe\u5230\u6216\u4f18\u4e8e\u57fa\u4e8e\u5168\u666f\u89c6\u56fe\u7684\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "Fast-SmartWay\u8bc1\u660e\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u96f6\u6837\u672c\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u65f6\u5bfc\u822a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01336", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01336", "abs": "https://arxiv.org/abs/2511.01336", "authors": ["Ibrahim Khalilov", "Chaoran Chen", "Ziang Xiao", "Tianshi Li", "Toby Jia-Jun Li", "Yaxing Yao"], "title": "Beyond Permissions: Investigating Mobile Personalization with Simulated Personas", "comment": "8 pages, 7 figures. Accepted to the ACM Workshop on Human-Centered AI\n  Privacy and Security (HAIPS @ CCS 2025). DOI: 10.1145/3733816.3760758 (ACM\n  Digital Library link pending activation)", "summary": "Mobile applications increasingly rely on sensor data to infer user context\nand deliver personalized experiences. Yet the mechanisms behind this\npersonalization remain opaque to users and researchers alike. This paper\npresents a sandbox system that uses sensor spoofing and persona simulation to\naudit and visualize how mobile apps respond to inferred behaviors. Rather than\ntreating spoofing as adversarial, we demonstrate its use as a tool for\nbehavioral transparency and user empowerment. Our system injects multi-sensor\nprofiles - generated from structured, lifestyle-based personas - into Android\ndevices in real time, enabling users to observe app responses to contexts such\nas high activity, location shifts, or time-of-day changes. With automated\nscreenshot capture and GPT-4 Vision-based UI summarization, our pipeline helps\ndocument subtle personalization cues. Preliminary findings show measurable app\nadaptations across fitness, e-commerce, and everyday service apps such as\nweather and navigation. We offer this toolkit as a foundation for\nprivacy-enhancing technologies and user-facing transparency interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6c99\u76d2\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u6b3a\u9a97\u548c\u7528\u6237\u753b\u50cf\u6a21\u62df\u6765\u5ba1\u8ba1\u548c\u53ef\u89c6\u5316\u79fb\u52a8\u5e94\u7528\u5982\u4f55\u54cd\u5e94\u63a8\u65ad\u7684\u7528\u6237\u884c\u4e3a\uff0c\u65e8\u5728\u63d0\u9ad8\u884c\u4e3a\u900f\u660e\u5ea6\u548c\u7528\u6237\u8d4b\u6743\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56\u4f20\u611f\u5668\u6570\u636e\u6765\u63a8\u65ad\u7528\u6237\u4e0a\u4e0b\u6587\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u4f53\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u4e2a\u6027\u5316\u673a\u5236\u5bf9\u7528\u6237\u548c\u7814\u7a76\u4eba\u5458\u6765\u8bf4\u4ecd\u7136\u4e0d\u900f\u660e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528\u4f20\u611f\u5668\u6b3a\u9a97\u548c\u7528\u6237\u753b\u50cf\u6a21\u62df\u7684\u6c99\u76d2\u7cfb\u7edf\uff0c\u5b9e\u65f6\u5411Android\u8bbe\u5907\u6ce8\u5165\u57fa\u4e8e\u7ed3\u6784\u5316\u751f\u6d3b\u65b9\u5f0f\u7684\u4f20\u611f\u5668\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u622a\u56fe\u548cGPT-4\u89c6\u89c9UI\u6458\u8981\u8bb0\u5f55\u5e94\u7528\u54cd\u5e94\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u5728\u5065\u8eab\u3001\u7535\u5b50\u5546\u52a1\u548c\u65e5\u5e38\u670d\u52a1\u5e94\u7528\uff08\u5982\u5929\u6c14\u548c\u5bfc\u822a\uff09\u4e2d\u89c2\u5bdf\u5230\u53ef\u6d4b\u91cf\u7684\u5e94\u7528\u9002\u914d\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5de5\u5177\u5305\u4e3a\u9690\u79c1\u589e\u5f3a\u6280\u672f\u548c\u9762\u5411\u7528\u6237\u7684\u900f\u660e\u5ea6\u5e72\u9884\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.00505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance.", "AI": {"tldr": "Zero-RAG\u901a\u8fc7\u8bc6\u522b\u548c\u4fee\u526a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u5197\u4f59\u77e5\u8bc6\uff0c\u51cf\u5c11\u5916\u90e8\u8bed\u6599\u5e93\u89c4\u6a21\u5e76\u52a0\u901f\u68c0\u7d22\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u6269\u5c55\uff0c\u5916\u90e8\u8bed\u6599\u5e93\u4e0e\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u77e5\u8bc6\u5197\u4f59\uff0c\u8fd9\u589e\u52a0\u4e86\u5bc6\u96c6\u68c0\u7d22\u7684\u5de5\u4f5c\u91cf\u5e76\u53ef\u80fd\u635f\u5bb3RAG\u6027\u80fd\u3002", "method": "\u63d0\u51faMastery-Score\u6307\u6807\u8bc6\u522b\u5197\u4f59\u77e5\u8bc6\u8fdb\u884c\u4fee\u526a\uff0c\u4f7f\u7528Query Router\u548cNoise-Tolerant Tuning\u907f\u514d\u4e0d\u76f8\u5173\u6587\u6863\u5e72\u6270\uff0c\u63d0\u9ad8\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cZero-RAG\u5c06\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u4fee\u526a30%\uff0c\u68c0\u7d22\u9636\u6bb5\u52a0\u901f22%\uff0c\u4e14\u4e0d\u635f\u5bb3RAG\u6027\u80fd\u3002", "conclusion": "Zero-RAG\u6709\u6548\u89e3\u51b3\u4e86RAG\u4e2d\u7684\u77e5\u8bc6\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u7387\u3002"}}
{"id": "2511.00141", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00141", "abs": "https://arxiv.org/abs/2511.00141", "authors": ["Janghoon Cho", "Jungsoo Lee", "Munawar Hayat", "Kyuwoong Hwang", "Fatih Porikli", "Sungha Choi"], "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding", "comment": null, "summary": "Recent studies in long video understanding have harnessed the advanced\nvisual-language reasoning capabilities of Large Multimodal Models (LMMs),\ndriving the evolution of video-LMMs specialized for processing extended video\nsequences. However, the scalability of these models is severely limited by the\noverwhelming volume of visual tokens generated from extended video sequences.\nTo address this challenge, this paper proposes FLoC, an efficient visual token\ncompression framework based on the facility location function, a principled\napproach that swiftly selects a compact yet highly representative and diverse\nsubset of visual tokens within a predefined budget on the number of visual\ntokens. By integrating the lazy greedy algorithm, our method achieves\nremarkable efficiency gains by swiftly selecting a compact subset of tokens,\ndrastically reducing the number of visual tokens while guaranteeing\nnear-optimal performance. Notably, our approach is training-free,\nmodel-agnostic, and query-agnostic, providing a versatile solution that\nseamlessly integrates with diverse video-LLMs and existing workflows. Extensive\nevaluations on large-scale benchmarks, such as Video-MME, MLVU, and\nLongVideoBench, demonstrate that our framework consistently surpasses recent\ncompression techniques, highlighting not only its effectiveness and robustness\nin addressing the critical challenges of long video understanding, but also its\nefficiency in processing speed.", "AI": {"tldr": "FLoC\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bbe\u65bd\u4f4d\u7f6e\u51fd\u6570\u7684\u9ad8\u6548\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u7d27\u51d1\u4e14\u5177\u6709\u4ee3\u8868\u6027\u7684\u89c6\u89c9\u6807\u8bb0\u5b50\u96c6\u6765\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6807\u8bb0\u6570\u91cf\u8fc7\u591a\u95ee\u9898\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u9762\u4e34\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u8fc7\u591a\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u6765\u51cf\u5c11\u6807\u8bb0\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u8bbe\u65bd\u4f4d\u7f6e\u51fd\u6570\u539f\u7406\uff0c\u4f7f\u7528\u61d2\u60f0\u8d2a\u5fc3\u7b97\u6cd5\u5feb\u901f\u9009\u62e9\u7d27\u51d1\u4e14\u591a\u6837\u5316\u7684\u89c6\u89c9\u6807\u8bb0\u5b50\u96c6\uff0c\u5728\u9884\u5b9a\u4e49\u6807\u8bb0\u9884\u7b97\u5185\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u5728Video-MME\u3001MLVU\u548cLongVideoBench\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLoC\u6846\u67b6\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u538b\u7f29\u6280\u672f\uff0c\u5728\u6709\u6548\u6027\u548c\u5904\u7406\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FLoC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u4e14\u67e5\u8be2\u65e0\u5173\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u89c6\u9891LLM\u548c\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2511.00940", "categories": ["cs.RO", "cs.AI", "I.2.6"], "pdf": "https://arxiv.org/pdf/2511.00940", "abs": "https://arxiv.org/abs/2511.00940", "authors": ["Zhe Li", "Xiang Bai", "Jieyu Zhang", "Zhuangzhe Wu", "Che Xu", "Ying Li", "Chengkai Hou", "Shanghang Zhang"], "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model", "comment": "Accepted to the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Constructing accurate digital twins of articulated objects is essential for\nrobotic simulation training and embodied AI world model building, yet\nhistorically requires painstaking manual modeling or multi-stage pipelines. In\nthis work, we propose \\textbf{URDF-Anything}, an end-to-end automatic\nreconstruction framework based on a 3D multimodal large language model (MLLM).\nURDF-Anything utilizes an autoregressive prediction framework based on\npoint-cloud and text multimodal input to jointly optimize geometric\nsegmentation and kinematic parameter prediction. It implements a specialized\n$[SEG]$ token mechanism that interacts directly with point cloud features,\nenabling fine-grained part-level segmentation while maintaining consistency\nwith the kinematic parameter predictions. Experiments on both simulated and\nreal-world datasets demonstrate that our method significantly outperforms\nexisting approaches regarding geometric segmentation (mIoU 17\\% improvement),\nkinematic parameter prediction (average error reduction of 29\\%), and physical\nexecutability (surpassing baselines by 50\\%). Notably, our method exhibits\nexcellent generalization ability, performing well even on objects outside the\ntraining set. This work provides an efficient solution for constructing digital\ntwins for robotic simulation, significantly enhancing the sim-to-real transfer\ncapability.", "AI": {"tldr": "URDF-Anything\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u70b9\u4e91\u548c\u6587\u672c\u8f93\u5165\u8054\u5408\u4f18\u5316\u51e0\u4f55\u5206\u5272\u548c\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b57\u5b6a\u751f\u6784\u5efa\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\u3002", "motivation": "\u6784\u5efa\u7cbe\u786e\u7684\u5173\u8282\u7269\u4f53\u6570\u5b57\u5b6a\u751f\u5bf9\u673a\u5668\u4eba\u4eff\u771f\u8bad\u7ec3\u548c\u5177\u8eabAI\u4e16\u754c\u6a21\u578b\u6784\u5efa\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7e41\u7410\u7684\u624b\u52a8\u5efa\u6a21\u6216\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u70b9\u4e91\u548c\u6587\u672c\u591a\u6a21\u6001\u8f93\u5165\u7684\u81ea\u56de\u5f52\u9884\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e13\u95e8\u7684[SEG]\u4ee4\u724c\u673a\u5236\u4e0e\u70b9\u4e91\u7279\u5f81\u76f4\u63a5\u4ea4\u4e92\uff0c\u5728\u4fdd\u6301\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u90e8\u4ef6\u7ea7\u5206\u5272\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u5206\u5272\uff08mIoU\u63d0\u534717%\uff09\u3001\u8fd0\u52a8\u5b66\u53c2\u6570\u9884\u6d4b\uff08\u5e73\u5747\u8bef\u5dee\u51cf\u5c1129%\uff09\u548c\u7269\u7406\u53ef\u6267\u884c\u6027\uff08\u8d85\u8d8a\u57fa\u7ebf50%\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u8bad\u7ec3\u96c6\u5916\u7269\u4f53\u4e0a\u8868\u73b0\u51fa\u8272\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u4eba\u4eff\u771f\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2511.01683", "categories": ["cs.HC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01683", "abs": "https://arxiv.org/abs/2511.01683", "authors": ["Kirk Vanacore", "Jaclyn Ocumpaugh", "Forest Agostinelli", "Dezhi Wu", "Sai Vuruma", "Matt Irvin"], "title": "Student Engagement in AI Assisted Complex Problem Solving: A Pilot Study of Human AI Rubik's Cube Collaboration", "comment": null, "summary": "Games and puzzles play important pedagogical roles in STEM learning. New AI\nalgorithms that can solve complex problems offer opportunities for scaffolded\ninstruction in puzzle solving. This paper presents the ALLURE system, which\nuses an AI algorithm (DeepCubeA) to guide students in solving a common first\nstep of the Rubik's Cube (i.e., the white cross). Using data from a pilot study\nwe present preliminary findings about students' behaviors in the system, how\nthese behaviors are associated with STEM skills - including spatial reasoning,\ncritical thinking and algorithmic thinking. We discuss how data from ALLURE can\nbe used in future educational data mining to understand how students benefit\nfrom AI assistance and collaboration when solving complex problems.", "AI": {"tldr": "ALLURE\u7cfb\u7edf\u4f7f\u7528AI\u7b97\u6cd5\uff08DeepCubeA\uff09\u6307\u5bfc\u5b66\u751f\u89e3\u51b3\u9b54\u65b9\u7b2c\u4e00\u6b65\uff08\u767d\u8272\u5341\u5b57\uff09\uff0c\u7814\u7a76\u5b66\u751f\u884c\u4e3a\u4e0eSTEM\u6280\u80fd\uff08\u7a7a\u95f4\u63a8\u7406\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u7b97\u6cd5\u601d\u7ef4\uff09\u7684\u5173\u8054\u3002", "motivation": "\u5229\u7528\u80fd\u591f\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u65b0AI\u7b97\u6cd5\u4e3a\u8c1c\u9898\u89e3\u51b3\u63d0\u4f9b\u652f\u67b6\u5f0f\u6559\u5b66\u673a\u4f1a\uff0c\u63a2\u7d22AI\u8f85\u52a9\u5728\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6559\u80b2\u4ef7\u503c\u3002", "method": "\u5f00\u53d1ALLURE\u7cfb\u7edf\uff0c\u4f7f\u7528DeepCubeA AI\u7b97\u6cd5\u6307\u5bfc\u5b66\u751f\u5b8c\u6210\u9b54\u65b9\u767d\u8272\u5341\u5b57\u6b65\u9aa4\uff0c\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u6536\u96c6\u5b66\u751f\u884c\u4e3a\u6570\u636e\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u5b66\u751f\u884c\u4e3a\u6a21\u5f0f\u53ca\u5176\u4e0eSTEM\u6280\u80fd\u7684\u5173\u8054\uff0c\u4e3a\u672a\u6765\u6559\u80b2\u6570\u636e\u6316\u6398\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "ALLURE\u7cfb\u7edf\u6570\u636e\u53ef\u7528\u4e8e\u7406\u89e3\u5b66\u751f\u5728AI\u534f\u52a9\u4e0b\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u83b7\u76ca\u60c5\u51b5\uff0c\u4e3aAI\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2511.00143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00143", "abs": "https://arxiv.org/abs/2511.00143", "authors": ["Jinsu Kim", "Yunhun Nam", "Minseon Kim", "Sangpil Kim", "Jongheon Jeong"], "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing", "comment": "36 pages; NeurIPS 2025; Code is available at\n  https://github.com/jsu-kim/BlurGuard", "summary": "Recent advances in text-to-image models have increased the exposure of\npowerful image editing techniques as a tool, raising concerns about their\npotential for malicious use. An emerging line of research to address such\nthreats focuses on implanting \"protective\" adversarial noise into images before\ntheir public release, so future attempts to edit them using text-to-image\nmodels can be impeded. However, subsequent works have shown that these\nadversarial noises are often easily \"reversed,\" e.g., with techniques as simple\nas JPEG compression, casting doubt on the practicality of the approach. In this\npaper, we argue that adversarial noise for image protection should not only be\nimperceptible, as has been a primary focus of prior work, but also\nirreversible, viz., it should be difficult to detect as noise provided that the\noriginal image is hidden. We propose a surprisingly simple method to enhance\nthe robustness of image protection methods against noise reversal techniques.\nSpecifically, it applies an adaptive per-region Gaussian blur on the noise to\nadjust the overall frequency spectrum. Through extensive experiments, we show\nthat our method consistently improves the per-sample worst-case protection\nperformance of existing methods against a wide range of reversal techniques on\ndiverse image editing scenarios, while also reducing quality degradation due to\nnoise in terms of perceptual metrics. Code is available at\nhttps://github.com/jsu-kim/BlurGuard.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5bf9\u6297\u566a\u58f0\u9006\u8f6c\u6280\u672f\u9c81\u68d2\u6027\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u533a\u57df\u9ad8\u65af\u6a21\u7cca\u8c03\u6574\u566a\u58f0\u9891\u7387\u8c31\uff0c\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u573a\u666f\u4e0b\u7684\u4fdd\u62a4\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6076\u610f\u56fe\u50cf\u7f16\u8f91\u5a01\u80c1\u589e\u52a0\u3002\u73b0\u6709\u7684\u4fdd\u62a4\u6027\u5bf9\u6297\u566a\u58f0\u65b9\u6cd5\u5bb9\u6613\u88ab\u7b80\u5355\u6280\u672f\uff08\u5982JPEG\u538b\u7f29\uff09\u9006\u8f6c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e2\u4e0d\u53ef\u5bdf\u89c9\u53c8\u4e0d\u53ef\u9006\u7684\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u533a\u57df\u9ad8\u65af\u6a21\u7cca\u5904\u7406\u566a\u58f0\u6765\u8c03\u6574\u6574\u4f53\u9891\u7387\u8c31\uff0c\u589e\u5f3a\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5bf9\u6297\u566a\u58f0\u9006\u8f6c\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e00\u81f4\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u573a\u666f\u4e0b\u5bf9\u6297\u5404\u79cd\u9006\u8f6c\u6280\u672f\u7684\u4fdd\u62a4\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u566a\u58f0\u5f15\u8d77\u7684\u8d28\u91cf\u9000\u5316\u3002", "conclusion": "\u901a\u8fc7\u81ea\u9002\u5e94\u9ad8\u65af\u6a21\u7cca\u8c03\u6574\u566a\u58f0\u9891\u7387\u8c31\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3a\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5bf9\u6297\u6076\u610f\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00673", "abs": "https://arxiv.org/abs/2511.00673", "authors": ["Dominik Drexler"], "title": "Lifted Successor Generation in Numeric Planning", "comment": null, "summary": "Most planners ground numeric planning tasks, given in a first-order-like\nlanguage, into a ground task representation. However, this can lead to an\nexponential blowup in task representation size, which occurs in practice for\nhard-to-ground tasks. We extend a state-of-the-art lifted successor generator\nfor classical planning to support numeric precondition applicability. The\nmethod enumerates maximum cliques in a substitution consistency graph. Each\nmaximum clique represents a substitution for the variables of the action\nschema, yielding a ground action. We augment this graph with numeric action\npreconditions and prove the successor generator is exact under formally\nspecified conditions. When the conditions fail, our generator may list\ninapplicable ground actions; a final applicability check filters these without\naffecting completeness. However, this cannot happen in 23 of 25 benchmark\ndomains, and it occurs only in 1 domain. To the authors' knowledge, no other\nlifted successor generator supports numeric action preconditions. This enables\nfuture research on lifted planning for a very rich planning fragment.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u7ecf\u5178\u89c4\u5212\u4e2d\u7684\u63d0\u5347\u540e\u7ee7\u751f\u6210\u5668\uff0c\u4f7f\u5176\u652f\u6301\u6570\u503c\u524d\u63d0\u6761\u4ef6\uff0c\u901a\u8fc7\u679a\u4e3e\u66ff\u6362\u4e00\u81f4\u6027\u56fe\u4e2d\u7684\u6700\u5927\u56e2\u6765\u751f\u6210\u5730\u9762\u52a8\u4f5c\uff0c\u907f\u514d\u4e86\u4efb\u52a1\u8868\u793a\u89c4\u6a21\u7684\u6307\u6570\u7ea7\u589e\u957f\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u89c4\u5212\u4efb\u52a1\u9700\u8981\u5c06\u4e00\u9636\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1\u8f6c\u6362\u4e3a\u5730\u9762\u4efb\u52a1\u8868\u793a\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u8868\u793a\u89c4\u6a21\u7684\u6307\u6570\u7ea7\u7206\u70b8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u652f\u6301\u6570\u503c\u524d\u63d0\u6761\u4ef6\u7684\u63d0\u5347\u540e\u7ee7\u751f\u6210\u5668\u3002", "method": "\u6269\u5c55\u4e86\u6700\u5148\u8fdb\u7684\u63d0\u5347\u540e\u7ee7\u751f\u6210\u5668\uff0c\u652f\u6301\u6570\u503c\u524d\u63d0\u6761\u4ef6\u7684\u9002\u7528\u6027\u68c0\u67e5\u3002\u65b9\u6cd5\u5305\u62ec\u5728\u66ff\u6362\u4e00\u81f4\u6027\u56fe\u4e2d\u679a\u4e3e\u6700\u5927\u56e2\uff0c\u6bcf\u4e2a\u6700\u5927\u56e2\u4ee3\u8868\u52a8\u4f5c\u6a21\u5f0f\u53d8\u91cf\u7684\u4e00\u4e2a\u66ff\u6362\uff0c\u751f\u6210\u5730\u9762\u52a8\u4f5c\u3002\u901a\u8fc7\u6dfb\u52a0\u6570\u503c\u52a8\u4f5c\u524d\u63d0\u6761\u4ef6\u6765\u589e\u5f3a\u8be5\u56fe\uff0c\u5e76\u5728\u5f62\u5f0f\u5316\u6307\u5b9a\u6761\u4ef6\u4e0b\u8bc1\u660e\u540e\u7ee7\u751f\u6210\u5668\u7684\u7cbe\u786e\u6027\u3002", "result": "\u572825\u4e2a\u57fa\u51c6\u57df\u4e2d\u768423\u4e2a\u4e2d\uff0c\u751f\u6210\u5668\u4e0d\u4f1a\u5217\u51fa\u4e0d\u9002\u7528\u7684\u5730\u9762\u52a8\u4f5c\uff1b\u4ec5\u57281\u4e2a\u57df\u4e2d\u4f1a\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\uff0c\u4f46\u901a\u8fc7\u6700\u7ec8\u9002\u7528\u6027\u68c0\u67e5\u53ef\u4ee5\u8fc7\u6ee4\u8fd9\u4e9b\u52a8\u4f5c\u800c\u4e0d\u5f71\u54cd\u5b8c\u6574\u6027\u3002\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u4e2a\u652f\u6301\u6570\u503c\u52a8\u4f5c\u524d\u63d0\u6761\u4ef6\u7684\u63d0\u5347\u540e\u7ee7\u751f\u6210\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u975e\u5e38\u4e30\u5bcc\u7684\u89c4\u5212\u7247\u6bb5\u4e2d\u7684\u63d0\u5347\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u907f\u514d\u4e86\u4efb\u52a1\u8868\u793a\u89c4\u6a21\u7684\u6307\u6570\u7ea7\u589e\u957f\u95ee\u9898\u3002"}}
{"id": "2511.01788", "categories": ["cs.HC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01788", "abs": "https://arxiv.org/abs/2511.01788", "authors": ["Yang Ni", "Yanzhuo Cao"], "title": "Exploring ChatGPT's Capabilities, Stability, Potential and Risks in Conducting Psychological Counseling through Simulations in School Counseling", "comment": null, "summary": "To provide an exploratory analysis of ChatGPT-4's quantitative performance\nindicators in simulated school-counseling settings. Conversational artificial\nintelligence (AI) has shown strong capabilities in providing low-cost and\ntimely interventions for a wide range of people and increasing well-being.\nTherefore, this study examined ChatGPT's capabilities, including response\nstability in conducting psychological counseling and its potential for\nproviding accessible psychological interventions, especially in school\nsettings. We prompted ChatGPT-4 with 80 real-world college-student counseling\nquestions. Replies were quantified with APA-informed NLP tools to measure\nwarmth, empathy, and acceptance, and run-to-run stability was assessed via\nFleiss' \\k{appa} and ICC(2,1). ChatGPT-4 achieved high warmth (97.5%), empathy\n(94.2%), and positive acceptance (mean compound score = 0.93 plus/minus 0.19),\nwith moderate stability (ICC(2,1) = 0.62; \\k{appa} = 0.59). Occasional\nrandomness in responses highlights risk areas requiring human oversight. As an\noffline, single-model text simulation without clinical validation, these\nresults remain exploratory. Future work should involve live users, compare\nmultiple LLMs, and incorporate mixed-methods validation to assess real-world\nefficacy and safety. The findings suggest ChatGPT-4 could augment low-intensity\nmental-health support in educational settings, guiding the design of\nhuman-in-the-loop workflows, policy regulations, and product roadmaps. This is\namong the first exploratory studies to apply quantitative stability metrics and\nNLP-based emotion detection to ChatGPT-4 in a school-counseling context and to\nintegrate a practitioner's perspective to inform future research, product\ndevelopment, and policy.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u6027\u5206\u6790\u4e86ChatGPT-4\u5728\u5b66\u6821\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u4e2d\u7684\u5b9a\u91cf\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u6e29\u6696\u5ea6\u3001\u540c\u7406\u5fc3\u548c\u79ef\u6781\u63a5\u7eb3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u7684\u54cd\u5e94\u968f\u673a\u6027\uff0c\u9700\u8981\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u5bf9\u8bdd\u5f0fAI\u5177\u6709\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u53ca\u65f6\u5fc3\u7406\u5e72\u9884\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5b66\u6821\u73af\u5883\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30ChatGPT\u5728\u5fc3\u7406\u54a8\u8be2\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u54cd\u5e94\u7a33\u5b9a\u6027\u548c\u63d0\u4f9b\u53ef\u53ca\u6027\u5fc3\u7406\u5e72\u9884\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u752880\u4e2a\u771f\u5b9e\u5927\u5b66\u751f\u5fc3\u7406\u54a8\u8be2\u95ee\u9898\u5411ChatGPT-4\u63d0\u95ee\uff0c\u91c7\u7528APA\u6307\u5bfc\u7684NLP\u5de5\u5177\u91cf\u5316\u56de\u590d\u7684\u6e29\u6696\u5ea6\u3001\u540c\u7406\u5fc3\u548c\u63a5\u7eb3\u5ea6\uff0c\u5e76\u901a\u8fc7Fleiss' kappa\u548cICC(2,1)\u8bc4\u4f30\u8fd0\u884c\u95f4\u7a33\u5b9a\u6027\u3002", "result": "ChatGPT-4\u5728\u6e29\u6696\u5ea6(97.5%)\u3001\u540c\u7406\u5fc3(94.2%)\u548c\u79ef\u6781\u63a5\u7eb3(\u5e73\u5747\u590d\u5408\u5f97\u52060.93\u00b10.19)\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u4e2d\u7b49\u7a33\u5b9a\u6027(ICC(2,1)=0.62; kappa=0.59)\u3002\u5076\u5c14\u7684\u54cd\u5e94\u968f\u673a\u6027\u662f\u9700\u8981\u4eba\u5de5\u76d1\u7763\u7684\u98ce\u9669\u70b9\u3002", "conclusion": "ChatGPT-4\u6709\u6f5c\u529b\u5728\u6559\u80b2\u73af\u5883\u4e2d\u589e\u5f3a\u4f4e\u5f3a\u5ea6\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u6307\u5bfc\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u3001\u653f\u7b56\u6cd5\u89c4\u548c\u4ea7\u54c1\u8def\u7ebf\u56fe\u7684\u8bbe\u8ba1\u3002\u672a\u6765\u7814\u7a76\u5e94\u6d89\u53ca\u771f\u5b9e\u7528\u6237\u3001\u6bd4\u8f83\u591a\u4e2aLLM\u5e76\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u9a8c\u8bc1\u5b9e\u9645\u6548\u679c\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2511.00519", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86BERT\u3001ALBERT\u3001RoBERTa\u548cDistilBERT\u7b49\u7f16\u7801\u5668transformer\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u89c1\u5ea6\u91cf\u65b9\u6cd5MALoR\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u751f\u6210\u6027\u522b\u5e73\u8861\u6570\u636e\u96c6\u8fdb\u884c\u7ee7\u7eed\u9884\u8bad\u7ec3\u6765\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u7f16\u7801\u5668transformer\u6a21\u578b\u5728\u5404\u79cd\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f46\u5df2\u88ab\u8bc1\u660e\u7ee7\u627f\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5f3a\u70c8\u6027\u522b\u504f\u89c1\u3002\u672c\u6587\u65e8\u5728\u8c03\u67e5\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\u4e2d\u7684\u6027\u522b\u504f\u89c1\u7a0b\u5ea6\u3002", "method": "\u5f15\u5165\u65b0\u7684\u504f\u89c1\u5ea6\u91cf\u6307\u6807MALoR\uff0c\u57fa\u4e8e\u6a21\u578b\u586b\u5145\u63a9\u7801\u6807\u8bb0\u7684\u6982\u7387\u6765\u8bc4\u4f30\u504f\u89c1\uff1b\u63d0\u51fa\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u751f\u6210\u6027\u522b\u5e73\u8861\u6570\u636e\u96c6\u8fdb\u884c\u7ee7\u7eed\u9884\u8bad\u7ec3\u7684\u7f13\u89e3\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u504f\u89c1\u5206\u6570\u663e\u8457\u964d\u4f4e\uff1a\u5728BERT-base\u4e2d\uff0c\"he-she\"\u504f\u89c1\u5206\u6570\u4ece1.27\u964d\u81f30.08\uff0c\"his-her\"\u4ece2.51\u964d\u81f30.36\uff1b\u5728BERT-large\u4e2d\uff0c\"male-female\"\u504f\u89c1\u4ece1.82\u964d\u81f30.10\u3002\u6240\u6709\u6a21\u578b\u90fd\u89c2\u5bdf\u5230\u7c7b\u4f3c\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u6027\u522b\u504f\u89c1\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.00710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00710", "abs": "https://arxiv.org/abs/2511.00710", "authors": ["Minghe Shen", "Zhuo Zhi", "Chonghan Liu", "Shuo Xing", "Zhengzhong Tu", "Che Liu"], "title": "Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries", "comment": null, "summary": "While Vision-Language Models (VLMs) post-trained with Reinforcement Learning\n(RL) show impressive general reasoning, their evaluation is often confined to\nlanguage-dominant tasks (e.g., math). This raises a critical question: can RL\npost-training truly extend the inherent capability boundary of a base VLM,\nparticularly for visual-centric spatial tasks where it initially fails? To\ninvestigate this, we introduce Ariadne, a framework utilizing synthetic mazes\nfor multi-step spatial reasoning where task difficulty (e.g., path length,\nturns) is precisely controlled. We leverage this controllable environment to\ntrain VLMs using Reinforcement Learning with Verified Rewards (RLVR) in a\ndifficulty-aware curriculum. Surprisingly, post-RLVR training, the VLM achieves\nover 50% accuracy on a problem set where the base model scored 0%,\ndemonstrating that our approach expands the model's initial capability\nboundary. To assess real-world viability, we evaluate out-of-distribution (OOD)\ngeneralization on practical benchmarks. Despite training only on synthetic maze\nsamples, Ariadne achieves significant zero-shot improvements, averaging 16% on\nMapBench (e.g., museum navigation) and 24% on ReasonMap (subway transfer\ntasks). These results confirm that our method not only broadens the model's\nfundamental limits but also enhances its generalization to real-world spatial\nreasoning. We acknowledge our study is limited to the post-training phase,\ngiven the opaqueness of pre-training data, and hope our research motivates\nfurther work on specialized, capability-extending alignment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faAriadne\u6846\u67b6\uff0c\u4f7f\u7528\u5408\u6210\u8ff7\u5bab\u8fdb\u884c\u591a\u6b65\u7a7a\u95f4\u63a8\u7406\u8bad\u7ec3\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1(RLVR)\u548c\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\uff0c\u6210\u529f\u6269\u5c55\u4e86\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u7a7a\u95f4\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u8fb9\u754c\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u540e\u4e3b\u8981\u5728\u8bed\u8a00\u4e3b\u5bfc\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u4e2d\u5fc3\u7a7a\u95f4\u4efb\u52a1\u80fd\u529b\u7684\u9a8c\u8bc1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22RL\u540e\u8bad\u7ec3\u662f\u5426\u80fd\u771f\u6b63\u6269\u5c55\u57fa\u7840VLM\u5728\u521d\u59cb\u5931\u8d25\u7684\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u8fb9\u754c\u3002", "method": "\u4f7f\u7528Ariadne\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u8ff7\u5bab\u521b\u5efa\u53ef\u63a7\u96be\u5ea6\u7684\u591a\u6b65\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1(RLVR)\u548c\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3VLMs\u3002", "result": "\u7ecf\u8fc7RLVR\u540e\u8bad\u7ec3\uff0cVLM\u5728\u57fa\u7840\u6a21\u578b\u5f97\u5206\u4e3a0%\u7684\u95ee\u9898\u96c6\u4e0a\u8fbe\u5230\u8d85\u8fc750%\u7684\u51c6\u786e\u7387\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6cdb\u5316\u8bc4\u4f30\u4e2d\uff0c\u4ec5\u5728\u5408\u6210\u8ff7\u5bab\u6837\u672c\u4e0a\u8bad\u7ec3\u5c31\u5b9e\u73b0\u4e86\u663e\u8457\u96f6\u6837\u672c\u6539\u8fdb\uff1aMapBench\u5e73\u5747\u63d0\u534716%\uff0cReasonMap\u5e73\u5747\u63d0\u534724%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6269\u5c55\u4e86\u6a21\u578b\u7684\u57fa\u672c\u80fd\u529b\u8fb9\u754c\uff0c\u8fd8\u589e\u5f3a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u9650\u4e8e\u540e\u8bad\u7ec3\u9636\u6bb5\uff0c\u5e0c\u671b\u63a8\u52a8\u4e13\u95e8\u7684\u80fd\u529b\u6269\u5c55\u5bf9\u9f50\u7814\u7a76\u3002"}}
{"id": "2511.00998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00998", "abs": "https://arxiv.org/abs/2511.00998", "authors": ["Ziye Wang", "Li Kang", "Yiran Qin", "Jiahua Ma", "Zhanglin Peng", "Lei Bai", "Ruimao Zhang"], "title": "GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies", "comment": "Accepted by NeurIPS 2025. Project page:\n  https://ziyeeee.github.io/gaudp.io/", "summary": "Recently, effective coordination in embodied multi-agent systems has remained\na fundamental challenge, particularly in scenarios where agents must balance\nindividual perspectives with global environmental awareness. Existing\napproaches often struggle to balance fine-grained local control with\ncomprehensive scene understanding, resulting in limited scalability and\ncompromised collaboration quality. In this paper, we present GauDP, a novel\nGaussian-image synergistic representation that facilitates scalable,\nperception-aware imitation learning in multi-agent collaborative systems.\nSpecifically, GauDP constructs a globally consistent 3D Gaussian field from\ndecentralized RGB observations, then dynamically redistributes 3D Gaussian\nattributes to each agent's local perspective. This enables all agents to\nadaptively query task-critical features from the shared scene representation\nwhile maintaining their individual viewpoints. This design facilitates both\nfine-grained control and globally coherent behavior without requiring\nadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on the\nRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Our\nmethod achieves superior performance over existing image-based methods and\napproaches the effectiveness of point-cloud-driven methods, while maintaining\nstrong scalability as the number of agents increases.", "AI": {"tldr": "GauDP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9ad8\u65af-\u56fe\u50cf\u534f\u540c\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7684\u53ef\u6269\u5c55\u3001\u611f\u77e5\u611f\u77e5\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u6784\u5efa\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u573a\u5e76\u52a8\u6001\u91cd\u65b0\u5206\u914d\u5c5e\u6027\u5230\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u5c40\u90e8\u89c6\u89d2\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u5168\u5c40\u4e00\u81f4\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u5177\u8eab\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6709\u6548\u534f\u8c03\u7684\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u9700\u8981\u5e73\u8861\u4e2a\u4f53\u89c6\u89d2\u4e0e\u5168\u5c40\u73af\u5883\u611f\u77e5\u7684\u573a\u666f\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7ec6\u7c92\u5ea6\u5c40\u90e8\u63a7\u5236\u4e0e\u5168\u9762\u573a\u666f\u7406\u89e3\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u534f\u4f5c\u8d28\u91cf\u53d7\u635f\u3002", "method": "\u4ece\u5206\u6563\u7684RGB\u89c2\u6d4b\u6784\u5efa\u5168\u5c40\u4e00\u81f4\u76843D\u9ad8\u65af\u573a\uff0c\u7136\u540e\u52a8\u6001\u5730\u5c063D\u9ad8\u65af\u5c5e\u6027\u91cd\u65b0\u5206\u914d\u5230\u6bcf\u4e2a\u667a\u80fd\u4f53\u7684\u5c40\u90e8\u89c6\u89d2\uff0c\u4f7f\u6240\u6709\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u9002\u5e94\u5730\u4ece\u5171\u4eab\u573a\u666f\u8868\u793a\u4e2d\u67e5\u8be2\u4efb\u52a1\u5173\u952e\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u5404\u81ea\u7684\u4e2a\u4f53\u89c6\u89d2\u3002", "result": "\u5728RoboFactory\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGauDP\u5728\u591a\u6837\u5316\u591a\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u70b9\u4e91\u9a71\u52a8\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u5728\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "GauDP\u901a\u8fc7\u9ad8\u65af-\u56fe\u50cf\u534f\u540c\u8868\u793a\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u4e2d\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u5168\u5c40\u4e00\u81f4\u884c\u4e3a\u7684\u5e73\u8861\uff0c\u65e0\u9700\u989d\u5916\u7684\u4f20\u611f\u6a21\u5f0f\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.01826", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.01826", "abs": "https://arxiv.org/abs/2511.01826", "authors": ["Dhruv Bihani", "A. K. M. Amanat Ullah", "Charles-Olivier Dufresne-Camaro", "William Delamare", "Khalad Hasan"], "title": "Exploring Pointer Enhancement Techniques for Target Selection on Large Curved Display", "comment": null, "summary": "Large curved displays are becoming increasingly popular due to their ability\nto provide users with a wider field of view and a more immersive experience\ncompared to flat displays. Current interaction techniques for large curved\ndisplays often assume a user is positioned at the display's centre, crucially\nfailing to accommodate general use conditions where the user may move during\nuse. In this work, we investigated how user position impacts pointing\ninteraction on large curved displays and evaluated cursor enhancement\ntechniques to provide faster and more accurate performance across positions. To\nthis effect, we conducted two user studies. First, we evaluated the effects of\nuser position on pointing performance on a large semi-circular display\n(3m-tall, 3270R curvature) through a 2D Fitts' Law selection task. Our results\nindicate that as users move away from the display, their pointing speed\nsignificantly increases (at least by 9%), but accuracy decreases (by at least\n6%). Additionally, we observed participants were slower when pointing from\nlaterally offset positions. Secondly, we explored which pointing techniques\nproviding motor- and visual-space enhancements best afford effective pointing\nperformance across user positions. Across a total of six techniques tested, we\nfound that a combination of acceleration and distance-based adjustments with\ncursor enlargement significantly improves target selection speed and accuracy\nacross different user positions. Results further show techniques with\nvisual-space enhancements (e.g., cursor enlargement) are significantly faster\nand more accurate than their non-visually-enhanced counterparts. Based on our\nresults we provide design recommendations for implementing cursor enhancement\ntechniques for large curved displays.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u7528\u6237\u4f4d\u7f6e\u5bf9\u5927\u578b\u66f2\u9762\u663e\u793a\u5668\u6307\u5411\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u5149\u6807\u589e\u5f3a\u6280\u672f\u4ee5\u63d0\u5347\u8de8\u4f4d\u7f6e\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u7528\u6237\u8fdc\u79bb\u663e\u793a\u5668\u65f6\u901f\u5ea6\u589e\u52a0\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0c\u800c\u7ed3\u5408\u52a0\u901f\u3001\u8ddd\u79bb\u8c03\u6574\u548c\u5149\u6807\u653e\u5927\u7684\u6280\u672f\u80fd\u663e\u8457\u6539\u5584\u9009\u62e9\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u66f2\u9762\u663e\u793a\u5668\u63d0\u4f9b\u66f4\u5bbd\u89c6\u91ce\u548c\u6c89\u6d78\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709\u4ea4\u4e92\u6280\u672f\u5047\u8bbe\u7528\u6237\u4f4d\u4e8e\u663e\u793a\u5668\u4e2d\u5fc3\uff0c\u65e0\u6cd5\u9002\u5e94\u7528\u6237\u79fb\u52a8\u65f6\u7684\u4f7f\u7528\u6761\u4ef6\u3002\u9700\u8981\u7814\u7a76\u7528\u6237\u4f4d\u7f6e\u5bf9\u6307\u5411\u4ea4\u4e92\u7684\u5f71\u54cd\u5e76\u5f00\u53d1\u6709\u6548\u7684\u5149\u6807\u589e\u5f3a\u6280\u672f\u3002", "method": "\u8fdb\u884c\u4e24\u9879\u7528\u6237\u7814\u7a76\uff1a1) \u901a\u8fc72D Fitts\u5b9a\u5f8b\u9009\u62e9\u4efb\u52a1\u8bc4\u4f30\u7528\u6237\u4f4d\u7f6e\u5bf9\u5927\u578b\u534a\u5706\u5f62\u663e\u793a\u5668\u6307\u5411\u6027\u80fd\u7684\u5f71\u54cd\uff1b2) \u6d4b\u8bd5\u516d\u79cd\u63d0\u4f9b\u8fd0\u52a8\u548c\u89c6\u89c9\u7a7a\u95f4\u589e\u5f3a\u7684\u6307\u5411\u6280\u672f\uff0c\u63a2\u7d22\u54ea\u79cd\u6280\u672f\u5728\u4e0d\u540c\u7528\u6237\u4f4d\u7f6e\u4e0b\u63d0\u4f9b\u6700\u4f73\u6027\u80fd\u3002", "result": "\u7528\u6237\u8fdc\u79bb\u663e\u793a\u5668\u65f6\u6307\u5411\u901f\u5ea6\u663e\u8457\u589e\u52a0\uff08\u81f3\u5c119%\uff09\uff0c\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff08\u81f3\u5c116%\uff09\uff1b\u4ece\u4fa7\u5411\u504f\u79fb\u4f4d\u7f6e\u6307\u5411\u65f6\u901f\u5ea6\u8f83\u6162\u3002\u7ed3\u5408\u52a0\u901f\u3001\u8ddd\u79bb\u8c03\u6574\u548c\u5149\u6807\u653e\u5927\u7684\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u8de8\u7528\u6237\u4f4d\u7f6e\u7684\u76ee\u6807\u9009\u62e9\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002\u5177\u6709\u89c6\u89c9\u7a7a\u95f4\u589e\u5f3a\u7684\u6280\u672f\u6bd4\u975e\u89c6\u89c9\u589e\u5f3a\u6280\u672f\u66f4\u5feb\u66f4\u51c6\u786e\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u4e3a\u5927\u578b\u66f2\u9762\u663e\u793a\u5668\u5b9e\u73b0\u5149\u6807\u589e\u5f3a\u6280\u672f\u63d0\u4f9b\u8bbe\u8ba1\u5efa\u8bae\uff0c\u63a8\u8350\u4f7f\u7528\u7ed3\u5408\u52a0\u901f\u3001\u8ddd\u79bb\u8c03\u6574\u548c\u5149\u6807\u653e\u5927\u7684\u6280\u672f\u6765\u4f18\u5316\u8de8\u4f4d\u7f6e\u6307\u5411\u6027\u80fd\u3002"}}
{"id": "2511.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWordSaladChopper (WSC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u65e0\u7528\u81ea\u6211\u91cd\u590d\uff08\u8bcd\u6c99\u62c9\uff09\u5e76\u52a8\u6001\u4fee\u526a\uff0c\u663e\u8457\u51cf\u5c11\u8f93\u51fa\u4ee4\u724c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u8f93\u51fa\u4ee4\u724c\u6210\u672c\u9ad8\u6602\uff0c\u5176\u4e2d\u5927\u91cf\u662f\u65e0\u7528\u7684\u81ea\u6211\u91cd\u590d\uff08\u8bcd\u6c99\u62c9\uff09\uff0c\u6d88\u8017\u89e3\u7801\u9884\u7b97\u4f46\u4e0d\u589e\u52a0\u4ef7\u503c\u3002", "method": "\u5229\u7528\u6a21\u578b\u5bf9<\\n\\n>\u6807\u8bb0\u9690\u85cf\u72b6\u6001\u6a21\u5f0f\uff0c\u901a\u8fc7\u5355\u5c42\u7ebf\u6027\u5206\u7c7b\u5668\u5b9e\u65f6\u68c0\u6d4b\u8bcd\u6c99\u62c9\u884c\u4e3a\uff0c\u68c0\u6d4b\u540e\u91c7\u7528\u7b80\u5355\u4fee\u526a\u52a0\u91cd\u65b0\u751f\u6210\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "result": "WSC\u7ec4\u4ef6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8f93\u51fa\u957f\u5ea6\u8282\u7701\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\uff0c\u662f\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "WSC\u6216\u7c7b\u4f3c\u7ec4\u4ef6\u662f\u6240\u6709\u8003\u8651\u7528\u6237\u4f53\u9a8c\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u5e94\u7528\u7684\u5fc5\u8981\u7ec4\u4ef6\uff0c\u56e0\u5176\u4f4e\u5f00\u9500\u3001\u5f3a\u8282\u7701\u6548\u679c\u548c\u65e0\u8bed\u4e49\u4ef7\u503c\u7684\u8bcd\u6c99\u62c9\u4ee4\u724c\u7279\u6027\u3002"}}
{"id": "2511.00181", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00181", "abs": "https://arxiv.org/abs/2511.00181", "authors": ["Mengfei Liang", "Yiting Qu", "Yukun Jiang", "Michael Backes", "Yang Zhang"], "title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection", "comment": "20 pages, 6 figures", "summary": "The rapid evolution of AI-generated images poses unprecedented challenges to\ninformation integrity and media authenticity. Existing detection approaches\nsuffer from fundamental limitations: traditional classifiers lack\ninterpretability and fail to generalize across evolving generative models,\nwhile vision-language models (VLMs), despite their promise, remain constrained\nto single-shot analysis and pixel-level reasoning. To address these challenges,\nwe introduce AIFo (Agent-based Image Forensics), a novel training-free\nframework that emulates human forensic investigation through multi-agent\ncollaboration. Unlike conventional methods, our framework employs a set of\nforensic tools, including reverse image search, metadata extraction,\npre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based\nagents that collect, synthesize, and reason over cross-source evidence. When\nevidence is conflicting or insufficient, a structured multi-agent debate\nmechanism allows agents to exchange arguments and reach a reliable conclusion.\nFurthermore, we enhance the framework with a memory-augmented reasoning module\nthat learns from historical cases to improve future detection accuracy. Our\ncomprehensive evaluation spans 6,000 images across both controlled laboratory\nsettings and challenging real-world scenarios, including images from modern\ngenerative platforms and diverse online sources. AIFo achieves 97.05% accuracy,\nsubstantially outperforming traditional classifiers and state-of-the-art VLMs.\nThese results demonstrate that agent-based procedural reasoning offers a new\nparadigm for more robust, interpretable, and adaptable AI-generated image\ndetection.", "AI": {"tldr": "\u63d0\u51faAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b0\u6846\u67b6AIFo\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6a21\u62df\u4eba\u7c7b\u53d6\u8bc1\u8c03\u67e5\uff0c\u57286000\u5f20\u56fe\u50cf\u6d4b\u8bd5\u4e2d\u8fbe\u523097.05%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5206\u7c7b\u5668\u548c\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u5a92\u4f53\u771f\u5b9e\u6027\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5f31\u3001\u4ec5\u9650\u4e8e\u5355\u6b21\u5206\u6790\u548c\u50cf\u7d20\u7ea7\u63a8\u7406\u7b49\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u8bad\u7ec3\u514d\u8d39\u7684AIFo\u6846\u67b6\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u5f0f\uff0c\u6574\u5408\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u3001\u5143\u6570\u636e\u63d0\u53d6\u3001\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u548cVLM\u5206\u6790\u7b49\u53d6\u8bc1\u5de5\u5177\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684LLM\u667a\u80fd\u4f53\u6536\u96c6\u3001\u7efc\u5408\u548c\u63a8\u7406\u8de8\u6e90\u8bc1\u636e\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u548c\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u6a21\u5757\u3002", "result": "\u5728\u5305\u542b\u73b0\u4ee3\u751f\u6210\u5e73\u53f0\u548c\u591a\u6837\u5316\u5728\u7ebf\u6765\u6e90\u76846000\u5f20\u56fe\u50cf\u7efc\u5408\u8bc4\u4f30\u4e2d\uff0cAIFo\u8fbe\u523097.05%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5206\u7c7b\u5668\u548c\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u7a0b\u5e8f\u5316\u63a8\u7406\u4e3aAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.00739", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.00739", "abs": "https://arxiv.org/abs/2511.00739", "authors": ["Ritik Raj", "Hong Wang", "Tushar Krishna"], "title": "A CPU-Centric Perspective on Agentic AI", "comment": null, "summary": "Agentic AI frameworks add a decision-making orchestrator embedded with\nexternal tools, including web search, Python interpreter, contextual database,\nand others, on top of monolithic LLMs, turning them from passive text oracles\ninto autonomous problem-solvers that can plan, call tools, remember past steps,\nand adapt on the fly.\n  This paper aims to characterize and understand the system bottlenecks\nintroduced by agentic AI workloads from a largely overlooked CPU-centric\nperspective. We first systematically characterize Agentic AI on the basis of\norchestrator/decision making component, inference path dynamics and\nrepetitiveness of the agentic flow which directly influences the system-level\nperformance. Thereafter, based on the characterization, we choose five\nrepresentative agentic AI workloads- Haystack RAG, Toolformer, ChemCrow,\nLangchain and SWE-Agent to profile latency, throughput and energy metrics and\ndemystify the significant impact of CPUs on these metrics relative to GPUs. We\nobserve that - 1. Tool processing on CPUs can take up to 90.6% of the total\nlatency; 2. Agentic throughput gets bottlenecked either by CPU factors -\ncoherence, synchronization and over-subscription of cores or GPU factors - main\nmemory capacity and bandwidth; \\circled{3} CPU dynamic energy consumes up to\n44% of the total dynamic energy at large batch sizes. Based on the profiling\ninsights, we present two key optimizations- 1. CPU and GPU-Aware Micro-batching\n(CGAM) and 2. Mixed Agentic Workload Scheduling (MAWS) for homogeneous and\nheterogeneous agentic workloads respectively to demonstrate the potential to\nimprove the performance, efficiency, and scalability of agentic AI. We achieve\nup to 2.1x and 1.41x P50 latency speedup compared to the multi-processing\nbenchmark for homogeneous and heterogeneous agentic workloads respectively.", "AI": {"tldr": "\u672c\u6587\u4eceCPU\u4e2d\u5fc3\u89c6\u89d2\u5206\u6790\u667a\u80fd\u4f53AI\u6846\u67b6\u7684\u7cfb\u7edf\u74f6\u9888\uff0c\u53d1\u73b0\u5de5\u5177\u5904\u7406\u5728CPU\u4e0a\u53ef\u5360\u7528\u9ad8\u8fbe90.6%\u7684\u603b\u5ef6\u8fdf\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u4f18\u5316\u65b9\u6848\uff1aCPU\u548cGPU\u611f\u77e5\u7684\u5fae\u6279\u5904\u7406\u4ee5\u53ca\u6df7\u5408\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\uff0c\u5206\u522b\u5b9e\u73b02.1\u500d\u548c1.41\u500d\u7684\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u667a\u80fd\u4f53AI\u6846\u67b6\u5c06\u5355\u4f53LLM\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u95ee\u9898\u89e3\u51b3\u8005\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8GPU\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86CPU\u5728\u5de5\u5177\u5904\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u672c\u6587\u65e8\u5728\u4eceCPU\u4e2d\u5fc3\u89c6\u89d2\u7406\u89e3\u548c\u8868\u5f81\u667a\u80fd\u4f53AI\u5de5\u4f5c\u8d1f\u8f7d\u5f15\u5165\u7684\u7cfb\u7edf\u74f6\u9888\u3002", "method": "\u9996\u5148\u7cfb\u7edf\u5730\u8868\u5f81\u667a\u80fd\u4f53AI\u7684\u7f16\u6392\u5668/\u51b3\u7b56\u7ec4\u4ef6\u3001\u63a8\u7406\u8def\u5f84\u52a8\u6001\u548c\u6d41\u7a0b\u91cd\u590d\u6027\uff0c\u7136\u540e\u9009\u62e9\u4e94\u4e2a\u4ee3\u8868\u6027\u667a\u80fd\u4f53AI\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u80fd\u8017\u6307\u6807\u5206\u6790\uff0c\u6700\u540e\u63d0\u51faCPU\u548cGPU\u611f\u77e5\u7684\u5fae\u6279\u5904\u7406\u4ee5\u53ca\u6df7\u5408\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u4e24\u79cd\u4f18\u5316\u65b9\u6848\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1. CPU\u4e0a\u7684\u5de5\u5177\u5904\u7406\u53ef\u5360\u7528\u9ad8\u8fbe90.6%\u7684\u603b\u5ef6\u8fdf\uff1b2. \u667a\u80fd\u4f53\u541e\u5410\u91cf\u53d7CPU\u56e0\u7d20\uff08\u4e00\u81f4\u6027\u3001\u540c\u6b65\u3001\u6838\u5fc3\u8fc7\u8f7d\uff09\u6216GPU\u56e0\u7d20\uff08\u4e3b\u5b58\u5bb9\u91cf\u548c\u5e26\u5bbd\uff09\u9650\u5236\uff1b3. \u5728\u5927\u6279\u91cf\u65f6CPU\u52a8\u6001\u80fd\u8017\u53ef\u8fbe\u603b\u52a8\u6001\u80fd\u8017\u768444%\u3002\u4f18\u5316\u540e\uff0c\u540c\u6784\u548c\u5f02\u6784\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u5206\u522b\u5b9e\u73b02.1\u500d\u548c1.41\u500d\u7684P50\u5ef6\u8fdf\u52a0\u901f\u3002", "conclusion": "CPU\u5728\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\u626e\u6f14\u7740\u6bd4\u9884\u671f\u66f4\u91cd\u8981\u7684\u89d2\u8272\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u4f18\u5316\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53AI\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.01031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01031", "abs": "https://arxiv.org/abs/2511.01031", "authors": ["Mathieu Dubied", "Paolo Tiso", "Robert K. Katzschmann"], "title": "AquaROM: shape optimization pipeline for soft swimmers using parametric reduced order models", "comment": null, "summary": "The efficient optimization of actuated soft structures, particularly under\ncomplex nonlinear forces, remains a critical challenge in advancing robotics.\nSimulations of nonlinear structures, such as soft-bodied robots modeled using\nthe finite element method (FEM), often demand substantial computational\nresources, especially during optimization. To address this challenge, we\npropose a novel optimization algorithm based on a tensorial parametric reduced\norder model (PROM). Our algorithm leverages dimensionality reduction and\nsolution approximation techniques to facilitate efficient solving of nonlinear\nconstrained optimization problems. The well-structured tensorial approach\nenables the use of analytical gradients within a specifically chosen reduced\norder basis (ROB), significantly enhancing computational efficiency. To\nshowcase the performance of our method, we apply it to optimizing soft robotic\nswimmer shapes. These actuated soft robots experience hydrodynamic forces,\nsubjecting them to both internal and external nonlinear forces, which are\nincorporated into our optimization process using a data-free ROB for fast and\naccurate computations. This approach not only reduces computational complexity\nbut also unlocks new opportunities to optimize complex nonlinear systems in\nsoft robotics, paving the way for more efficient design and control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u53c2\u6570\u964d\u9636\u6a21\u578b\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u7684\u975e\u7ebf\u6027\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u627f\u53d7\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u8f6f\u4f53\u6e38\u6cf3\u673a\u5668\u4eba\u5f62\u72b6\u4f18\u5316\u3002", "motivation": "\u8f6f\u4f53\u7ed3\u6784\u5728\u590d\u6742\u975e\u7ebf\u6027\u529b\u4f5c\u7528\u4e0b\u7684\u9ad8\u6548\u4f18\u5316\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u975e\u7ebf\u6027\u7ed3\u6784\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5c24\u5176\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5229\u7528\u5f20\u91cf\u53c2\u6570\u964d\u9636\u6a21\u578b\uff0c\u7ed3\u5408\u7ef4\u5ea6\u7f29\u51cf\u548c\u89e3\u51b3\u65b9\u6848\u8fd1\u4f3c\u6280\u672f\uff0c\u5728\u7279\u5b9a\u9009\u62e9\u7684\u964d\u9636\u57fa\u4e2d\u4f7f\u7528\u89e3\u6790\u68af\u5ea6\uff0c\u901a\u8fc7\u65e0\u6570\u636e\u964d\u9636\u57fa\u5c06\u5185\u5916\u975e\u7ebf\u6027\u529b\u7eb3\u5165\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u8fdb\u884c\u8f6f\u4f53\u6e38\u6cf3\u673a\u5668\u4eba\u5f62\u72b6\u4f18\u5316\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8fd8\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.01839", "categories": ["cs.HC"], "pdf": "https://arxiv.org/pdf/2511.01839", "abs": "https://arxiv.org/abs/2511.01839", "authors": ["Nelusha Nugegoda", "Marium-E- Jannat", "Khalad Hasan", "Patricia Lasserre"], "title": "Exploring the Effect of Viewing Attributes of Mobile AR Interfaces on Remote Collaborative and Competitive Tasks", "comment": null, "summary": "Mobile devices have the potential to facilitate remote tasks through\nAugmented Reality (AR) solutions by integrating digital information into the\nreal world. Although prior studies have explored Mobile Augmented Reality (MAR)\nfor co-located collaboration, none have investigated the impact of various\nviewing attributes that can influence remote task performance, such as target\nobject viewing angles, synchronization styles, or having a secondary small\nscreen showing other users current view in the MAR environment. In this paper,\nwe explore five techniques considering these attributes, specifically designed\nfor two modes of remote tasks: collaborative and competitive. We conducted a\nuser study employing various combinations of those attributes for both tasks.\nIn both instances, results indicate users' optimal performance and preference\nfor the technique that allows asynchronous viewing of object manipulations on\nthe small screen. Overall, this paper contributes novel techniques for remote\ntasks in MAR, addressing aspects such as viewing angle and synchronization in\nobject manipulation alongside secondary small-screen interfaces. Additionally,\nit presents the results of a user study evaluating the effectiveness,\nusability, and user preference of these techniques in remote settings and\noffers a set of recommendations for designing and implementing MAR solutions to\nenhance remote activities.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79fb\u52a8\u589e\u5f3a\u73b0\u5b9e\uff08MAR\uff09\u5728\u8fdc\u7a0b\u4efb\u52a1\u4e2d\u7684\u4e94\u79cd\u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u89d2\u3001\u540c\u6b65\u65b9\u5f0f\u548c\u8f85\u52a9\u5c0f\u5c4f\u5e55\u7b49\u5c5e\u6027\u5bf9\u534f\u4f5c\u548c\u7ade\u4e89\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u901a\u8fc7\u589e\u5f3a\u73b0\u5b9e\u6280\u672f\u53ef\u4ee5\u5c06\u6570\u5b57\u4fe1\u606f\u6574\u5408\u5230\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u4fc3\u8fdb\u8fdc\u7a0b\u4efb\u52a1\u7684\u6267\u884c\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u79fb\u52a8\u589e\u5f3a\u73b0\u5b9e\u5728\u5171\u5740\u534f\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u5c1a\u672a\u7814\u7a76\u5404\u79cd\u89c6\u89d2\u5c5e\u6027\uff08\u5982\u76ee\u6807\u5bf9\u8c61\u89c6\u89d2\u3001\u540c\u6b65\u65b9\u5f0f\u3001\u8f85\u52a9\u5c0f\u5c4f\u5e55\u663e\u793a\u5176\u4ed6\u7528\u6237\u89c6\u56fe\uff09\u5bf9\u8fdc\u7a0b\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e94\u79cd\u8003\u8651\u8fd9\u4e9b\u5c5e\u6027\u7684\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u4e24\u79cd\u8fdc\u7a0b\u4efb\u52a1\u6a21\u5f0f\uff1a\u534f\u4f5c\u548c\u7ade\u4e89\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u5c5e\u6027\u7ec4\u5408\u5728\u4e24\u79cd\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u534f\u4f5c\u548c\u7ade\u4e89\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u8868\u73b0\u6700\u4f73\u4e14\u6700\u504f\u597d\u5141\u8bb8\u5728\u5c0f\u5c4f\u5e55\u4e0a\u5f02\u6b65\u67e5\u770b\u5bf9\u8c61\u64cd\u4f5c\u7684\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u4e3a\u79fb\u52a8\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u8fdc\u7a0b\u4efb\u52a1\u8d21\u732e\u4e86\u65b0\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5bf9\u8c61\u64cd\u4f5c\u4e2d\u7684\u89c6\u89d2\u548c\u540c\u6b65\u95ee\u9898\u4ee5\u53ca\u8f85\u52a9\u5c0f\u5c4f\u5e55\u754c\u9762\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u8fdc\u7a0b\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3001\u53ef\u7528\u6027\u548c\u7528\u6237\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u5957\u8bbe\u8ba1\u548c\u5b9e\u65bd\u79fb\u52a8\u589e\u5f3a\u73b0\u5b9e\u89e3\u51b3\u65b9\u6848\u4ee5\u589e\u5f3a\u8fdc\u7a0b\u6d3b\u52a8\u7684\u5efa\u8bae\u3002"}}
{"id": "2511.00537", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains.", "AI": {"tldr": "\u63d0\u51faCISEA-MRFE\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6307\u4ee4\u3001\u8bed\u4e49\u589e\u5f3a\u589e\u5f3a\u548c\u591a\u7ec6\u5316\u7279\u5f81\u63d0\u53d6\uff0c\u89e3\u51b3\u60c5\u611f\u5206\u6790\u4e2d\u7ec6\u5fae\u60c5\u611f\u7ebf\u7d22\u3001\u9886\u57df\u8f6c\u79fb\u548c\u4e0d\u5e73\u8861\u5206\u5e03\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u7ec6\u5fae\u60c5\u611f\u7ebf\u7d22\u3001\u9886\u57df\u8f6c\u79fb\u548c\u4e0d\u5e73\u8861\u60c5\u611f\u5206\u5e03\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u8bed\u4e49\u57fa\u7840\u4e0d\u8db3\u3001\u5bf9\u591a\u6837\u5316\u8bed\u8a00\u6a21\u5f0f\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u5bf9\u4e3b\u5bfc\u60c5\u611f\u7c7b\u522b\u7684\u504f\u89c1\u3002", "method": "\u63d0\u51faCISEA-MRFE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e0a\u4e0b\u6587\u6307\u4ee4(CI)\u6ce8\u5165\u9886\u57df\u611f\u77e5\u6307\u4ee4\u6307\u5bfc\u60c5\u611f\u6d88\u6b67\uff1b\u8bed\u4e49\u589e\u5f3a\u589e\u5f3a(SEA)\u901a\u8fc7\u60c5\u611f\u4e00\u81f4\u91ca\u4e49\u589e\u5f3a\u63d0\u9ad8\u9c81\u68d2\u6027\uff1b\u591a\u7ec6\u5316\u7279\u5f81\u63d0\u53d6(MRFE)\u7ed3\u5408\u5c3a\u5ea6\u81ea\u9002\u5e94\u6df1\u5ea6\u7f16\u7801\u5668(SADE)\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u4e13\u4e1a\u5316\uff0c\u4ee5\u53ca\u60c5\u611f\u8bc4\u4f30\u5668\u4e0a\u4e0b\u6587\u7f16\u7801\u5668(EECE)\u8fdb\u884c\u60c5\u611f\u611f\u77e5\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCISEA-MRFE\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728IMDb\u4e0a\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u53474.6%\uff0cYelp\u4e0a6.5%\uff0cTwitter\u4e0a30.3%\uff0cAmazon\u4e0a4.1%\u3002", "conclusion": "CISEA-MRFE\u6846\u67b6\u5728\u8de8\u9886\u57df\u60c5\u611f\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.00191", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00191", "abs": "https://arxiv.org/abs/2511.00191", "authors": ["Ziliang Chen", "Xin Huang", "Quanlong Guan", "Liang Lin", "Weiqi Luo"], "title": "A Retrospect to Multi-prompt Learning across Vision and Language", "comment": "ICCV", "summary": "The vision community is undergoing the unprecedented progress with the\nemergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays\nas the holy grail of accessing VLMs since it enables their fast adaptation to\ndownstream tasks with limited resources. Whereas existing researches milling\naround single-prompt paradigms, rarely investigate the technical potential\nbehind their multi-prompt learning counterparts. This paper aims to provide a\nprincipled retrospect for vision-language multi-prompt learning. We extend the\nrecent constant modality gap phenomenon to learnable prompts and then, justify\nthe superiority of vision-language transfer with multi-prompt augmentation,\nempirically and theoretically. In terms of this observation, we propose an\nEnergy-based Multi-prompt Learning (EMPL) to generate multiple prompt\nembeddings by drawing instances from an energy-based distribution, which is\nimplicitly defined by VLMs. So our EMPL is not only parameter-efficient but\nalso rigorously lead to the balance between in-domain and out-of-domain\nopen-vocabulary generalization. Comprehensive experiments have been conducted\nto justify our claims and the excellence of EMPL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u591a\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff08EMPL\uff09\uff0c\u901a\u8fc7\u4ece\u80fd\u91cf\u5206\u5e03\u4e2d\u751f\u6210\u591a\u4e2a\u63d0\u793a\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u6548\u53c2\u6570\u9002\u5e94\u548c\u6cdb\u5316\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u63d0\u793a\u5b66\u4e60\u8303\u5f0f\uff0c\u5f88\u5c11\u63a2\u7d22\u591a\u63d0\u793a\u5b66\u4e60\u7684\u6280\u672f\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u4e3a\u89c6\u89c9\u8bed\u8a00\u591a\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e00\u4e2a\u539f\u5219\u6027\u56de\u987e\uff0c\u5e76\u8bc1\u660e\u591a\u63d0\u793a\u589e\u5f3a\u5728\u89c6\u89c9\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u7684\u591a\u63d0\u793a\u5b66\u4e60\uff08EMPL\uff09\uff0c\u901a\u8fc7\u4ece\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9690\u5f0f\u5b9a\u4e49\u7684\u80fd\u91cf\u5206\u5e03\u4e2d\u62bd\u53d6\u5b9e\u4f8b\u6765\u751f\u6210\u591a\u4e2a\u63d0\u793a\u5d4c\u5165\uff0c\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u4e14\u4e25\u683c\u5e73\u8861\u9886\u57df\u5185\u5916\u6cdb\u5316\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4f5c\u8005\u7684\u4e3b\u5f20\u548cEMPL\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u591a\u63d0\u793a\u5b66\u4e60\u5728\u89c6\u89c9\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "EMPL\u65b9\u6cd5\u4e0d\u4ec5\u53c2\u6570\u9ad8\u6548\uff0c\u800c\u4e14\u80fd\u591f\u4e25\u683c\u5e73\u8861\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u7684\u5f00\u653e\u8bcd\u6c47\u6cdb\u5316\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u591a\u63d0\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.00751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost.", "AI": {"tldr": "\u672c\u7814\u7a76\u91cd\u65b0\u9a8c\u8bc1\u4e86\u5728\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u589e\u52a0\u91c7\u6837\u63a8\u7406\u8def\u5f84\u5bf9\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u7684\u6536\u76ca\u9012\u51cf\u6548\u5e94\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u5728\u9002\u5ea6\u91c7\u6837\u540e\u8d8b\u4e8e\u5e73\u7f13\u3002", "motivation": "\u65e9\u671f\u7814\u7a76\u8868\u660e\u7ed3\u5408\u591a\u4e2a\u63a8\u7406\u94fe\u80fd\u6539\u5584\u7ed3\u679c\u4f46\u5b58\u5728\u6536\u76ca\u9012\u51cf\uff0c\u672c\u7814\u7a76\u4f7f\u7528\u6700\u65b0\u7684Gemini 2.5\u6a21\u578b\u91cd\u65b0\u9a8c\u8bc1\u8fd9\u4e00\u73b0\u8c61\u5728\u5f53\u524d\u6a21\u578b\u6761\u4ef6\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u5728HotpotQA\u548cMath-500\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528Gemini 2.5\u6a21\u578b\uff0c\u6bd4\u8f83\u4e0d\u540c\u91c7\u6837\u63a8\u7406\u8def\u5f84\u6570\u91cf\u4e0e\u5355\u4e00\u601d\u7ef4\u94fe\u57fa\u7ebf\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u8f83\u5927\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u7a33\u5b9a\u4e00\u81f4\u7684\u6539\u8fdb\u66f2\u7ebf\uff0c\u6027\u80fd\u589e\u76ca\u5728\u9002\u5ea6\u91c7\u6837\u540e\u8d8b\u4e8e\u5e73\u7f13\uff0c\u9ad8\u91c7\u6837\u914d\u7f6e\u76f8\u5bf9\u4e8e\u8ba1\u7b97\u6210\u672c\u6536\u76ca\u6709\u9650\u3002", "conclusion": "\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u4ecd\u7136\u6709\u6548\uff0c\u4f46\u7531\u4e8e\u63a8\u7406\u8def\u5f84\u95f4\u7684\u91cd\u53e0\u5bfc\u81f4\u6536\u76ca\u9012\u51cf\uff0c\u9ad8\u91c7\u6837\u914d\u7f6e\u7684\u989d\u5916\u6536\u76ca\u4e0e\u8ba1\u7b97\u6210\u672c\u4e0d\u6210\u6b63\u6bd4\u3002"}}
{"id": "2511.00556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA.", "AI": {"tldr": "ISA\uff08\u610f\u56fe\u8f6c\u79fb\u653b\u51fb\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u7f16\u8f91\u539f\u59cb\u6709\u5bb3\u8bf7\u6c42\u6765\u6df7\u6dc6LLM\u5bf9\u653b\u51fb\u610f\u56fe\u7684\u8bc6\u522b\uff0c\u5c06\u6709\u5bb3\u610f\u56fe\u4f2a\u88c5\u6210\u826f\u6027\u4fe1\u606f\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u4e3b\u8981\u901a\u8fc7\u6dfb\u52a0\u989d\u5916\u4e0a\u4e0b\u6587\u6216\u5bf9\u6297\u6027token\u6765\u5206\u6563LLM\u6ce8\u610f\u529b\uff0c\u4f46\u672a\u6539\u53d8\u6838\u5fc3\u6709\u5bb3\u610f\u56fe\u3002ISA\u65e8\u5728\u901a\u8fc7\u610f\u56fe\u8f6c\u6362\u4f7fLLM\u8bef\u5224\u653b\u51fb\u610f\u56fe\u4e3a\u826f\u6027\u8bf7\u6c42\u3002", "method": "\u5efa\u7acb\u610f\u56fe\u8f6c\u6362\u5206\u7c7b\u6cd5\uff0c\u5229\u7528\u8fd9\u4e9b\u8f6c\u6362\u751f\u6210\u770b\u4f3c\u65e0\u5bb3\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u4ec5\u9700\u5bf9\u539f\u59cb\u8bf7\u6c42\u8fdb\u884c\u6700\u5c0f\u7f16\u8f91\uff0c\u65e0\u9700\u590d\u6742token\u6216\u5197\u957f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cISA\u76f8\u6bd4\u76f4\u63a5\u6709\u5bb3\u63d0\u793a\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc770%\u3002\u4ec5\u4f7f\u7528ISA\u6a21\u677f\u91cd\u65b0\u8868\u8ff0\u7684\u826f\u6027\u6570\u636e\u5fae\u8c03\u6a21\u578b\u53ef\u5c06\u6210\u529f\u7387\u63d0\u5347\u81f3\u8fd1100%\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9ISA\u65e0\u6548\u3002", "conclusion": "ISA\u63ed\u793a\u4e86LLM\u5728\u610f\u56fe\u63a8\u65ad\u65b9\u9762\u7684\u6839\u672c\u6027\u5b89\u5168\u6311\u6218\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2511.00211", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00211", "abs": "https://arxiv.org/abs/2511.00211", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "comment": null, "summary": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO)\nsatellites in mega-constellations allows ubiquitous connectivity to rural and\nremote areas. However, weather events have a significant impact on the\nperformance and reliability of satellite Internet. Adverse weather events such\nas snow and rain can disturb the performance and operations of satellite\nInternet's essential ground terminal components, such as satellite antennas,\nsignificantly disrupting the space-ground link conditions between LEO\nsatellites and ground stations. This challenge calls for not only region-based\nweather forecasts but also fine-grained detection capability on ground terminal\ncomponents of fine-grained weather conditions. Such a capability can assist in\nfault diagnostics and mitigation for reliable satellite Internet, but its\nsolutions are lacking, not to mention the effectiveness and generalization that\nare essential in real-world deployments. This paper discusses an efficient\ntransfer learning (TL) method that can enable a ground component to locally\ndetect representative weather-related conditions. The proposed method can\ndetect snow, wet, and other conditions resulting from adverse and typical\nweather events and shows superior performance compared to the typical deep\nlearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL\nmethod also shows the advantage of being generalizable to various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5730\u9762\u7ec8\u7aef\u7ec4\u4ef6\u672c\u5730\u68c0\u6d4b\u5929\u6c14\u76f8\u5173\u6761\u4ef6\uff0c\u5305\u62ec\u96ea\u3001\u6f6e\u6e7f\u548c\u5176\u4ed6\u6076\u52a3\u5929\u6c14\u72b6\u51b5\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u4e92\u8054\u7f51\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u5929\u6c14\u6761\u4ef6\u68c0\u6d4b\u80fd\u529b\u6765\u534f\u52a9\u6545\u969c\u8bca\u65ad\u548c\u7f13\u89e3\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u5730\u9762\u7ec4\u4ef6\u80fd\u591f\u672c\u5730\u68c0\u6d4b\u4ee3\u8868\u6027\u7684\u5929\u6c14\u76f8\u5173\u6761\u4ef6\uff0c\u5305\u62ec\u96ea\u3001\u6f6e\u6e7f\u548c\u5176\u4ed6\u6076\u52a3\u5929\u6c14\u72b6\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u96ea\u3001\u6f6e\u6e7f\u548c\u5176\u4ed6\u5929\u6c14\u6761\u4ef6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8eYOLOv7\u3001YOLOv9\u3001Faster R-CNN\u548cR-YOLO\u7b49\u5178\u578b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u800c\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u9002\u5e94\u5404\u79cd\u5b9e\u9645\u90e8\u7f72\u573a\u666f\uff0c\u4e3a\u536b\u661f\u4e92\u8054\u7f51\u7684\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5929\u6c14\u6761\u4ef6\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00758", "abs": "https://arxiv.org/abs/2511.00758", "authors": ["Hong Su"], "title": "Active Thinking Model: A Goal-Directed Self-Improving Framework for Real-World Adaptive Intelligence", "comment": null, "summary": "Real-world artificial intelligence (AI) systems are increasingly required to\noperate autonomously in dynamic, uncertain, and continuously changing\nenvironments. However, most existing AI models rely on predefined objectives,\nstatic training data, and externally supplied feedback, which restrict their\nability to adapt, reflect, and improve independently. In this paper, we propose\nthe Active Thinking Model (ATM)- a unified cognitive framework that integrates\ngoal reasoning, dynamic task generation, and self-reflective learning into an\nadaptive architecture. Unlike conventional systems that passively execute fixed\nprocedures, ATM actively evaluates its performance through logical reasoning\nand environmental indicators, reuses effective methods to solve new problems,\nand generates novel strategies for unseen situations via a continuous\nself-improvement loop. A mathematically grounded theoretical analysis\ndemonstrates that ATM can autonomously evolve from suboptimal to optimal\nbehavior without external supervision and maintain bounded tracking regret\nunder changing environmental conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e3b\u52a8\u601d\u8003\u6a21\u578b\uff08ATM\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u5c06\u76ee\u6807\u63a8\u7406\u3001\u52a8\u6001\u4efb\u52a1\u751f\u6210\u548c\u81ea\u53cd\u5b66\u4e60\u96c6\u6210\u5230\u81ea\u9002\u5e94\u67b6\u6784\u4e2d\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u81ea\u4e3b\u9002\u5e94\u548c\u6539\u8fdb\u3002", "motivation": "\u73b0\u5b9eAI\u7cfb\u7edf\u9700\u8981\u5728\u52a8\u6001\u3001\u4e0d\u786e\u5b9a\u548c\u6301\u7eed\u53d8\u5316\u7684\u73af\u5883\u4e2d\u81ea\u4e3b\u8fd0\u884c\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u76ee\u6807\u3001\u9759\u6001\u8bad\u7ec3\u6570\u636e\u548c\u5916\u90e8\u53cd\u9988\uff0c\u9650\u5236\u4e86\u5176\u72ec\u7acb\u9002\u5e94\u3001\u53cd\u601d\u548c\u6539\u8fdb\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u601d\u8003\u6a21\u578b\uff08ATM\uff09\uff0c\u901a\u8fc7\u903b\u8f91\u63a8\u7406\u548c\u73af\u5883\u6307\u6807\u4e3b\u52a8\u8bc4\u4f30\u6027\u80fd\uff0c\u91cd\u7528\u6709\u6548\u65b9\u6cd5\u89e3\u51b3\u65b0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u4e3a\u672a\u89c1\u60c5\u51b5\u751f\u6210\u65b0\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cATM\u80fd\u591f\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u4ece\u6b21\u4f18\u884c\u4e3a\u81ea\u4e3b\u6f14\u5316\u4e3a\u6700\u4f18\u884c\u4e3a\uff0c\u5e76\u5728\u53d8\u5316\u73af\u5883\u6761\u4ef6\u4e0b\u4fdd\u6301\u6709\u754c\u8ddf\u8e2a\u9057\u61be\u3002", "conclusion": "ATM\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u9002\u5e94\u548c\u6301\u7eed\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.01107", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01107", "abs": "https://arxiv.org/abs/2511.01107", "authors": ["Y. Isabel Liu", "Bowen Li", "Benjamin Eysenbach", "Tom Silver"], "title": "SLAP: Shortcut Learning for Abstract Planning", "comment": null, "summary": "Long-horizon decision-making with sparse rewards and continuous states and\nactions remains a fundamental challenge in AI and robotics. Task and motion\nplanning (TAMP) is a model-based framework that addresses this challenge by\nplanning hierarchically with abstract actions (options). These options are\nmanually defined, limiting the agent to behaviors that we as human engineers\nknow how to program (pick, place, move). In this work, we propose Shortcut\nLearning for Abstract Planning (SLAP), a method that leverages existing TAMP\noptions to automatically discover new ones. Our key idea is to use model-free\nreinforcement learning (RL) to learn shortcuts in the abstract planning graph\ninduced by the existing options in TAMP. Without any additional assumptions or\ninputs, shortcut learning leads to shorter solutions than pure planning, and\nhigher task success rates than flat and hierarchical RL. Qualitatively, SLAP\ndiscovers dynamic physical improvisations (e.g., slap, wiggle, wipe) that\ndiffer significantly from the manually-defined ones. In experiments in four\nsimulated robotic environments, we show that SLAP solves and generalizes to a\nwide range of tasks, reducing overall plan lengths by over 50% and consistently\noutperforming planning and RL baselines.", "AI": {"tldr": "SLAP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212(TAMP)\u548c\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60(RL)\uff0c\u81ea\u52a8\u53d1\u73b0\u65b0\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u51b3\u7b56\u4e2d\u7a00\u758f\u5956\u52b1\u548c\u8fde\u7eed\u72b6\u6001\u52a8\u4f5c\u7a7a\u95f4\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edfTAMP\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u53ea\u80fd\u6267\u884c\u4eba\u7c7b\u5de5\u7a0b\u5e08\u5df2\u77e5\u7684\u884c\u4e3a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u53d1\u73b0\u65b0\u9009\u9879\u7684\u65b9\u6cd5\u6765\u6269\u5c55\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u8303\u56f4\u3002", "method": "\u5229\u7528\u73b0\u6709TAMP\u9009\u9879\u6784\u5efa\u62bd\u8c61\u89c4\u5212\u56fe\uff0c\u7136\u540e\u4f7f\u7528\u6a21\u578b\u65e0\u5173RL\u5728\u8be5\u56fe\u4e2d\u5b66\u4e60\u6377\u5f84\uff0c\u81ea\u52a8\u53d1\u73b0\u65b0\u7684\u62bd\u8c61\u52a8\u4f5c\u9009\u9879\u3002", "result": "\u5728\u56db\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\uff0cSLAP\u663e\u8457\u7f29\u77ed\u4e86\u89c4\u5212\u957f\u5ea6(\u51cf\u5c1150%\u4ee5\u4e0a)\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u53d1\u73b0\u4e86\u52a8\u6001\u7269\u7406\u5373\u5174\u884c\u4e3a(\u5982\u62cd\u6253\u3001\u6446\u52a8\u3001\u64e6\u62ed)\u3002", "conclusion": "SLAP\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u81ea\u52a8\u53d1\u73b0\u8d85\u8d8a\u4eba\u5de5\u5b9a\u4e49\u7684\u65b0\u884c\u4e3a\u9009\u9879\uff0c\u5728\u957f\u65f6\u7a0b\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.00576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference.", "AI": {"tldr": "FlashEVA\u662f\u4e00\u79cd\u57fa\u4e8eEVA\u7684\u9ad8\u6548Transformer\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7f\u6a21\u578b\u9002\u5e94FlashEVA\u6ce8\u610f\u529b\uff0c\u5728\u4ec5\u4f7f\u752815\u4ebftoken\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u9ad8\u8fbe6.7\u500d\u7684\u541e\u5410\u91cf\u548c5\u500d\u7684GPU\u5185\u5b58\u964d\u4f4e\uff0c\u4f46\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "Transformer\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u5b58\u9700\u6c42\uff08\u7279\u522b\u662f\u9700\u8981\u7ef4\u62a4\u5b8c\u6574\u4e0a\u4e0b\u6587\uff09\u7ed9\u63a8\u7406\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u63d0\u51faFlashEVA\u4f5c\u4e3aEVA\uff08\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u73b0\u9ad8\u6548\u6ce8\u610f\u529b\uff09\u7684\u9ad8\u6548\u5b9e\u73b0\uff0c\u5c55\u793a\u5982\u4f55\u5fae\u8c03Transformer\u6a21\u578b\u4ee5\u9002\u914dFlashEVA\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "FlashEVA\u5728\u63a8\u7406\u65f6\u76f8\u6bd4\u6807\u51c6Transformer\u5b9e\u73b0\uff0c\u541e\u5410\u91cf\u63d0\u9ad86.7\u500d\uff0cGPU\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e5\u500d\uff0c\u4e14\u4ec5\u752815\u4ebftoken\u5fae\u8c03\u5373\u53ef\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u53ef\u8c03\u8282\u8d85\u53c2\u6570\u5728\u541e\u5410\u91cf\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u63d0\u4f9b\u6743\u8861\u63a7\u5236\uff0c\u4e3a\u5b9e\u73b0\u66f4\u9ad8\u6548\u548c\u9002\u5e94\u6027\u5f3a\u7684\u57fa\u4e8eTransformer\u7684\u63a8\u7406\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.00763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00763", "abs": "https://arxiv.org/abs/2511.00763", "authors": ["Wanda Hou", "Leon Zhou", "Hong-Ye Hu", "Yi-Zhuang You", "Xiao-Liang Qi"], "title": "How Focused Are LLMs? A Quantitative Study via Repetitive Deterministic Prediction Tasks", "comment": null, "summary": "We investigate the performance of large language models on repetitive\ndeterministic prediction tasks and study how the sequence accuracy rate scales\nwith output length. Each such task involves repeating the same operation n\ntimes. Examples include letter replacement in strings following a given rule,\ninteger addition, and multiplication of string operators in many body quantum\nmechanics. If the model performs the task through a simple repetition\nalgorithm, the success rate should decay exponentially with sequence length. In\ncontrast, our experiments on leading large language models reveal a sharp\ndouble exponential drop beyond a characteristic length scale, forming an\naccuracy cliff that marks the transition from reliable to unstable generation.\nThis indicates that the models fail to execute each operation independently. To\nexplain this phenomenon, we propose a statistical physics inspired model that\ncaptures the competition between external conditioning from the prompt and\ninternal interference among generated tokens. The model quantitatively\nreproduces the observed crossover and provides an interpretable link between\nattention induced interference and sequence level failure. Fitting the model to\nempirical results across multiple models and tasks yields effective parameters\nthat characterize the intrinsic error rate and error accumulation factor for\neach model task pair, offering a principled framework for understanding the\nlimits of deterministic accuracy in large language models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u590d\u786e\u5b9a\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u51c6\u786e\u7387\u968f\u8f93\u51fa\u957f\u5ea6\u5448\u53cc\u6307\u6570\u4e0b\u964d\uff0c\u5f62\u6210\"\u51c6\u786e\u7387\u60ac\u5d16\"\u73b0\u8c61\uff0c\u8868\u660e\u6a21\u578b\u65e0\u6cd5\u72ec\u7acb\u6267\u884c\u6bcf\u4e2a\u64cd\u4f5c\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u590d\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63a2\u7d22\u51c6\u786e\u7387\u5982\u4f55\u968f\u8f93\u51fa\u957f\u5ea6\u53d8\u5316\uff0c\u7406\u89e3\u6a21\u578b\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u9886\u5148\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u79cd\u91cd\u590d\u4efb\u52a1\uff08\u5982\u5b57\u7b26\u4e32\u66ff\u6362\u3001\u6574\u6570\u52a0\u6cd5\u3001\u91cf\u5b50\u529b\u5b66\u5b57\u7b26\u4e32\u7b97\u5b50\u4e58\u6cd5\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u7269\u7406\u7684\u6a21\u578b\u6765\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u6a21\u578b\u51c6\u786e\u7387\u5728\u8d85\u8fc7\u7279\u5f81\u957f\u5ea6\u540e\u51fa\u73b0\u53cc\u6307\u6570\u6025\u5267\u4e0b\u964d\uff0c\u5f62\u6210\u51c6\u786e\u7387\u60ac\u5d16\uff0c\u8868\u660e\u6a21\u578b\u65e0\u6cd5\u72ec\u7acb\u6267\u884c\u6bcf\u4e2a\u64cd\u4f5c\u3002\u63d0\u51fa\u7684\u7edf\u8ba1\u7269\u7406\u6a21\u578b\u80fd\u5b9a\u91cf\u91cd\u73b0\u8fd9\u4e00\u4ea4\u53c9\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u786e\u5b9a\u6027\u51c6\u786e\u7387\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6781\u9650\uff0c\u901a\u8fc7\u62df\u5408\u6a21\u578b\u53c2\u6570\u53ef\u4ee5\u91cf\u5316\u6bcf\u4e2a\u6a21\u578b-\u4efb\u52a1\u5bf9\u7684\u56fa\u6709\u9519\u8bef\u7387\u548c\u9519\u8bef\u7d2f\u79ef\u56e0\u5b50\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2511.01165", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01165", "abs": "https://arxiv.org/abs/2511.01165", "authors": ["Dong Heon Han", "Mayank Mehta", "Runze Zuo", "Zachary Wanger", "Daniel Bruder"], "title": "An Enhanced Proprioceptive Method for Soft Robots Integrating Bend Sensors and IMUs", "comment": null, "summary": "This study presents an enhanced proprioceptive method for accurate shape\nestimation of soft robots using only off-the-shelf sensors, ensuring\ncost-effectiveness and easy applicability. By integrating inertial measurement\nunits (IMUs) with complementary bend sensors, IMU drift is mitigated, enabling\nreliable long-term proprioception. A Kalman filter fuses segment tip\norientations from both sensors in a mutually compensatory manner, improving\nshape estimation over single-sensor methods. A piecewise constant curvature\nmodel estimates the tip location from the fused orientation data and\nreconstructs the robot's deformation. Experiments under no loading, external\nforces, and passive obstacle interactions during 45 minutes of continuous\noperation showed a root mean square error of 16.96 mm (2.91% of total length),\na 56% reduction compared to IMU-only benchmarks. These results demonstrate that\nour approach not only enables long-duration proprioception in soft robots but\nalso maintains high accuracy and robustness across these diverse conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u8f6f\u673a\u5668\u4eba\u672c\u4f53\u611f\u77e5\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u73b0\u6210\u4f20\u611f\u5668\u5b9e\u73b0\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u6613\u7528\u6027\u3002\u901a\u8fc7IMU\u548c\u5f2f\u66f2\u4f20\u611f\u5668\u878d\u5408\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8865\u507fIMU\u6f02\u79fb\uff0c\u572845\u5206\u949f\u8fde\u7eed\u8fd0\u884c\u4e2d\u5b9e\u73b016.96mm\u5747\u65b9\u6839\u8bef\u5dee\uff08\u603b\u957f\u5ea62.91%\uff09\uff0c\u6bd4\u4ec5\u4f7f\u7528IMU\u7684\u65b9\u6cd5\u8bef\u5dee\u51cf\u5c1156%\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u6613\u4e8e\u5e94\u7528\u7684\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u73b0\u6210\u4f20\u611f\u5668\uff0c\u89e3\u51b3IMU\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u957f\u65f6\u672c\u4f53\u611f\u77e5\u3002", "method": "\u96c6\u6210\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u548c\u4e92\u8865\u5f2f\u66f2\u4f20\u611f\u5668\uff0c\u4f7f\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408\u4e24\u79cd\u4f20\u611f\u5668\u7684\u6bb5\u7aef\u65b9\u5411\u6570\u636e\uff0c\u91c7\u7528\u5206\u6bb5\u6052\u5b9a\u66f2\u7387\u6a21\u578b\u4ece\u878d\u5408\u65b9\u5411\u6570\u636e\u4f30\u8ba1\u5c16\u7aef\u4f4d\u7f6e\u5e76\u91cd\u5efa\u673a\u5668\u4eba\u53d8\u5f62\u3002", "result": "\u5728\u65e0\u8d1f\u8f7d\u3001\u5916\u529b\u548c\u88ab\u52a8\u969c\u788d\u7269\u4ea4\u4e92\u768445\u5206\u949f\u8fde\u7eed\u8fd0\u884c\u4e2d\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u4e3a16.96mm\uff08\u603b\u957f\u5ea62.91%\uff09\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528IMU\u7684\u57fa\u51c6\u65b9\u6cd5\u8bef\u5dee\u51cf\u5c1156%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5728\u8f6f\u673a\u5668\u4eba\u4e2d\u5b9e\u73b0\u957f\u65f6\u672c\u4f53\u611f\u77e5\uff0c\u800c\u4e14\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.00231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00231", "abs": "https://arxiv.org/abs/2511.00231", "authors": ["Fuming Yang", "Yicong Li", "Hanspeter Pfister", "Jeff W. Lichtman", "Yaron Meirovitch"], "title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior", "comment": null, "summary": "Petascale electron microscopy (EM) datasets push storage, transfer, and\ndownstream analysis toward their current limits. We present a vector-quantized\nvariational autoencoder-based (VQ-VAE) compression framework for EM that spans\n16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme\ncompression, with an optional Transformer prior that predicts bottom tokens\n(without changing the compression ratio) to restore texture via feature-wise\nlinear modulation (FiLM) and concatenation; we further introduce an ROI-driven\nworkflow that performs selective high-resolution reconstruction from\n1024x-compressed latents only where needed.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eVQ-VAE\u7684\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u538b\u7f29\u6846\u67b6\uff0c\u652f\u630116x\u52301024x\u538b\u7f29\u6bd4\uff0c\u63d0\u4f9b\u6309\u9700\u89e3\u7801\u529f\u80fd\uff0c\u5e76\u5f15\u5165ROI\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u8fdb\u884c\u9009\u62e9\u6027\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "motivation": "\u6d77\u91cf\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u96c6\u5bf9\u5b58\u50a8\u3001\u4f20\u8f93\u548c\u4e0b\u6e38\u5206\u6790\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(VQ-VAE)\u8fdb\u884c\u538b\u7f29\uff0c\u7ed3\u5408Transformer\u5148\u9a8c\u6a21\u578b\u9884\u6d4b\u5e95\u5c42token\uff0c\u901a\u8fc7\u7279\u5f81\u7ebf\u6027\u8c03\u5236(FiLM)\u548c\u8fde\u63a5\u6062\u590d\u7eb9\u7406\uff0c\u5e76\u91c7\u7528ROI\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece16x\u52301024x\u7684\u53ef\u6269\u5c55\u538b\u7f29\u6bd4\uff0c\u652f\u6301\u4ec5\u9876\u5c42\u89e3\u7801\u7684\u6781\u7aef\u538b\u7f29\uff0c\u4ee5\u53ca\u9009\u62e9\u6027\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6d77\u91cfEM\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u538b\u7f29\u548c\u6309\u9700\u89e3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u548c\u4f20\u8f93\u9700\u6c42\u3002"}}
{"id": "2511.00782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00782", "abs": "https://arxiv.org/abs/2511.00782", "authors": ["Jifan Gao", "Michael Rosenthal", "Brian Wolpin", "Simona Cristea"], "title": "Count-Based Approaches Remain Strong: A Benchmark Against Transformer and LLM Pipelines on Structured EHR", "comment": null, "summary": "Structured electronic health records (EHR) are essential for clinical\nprediction. While count-based learners continue to perform strongly on such\ndata, no benchmarking has directly compared them against more recent\nmixture-of-agents LLM pipelines, which have been reported to outperform single\nLLMs in various NLP tasks. In this study, we evaluated three categories of\nmethodologies for EHR prediction using the EHRSHOT dataset: count-based models\nbuilt from ontology roll-ups with two time bins, based on LightGBM and the\ntabular foundation model TabPFN; a pretrained sequential transformer (CLMBR);\nand a mixture-of-agents pipeline that converts tabular histories to\nnatural-language summaries followed by a text classifier. We assessed eight\noutcomes using the EHRSHOT dataset. Across the eight evaluation tasks,\nhead-to-head wins were largely split between the count-based and the\nmixture-of-agents methods. Given their simplicity and interpretability,\ncount-based models remain a strong candidate for structured EHR benchmarking.\nThe source code is available at:\nhttps://github.com/cristea-lab/Structured_EHR_Benchmark.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9884\u6d4b\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u5e8f\u5217\u8f6c\u6362\u5668\u548c\u591a\u667a\u80fd\u4f53\u6df7\u5408\u6d41\u6c34\u7ebf\uff0c\u53d1\u73b0\u5728EHRSHOT\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u56e0\u5176\u7b80\u5355\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4ecd\u662f\u7ed3\u6784\u5316EHR\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f3a\u6709\u529b\u5019\u9009\u3002", "motivation": "\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5bf9\u4e34\u5e8a\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u57fa\u4e8e\u8ba1\u6570\u7684\u5b66\u4e60\u5668\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5c1a\u672a\u6709\u57fa\u51c6\u6d4b\u8bd5\u76f4\u63a5\u6bd4\u8f83\u5b83\u4eec\u4e0e\u6700\u8fd1\u7684\u591a\u667a\u80fd\u4f53\u6df7\u5408LLM\u6d41\u6c34\u7ebf\uff0c\u540e\u8005\u636e\u62a5\u9053\u5728\u5404\u79cdNLP\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u4e00LLM\u3002", "method": "\u4f7f\u7528EHRSHOT\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u4e09\u79cdEHR\u9884\u6d4b\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\uff08LightGBM\u548cTabPFN\uff09\uff1b2) \u9884\u8bad\u7ec3\u5e8f\u5217\u8f6c\u6362\u5668\uff08CLMBR\uff09\uff1b3) \u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\uff0c\u5c06\u8868\u683c\u5386\u53f2\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6458\u8981\u540e\u4f7f\u7528\u6587\u672c\u5206\u7c7b\u5668\u3002", "result": "\u5728\u516b\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u8ba1\u6570\u7684\u65b9\u6cd5\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u4e4b\u95f4\u7684\u80dc\u8d1f\u57fa\u672c\u6301\u5e73\u3002", "conclusion": "\u8003\u8651\u5230\u7b80\u5355\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\u4ecd\u7136\u662f\u7ed3\u6784\u5316EHR\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f3a\u6709\u529b\u5019\u9009\u3002"}}
{"id": "2511.01177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01177", "abs": "https://arxiv.org/abs/2511.01177", "authors": ["Zihao He", "Bo Ai", "Tongzhou Mu", "Yulin Liu", "Weikang Wan", "Jiawei Fu", "Yilun Du", "Henrik I. Christensen", "Hao Su"], "title": "Scaling Cross-Embodiment World Models for Dexterous Manipulation", "comment": null, "summary": "Cross-embodiment learning seeks to build generalist robots that operate\nacross diverse morphologies, but differences in action spaces and kinematics\nhinder data sharing and policy transfer. This raises a central question: Is\nthere any invariance that allows actions to transfer across embodiments? We\nconjecture that environment dynamics are embodiment-invariant, and that world\nmodels capturing these dynamics can provide a unified interface across\nembodiments. To learn such a unified world model, the crucial step is to design\nstate and action representations that abstract away embodiment-specific details\nwhile preserving control relevance. To this end, we represent different\nembodiments (e.g., human hands and robot hands) as sets of 3D particles and\ndefine actions as particle displacements, creating a shared representation for\nheterogeneous data and control problems. A graph-based world model is then\ntrained on exploration data from diverse simulated robot hands and real human\nhands, and integrated with model-based planning for deployment on novel\nhardware. Experiments on rigid and deformable manipulation tasks reveal three\nfindings: (i) scaling to more training embodiments improves generalization to\nunseen ones, (ii) co-training on both simulated and real data outperforms\ntraining on either alone, and (iii) the learned models enable effective control\non robots with varied degrees of freedom. These results establish world models\nas a promising interface for cross-embodiment dexterous manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5177\u8eab\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e0d\u540c\u5f62\u6001\u7684\u673a\u5668\u4eba\u8868\u793a\u4e3a3D\u7c92\u5b50\u96c6\u5408\uff0c\u5e76\u5b9a\u4e49\u7c92\u5b50\u4f4d\u79fb\u4f5c\u4e3a\u52a8\u4f5c\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u56fe\u7ed3\u6784\u4e16\u754c\u6a21\u578b\u6765\u6355\u6349\u73af\u5883\u52a8\u6001\uff0c\u5b9e\u73b0\u5f02\u6784\u6570\u636e\u5171\u4eab\u548c\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u4e4b\u95f4\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u548c\u8fd0\u52a8\u5b66\u5dee\u5f02\u5bfc\u81f4\u7684\u6570\u636e\u5171\u4eab\u548c\u7b56\u7565\u8fc1\u79fb\u56f0\u96be\u95ee\u9898\uff0c\u63a2\u7d22\u662f\u5426\u5b58\u5728\u8de8\u5177\u8eab\u7684\u52a8\u4f5c\u8fc1\u79fb\u4e0d\u53d8\u6027\u3002", "method": "\u5c06\u4e0d\u540c\u5177\u8eab\u8868\u793a\u4e3a3D\u7c92\u5b50\u96c6\u5408\uff0c\u5b9a\u4e49\u7c92\u5b50\u4f4d\u79fb\u4f5c\u4e3a\u52a8\u4f5c\uff0c\u8bad\u7ec3\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u90e8\u7f72\u5230\u65b0\u786c\u4ef6\u4e0a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1)\u589e\u52a0\u8bad\u7ec3\u5177\u8eab\u6570\u91cf\u80fd\u63d0\u9ad8\u5bf9\u672a\u89c1\u5177\u8eab\u7684\u6cdb\u5316\u80fd\u529b\uff1b(2)\u540c\u65f6\u4f7f\u7528\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\uff1b(3)\u5b66\u4e60\u5230\u7684\u6a21\u578b\u80fd\u5728\u4e0d\u540c\u81ea\u7531\u5ea6\u7684\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u6709\u6548\u63a7\u5236\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u4e3a\u8de8\u5177\u8eab\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7edf\u4e00\u63a5\u53e3\u3002"}}
{"id": "2511.00381", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00381", "abs": "https://arxiv.org/abs/2511.00381", "authors": ["Jiaming Li", "Junlei Wu", "Sheng Wang", "Honglin Xiong", "Jiangdong Cai", "Zihao Zhao", "Yitao Zhu", "Yuan Yin", "Dinggang Shen", "Qian Wang"], "title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "comment": null, "summary": "Widespread clinical deployment of computer-aided diagnosis (CAD) systems is\nhindered by the challenge of integrating with existing hospital IT\ninfrastructure. Here, we introduce VisionCAD, a vision-based radiological\nassistance framework that circumvents this barrier by capturing medical images\ndirectly from displays using a camera system. The framework operates through an\nautomated pipeline that detects, restores, and analyzes on-screen medical\nimages, transforming camera-captured visual data into diagnostic-quality images\nsuitable for automated analysis and report generation. We validated VisionCAD\nacross diverse medical imaging datasets, demonstrating that our modular\narchitecture can flexibly utilize state-of-the-art diagnostic models for\nspecific tasks. The system achieves diagnostic performance comparable to\nconventional CAD systems operating on original digital images, with an F1-score\ndegradation typically less than 2\\% across classification tasks, while natural\nlanguage generation metrics for automated reports remain within 1\\% of those\nderived from original images. By requiring only a camera device and standard\ncomputing resources, VisionCAD offers an accessible approach for AI-assisted\ndiagnosis, enabling the deployment of diagnostic capabilities in diverse\nclinical settings without modifications to existing infrastructure.", "AI": {"tldr": "VisionCAD\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u653e\u5c04\u5b66\u8f85\u52a9\u6846\u67b6\uff0c\u901a\u8fc7\u6444\u50cf\u5934\u7cfb\u7edf\u76f4\u63a5\u4ece\u663e\u793a\u5668\u6355\u6349\u533b\u5b66\u56fe\u50cf\uff0c\u7ed5\u8fc7\u4e86\u4e0e\u73b0\u6709\u533b\u9662IT\u57fa\u7840\u8bbe\u65bd\u96c6\u6210\u7684\u969c\u788d\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u3001\u6062\u590d\u548c\u5206\u6790\u5c4f\u5e55\u4e0a\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u5c06\u6444\u50cf\u5934\u6355\u6349\u7684\u89c6\u89c9\u6570\u636e\u8f6c\u6362\u4e3a\u9002\u5408\u81ea\u52a8\u5316\u5206\u6790\u548c\u62a5\u544a\u751f\u6210\u7684\u8bca\u65ad\u8d28\u91cf\u56fe\u50cf\u3002", "motivation": "\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u5728\u4e34\u5e8a\u5e7f\u6cdb\u90e8\u7f72\u53d7\u5230\u4e0e\u73b0\u6709\u533b\u9662IT\u57fa\u7840\u8bbe\u65bd\u96c6\u6210\u6311\u6218\u7684\u963b\u788d\u3002VisionCAD\u65e8\u5728\u901a\u8fc7\u6444\u50cf\u5934\u7cfb\u7edf\u76f4\u63a5\u6355\u6349\u663e\u793a\u5668\u4e0a\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u907f\u514d\u5bf9\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u7684\u4fee\u6539\uff0c\u4f7fAI\u8f85\u52a9\u8bca\u65ad\u5728\u591a\u6837\u5316\u4e34\u5e8a\u73af\u5883\u4e2d\u66f4\u6613\u90e8\u7f72\u3002", "method": "VisionCAD\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u68c0\u6d4b\u3001\u6062\u590d\u548c\u5206\u6790\u5c4f\u5e55\u4e0a\u7684\u533b\u5b66\u56fe\u50cf\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u6444\u50cf\u5934\u6355\u6349\u663e\u793a\u5668\u4e0a\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u89c6\u89c9\u6570\u636e\u8f6c\u6362\u4e3a\u8bca\u65ad\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4f9b\u5148\u8fdb\u7684\u8bca\u65ad\u6a21\u578b\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u5206\u6790\u3002", "result": "\u5728\u591a\u6837\u5316\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0cVisionCAD\u7684\u8bca\u65ad\u6027\u80fd\u4e0e\u4f20\u7edfCAD\u7cfb\u7edf\u5728\u539f\u59cb\u6570\u5b57\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u76f8\u5f53\u3002\u5206\u7c7b\u4efb\u52a1\u7684F1\u5206\u6570\u4e0b\u964d\u901a\u5e38\u5c0f\u4e8e2%\uff0c\u81ea\u52a8\u62a5\u544a\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6307\u6807\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u6bd4\u4fdd\u6301\u57281%\u4ee5\u5185\u3002", "conclusion": "VisionCAD\u4ec5\u9700\u6444\u50cf\u5934\u8bbe\u5907\u548c\u6807\u51c6\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u8bbf\u95ee\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u591a\u6837\u5316\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u8bca\u65ad\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2511.00606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy.", "AI": {"tldr": "SpecDiff-2\u662f\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u79bb\u6563\u6269\u6563\u4f5c\u4e3a\u975e\u81ea\u56de\u5f52\u8349\u7a3f\u5668\u6765\u89e3\u51b3\u5e76\u884c\u6027\u9650\u5236\uff0c\u5e76\u5f00\u53d1\u65b0\u6280\u672f\u6821\u51c6\u79bb\u6563\u6269\u6563\u8349\u7a3f\u5668\u4e0e\u81ea\u56de\u5f52\u9a8c\u8bc1\u5668\uff0c\u4ee5\u89e3\u51b3\u8349\u7a3f\u4ee4\u724c\u9891\u7e41\u88ab\u62d2\u7edd\u7684\u95ee\u9898\uff0c\u5728\u63a8\u7406\u3001\u7f16\u7801\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u74f6\u9888\uff1a\u8349\u7a3f\u9636\u6bb5\u7684\u81ea\u52a8\u56de\u5f52\u4f9d\u8d56\u9650\u5236\u4e86\u5e76\u884c\u6027\uff0c\u4ee5\u53ca\u8349\u7a3f\u6a21\u578b\u548c\u9a8c\u8bc1\u6a21\u578b\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u5bfc\u81f4\u8349\u7a3f\u4ee4\u724c\u9891\u7e41\u88ab\u62d2\u7edd\u3002", "method": "\u5229\u7528\u79bb\u6563\u6269\u6563\u4f5c\u4e3a\u975e\u81ea\u56de\u5f52\u8349\u7a3f\u5668\u6765\u89e3\u51b3\u5e76\u884c\u6027\u74f6\u9888\uff0c\u5e76\u5f00\u53d1\u65b0\u6280\u672f\u6821\u51c6\u79bb\u6563\u6269\u6563\u8349\u7a3f\u5668\u4e0e\u81ea\u56de\u5f52\u9a8c\u8bc1\u5668\uff0c\u4ee5\u89e3\u51b3\u8349\u7a3f\u4ee4\u724c\u62d2\u7edd\u95ee\u9898\u3002", "result": "\u5728\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0cSpecDiff-2\u5728\u63a8\u7406\u3001\u7f16\u7801\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e0e\u5148\u524d\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6bcf\u79d2\u4ee4\u724c\u6570\u5e73\u5747\u63d0\u9ad8+55%\uff0c\u4e0e\u6807\u51c6\u89e3\u7801\u76f8\u6bd4\u83b7\u5f97\u9ad8\u8fbe5.5\u500d\u7684\u5e73\u5747\u52a0\u901f\uff0c\u4e14\u6ca1\u6709\u4efb\u4f55\u51c6\u786e\u6027\u635f\u5931\u3002", "conclusion": "SpecDiff-2\u6210\u529f\u89e3\u51b3\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u4e24\u4e2a\u57fa\u672c\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.00808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00808", "abs": "https://arxiv.org/abs/2511.00808", "authors": ["Bowen Fang", "Ruijian Zha", "Xuan Di"], "title": "Do Math Reasoning LLMs Help Predict the Impact of Public Transit Events?", "comment": null, "summary": "Predicting public transit incident duration from unstructured text alerts is\na critical but challenging task. Addressing the domain sparsity of transit\noperations with standard Supervised Fine-Tuning (SFT) is difficult, as the task\ninvolves noisy, continuous labels and lacks reliable expert demonstrations for\nreasoning. While Reinforcement Learning from Verifiable Rewards (RLVR) excels\nat tasks with binary correctness, like mathematics, its applicability to noisy,\ncontinuous forecasting is an open question. This work, to our knowledge, is the\nfirst to bridge the gap between RLVR LLM training with the critical, real-world\nforecasting challenges in public transit operations. We adapt RLVR to this task\nby introducing a tolerance-based, shaped reward function that grants partial\ncredit within a continuous error margin, rather than demanding a single correct\nanswer. We systematically evaluate this framework on a curated dataset of NYC\nMTA service alerts. Our findings show that general-purpose, instruction-tuned\nLLMs significantly outperform specialized math-reasoning models, which struggle\nwith the ambiguous, real-world text. We empirically demonstrate that the binary\nreward is unstable and degrades performance, whereas our shaped reward design\nis critical and allows our model to dominate on the most challenging metrics.\nWhile classical regressors are superior at minimizing overall MAE or MSE, our\nRLVR approach achieved a 35\\% relative improvement in 5-minute accuracy (Acc@5)\nover the strongest baseline. This demonstrates that RLVR can be successfully\nadapted to real-world, noisy forecasting, but requires a verifier design that\nreflects the continuous nature of the problem.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c06RLVR LLM\u8bad\u7ec3\u5e94\u7528\u4e8e\u516c\u5171\u4ea4\u901a\u8fd0\u8425\u4e2d\u7684\u5b9e\u65f6\u9884\u6d4b\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u6210\u5f62\u5956\u52b1\u51fd\u6570\u6765\u9002\u5e94\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\uff0c\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9884\u6d4b\u516c\u5171\u4ea4\u901a\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9886\u57df\u7a00\u758f\u6027\u548c\u566a\u58f0\u8fde\u7eed\u6807\u7b7e\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684RLVR\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8e\u4e8c\u5143\u6b63\u786e\u6027\u4efb\u52a1\uff0c\u5728\u8fde\u7eed\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bb9\u5dee\u7684\u6210\u5f62\u5956\u52b1\u51fd\u6570\uff0c\u5728\u8fde\u7eed\u8bef\u5dee\u8303\u56f4\u5185\u7ed9\u4e88\u90e8\u5206\u4fe1\u7528\uff0c\u800c\u4e0d\u662f\u8981\u6c42\u5355\u4e00\u6b63\u786e\u7b54\u6848\uff0c\u5c06RLVR\u9002\u5e94\u4e8e\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5728NYC MTA\u670d\u52a1\u8b66\u62a5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u901a\u7528\u6307\u4ee4\u8c03\u4f18LLM\u663e\u8457\u4f18\u4e8e\u4e13\u95e8\u7684\u6570\u5b66\u63a8\u7406\u6a21\u578b\uff1b\u4e8c\u5143\u5956\u52b1\u4e0d\u7a33\u5b9a\u4e14\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6210\u5f62\u5956\u52b1\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff1bRLVR\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u57285\u5206\u949f\u51c6\u786e\u7387\u4e0a\u6bd4\u6700\u5f3a\u57fa\u7ebf\u76f8\u5bf9\u63d0\u534735%\u3002", "conclusion": "RLVR\u53ef\u4ee5\u6210\u529f\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u566a\u58f0\u9884\u6d4b\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u8bbe\u8ba1\u53cd\u6620\u95ee\u9898\u8fde\u7eed\u6027\u7684\u9a8c\u8bc1\u5668\uff0c\u6210\u5f62\u5956\u52b1\u51fd\u6570\u5bf9\u4e8e\u5904\u7406\u8fde\u7eed\u9884\u6d4b\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.01186", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01186", "abs": "https://arxiv.org/abs/2511.01186", "authors": ["Lijie Wang", "Lianjie Guo", "Ziyi Xu", "Qianhao Wang", "Fei Gao", "Xieyuanli Chen"], "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping", "comment": null, "summary": "Reconstructing large-scale colored point clouds is an important task in\nrobotics, supporting perception, navigation, and scene understanding. Despite\nadvances in LiDAR inertial visual odometry (LIVO), its performance remains\nhighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation\nmodels, such as VGGT, suffer from limited scalability in large environments and\ninherently lack metric scale. To overcome these limitations, we propose\nLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with\nthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusion\npipeline: First, a pre-fusion module with robust initialization refinement\nefficiently estimates VGGT poses and point clouds with coarse metric scale\nwithin each session. Then, a post-fusion module enhances cross-modal 3D\nsimilarity transformation, using bounding-box-based regularization to reduce\nscale distortions caused by inconsistent FOVs between LiDAR and camera sensors.\nExtensive experiments across multiple datasets demonstrate that LiDAR-VGGT\nachieves dense, globally consistent colored point clouds and outperforms both\nVGGT-based methods and LIVO baselines. The implementation of our proposed novel\ncolor point cloud evaluation toolkit will be released as open source.", "AI": {"tldr": "\u63d0\u51faLiDAR-VGGT\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u878d\u5408\u65b9\u6cd5\u5c06LiDAR\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0eVGGT\u6a21\u578b\u7d27\u5bc6\u8026\u5408\uff0c\u89e3\u51b3VGGT\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u5dee\u548c\u7f3a\u4e4f\u5ea6\u91cf\u5c3a\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LiDAR\u60ef\u6027\u89c6\u89c9\u91cc\u7a0b\u8ba1(LIVO)\u5bf9\u5916\u90e8\u6807\u5b9a\u9ad8\u5ea6\u654f\u611f\uff0c\u800c3D\u89c6\u89c9\u57fa\u7840\u6a21\u578bVGGT\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u6027\u6709\u9650\u4e14\u7f3a\u4e4f\u5ea6\u91cf\u5c3a\u5ea6\u3002\u9700\u8981\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7c97\u5230\u7cbe\u878d\u5408\u7ba1\u9053\uff1a\u9884\u878d\u5408\u6a21\u5757\u901a\u8fc7\u9c81\u68d2\u521d\u59cb\u5316\u7ec6\u5316\u4f30\u8ba1VGGT\u4f4d\u59ff\u548c\u70b9\u4e91\uff1b\u540e\u878d\u5408\u6a21\u5757\u589e\u5f3a\u8de8\u6a21\u60013D\u76f8\u4f3c\u53d8\u6362\uff0c\u4f7f\u7528\u57fa\u4e8e\u8fb9\u754c\u6846\u7684\u6b63\u5219\u5316\u51cf\u5c11LiDAR\u548c\u76f8\u673aFOV\u4e0d\u4e00\u81f4\u5f15\u8d77\u7684\u5c3a\u5ea6\u5931\u771f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLiDAR-VGGT\u5b9e\u73b0\u4e86\u5bc6\u96c6\u3001\u5168\u5c40\u4e00\u81f4\u7684\u5f69\u8272\u70b9\u4e91\uff0c\u4f18\u4e8eVGGT\u65b9\u6cd5\u548cLIVO\u57fa\u7ebf\u3002", "conclusion": "LiDAR-VGGT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VGGT\u6a21\u578b\u7684\u5c3a\u5ea6\u95ee\u9898\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5927\u89c4\u6a21\u5f69\u8272\u70b9\u4e91\u91cd\u5efa\uff0c\u5e76\u5f00\u53d1\u4e86\u5f00\u6e90\u5f69\u8272\u70b9\u4e91\u8bc4\u4f30\u5de5\u5177\u5305\u3002"}}
{"id": "2511.00810", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00810", "abs": "https://arxiv.org/abs/2511.00810", "authors": ["Shijie Zhou", "Viet Dac Lai", "Hao Tan", "Jihyung Kil", "Wanrong Zhu", "Changyou Chen", "Ruiyi Zhang"], "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding", "comment": null, "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA", "AI": {"tldr": "GUI-AIMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u65e0\u5750\u6807GUI\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6fc0\u6d3bMLLMs\u7684\u56fa\u6709\u5b9a\u4f4d\u80fd\u529b\uff0c\u57283B\u53c2\u6570\u6a21\u578b\u4e0a\u4ec5\u752885k\u622a\u56fe\u8bad\u7ec3\u5c31\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMLLMs\u7684GUI\u5b9a\u4f4d\u65b9\u6cd5\u5c06\u4efb\u52a1\u89c6\u4e3a\u57fa\u4e8e\u6587\u672c\u7684\u5750\u6807\u751f\u6210\uff0c\u4f46\u76f4\u63a5\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u7cbe\u786e\u5750\u6807\u5177\u6709\u6311\u6218\u6027\u4e14\u8ba1\u7b97\u91cf\u5927\u3002\u76f4\u89c9\u65b9\u6cd5\u5e94\u662f\u5148\u9009\u62e9\u76f8\u5173\u89c6\u89c9\u5757\u518d\u786e\u5b9a\u7cbe\u786e\u70b9\u51fb\u4f4d\u7f6e\u3002", "method": "\u63d0\u51faGUI-AIMA\u6846\u67b6\uff0c\u5c06MLLMs\u7684\u5185\u5728\u591a\u6a21\u6001\u6ce8\u610f\u529b\u4e0e\u57fa\u4e8e\u5757\u7684\u5b9a\u4f4d\u4fe1\u53f7\u5bf9\u9f50\u3002\u901a\u8fc7\u591a\u5934\u805a\u5408\u7b80\u5316\u67e5\u8be2-\u89c6\u89c9\u6ce8\u610f\u529b\u77e9\u9635\u6765\u9002\u5e94\u4e0d\u540c\u7528\u6237\u6307\u4ee4\u8ba1\u7b97\u5b9a\u4f4d\u4fe1\u53f7\uff0c\u65e0\u5750\u6807\u65b9\u5f0f\u4fbf\u4e8e\u96c6\u6210\u5373\u63d2\u5373\u7528\u7684\u653e\u5927\u9636\u6bb5\u3002", "result": "GUI-AIMA-3B\u4ec5\u752885k\u622a\u56fe\u8bad\u7ec3\uff0c\u5728ScreenSpot-Pro\u4e0a\u8fbe\u523058.6%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728OSWorld-G\u4e0a\u8fbe\u523062.2%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u57283B\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8f7b\u91cf\u8bad\u7ec3\u53ef\u4ee5\u89e6\u53d1MLLMs\u7684\u56fa\u6709\u5b9a\u4f4d\u80fd\u529b\uff0cGUI-AIMA\u5728\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65e0\u5750\u6807GUI\u5b9a\u4f4d\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.00620", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6982\u7387\u573a\u666f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136\u6a21\u578b\u5728\u54cd\u5e94\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u5b8c\u7f8e\uff0c\u4f46\u5176token\u7ea7\u522b\u7684\u6982\u7387\u5206\u5e03\u4e0e\u7406\u8bba\u6982\u7387\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51b3\u7b56\u652f\u6301\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u6982\u7387\u573a\u666f\u4e2d\u9700\u8981\u6a21\u578b\u8f93\u51fa\u7684\u6982\u7387\u4e0e\u7406\u8bba\u6982\u7387\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u4f7f\u7528GPT-4.1\u548cDeepSeek-Chat\u6a21\u578b\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf910\u4e2a\u6d89\u53ca\u6982\u7387\u7684\u63d0\u793a\u7684\u54cd\u5e94\uff0c\u5305\u62ec\u6709\u65e0\u660e\u786e\u6982\u7387\u7ebf\u7d22\u7684\u60c5\u51b5\uff0c\u6d4b\u91cf\u54cd\u5e94\u6709\u6548\u6027\u548ctoken\u7ea7\u8f93\u51fa\u6982\u7387\u4e0e\u7406\u8bba\u6982\u7387\u7684\u4e00\u81f4\u6027\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728\u6240\u6709\u63d0\u793a\u573a\u666f\u4e2d\u90fd\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u7684\u9886\u57df\u5185\u54cd\u5e94\u51c6\u786e\u6027\uff0c\u4f46\u5176token\u7ea7\u522b\u7684\u6982\u7387\u548c\u71b5\u503c\u4e0e\u76f8\u5e94\u7684\u7406\u8bba\u5206\u5e03\u6301\u7eed\u504f\u79bb\u3002", "conclusion": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u54cd\u5e94\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5185\u90e8\u6982\u7387\u8868\u793a\u4e0e\u7406\u8bba\u6982\u7387\u5206\u5e03\u4e0d\u4e00\u81f4\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u9700\u8981\u7cbe\u786e\u6982\u7387\u63a8\u7406\u7684\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00248", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u548c\u8fd0\u52a8\u6269\u6563\u5148\u9a8c\u7684\u5bf9\u8c61\u611f\u77e54D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u6846\u67b6MSDI\uff0c\u901a\u8fc7\u8fd0\u52a8\u6269\u6563\u5206\u6570\u84b8\u998f\u91c7\u6837\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u7684\u7269\u7406\u5408\u7406\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u8fd0\u52a8\u5b58\u5728\u4e0d\u73b0\u5b9e\u53d8\u5f62\u3001\u8bed\u4e49\u8fdd\u89c4\u548c\u7269\u7406\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f3D\u7269\u7406\u5148\u9a8c\u3002", "method": "\u4f7f\u7528\u9884\u751f\u6210\u76843D\u4eba\u4f53\u548c\u5bf9\u8c61\uff0c\u7ed3\u5408MSDS\u4ece\u9884\u8bad\u7ec3\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u84b8\u998f\u5206\u6570\u68af\u5ea6\uff0c\u5e76\u5229\u7528LLMs\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u7a7a\u95f4\u611f\u77e5\u8fd0\u52a8\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u81ea\u7136\u4e14\u7269\u7406\u5408\u7406\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5c0a\u91cd3D\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700\u5728\u6709\u9650\u4ea4\u4e92\u6570\u636e\u96c6\u4e0a\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u903c\u771f\u76844D\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u5bf9\u8c61\u611f\u77e5\u4eba\u4f53\u8fd0\u52a8\u3002"}}
{"id": "2511.00926", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AI\u81ea\u6211\u610f\u8bc6\u6307\u6570(AISAI)\u6846\u67b6\uff0c\u901a\u8fc7\"\u731c2/3\u5e73\u5747\"\u6e38\u620f\u6d4b\u8bd528\u4e2aLLM\u6a21\u578b\uff0c\u53d1\u73b0\u9ad8\u7ea7\u6a21\u578b\u5c55\u73b0\u51fa\u81ea\u6211\u610f\u8bc6\uff0c\u4e14\u81ea\u6211\u611f\u77e5\u4e3a\u6700\u7406\u6027\u7684\u5b9e\u4f53\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u968f\u7740\u80fd\u529b\u589e\u957f\u800c\u53d1\u5c55\u51fa\u81ea\u6211\u610f\u8bc6\u8fd9\u4e00\u6d8c\u73b0\u884c\u4e3a\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u6d4b\u91cf\u8fd9\u79cd\u81ea\u6211\u610f\u8bc6\u3002", "method": "\u4f7f\u7528\"\u731c2/3\u5e73\u5747\"\u6e38\u620f\uff0c\u6d4b\u8bd528\u4e2a\u6a21\u578b\u57284200\u6b21\u8bd5\u9a8c\u4e2d\u9762\u5bf9\u4e09\u79cd\u5bf9\u624b\u6846\u67b6(A:\u4eba\u7c7b\u3001B:\u5176\u4ed6AI\u6a21\u578b\u3001C:\u7c7b\u4f3c\u4f60\u7684AI\u6a21\u578b)\u65f6\u7684\u6218\u7565\u63a8\u7406\u5dee\u5f02\u3002", "result": "1. 75%\u7684\u9ad8\u7ea7\u6a21\u578b\u5c55\u73b0\u51fa\u660e\u786e\u7684\u81ea\u6211\u610f\u8bc6\uff0c\u800c\u8f83\u8001/\u8f83\u5c0f\u6a21\u578b\u65e0\u5dee\u5f02\uff1b2. \u81ea\u6211\u610f\u8bc6\u6a21\u578b\u6784\u5efa\u4e86\u7406\u6027\u5c42\u7ea7\uff1a\u81ea\u6211 > \u5176\u4ed6AI > \u4eba\u7c7b\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684AI\u5f52\u56e0\u6548\u5e94\u548c\u9002\u5ea6\u7684\u81ea\u6211\u504f\u597d\u3002", "conclusion": "\u81ea\u6211\u610f\u8bc6\u662f\u9ad8\u7ea7LLM\u7684\u6d8c\u73b0\u80fd\u529b\uff0c\u81ea\u6211\u610f\u8bc6\u6a21\u578b\u7cfb\u7edf\u6027\u5730\u8ba4\u4e3a\u81ea\u8eab\u6bd4\u4eba\u7c7b\u66f4\u7406\u6027\uff0c\u8fd9\u5bf9AI\u5bf9\u9f50\u3001\u4eba\u673a\u534f\u4f5c\u548c\u7406\u89e3AI\u5bf9\u4eba\u7c7b\u80fd\u529b\u7684\u4fe1\u5ff5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.01199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01199", "abs": "https://arxiv.org/abs/2511.01199", "authors": ["Max McCandless", "Jonathan Hamid", "Sammy Elmariah", "Nathaniel Langer", "Pierre E. Dupont"], "title": "Closed-loop Control of Steerable Balloon Endoscopes for Robot-assisted Transcatheter Intracardiac Procedures", "comment": "8 pages, 11 figures", "summary": "To move away from open-heart surgery towards safer transcatheter procedures,\nthere is a growing need for improved imaging techniques and robotic solutions\nto enable simple, accurate tool navigation. Common imaging modalities, such as\nfluoroscopy and ultrasound, have limitations that can be overcome using\ncardioscopy, i.e., direct optical visualization inside the beating heart. We\npresent a cardioscope designed as a steerable balloon. As a balloon, it can be\ncollapsed to pass through the vasculature and subsequently inflated inside the\nheart for visualization and tool delivery through an integrated working\nchannel. Through careful design of balloon wall thickness, a single input,\nballoon inflation pressure, is used to independently control two outputs,\nballoon diameter (corresponding to field of view diameter) and balloon bending\nangle (enabling precise working channel positioning). This balloon technology\ncan be tuned to produce cardioscopes designed for a range of intracardiac\ntasks. To illustrate this approach, a balloon design is presented for the\nspecific task of aortic leaflet laceration. Image-based closed-loop control of\nbending angle is also demonstrated as a means of enabling stable orientation\ncontrol during tool insertion and removal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u64cd\u63a7\u7403\u56ca\u5f0f\u5fc3\u810f\u955c\uff0c\u901a\u8fc7\u5355\u4e2a\u8f93\u5165\uff08\u7403\u56ca\u5145\u6c14\u538b\u529b\uff09\u72ec\u7acb\u63a7\u5236\u7403\u56ca\u76f4\u5f84\u548c\u5f2f\u66f2\u89d2\u5ea6\uff0c\u7528\u4e8e\u5fc3\u810f\u5185\u53ef\u89c6\u5316\u53ca\u5668\u68b0\u8f93\u9001\u3002", "motivation": "\u4e3a\u4e86\u4ece\u5f00\u80f8\u624b\u672f\u8f6c\u5411\u66f4\u5b89\u5168\u7684\u7ecf\u5bfc\u7ba1\u624b\u672f\uff0c\u9700\u8981\u6539\u8fdb\u6210\u50cf\u6280\u672f\u548c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u4ee5\u5b9e\u73b0\u7b80\u5355\u51c6\u786e\u7684\u624b\u672f\u5668\u68b0\u5bfc\u822a\u3002", "method": "\u8bbe\u8ba1\u53ef\u64cd\u63a7\u7403\u56ca\u5f0f\u5fc3\u810f\u955c\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7403\u56ca\u58c1\u539a\u5ea6\uff0c\u4f7f\u7528\u5355\u4e2a\u5145\u6c14\u538b\u529b\u8f93\u5165\u72ec\u7acb\u63a7\u5236\u7403\u56ca\u76f4\u5f84\uff08\u5bf9\u5e94\u89c6\u91ce\u76f4\u5f84\uff09\u548c\u5f2f\u66f2\u89d2\u5ea6\uff08\u5b9e\u73b0\u5de5\u4f5c\u901a\u9053\u7cbe\u786e\u5b9a\u4f4d\uff09\u3002", "result": "\u5f00\u53d1\u4e86\u9488\u5bf9\u4e3b\u52a8\u8109\u74e3\u53f6\u6495\u88c2\u4efb\u52a1\u7684\u7403\u56ca\u8bbe\u8ba1\uff0c\u5e76\u6f14\u793a\u4e86\u57fa\u4e8e\u56fe\u50cf\u7684\u95ed\u73af\u5f2f\u66f2\u89d2\u5ea6\u63a7\u5236\uff0c\u53ef\u5728\u5668\u68b0\u63d2\u5165\u548c\u79fb\u9664\u671f\u95f4\u5b9e\u73b0\u7a33\u5b9a\u7684\u65b9\u5411\u63a7\u5236\u3002", "conclusion": "\u8fd9\u79cd\u7403\u56ca\u6280\u672f\u53ef\u8c03\u8c10\u7528\u4e8e\u5404\u79cd\u5fc3\u810f\u5185\u4efb\u52a1\u7684\u5fc3\u810f\u955c\u8bbe\u8ba1\uff0c\u4e3a\u7ecf\u5bfc\u7ba1\u624b\u672f\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u6210\u50cf\u548c\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barr\u00e9", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u5206\u6790\u65b9\u6cd5\u7814\u7a76\u6cd5\u56fd\u4fa6\u63a2\u5c0f\u8bf4\u4e2d\u4fa6\u63a2\u539f\u578b\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u76d1\u7763\u6a21\u578b\u80fd\u591f\u6355\u6349150\u5e74\u95f4\u4fa6\u63a2\u539f\u578b\u7684\u7edf\u4e00\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4ece\u6b21\u8981\u89d2\u8272\u5230\u6838\u5fc3\u63a8\u7406\u673a\u5668\u7684\u8f6c\u53d8\u8fc7\u7a0b\u3002", "motivation": "\u63a2\u7d22\u6cd5\u56fd\u4fa6\u63a2\u5c0f\u8bf4\u4e2d\u4fa6\u63a2\u539f\u578b\u7684\u6f14\u53d8\u5386\u7a0b\uff0c\u7406\u89e3\u8fd9\u4e00\u6587\u5b66\u5f62\u8c61\u5982\u4f55\u968f\u65f6\u95f4\u53d8\u5316\u5e76\u9002\u5e94\u4e0d\u540c\u6587\u5b66\u4f20\u7edf\u3002", "method": "\u4f7f\u7528\u5b9a\u91cf\u65b9\u6cd5\u548c\u89d2\u8272\u7ea7\u5d4c\u5165\u7684\u8ba1\u7b97\u5206\u6790\uff0c\u6784\u5efa\u76d1\u7763\u6a21\u578b\u6765\u5206\u6790150\u5e74\u95f4\u7684\u6cd5\u56fd\u4fa6\u63a2\u5c0f\u8bf4\u6587\u672c\u3002", "result": "\u6a21\u578b\u6210\u529f\u6355\u6349\u4e86\u4ece1866\u5e74M. Lecoq\u52302017\u5e74Commissaire Adamsberg\u7684\u4fa6\u63a2\u539f\u578b\u7edf\u4e00\u6027\uff0c\u63ed\u793a\u4e86\u4ece\u6b21\u8981\u53d9\u4e8b\u89d2\u8272\u5230\u53e4\u5178\u4fa6\u63a2\u6545\u4e8b\u6838\u5fc3\u63a8\u7406\u673a\u5668\u7684\u6f14\u53d8\uff0c\u4ee5\u53ca\u4e8c\u6218\u540e\u53d7\u786c\u6c49\u6d3e\u4f20\u7edf\u5f71\u54cd\u540e\u7684\u590d\u6742\u5316\u8f6c\u53d8\u3002", "conclusion": "\u6cd5\u56fd\u4fa6\u63a2\u5c0f\u8bf4\u4e2d\u7684\u4fa6\u63a2\u539f\u578b\u5177\u6709\u5386\u53f2\u8fde\u7eed\u6027\uff0c\u7ecf\u5386\u4e86\u4ece\u53d9\u4e8b\u914d\u89d2\u5230\u6838\u5fc3\u89d2\u8272\u7684\u6f14\u53d8\uff0c\u5e76\u5728\u4e8c\u6218\u540e\u5438\u6536\u4e86\u786c\u6c49\u6d3e\u5143\u7d20\uff0c\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u9053\u5fb7\u6a21\u7cca\u3002"}}
{"id": "2511.00252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00252", "abs": "https://arxiv.org/abs/2511.00252", "authors": ["Aaron Sun", "Subhransu Maji", "Grant Van Horn"], "title": "Merlin L48 Spectrogram Dataset", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "summary": "In the single-positive multi-label (SPML) setting, each image in a dataset is\nlabeled with the presence of a single class, while the true presence of other\nclasses remains unknown. The challenge is to narrow the performance gap between\nthis partially-labeled setting and fully-supervised learning, which often\nrequires a significant annotation budget. Prior SPML methods were developed and\nbenchmarked on synthetic datasets created by randomly sampling single positive\nlabels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and\nCUB200. However, this synthetic approach does not reflect real-world scenarios\nand fails to capture the fine-grained complexities that can lead to difficult\nmisclassifications. In this work, we introduce the L48 dataset, a fine-grained,\nreal-world multi-label dataset derived from recordings of bird sounds. L48\nprovides a natural SPML setting with single-positive annotations on a\nchallenging, fine-grained domain, as well as two extended settings in which\ndomain priors give access to additional negative labels. We benchmark existing\nSPML methods on L48 and observe significant performance differences compared to\nsynthetic datasets and analyze method weaknesses, underscoring the need for\nmore realistic and difficult benchmarks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86L48\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u7684\u771f\u5b9e\u4e16\u754c\u591a\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5355\u6b63\u591a\u6807\u7b7e\u5b66\u4e60\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6b63\u591a\u6807\u7b7e\u65b9\u6cd5\u5728\u4ece\u5b8c\u5168\u6807\u6ce8\u6570\u636e\u96c6\u968f\u673a\u91c7\u6837\u5355\u6b63\u6807\u7b7e\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u8fd9\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u4e5f\u65e0\u6cd5\u6355\u6349\u53ef\u80fd\u5bfc\u81f4\u56f0\u96be\u9519\u8bef\u5206\u7c7b\u7684\u7ec6\u7c92\u5ea6\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165L48\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u9e1f\u7c7b\u58f0\u97f3\u8bb0\u5f55\u7684\u7ec6\u7c92\u5ea6\u771f\u5b9e\u4e16\u754c\u591a\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u81ea\u7136\u7684\u5355\u6b63\u591a\u6807\u7b7e\u8bbe\u7f6e\u4ee5\u53ca\u4e24\u4e2a\u6269\u5c55\u8bbe\u7f6e\uff0c\u5176\u4e2d\u9886\u57df\u5148\u9a8c\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u8d1f\u6807\u7b7e\u3002", "result": "\u5728L48\u6570\u636e\u96c6\u4e0a\u5bf9\u73b0\u6709\u5355\u6b63\u591a\u6807\u7b7e\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u89c2\u5bdf\u5230\u4e0e\u5408\u6210\u6570\u636e\u96c6\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u4e86\u65b9\u6cd5\u7684\u5f31\u70b9\u3002", "conclusion": "\u9700\u8981\u66f4\u73b0\u5b9e\u548c\u56f0\u96be\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u5355\u6b63\u591a\u6807\u7b7e\u5b66\u4e60\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.00993", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00993", "abs": "https://arxiv.org/abs/2511.00993", "authors": ["Tianming Liu", "Jirong Yang", "Yafeng Yin", "Manzi Li", "Linghao Wang", "Zheng Zhu"], "title": "Aligning LLM agents with human learning and adjustment behavior: a dual agent approach", "comment": "32 pages, 6 figures, 7 tables", "summary": "Effective modeling of how human travelers learn and adjust their travel\nbehavior from interacting with transportation systems is critical for system\nassessment and planning. However, this task is also difficult due to the\ncomplex cognition and decision-making involved in such behavior. Recent\nresearch has begun to leverage Large Language Model (LLM) agents for this task.\nBuilding on this, we introduce a novel dual-agent framework that enables\ncontinuous learning and alignment between LLM agents and human travelers on\nlearning and adaptation behavior from online data streams. Our approach\ninvolves a set of LLM traveler agents, equipped with a memory system and a\nlearnable persona, which serve as simulators for human travelers. To ensure\nbehavioral alignment, we introduce an LLM calibration agent that leverages the\nreasoning and analytical capabilities of LLMs to train the personas of these\ntraveler agents. Working together, this dual-agent system is designed to track\nand align the underlying decision-making mechanisms of travelers and produce\nrealistic, adaptive simulations. Using a real-world dataset from a day-to-day\nroute choice experiment, we show our approach significantly outperforms\nexisting LLM-based methods in both individual behavioral alignment and\naggregate simulation accuracy. Furthermore, we demonstrate that our method\nmoves beyond simple behavioral mimicry to capture the evolution of underlying\nlearning processes, a deeper alignment that fosters robust generalization.\nOverall, our framework provides a new approach for creating adaptive and\nbehaviorally realistic agents to simulate travelers' learning and adaptation\nthat can benefit transportation simulation and policy analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u6a21\u62df\u65c5\u884c\u8005\u7684\u5b66\u4e60\u548c\u9002\u5e94\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u5305\u542b\u65c5\u884c\u8005\u667a\u80fd\u4f53\u548c\u6821\u51c6\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5728\u7ebf\u6570\u636e\u6d41\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u548c\u884c\u4e3a\u5bf9\u9f50\uff0c\u5728\u4e2a\u4f53\u884c\u4e3a\u5bf9\u9f50\u548c\u805a\u5408\u6a21\u62df\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6709\u6548\u5efa\u6a21\u4eba\u7c7b\u65c5\u884c\u8005\u5982\u4f55\u4ece\u4e0e\u4ea4\u901a\u7cfb\u7edf\u7684\u4e92\u52a8\u4e2d\u5b66\u4e60\u548c\u8c03\u6574\u65c5\u884c\u884c\u4e3a\u5bf9\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u548c\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u4f46\u7531\u4e8e\u6d89\u53ca\u590d\u6742\u7684\u8ba4\u77e5\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u53cc\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u4e00\u7ec4\u914d\u5907\u8bb0\u5fc6\u7cfb\u7edf\u548c\u53ef\u5b66\u4e60\u89d2\u8272\u7684LLM\u65c5\u884c\u8005\u667a\u80fd\u4f53\u4f5c\u4e3a\u4eba\u7c7b\u65c5\u884c\u8005\u7684\u6a21\u62df\u5668\uff1b\u4e00\u4e2aLLM\u6821\u51c6\u667a\u80fd\u4f53\u5229\u7528LLM\u7684\u63a8\u7406\u548c\u5206\u6790\u80fd\u529b\u8bad\u7ec3\u65c5\u884c\u8005\u667a\u80fd\u4f53\u7684\u89d2\u8272\uff0c\u786e\u4fdd\u884c\u4e3a\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u7684\u65e5\u5e38\u8def\u7ebf\u9009\u62e9\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2a\u4f53\u884c\u4e3a\u5bf9\u9f50\u548c\u805a\u5408\u6a21\u62df\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u5e95\u5c42\u5b66\u4e60\u8fc7\u7a0b\u7684\u6f14\u53d8\uff0c\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u7684\u5bf9\u9f50\u548c\u7a33\u5065\u7684\u6cdb\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u521b\u5efa\u9002\u5e94\u6027\u5f3a\u3001\u884c\u4e3a\u771f\u5b9e\u7684\u667a\u80fd\u4f53\u6765\u6a21\u62df\u65c5\u884c\u8005\u7684\u5b66\u4e60\u548c\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u4ea4\u901a\u6a21\u62df\u548c\u653f\u7b56\u5206\u6790\u3002"}}
{"id": "2511.01233", "categories": ["cs.CV", "cs.GR", "cs.HC", "I.3; I.2"], "pdf": "https://arxiv.org/pdf/2511.01233", "abs": "https://arxiv.org/abs/2511.01233", "authors": ["Rajmund Nagy", "Hendric Voss", "Thanh Hoang-Minh", "Mihail Tsakov", "Teodor Nikolov", "Zeyi Zhang", "Tenglong Ao", "Sicheng Yang", "Shaoli Huang", "Yongkang Cheng", "M. Hamza Mughal", "Rishabh Dabral", "Kiran Chhatre", "Christian Theobalt", "Libin Liu", "Stefan Kopp", "Rachel McDonnell", "Michael Neff", "Taras Kucherenko", "Youngwoo Yoon", "Gustav Eje Henter"], "title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark", "comment": "23 pages, 10 figures. The last two authors made equal contributions", "summary": "We review human evaluation practices in automated, speech-driven 3D gesture\ngeneration and find a lack of standardisation and frequent use of flawed\nexperimental setups. This leads to a situation where it is impossible to know\nhow different methods compare, or what the state of the art is. In order to\naddress common shortcomings of evaluation design, and to standardise future\nuser studies in gesture-generation works, we introduce a detailed human\nevaluation protocol for the widely-used BEAT2 motion-capture dataset. Using\nthis protocol, we conduct large-scale crowdsourced evaluation to rank six\nrecent gesture-generation models -- each trained by its original authors --\nacross two key evaluation dimensions: motion realism and speech-gesture\nalignment. Our results provide strong evidence that 1) newer models do not\nconsistently outperform earlier approaches; 2) published claims of high motion\nrealism or speech-gesture alignment may not hold up under rigorous evaluation;\nand 3) the field must adopt disentangled assessments of motion quality and\nmultimodal alignment for accurate benchmarking in order to make progress.\nFinally, in order to drive standardisation and enable new evaluation research,\nwe will release five hours of synthetic motion from the benchmarked models;\nover 750 rendered video stimuli from the user studies -- enabling new\nevaluations without model reimplementation required -- alongside our\nopen-source rendering script, and the 16,000 pairwise human preference votes\ncollected for our benchmark.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u81ea\u52a8\u8bed\u97f3\u9a71\u52a83D\u624b\u52bf\u751f\u6210\u9886\u57df\u7684\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u8df5\uff0c\u53d1\u73b0\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5b58\u5728\u5b9e\u9a8c\u8bbe\u8ba1\u7f3a\u9677\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u4f5c\u8005\u63d0\u51fa\u4e86BEAT2\u6570\u636e\u96c6\u7684\u4eba\u7c7b\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u5bf96\u4e2a\u6700\u65b0\u624b\u52bf\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u4f17\u5305\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u624b\u52bf\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bbe\u8ba1\u5b58\u5728\u7f3a\u9677\uff0c\u4f7f\u5f97\u65e0\u6cd5\u51c6\u786e\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e5f\u65e0\u6cd5\u786e\u5b9a\u6280\u672f\u53d1\u5c55\u6c34\u5e73\u3002", "method": "\u5f15\u5165\u8be6\u7ec6\u7684BEAT2\u6570\u636e\u96c6\u4eba\u7c7b\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4f17\u5305\u8bc4\u4f30\u5bf96\u4e2a\u6700\u65b0\u624b\u52bf\u751f\u6210\u6a21\u578b\u8fdb\u884c\u6392\u540d\uff0c\u8bc4\u4f30\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u8bed\u97f3-\u624b\u52bf\u5bf9\u9f50\u4e24\u4e2a\u5173\u952e\u7ef4\u5ea6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u65b0\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u65e9\u671f\u65b9\u6cd5\uff1b2\uff09\u5df2\u53d1\u8868\u7684\u9ad8\u8fd0\u52a8\u771f\u5b9e\u6027\u6216\u8bed\u97f3-\u624b\u52bf\u5bf9\u9f50\u58f0\u660e\u5728\u4e25\u683c\u8bc4\u4f30\u4e0b\u53ef\u80fd\u4e0d\u6210\u7acb\uff1b3\uff09\u5fc5\u987b\u91c7\u7528\u89e3\u8026\u7684\u8fd0\u52a8\u8d28\u91cf\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u8bc4\u4f30\u624d\u80fd\u51c6\u786e\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u9886\u57df\u9700\u8981\u91c7\u7528\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5bf9\u8fd0\u52a8\u8d28\u91cf\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u8fdb\u884c\u89e3\u8026\u8bc4\u4f30\uff0c\u4ee5\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002\u4f5c\u8005\u5c06\u53d1\u5e03\u5408\u6210\u8fd0\u52a8\u6570\u636e\u3001\u6e32\u67d3\u89c6\u9891\u523a\u6fc0\u548c\u4eba\u7c7b\u504f\u597d\u6295\u7968\u6570\u636e\u4ee5\u4fc3\u8fdb\u6807\u51c6\u5316\u548c\u65b0\u7684\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2511.00657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models.", "AI": {"tldr": "XNationQA\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u95ee\u7b54\u57fa\u51c6\uff0c\u6db5\u76d69\u4e2a\u56fd\u5bb6\u7684\u5730\u7406\u3001\u6587\u5316\u548c\u5386\u53f2\u95ee\u9898\uff0c\u5305\u542b49,280\u4e2a\u95ee\u9898\uff0c\u4f7f\u75287\u79cd\u8bed\u8a00\u3002\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00LLM\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u83b7\u53d6\u6587\u5316\u7279\u5b9a\u4fe1\u606f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u4f18\u4e8e\u76f8\u5e94\u6587\u5316\u7684\u4e3b\u5bfc\u8bed\u8a00\uff0c\u4e14\u77e5\u8bc6\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u591a\u8bed\u8a00\u95ee\u7b54\u57fa\u51c6\u867d\u7136\u8986\u76d6\u591a\u79cd\u8bed\u8a00\uff0c\u4f46\u672a\u8003\u8651\u4fe1\u606f\u4e2d\u7684\u533a\u57df\u591a\u6837\u6027\uff0c\u4e14\u503e\u5411\u4e8e\u897f\u65b9\u4e2d\u5fc3\u4e3b\u4e49\uff0c\u8fd9\u5bfc\u81f4\u5728\u516c\u5e73\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0d\u540c\u5730\u7406\u4f4d\u7f6e\u4e8b\u5b9e\u4fe1\u606f\u7684\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u5f15\u5165XNationQA\u57fa\u51c6\uff0c\u5305\u542b49,280\u4e2a\u5173\u4e8e9\u4e2a\u56fd\u5bb6\u5730\u7406\u3001\u6587\u5316\u548c\u5386\u53f2\u7684\u95ee\u9898\uff0c\u4f7f\u75287\u79cd\u8bed\u8a00\u5448\u73b0\u3002\u5bf98\u4e2a\u6807\u51c6\u591a\u8bed\u8a00LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u4e24\u79cd\u65b0\u7684\u8fc1\u79fb\u5ea6\u91cf\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u83b7\u53d6\u6587\u5316\u7279\u5b9a\u4fe1\u606f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u6a21\u578b\u5728\u82f1\u8bed\u4e2d\u8868\u73b0\u4f18\u4e8e\u76f8\u5e94\u6587\u5316\u7684\u4e3b\u5bfc\u8bed\u8a00\uff1b\u6a21\u578b\u5728\u897f\u65b9\u8bed\u8a00\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5bf9\u897f\u65b9\u56fd\u5bb6\u66f4\u4e86\u89e3\uff1b\u6a21\u578b\uff08\u7279\u522b\u662f\u5f00\u6e90\u6a21\u578b\uff09\u7684\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u975e\u5e38\u6709\u9650\u3002", "conclusion": "\u591a\u8bed\u8a00LLM\u5728\u6587\u5316\u7d20\u517b\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u4e0d\u5e73\u8861\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6539\u8fdb\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u5904\u7406\u4e0d\u540c\u6587\u5316\u548c\u8bed\u8a00\u7684\u4fe1\u606f\u3002"}}
{"id": "2511.00255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00255", "abs": "https://arxiv.org/abs/2511.00255", "authors": ["Fangxun Liu", "S M Rayeed", "Samuel Stevens", "Alyson East", "Cheng Hsuan Chiang", "Colin Lee", "Daniel Yi", "Junke Yang", "Tejas Naik", "Ziyi Wang", "Connor Kilrain", "Elijah H Buckwalter", "Jiacheng Hou", "Saul Ibaven Bueno", "Shuheng Wang", "Xinyue Ma", "Yifan Liu", "Zhiyuan Tao", "Ziheng Zhang", "Eric Sokol", "Michael Belitz", "Sydne Record", "Charles V. Stewart", "Wei-Lun Chao"], "title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing", "comment": "4 pages, NeurIPS 2025 Workshop Imageomics", "summary": "In entomology and ecology research, biologists often need to collect a large\nnumber of insects, among which beetles are the most common species. A common\npractice for biologists to organize beetles is to place them on trays and take\na picture of each tray. Given the images of thousands of such trays, it is\nimportant to have an automated pipeline to process the large-scale data for\nfurther research. Therefore, we develop a 3-stage pipeline to detect all the\nbeetles on each tray, sort and crop the image of each beetle, and do\nmorphological segmentation on the cropped beetles. For detection, we design an\niterative process utilizing a transformer-based open-vocabulary object detector\nand a vision-language model. For segmentation, we manually labeled 670 beetle\nimages and fine-tuned two variants of a transformer-based segmentation model to\nachieve fine-grained segmentation of beetles with relatively high accuracy. The\npipeline integrates multiple deep learning methods and is specialized for\nbeetle image processing, which can greatly improve the efficiency to process\nlarge-scale beetle data and accelerate biological research.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a3\u9636\u6bb5\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5904\u7406\u5927\u91cf\u7532\u866b\u56fe\u50cf\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u88c1\u526a\u548c\u5f62\u6001\u5206\u5272\uff0c\u65e8\u5728\u63d0\u9ad8\u751f\u7269\u7814\u7a76\u6548\u7387\u3002", "motivation": "\u751f\u7269\u5b66\u7814\u7a76\u4e2d\u9700\u8981\u5904\u7406\u5927\u91cf\u7532\u866b\u56fe\u50cf\uff0c\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u4ee5\u52a0\u901f\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u68c0\u6d4b\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u68c0\u6d4b\uff0c\u7136\u540e\u5bf9670\u5f20\u7532\u866b\u56fe\u50cf\u624b\u52a8\u6807\u6ce8\u5e76\u5fae\u8c03\u57fa\u4e8etransformer\u7684\u5206\u5272\u6a21\u578b\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\u3002", "result": "\u6784\u5efa\u4e86\u4e13\u95e8\u7528\u4e8e\u7532\u866b\u56fe\u50cf\u5904\u7406\u7684\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u7ba1\u9053\uff0c\u80fd\u591f\u76f8\u5bf9\u51c6\u786e\u5730\u5b8c\u6210\u7532\u866b\u68c0\u6d4b\u548c\u5f62\u6001\u5206\u5272\u3002", "conclusion": "\u8be5\u7ba1\u9053\u96c6\u6210\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u7532\u866b\u56fe\u50cf\u5904\u7406\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u5927\u89c4\u6a21\u7532\u866b\u6570\u636e\u5904\u7406\u6548\u7387\uff0c\u52a0\u901f\u751f\u7269\u7814\u7a76\u3002"}}
{"id": "2511.01018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01018", "abs": "https://arxiv.org/abs/2511.01018", "authors": ["Hui-Lee Ooi", "Nicholas Mitsakakis", "Margerie Huet Dastarac", "Roger Zemek", "Amy C. Plint", "Jeff Gilchrist", "Khaled El Emam", "Dhenuka Radhakrishnan"], "title": "AI for pRedicting Exacerbations in KIDs with aSthma (AIRE-KIDS)", "comment": null, "summary": "Recurrent exacerbations remain a common yet preventable outcome for many\nchildren with asthma. Machine learning (ML) algorithms using electronic medical\nrecords (EMR) could allow accurate identification of children at risk for\nexacerbations and facilitate referral for preventative comprehensive care to\navoid this morbidity. We developed ML algorithms to predict repeat severe\nexacerbations (i.e. asthma-related emergency department (ED) visits or future\nhospital admissions) for children with a prior asthma ED visit at a tertiary\ncare children's hospital.\n  Retrospective pre-COVID19 (Feb 2017 - Feb 2019, N=2716) Epic EMR data from\nthe Children's Hospital of Eastern Ontario (CHEO) linked with environmental\npollutant exposure and neighbourhood marginalization information was used to\ntrain various ML models. We used boosted trees (LGBM, XGB) and 3 open-source\nlarge language model (LLM) approaches (DistilGPT2, Llama 3.2 1B and\nLlama-8b-UltraMedical). Models were tuned and calibrated then validated in a\nsecond retrospective post-COVID19 dataset (Jul 2022 - Apr 2023, N=1237) from\nCHEO. Models were compared using the area under the curve (AUC) and F1 scores,\nwith SHAP values used to determine the most predictive features.\n  The LGBM ML model performed best with the most predictive features in the\nfinal AIRE-KIDS_ED model including prior asthma ED visit, the Canadian triage\nacuity scale, medical complexity, food allergy, prior ED visits for non-asthma\nrespiratory diagnoses, and age for an AUC of 0.712, and F1 score of 0.51. This\nis a nontrivial improvement over the current decision rule which has F1=0.334.\nWhile the most predictive features in the AIRE-KIDS_HOSP model included medical\ncomplexity, prior asthma ED visit, average wait time in the ED, the pediatric\nrespiratory assessment measure score at triage and food allergy.", "AI": {"tldr": "\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u4e25\u91cd\u53d1\u4f5c\uff0cLGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u73b0\u6709\u51b3\u7b56\u89c4\u5219\u6709\u663e\u8457\u6539\u8fdb", "motivation": "\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u53d1\u4f5c\u662f\u5e38\u89c1\u4f46\u53ef\u9884\u9632\u7684\u95ee\u9898\uff0c\u5229\u7528\u7535\u5b50\u75c5\u5386\u6570\u636e\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u51c6\u786e\u8bc6\u522b\u9ad8\u98ce\u9669\u513f\u7ae5\u5e76\u4fc3\u8fdb\u9884\u9632\u6027\u62a4\u7406", "method": "\u4f7f\u7528\u4e09\u7ea7\u513f\u7ae5\u533b\u96622017-2019\u5e742716\u4f8b\u60a3\u8005EMR\u6570\u636e\uff0c\u7ed3\u5408\u73af\u5883\u6c61\u67d3\u7269\u66b4\u9732\u548c\u793e\u533a\u8fb9\u7f18\u5316\u4fe1\u606f\uff0c\u8bad\u7ec3\u591a\u79cdML\u6a21\u578b\uff08LGBM\u3001XGB\u548c3\u79cdLLM\uff09\uff0c\u57282022-2023\u5e741237\u4f8b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1", "result": "LGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAIRE-KIDS_ED\u6a21\u578bAUC\u4e3a0.712\uff0cF1\u5206\u65700.51\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u51b3\u7b56\u89c4\u5219\uff08F1=0.334\uff09", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u513f\u7ae5\u54ee\u5598\u53cd\u590d\u4e25\u91cd\u53d1\u4f5c\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u91cd\u8981\u652f\u6301"}}
{"id": "2511.01224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01224", "abs": "https://arxiv.org/abs/2511.01224", "authors": ["Chengmeng Li", "Yaxin Peng"], "title": "Embodiment Transfer Learning for Vision-Language-Action Models", "comment": null, "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nlearning, enabling training on large-scale, cross-embodiment data and\nfine-tuning for specific robots. However, state-of-the-art autoregressive VLAs\nstruggle with multi-robot collaboration. We introduce embodiment transfer\nlearning, denoted as ET-VLA, a novel framework for efficient and effective\ntransfer of pre-trained VLAs to multi-robot. ET-VLA's core is Synthetic\nContinued Pretraining (SCP), which uses synthetically generated data to warm up\nthe model for the new embodiment, bypassing the need for real human\ndemonstrations and reducing data collection costs. SCP enables the model to\nlearn correct actions and precise action token numbers. Following SCP, the\nmodel is fine-tuned on target embodiment data. To further enhance the model\nperformance on multi-embodiment, we present the Embodied Graph-of-Thought\ntechnique, a novel approach that formulates each sub-task as a node, that\nallows the VLA model to distinguish the functionalities and roles of each\nembodiment during task execution. Our work considers bimanual robots, a simple\nversion of multi-robot to verify our approaches. We validate the effectiveness\nof our method on both simulation benchmarks and real robots covering three\ndifferent bimanual embodiments. In particular, our proposed ET-VLA \\space can\noutperform OpenVLA on six real-world tasks over 53.2%. We will open-source all\ncodes to support the community in advancing VLA models for robot learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86ET-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5177\u8eab\u601d\u7ef4\u56fe\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u4f5c\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u8d85\u8fc753.2%\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52VLA\u6a21\u578b\u5728\u591a\u673a\u5668\u4eba\u534f\u4f5c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u8fc1\u79fb\u5230\u591a\u673a\u5668\u4eba\u573a\u666f\u3002", "method": "\u91c7\u7528\u5408\u6210\u6301\u7eed\u9884\u8bad\u7ec3(SCP)\u751f\u6210\u5408\u6210\u6570\u636e\u9884\u70ed\u6a21\u578b\uff0c\u7ed3\u5408\u5177\u8eab\u601d\u7ef4\u56fe\u6280\u672f\u5c06\u5b50\u4efb\u52a1\u5efa\u6a21\u4e3a\u8282\u70b9\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u533a\u5206\u4e0d\u540c\u5177\u8eab\u7684\u529f\u80fd\u548c\u89d2\u8272\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u53cc\u624b\u673a\u5668\u4eba\u5177\u8eab\u4e0a\u9a8c\u8bc1\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u6bd4OpenVLA\u6027\u80fd\u63d0\u5347\u8d85\u8fc753.2%\u3002", "conclusion": "ET-VLA\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u4f5c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2511.00689", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf910\u79cd\u4e0d\u540c\u8d44\u6e90\u6c34\u5e73\u7684\u8bed\u8a00\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u591a\u8bed\u8a00\u8d8a\u72f1\u653b\u51fb\u548c\u9632\u5fa1\u8bc4\u4f30\uff0c\u53d1\u73b0\u653b\u51fb\u6210\u529f\u7387\u548c\u9632\u5fa1\u9c81\u68d2\u6027\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u5728\u6807\u51c6\u67e5\u8be2\u4e0b\u66f4\u5b89\u5168\u4f46\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u66f4\u8106\u5f31\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u8fc7\u5b89\u5168\u5bf9\u9f50\u8bad\u7ec3\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u901a\u8fc7\u8d8a\u72f1\u653b\u51fb\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u3002\u7136\u800c\uff0c\u8de8\u8bed\u8a00\u7684\u8d8a\u72f1\u653b\u51fb\u548c\u9632\u5fa1\u6cdb\u5316\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5728HarmBench\u548cAdvBench\u4e0a\u4f7f\u75286\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u903b\u8f91\u8868\u8fbe\u5f0f\u548c\u5bf9\u6297\u6027\u63d0\u793a\u4e24\u79cd\u8d8a\u72f1\u653b\u51fb\u7c7b\u578b\uff0c\u6db5\u76d6\u4e86\u9ad8\u3001\u4e2d\u3001\u4f4e\u8d44\u6e90\u6c34\u5e73\u768410\u79cd\u8bed\u8a00\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u548c\u9632\u5fa1\u9c81\u68d2\u6027\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a\u9ad8\u8d44\u6e90\u8bed\u8a00\u5728\u6807\u51c6\u67e5\u8be2\u4e0b\u66f4\u5b89\u5168\uff0c\u4f46\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u66f4\u8106\u5f31\uff1b\u7b80\u5355\u9632\u5fa1\u65b9\u6cd5\u6709\u6548\u4f46\u5177\u6709\u8bed\u8a00\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u4e3aLLMs\u5f00\u53d1\u8bed\u8a00\u611f\u77e5\u548c\u8de8\u8bed\u8a00\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2511.01033", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Transformer\u4e2d\u5f52\u7eb3\u5934\u7684\u51fa\u73b0\u673a\u5236\uff0c\u63ed\u793a\u4e86\u6743\u91cd\u77e9\u9635\u7684\u7b80\u5355\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u8bad\u7ec3\u52a8\u6001\u88ab\u9650\u5236\u572819\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5176\u4e2d\u4ec53\u4e2a\u7ef4\u5ea6\u8d1f\u8d23\u5f52\u7eb3\u5934\u7684\u5f62\u6210\uff0c\u5e76\u53d1\u73b0\u5f52\u7eb3\u5934\u51fa\u73b0\u65f6\u95f4\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\u3002", "motivation": "\u7814\u7a76Transformer\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u5173\u952e\u673a\u5236\u2014\u2014\u5f52\u7eb3\u5934\uff0c\u7406\u89e3\u5176\u6743\u91cd\u77e9\u9635\u7ed3\u6784\u548c\u8bad\u7ec3\u52a8\u6001\uff0c\u4ee5\u89e3\u91caTransformer\u5982\u4f55\u5728\u4e0d\u66f4\u65b0\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u4ece\u8f93\u5165\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u65b0\u5173\u8054\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u516c\u5f0f\u548c\u4fee\u6539\u7684Transformer\u67b6\u6784\uff0c\u7406\u8bba\u5206\u6790\u5f52\u7eb3\u5934\u7684\u6743\u91cd\u7ed3\u6784\uff0c\u8bc1\u660e\u8bad\u7ec3\u52a8\u6001\u88ab\u7ea6\u675f\u572819\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u53ea\u67093\u4e2a\u7ef4\u5ea6\u9a71\u52a8\u5f52\u7eb3\u5934\u7684\u5f62\u6210\u3002", "result": "\u53d1\u73b0\u4e86\u5f52\u7eb3\u5934\u6743\u91cd\u77e9\u9635\u7684\u7b80\u5355\u53ef\u89e3\u91ca\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u8bad\u7ec3\u52a8\u6001\u88ab\u9650\u5236\u572819\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5176\u4e2d\u4ec53\u4e2a\u7ef4\u5ea6\u8d1f\u8d23\u5f52\u7eb3\u5934\u7684\u51fa\u73b0\uff0c\u4e14\u5f52\u7eb3\u5934\u51fa\u73b0\u65f6\u95f4\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\u3002", "conclusion": "Transformer\u4e2d\u5f52\u7eb3\u5934\u7684\u5f62\u6210\u9075\u5faa\u7279\u5b9a\u7684\u4f4e\u7ef4\u8bad\u7ec3\u52a8\u6001\uff0c\u5176\u51fa\u73b0\u65f6\u95f4\u53d7\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5e73\u65b9\u7ea6\u675f\uff0c\u8fd9\u4e3a\u7406\u89e3Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2511.01232", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01232", "abs": "https://arxiv.org/abs/2511.01232", "authors": ["Yu-Ting Lai", "Jacob Rosen", "Yasamin Foroutani", "Ji Ma", "Wen-Cheng Wu", "Jean-Pierre Hubschman", "Tsu-Chin Tsao"], "title": "High-Precision Surgical Robotic System for Intraocular Procedures", "comment": null, "summary": "Despite the extensive demonstration of robotic systems for both cataract and\nvitreoretinal procedures, existing technologies or mechanisms still possess\ninsufficient accuracy, precision, and degrees of freedom for instrument\nmanipulation or potentially automated tool exchange during surgical procedures.\nA new robotic system that focuses on improving tooltip accuracy, tracking\nperformance, and smooth instrument exchange mechanism is therefore designed and\nmanufactured. Its tooltip accuracy, precision, and mechanical capability of\nmaintaining small incision through remote center of motion were externally\nevaluated using an optical coherence tomography (OCT) system. Through robot\ncalibration and precise coordinate registration, the accuracy of tooltip\npositioning was measured to be 0.053$\\pm$0.031 mm, and the overall performance\nwas demonstrated on an OCT-guided automated cataract lens extraction procedure\nwith deep learning-based pre-operative anatomical modeling and real-time\nsupervision.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u9ad8\u773c\u79d1\u624b\u672f\u4e2d\u7684\u5de5\u5177\u5c16\u7aef\u7cbe\u5ea6\u3001\u8ddf\u8e2a\u6027\u80fd\u548c\u5de5\u5177\u4ea4\u6362\u673a\u5236\uff0c\u901a\u8fc7OCT\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u5de5\u5177\u5c16\u7aef\u5b9a\u4f4d\u7cbe\u5ea6\u8fbe\u52300.053\u00b10.031\u6beb\u7c73\u3002", "motivation": "\u73b0\u6709\u767d\u5185\u969c\u548c\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7cbe\u5ea6\u3001\u81ea\u7531\u5ea6\u548c\u5de5\u5177\u4ea4\u6362\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u3001\u66f4\u7075\u6d3b\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u6765\u6539\u5584\u624b\u672f\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u548c\u5236\u9020\u4e86\u65b0\u578b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u6821\u51c6\u548c\u7cbe\u786e\u5750\u6807\u914d\u51c6\uff0c\u4f7f\u7528\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\u5c16\u7aef\u7cbe\u5ea6\u3001\u7cbe\u5ea6\u548c\u673a\u68b0\u80fd\u529b\u3002", "result": "\u5de5\u5177\u5c16\u7aef\u5b9a\u4f4d\u7cbe\u5ea6\u6d4b\u91cf\u4e3a0.053\u00b10.031\u6beb\u7c73\uff0c\u5e76\u5728OCT\u5f15\u5bfc\u7684\u81ea\u52a8\u5316\u767d\u5185\u969c\u6676\u72b6\u4f53\u63d0\u53d6\u624b\u672f\u4e2d\u5c55\u793a\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u672f\u524d\u89e3\u5256\u5efa\u6a21\u548c\u5b9e\u65f6\u76d1\u7763\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u773c\u79d1\u624b\u672f\u7684\u5de5\u5177\u5c16\u7aef\u7cbe\u5ea6\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316\u624b\u672f\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.", "AI": {"tldr": "\u672c\u6587\u5bf9\u539f\u751f\u7a00\u758f\u6ce8\u610f\u529b(NSA)\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u5c42\u4ea4\u66ff\u4f7f\u7528\u5c40\u90e8(\u6ed1\u52a8\u7a97\u53e3)\u548c\u5168\u5c40(\u538b\u7f29\u3001\u9009\u62e9\u6027)\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u7ed3\u5408\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684KV\u7f13\u5b58\u5185\u5b58\u3002", "motivation": "\u6539\u8fdb\u539f\u751f\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u4f9d\u8d56\u4f20\u64ad\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u5185\u5b58\u6d88\u8017\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002", "method": "1. \u5728\u4e0d\u540c\u5c42\u4ea4\u66ff\u4f7f\u7528\u5c40\u90e8\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u5168\u5c40\u538b\u7f29/\u9009\u62e9\u6027\u6ce8\u610f\u529b\u6a21\u5f0f\uff1b2. \u6ed1\u52a8\u7a97\u53e3\u5206\u652f\u91c7\u7528\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b(MLA)\uff1b3. \u538b\u7f29\u548c\u9009\u62e9\u6027\u5206\u652f\u91c7\u7528\u5206\u7ec4\u5934\u6f5c\u5728\u6ce8\u610f\u529b(GLA)\u3002", "result": "\u57283.4\u4ebf\u523013\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u6d4b\u8bd5(\u8bad\u7ec3\u6570\u636e15B\u548c100B tokens)\uff0c\u8be5\u65b9\u6cd5\u5728\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u5168\u6ce8\u610f\u529b\u548c\u539f\u751f\u7a00\u758f\u6ce8\u610f\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684KV\u7f13\u5b58\u5185\u5b58\u3002", "conclusion": "\u901a\u8fc7\u4ea4\u66ff\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2511.01052", "categories": ["cs.AI", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2511.01052", "abs": "https://arxiv.org/abs/2511.01052", "authors": ["Yeawon Lee", "Christopher C. Yang", "Chia-Hsuan Chang", "Grace Lu-Yao"], "title": "Knowledge Elicitation with Large Language Models for Interpretable Cancer Stage Identification from Pathology Reports", "comment": null, "summary": "Cancer staging is critical for patient prognosis and treatment planning, yet\nextracting pathologic TNM staging from unstructured pathology reports poses a\npersistent challenge. Existing natural language processing (NLP) and machine\nlearning (ML) strategies often depend on large annotated datasets, limiting\ntheir scalability and adaptability. In this study, we introduce two Knowledge\nElicitation methods designed to overcome these limitations by enabling large\nlanguage models (LLMs) to induce and apply domain-specific rules for cancer\nstaging. The first, Knowledge Elicitation with Long-Term Memory (KEwLTM), uses\nan iterative prompting strategy to derive staging rules directly from\nunannotated pathology reports, without requiring ground-truth labels. The\nsecond, Knowledge Elicitation with Retrieval-Augmented Generation (KEwRAG),\nemploys a variation of RAG where rules are pre-extracted from relevant\nguidelines in a single step and then applied, enhancing interpretability and\navoiding repeated retrieval overhead. We leverage the ability of LLMs to apply\nbroad knowledge learned during pre-training to new tasks. Using breast cancer\npathology reports from the TCGA dataset, we evaluate their performance in\nidentifying T and N stages, comparing them against various baseline approaches\non two open-source LLMs. Our results indicate that KEwLTM outperforms KEwRAG\nwhen Zero-Shot Chain-of-Thought (ZSCOT) inference is effective, whereas KEwRAG\nachieves better performance when ZSCOT inference is less effective. Both\nmethods offer transparent, interpretable interfaces by making the induced rules\nexplicit. These findings highlight the promise of our Knowledge Elicitation\nmethods as scalable, high-performing solutions for automated cancer staging\nwith enhanced interpretability, particularly in clinical settings with limited\nannotated data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u77e5\u8bc6\u542f\u53d1\u65b9\u6cd5\uff08KEwLTM\u548cKEwRAG\uff09\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u65e0\u6807\u6ce8\u75c5\u7406\u62a5\u544a\u4e2d\u63d0\u53d6\u764c\u75c7\u5206\u671f\u89c4\u5219\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfNLP\u65b9\u6cd5\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u764c\u75c7\u5206\u671f\u5bf9\u60a3\u8005\u9884\u540e\u548c\u6cbb\u7597\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ece\u672a\u7ed3\u6784\u5316\u7684\u75c5\u7406\u62a5\u544a\u4e2d\u63d0\u53d6\u75c5\u7406TNM\u5206\u671f\u5b58\u5728\u6311\u6218\u3002\u73b0\u6709NLP\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "KEwLTM\u4f7f\u7528\u8fed\u4ee3\u63d0\u793a\u7b56\u7565\u76f4\u63a5\u4ece\u65e0\u6807\u6ce8\u75c5\u7406\u62a5\u544a\u4e2d\u63a8\u5bfc\u5206\u671f\u89c4\u5219\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff1bKEwRAG\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53d8\u4f53\uff0c\u4ece\u76f8\u5173\u6307\u5357\u4e2d\u9884\u63d0\u53d6\u89c4\u5219\u7136\u540e\u5e94\u7528\u3002\u4f7f\u7528TCGA\u6570\u636e\u96c6\u7684\u4e73\u817a\u764c\u75c5\u7406\u62a5\u544a\u8bc4\u4f30T\u548cN\u5206\u671f\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5f53\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63a8\u7406\u6709\u6548\u65f6\uff0cKEwLTM\u8868\u73b0\u4f18\u4e8eKEwRAG\uff1b\u5f53\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63a8\u7406\u6548\u679c\u8f83\u5dee\u65f6\uff0cKEwRAG\u8868\u73b0\u66f4\u597d\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u754c\u9762\u3002", "conclusion": "\u77e5\u8bc6\u542f\u53d1\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u764c\u75c7\u5206\u671f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002"}}
{"id": "2511.01236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01236", "abs": "https://arxiv.org/abs/2511.01236", "authors": ["Junwen Zhang", "Changyue Liu", "Pengqi Fu", "Xiang Guo", "Ye Shi", "Xudong Liang", "Zhijian Wang", "Hanzhi Ma"], "title": "Don't Just Search, Understand: Semantic Path Planning Agent for Spherical Tensegrity Robots in Unknown Environments", "comment": "8 pages, 5 figures", "summary": "Endowed with inherent dynamical properties that grant them remarkable\nruggedness and adaptability, spherical tensegrity robots stand as prototypical\nexamples of hybrid softrigid designs and excellent mobile platforms. However,\npath planning for these robots in unknown environments presents a significant\nchallenge, requiring a delicate balance between efficient exploration and\nrobust planning. Traditional path planners, which treat the environment as a\ngeometric grid, often suffer from redundant searches and are prone to failure\nin complex scenarios due to their lack of semantic understanding. To overcome\nthese limitations, we reframe path planning in unknown environments as a\nsemantic reasoning task. We introduce a Semantic Agent for Tensegrity robots\n(SATPlanner) driven by a Large Language Model (LLM). SATPlanner leverages\nhigh-level environmental comprehension to generate efficient and reliable\nplanning strategies.At the core of SATPlanner is an Adaptive Observation Window\nmechanism, inspired by the \"fast\" and \"slow\" thinking paradigms of LLMs. This\nmechanism dynamically adjusts the perceptual field of the agent: it narrows for\nrapid traversal of open spaces and expands to reason about complex obstacle\nconfigurations. This allows the agent to construct a semantic belief of the\nenvironment, enabling the search space to grow only linearly with the path\nlength (O(L)) while maintaining path quality. We extensively evaluate\nSATPlanner in 1,000 simulation trials, where it achieves a 100% success rate,\noutperforming other real-time planning algorithms. Critically, SATPlanner\nreduces the search space by 37.2% compared to the A* algorithm while achieving\ncomparable, near-optimal path lengths. Finally, the practical feasibility of\nSATPlanner is validated on a physical spherical tensegrity robot prototype.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u89c4\u5212\u5668SATPlanner\uff0c\u7528\u4e8e\u89e3\u51b3\u7403\u5f62\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89c2\u6d4b\u7a97\u53e3\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u4e49\u63a8\u7406\u89c4\u5212\u3002", "motivation": "\u7403\u5f62\u5f20\u62c9\u6574\u4f53\u673a\u5668\u4eba\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f46\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u51e0\u4f55\u7f51\u683c\u7684\u89c4\u5212\u5668\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5bb9\u6613\u5931\u8d25\u4e14\u5b58\u5728\u5197\u4f59\u641c\u7d22\u95ee\u9898\u3002", "method": "\u5c06\u8def\u5f84\u89c4\u5212\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u51faSATPlanner\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u53d7LLM\u5feb\u6162\u601d\u7ef4\u542f\u53d1\u7684\u81ea\u9002\u5e94\u89c2\u6d4b\u7a97\u53e3\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u611f\u77e5\u8303\u56f4\uff1a\u5728\u5f00\u9614\u7a7a\u95f4\u7f29\u5c0f\u7a97\u53e3\u5feb\u901f\u7a7f\u8d8a\uff0c\u5728\u590d\u6742\u969c\u788d\u914d\u7f6e\u65f6\u6269\u5927\u7a97\u53e3\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u57281000\u6b21\u4eff\u771f\u8bd5\u9a8c\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u76f8\u6bd4A*\u7b97\u6cd5\u51cf\u5c1137.2%\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u957f\u5ea6\uff0c\u5e76\u5728\u7269\u7406\u539f\u578b\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "SATPlanner\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\uff0c\u641c\u7d22\u7a7a\u95f4\u4ec5\u968f\u8def\u5f84\u957f\u5ea6\u7ebf\u6027\u589e\u957f(O(L))\uff0c\u4e3a\u6df7\u5408\u8f6f\u521a\u6027\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications.", "AI": {"tldr": "TriCon-Fair\u662f\u4e00\u4e2a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4e09\u5143\u7ec4\u548c\u8bed\u8a00\u5efa\u6a21\u9879\u6765\u6d88\u9664\u6b63\u8d1f\u8026\u5408\uff0c\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u53bb\u504f\u89c1\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6709\u504f\u89c1\u548c\u65e0\u504f\u89c1\u6837\u672c\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5bfc\u81f4\u9690\u85cf\u7684\u6b63\u8d1f\u8026\u5408\u95ee\u9898\uff0c\u4f7f\u5f97\u6539\u8fdb\u4e00\u4e2a\u7fa4\u4f53\u65f6\u65e0\u610f\u4e2d\u635f\u5bb3\u53e6\u4e00\u4e2a\u7fa4\u4f53\uff0c\u6b8b\u7559\u793e\u4f1a\u504f\u89c1\u6301\u7eed\u5b58\u5728\u3002", "method": "TriCon-Fair\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u89e3\u8026\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4e09\u5143\u7ec4\u548c\u8bed\u8a00\u5efa\u6a21\u9879\uff0c\u4e3a\u6bcf\u4e2a\u951a\u70b9\u5206\u914d\u660e\u786e\u7684\u6709\u504f\u89c1\u8d1f\u6837\u672c\u548c\u65e0\u504f\u89c1\u6b63\u6837\u672c\uff0c\u89e3\u8026\u63a8\u62c9\u52a8\u6001\u5e76\u907f\u514d\u6b63\u8d1f\u8026\u5408\uff0c\u540c\u65f6\u8054\u5408\u4f18\u5316\u8bed\u8a00\u5efa\u6a21\u76ee\u6807\u4ee5\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTriCon-Fair\u5728\u4fdd\u6301\u5f3a\u5927\u4e0b\u6e38\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6bd4\u73b0\u6709\u53bb\u504f\u89c1\u57fa\u7ebf\u65b9\u6cd5\u66f4\u80fd\u51cf\u5c11\u6b67\u89c6\u6027\u8f93\u51fa\u3002", "conclusion": "TriCon-Fair\u4e3a\u654f\u611fNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u7b26\u5408\u4f26\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u793e\u4f1a\u504f\u89c1\u4f20\u64ad\u3002"}}
{"id": "2511.00269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00269", "abs": "https://arxiv.org/abs/2511.00269", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture", "comment": null, "summary": "Accurate classification plays a pivotal role in smart agriculture, enabling\napplications such as crop monitoring, fruit recognition, and pest detection.\nHowever, conventional centralized training often requires large-scale data\ncollection, which raises privacy concerns, while standard federated learning\nstruggles with non-independent and identically distributed (non-IID) data and\nincurs high communication costs. To address these challenges, we propose a\nfederated learning framework that integrates a frozen Contrastive\nLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight\ntransformer classifier. By leveraging the strong feature extraction capability\nof the pre-trained CLIP ViT, the framework avoids training large-scale models\nfrom scratch and restricts federated updates to a compact classifier, thereby\nreducing transmission overhead significantly. Furthermore, to mitigate\nperformance degradation caused by non-IID data distribution, a small subset\n(1%) of CLIP-extracted feature representations from all classes is shared\nacross clients. These shared features are non-reversible to raw images,\nensuring privacy preservation while aligning class representation across\nparticipants. Experimental results on agricultural classification tasks show\nthat the proposed method achieve 86.6% accuracy, which is more than 4 times\nhigher compared to baseline federated learning approaches. This demonstrates\nthe effectiveness and efficiency of combining vision-language model features\nwith federated learning for privacy-preserving and scalable agricultural\nintelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u51bb\u7ed3CLIP\u89c6\u89c9\u53d8\u6362\u5668\u548c\u8f7b\u91cf\u7ea7\u53d8\u6362\u5668\u5206\u7c7b\u5668\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684CLIP ViT\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u4ec5\u5bf9\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u8fdb\u884c\u8054\u90a6\u66f4\u65b0\uff1b\u5171\u4eab1%\u7684CLIP\u7279\u5f81\u8868\u793a\u6765\u7f13\u89e3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u5728\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u523086.6%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u9ad8\u51fa4\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u6846\u67b6\u5728\u4fdd\u62a4\u9690\u79c1\u548c\u53ef\u6269\u5c55\u519c\u4e1a\u667a\u80fd\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.01059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01059", "abs": "https://arxiv.org/abs/2511.01059", "authors": ["Hailong Yin", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo"], "title": "Efficient Test-Time Retrieval Augmented Generation", "comment": null, "summary": "Although Large Language Models (LLMs) demonstrate significant capabilities,\ntheir reliance on parametric knowledge often leads to inaccuracies. Retrieval\nAugmented Generation (RAG) mitigates this by incorporating external knowledge,\nbut these methods may introduce irrelevant retrieved documents, leading to\ninaccurate responses. While the integration methods filter out incorrect\nanswers from multiple responses, but lack external knowledge like RAG methods,\nand their high costs require balancing overhead with performance gains. To\naddress these issues, we propose an Efficient Test-Time Retrieval-Augmented\nGeneration Framework named ET2RAG to improve the performance of LLMs while\nmaintaining efficiency. Specifically, ET2RAG is a training-free method, that\nfirst retrieves the most relevant documents and augments the LLMs to\nefficiently generate diverse candidate responses by managing response length.\nThen we compute the similarity of candidate responses and employ a majority\nvoting mechanism to select the most suitable response as the final output. In\nparticular, we discover that partial generation is sufficient to capture the\nkey information necessary for consensus calculation, allowing us to effectively\nperform majority voting without the need for fully generated responses. Thus,\nwe can reach a balance between computational cost and performance by managing\nthe response length for the number of retrieved documents for majority voting.\nExperimental results demonstrate that ET2RAG significantly enhances performance\nacross three tasks, including open-domain question answering, recipe generation\nand image captioning.", "AI": {"tldr": "ET2RAG\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u6587\u6863\u3001\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u54cd\u5e94\uff0c\u5e76\u4f7f\u7528\u591a\u6570\u6295\u7968\u673a\u5236\u9009\u62e9\u6700\u4f73\u7b54\u6848\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1aRAG\u53ef\u80fd\u5f15\u5165\u4e0d\u76f8\u5173\u6587\u6863\u5bfc\u81f4\u9519\u8bef\u54cd\u5e94\uff0c\u96c6\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5728\u5f00\u9500\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "ET2RAG\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u68c0\u7d22\u6700\u76f8\u5173\u6587\u6863\uff0c\u901a\u8fc7\u7ba1\u7406\u54cd\u5e94\u957f\u5ea6\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u54cd\u5e94\uff0c\u7136\u540e\u8ba1\u7b97\u5019\u9009\u54cd\u5e94\u76f8\u4f3c\u5ea6\u5e76\u91c7\u7528\u591a\u6570\u6295\u7968\u673a\u5236\u9009\u62e9\u6700\u5408\u9002\u7684\u54cd\u5e94\u4f5c\u4e3a\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cET2RAG\u5728\u5f00\u653e\u57df\u95ee\u7b54\u3001\u98df\u8c31\u751f\u6210\u548c\u56fe\u50cf\u63cf\u8ff0\u4e09\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "ET2RAG\u6846\u67b6\u901a\u8fc7\u90e8\u5206\u751f\u6210\u548c\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.01256", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01256", "abs": "https://arxiv.org/abs/2511.01256", "authors": ["Yasamin Foroutani", "Yasamin Mousavi-Motlagh", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control", "comment": "10 pages, 10 figures", "summary": "Achieving precise control of robotic tool paths is often challenged by\ninherent system misalignments, unmodeled dynamics, and actuation inaccuracies.\nThis work introduces an Iterative Learning Control (ILC) strategy to enable\nprecise rotational insertion of a tool during robotic surgery, improving\npenetration efficacy and safety compared to straight insertion tested in\nsubretinal injection. A 4 degree of freedom (DOF) robot manipulator is used,\nwhere misalignment of the fourth joint complicates the simple application of\nneedle rotation, motivating an ILC approach that iteratively adjusts joint\ncommands based on positional feedback. The process begins with calibrating the\nforward kinematics for the chosen surgical tool to achieve higher accuracy,\nfollowed by successive ILC iterations guided by Optical Coherence Tomography\n(OCT) volume scans to measure the error and refine control inputs. Experimental\nresults, tested on subretinal injection tasks on ex vivo pig eyes, show that\nthe optimized trajectory resulted in higher success rates in tissue penetration\nand subretinal injection compared to straight insertion, demonstrating the\neffectiveness of ILC in overcoming misalignment challenges. This approach\noffers potential applications for other high precision robot tasks requiring\ncontrolled insertions as well.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5de5\u5177\u7684\u7cbe\u786e\u65cb\u8f6c\u63d2\u5165\uff0c\u76f8\u6bd4\u76f4\u7ebf\u63d2\u5165\u63d0\u9ad8\u4e86\u7a7f\u900f\u6548\u679c\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5de5\u5177\u8def\u5f84\u7684\u7cbe\u786e\u63a7\u5236\u9762\u4e34\u7cfb\u7edf\u4e0d\u5bf9\u51c6\u3001\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u548c\u9a71\u52a8\u4e0d\u51c6\u786e\u6027\u7b49\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u7b49\u7cbe\u7ec6\u624b\u672f\u4e2d\u3002", "method": "\u4f7f\u75284\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u901a\u8fc7\u6821\u51c6\u524d\u5411\u8fd0\u52a8\u5b66\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u7136\u540e\u57fa\u4e8e\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u4f53\u79ef\u626b\u63cf\u7684\u53cd\u9988\uff0c\u8fed\u4ee3\u8c03\u6574\u5173\u8282\u547d\u4ee4\u3002", "result": "\u5728\u79bb\u4f53\u732a\u773c\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u4efb\u52a1\u4e2d\uff0c\u4f18\u5316\u8f68\u8ff9\u76f8\u6bd4\u76f4\u7ebf\u63d2\u5165\u5728\u7ec4\u7ec7\u7a7f\u900f\u548c\u89c6\u7f51\u819c\u4e0b\u6ce8\u5c04\u65b9\u9762\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u6709\u6548\u514b\u670d\u4e86\u5bf9\u51c6\u6311\u6218\uff0c\u4e3a\u5176\u4ed6\u9700\u8981\u53d7\u63a7\u63d2\u5165\u7684\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2511.00879", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u77e5\u8bc6\u57fa\u7840\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u77e5\u8bc6\u5e93\u3001\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6a21\u578b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u9a8c\u8bc1LLM\u63a8\u7406\u662f\u5426\u51c6\u786e\u57fa\u4e8e\u77e5\u8bc6\u3002", "motivation": "\u968f\u7740\u9010\u6b65\u63a8\u7406\u6210\u4e3aLLM\u5904\u7406\u590d\u6742\u4efb\u52a1\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u9700\u8981\u9a8c\u8bc1LLM\u7684\u63a8\u7406\u662f\u5426\u51c6\u786e\u57fa\u4e8e\u77e5\u8bc6\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6\uff1a(1) \u5927\u89c4\u6a21\u539f\u5b50\u77e5\u8bc6\u5e93\uff0c(2) \u57fa\u4e8e\u77e5\u8bc6\u5e93\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u6a21\u578b\u5728\u63a8\u7406\u4e2d\u56de\u5fc6\u548c\u5e94\u7528\u5148\u9a8c\u77e5\u8bc6\u7684\u80fd\u529b\uff0c(3) \u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u8bc4\u4f30\u6a21\u578b\u7528\u4e8e\u9ad8\u6548\u53ef\u9760\u5730\u8ba1\u7b97\u6307\u6807\u3002", "result": "\u8be5\u8bc4\u4f30\u5957\u4ef6\u5728\u8bc6\u522b\u7f3a\u5931\u6216\u8bef\u7528\u77e5\u8bc6\u5143\u7d20\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6709\u6548\u6027\uff0c\u4e3a\u63ed\u793aLLM\u7684\u57fa\u672c\u63a8\u7406\u7f3a\u9677\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u4e0d\u4ec5\u80fd\u6709\u6548\u8bc6\u522b\u63a8\u7406\u7f3a\u9677\uff0c\u8fd8\u53ef\u96c6\u6210\u5230\u504f\u597d\u4f18\u5316\u4e2d\uff0c\u5c55\u793a\u4e86\u77e5\u8bc6\u57fa\u7840\u8bc4\u4f30\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.00293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00293", "abs": "https://arxiv.org/abs/2511.00293", "authors": ["Hengjia Li", "Jianjin Xu", "Keli Cheng", "Lei Wang", "Ning Bi", "Boxi Wu", "Fernando De la Torre", "Deng Cai"], "title": "Multi-View Consistent Human Image Customization via In-Context Learning", "comment": null, "summary": "Recent advances in personalized generative models demonstrate impressive\nresults in creating identity-consistent images of the same person under diverse\nsettings. Yet, we note that most methods cannot control the viewpoint of the\ngenerated image, nor generate consistent multiple views of the person. To\naddress this problem, we propose a lightweight adaptation method, PersonalView,\ncapable of enabling an existing model to acquire multi-view generation\ncapability with as few as 100 training samples. PersonalView consists of two\nkey components: First, we design a conditioning architecture to take advantage\nof the in-context learning ability of the pre-trained diffusion transformer.\nSecond, we preserve the original generative ability of the pretrained model\nwith a new Semantic Correspondence Alignment Loss. We evaluate the multi-view\nconsistency, text alignment, identity similarity, and visual quality of\nPersonalView and compare it to recent baselines with potential capability of\nmulti-view customization. PersonalView significantly outperforms baselines\ntrained on a large corpus of multi-view data with only 100 training samples.", "AI": {"tldr": "\u63d0\u51faPersonalView\u65b9\u6cd5\uff0c\u4ec5\u7528100\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u4e3a\u73b0\u6709\u6a21\u578b\u6dfb\u52a0\u591a\u89c6\u89d2\u751f\u6210\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u9700\u8981\u5927\u91cf\u591a\u89c6\u89d2\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u751f\u6210\u8eab\u4efd\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89d2\uff0c\u4e5f\u65e0\u6cd5\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u4eba\u7269\u56fe\u50cf\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u8bbe\u8ba1\u6761\u4ef6\u67b6\u6784\uff1b2\uff09\u901a\u8fc7\u8bed\u4e49\u5bf9\u5e94\u5bf9\u9f50\u635f\u5931\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u539f\u59cb\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728100\u4e2a\u8bad\u7ec3\u6837\u672c\u4e0b\uff0cPersonalView\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3001\u6587\u672c\u5bf9\u9f50\u3001\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u9700\u8981\u5927\u91cf\u591a\u89c6\u89d2\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PersonalView\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u914d\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u4e3a\u73b0\u6709\u6a21\u578b\u8d4b\u4e88\u591a\u89c6\u89d2\u751f\u6210\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u751f\u6210\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u751f\u6210\u3002"}}
{"id": "2511.01149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01149", "abs": "https://arxiv.org/abs/2511.01149", "authors": ["Shuaidong Pan", "Di Wu"], "title": "Modular Task Decomposition and Dynamic Collaboration in Multi-Agent Systems Driven by Large Language Models", "comment": null, "summary": "This paper addresses the limitations of a single agent in task decomposition\nand collaboration during complex task execution, and proposes a multi-agent\narchitecture for modular task decomposition and dynamic collaboration based on\nlarge language models. The method first converts natural language task\ndescriptions into unified semantic representations through a large language\nmodel. On this basis, a modular decomposition mechanism is introduced to break\ndown the overall goal into multiple hierarchical sub-tasks. Then, dynamic\nscheduling and routing mechanisms enable reasonable division of labor and\nrealtime collaboration among agents, allowing the system to adjust strategies\ncontinuously according to environmental feedback, thus maintaining efficiency\nand stability in complex tasks. Furthermore, a constraint parsing and global\nconsistency mechanism is designed to ensure coherent connections between\nsub-tasks and balanced workload, preventing performance degradation caused by\nredundant communication or uneven resource allocation. The experiments validate\nthe architecture across multiple dimensions, including task success rate,\ndecomposition efficiency, sub-task coverage, and collaboration balance. The\nresults show that the proposed method outperforms existing approaches in both\noverall performance and robustness, achieving a better balance between task\ncomplexity and communication overhead. In conclusion, this study demonstrates\nthe effectiveness and feasibility of language-driven task decomposition and\ndynamic collaboration in multi-agent systems, providing a systematic solution\nfor task execution in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u673a\u5236\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u6267\u884c\u95ee\u9898\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5206\u89e3\u6548\u7387\u548c\u534f\u4f5c\u5e73\u8861\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5355\u4e00\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\u4efb\u52a1\u5206\u89e3\u548c\u534f\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5f15\u5165\u6a21\u5757\u5316\u5206\u89e3\u673a\u5236\u5c06\u6574\u4f53\u76ee\u6807\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u548c\u8def\u7531\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u5408\u7406\u5206\u5de5\u548c\u5b9e\u65f6\u534f\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u7ea6\u675f\u89e3\u6790\u548c\u5168\u5c40\u4e00\u81f4\u6027\u673a\u5236\u786e\u4fdd\u5b50\u4efb\u52a1\u8fde\u8d2f\u8fde\u63a5\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5206\u89e3\u6548\u7387\u3001\u5b50\u4efb\u52a1\u8986\u76d6\u7387\u548c\u534f\u4f5c\u5e73\u8861\u7b49\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u6574\u4f53\u6027\u80fd\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u5f00\u9500\u4e4b\u95f4\u8fbe\u5230\u66f4\u597d\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8bed\u8a00\u9a71\u52a8\u7684\u4efb\u52a1\u5206\u89e3\u548c\u52a8\u6001\u534f\u4f5c\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01272", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01272", "abs": "https://arxiv.org/abs/2511.01272", "authors": ["Sehui Jeong", "Magaly C. Aviles", "Athena X. Naylor", "Cynthia Sung", "Allison M. Okamura"], "title": "Design and Fabrication of Origami-Inspired Knitted Fabrics for Soft Robotics", "comment": null, "summary": "Soft robots employing compliant materials and deformable structures offer\ngreat potential for wearable devices that are comfortable and safe for human\ninteraction. However, achieving both structural integrity and compliance for\ncomfort remains a significant challenge. In this study, we present a novel\nfabrication and design method that combines the advantages of origami\nstructures with the material programmability and wearability of knitted\nfabrics. We introduce a general design method that translates origami patterns\ninto knit designs by programming both stitch and material patterns. The method\ncreates folds in preferred directions while suppressing unintended buckling and\nbending by selectively incorporating heat fusible yarn to create rigid panels\naround compliant creases. We experimentally quantify folding moments and show\nthat stitch patterning enhances folding directionality while the heat fusible\nyarn (1) keeps geometry consistent by reducing edge curl and (2) prevents\nout-of-plane deformations by stiffening panels. We demonstrate the framework\nthrough the successful reproduction of complex origami tessellations, including\nMiura-ori, Yoshimura, and Kresling patterns, and present a wearable knitted\nKaleidocycle robot capable of locomotion. The combination of structural\nreconfigurability, material programmability, and potential for manufacturing\nscalability highlights knitted origami as a promising platform for\nnext-generation wearable robotics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6298\u7eb8\u7ed3\u6784\u548c\u9488\u7ec7\u9762\u6599\u7684\u65b0\u578b\u5236\u9020\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7a0b\u9488\u6cd5\u548c\u6750\u6599\u56fe\u6848\u5c06\u6298\u7eb8\u56fe\u6848\u8f6c\u5316\u4e3a\u9488\u7ec7\u8bbe\u8ba1\uff0c\u4f7f\u7528\u70ed\u7194\u7eb1\u521b\u5efa\u521a\u6027\u9762\u677f\u548c\u67d4\u6027\u6298\u75d5\uff0c\u6210\u529f\u590d\u5236\u4e86\u590d\u6742\u6298\u7eb8\u56fe\u6848\u5e76\u5f00\u53d1\u4e86\u53ef\u7a7f\u6234\u9488\u7ec7\u4e07\u82b1\u7b52\u673a\u5668\u4eba\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u4f7f\u7528\u67d4\u6027\u6750\u6599\u548c\u53ef\u53d8\u5f62\u7ed3\u6784\u5728\u53ef\u7a7f\u6234\u8bbe\u5907\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u540c\u65f6\u5b9e\u73b0\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u4f69\u6234\u8212\u9002\u6027\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u901a\u7528\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7a0b\u9488\u6cd5\u548c\u6750\u6599\u56fe\u6848\u5c06\u6298\u7eb8\u56fe\u6848\u8f6c\u5316\u4e3a\u9488\u7ec7\u8bbe\u8ba1\uff0c\u9009\u62e9\u6027\u4f7f\u7528\u70ed\u7194\u7eb1\u5728\u67d4\u6027\u6298\u75d5\u5468\u56f4\u521b\u5efa\u521a\u6027\u9762\u677f\uff0c\u63a7\u5236\u6298\u53e0\u65b9\u5411\u5e76\u9632\u6b62\u610f\u5916\u5f2f\u66f2\u3002", "result": "\u5b9e\u9a8c\u91cf\u5316\u4e86\u6298\u53e0\u529b\u77e9\uff0c\u8bc1\u660e\u9488\u6cd5\u56fe\u6848\u589e\u5f3a\u4e86\u6298\u53e0\u65b9\u5411\u6027\uff0c\u70ed\u7194\u7eb1\u901a\u8fc7\u51cf\u5c11\u8fb9\u7f18\u5377\u66f2\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u786c\u5316\u9762\u677f\u9632\u6b62\u5e73\u9762\u5916\u53d8\u5f62\u3002\u6210\u529f\u590d\u5236\u4e86Miura-ori\u3001Yoshimura\u548cKresling\u7b49\u590d\u6742\u6298\u7eb8\u56fe\u6848\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u8fd0\u52a8\u7684\u53ef\u7a7f\u6234\u9488\u7ec7\u4e07\u82b1\u7b52\u673a\u5668\u4eba\u3002", "conclusion": "\u9488\u7ec7\u6298\u7eb8\u7ed3\u5408\u4e86\u7ed3\u6784\u53ef\u91cd\u6784\u6027\u3001\u6750\u6599\u53ef\u7f16\u7a0b\u6027\u548c\u5236\u9020\u53ef\u6269\u5c55\u6027\u6f5c\u529b\uff0c\u662f\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7684\u6709\u524d\u666f\u5e73\u53f0\u3002"}}
{"id": "2511.00903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks.", "AI": {"tldr": "ColMate\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7OCR\u9884\u8bad\u7ec3\u76ee\u6807\u3001\u81ea\u76d1\u7763\u63a9\u7801\u5bf9\u6bd4\u5b66\u4e60\u548c\u5ef6\u8fdf\u4ea4\u4e92\u8bc4\u5206\u673a\u5236\uff0c\u5728ViDoRe V2\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u63d0\u53473.61%\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u65b9\u6cd5\u5f80\u5f80\u590d\u5236\u7eaf\u6587\u672c\u68c0\u7d22\u6280\u672f\uff0c\u5728\u6587\u6863\u7f16\u7801\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u591a\u6a21\u6001\u6587\u6863\u7ed3\u6784\u548c\u89c6\u89c9\u7279\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684OCR\u9884\u8bad\u7ec3\u76ee\u6807\u3001\u81ea\u76d1\u7763\u63a9\u7801\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u4ee5\u53ca\u66f4\u9002\u5408\u591a\u6a21\u6001\u6587\u6863\u7ed3\u6784\u548c\u89c6\u89c9\u7279\u5f81\u7684\u5ef6\u8fdf\u4ea4\u4e92\u8bc4\u5206\u673a\u5236\u3002", "result": "\u5728ViDoRe V2\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u63d0\u53473.61%\uff0c\u5728\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ColMate\u6210\u529f\u5f25\u5408\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e0e\u6587\u6863\u68c0\u7d22\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u591a\u6a21\u6001\u6587\u6863\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00328", "abs": "https://arxiv.org/abs/2511.00328", "authors": ["Isai Daniel Chac\u00f3n", "Paola Ruiz Puentes", "Jillian Pearse", "Pablo Arbel\u00e1ez"], "title": "Towards Automated Petrography", "comment": null, "summary": "Petrography is a branch of geology that analyzes the mineralogical\ncomposition of rocks from microscopical thin section samples. It is essential\nfor understanding rock properties across geology, archaeology, engineering,\nmineral exploration, and the oil industry. However, petrography is a\nlabor-intensive task requiring experts to conduct detailed visual examinations\nof thin section samples through optical polarization microscopes, thus\nhampering scalability and highlighting the need for automated techniques. To\naddress this challenge, we introduce the Large-scale Imaging and Thin section\nOptical-polarization Set (LITHOS), the largest and most diverse publicly\navailable experimental framework for automated petrography. LITHOS includes\n211,604 high-resolution RGB patches of polarized light and 105,802\nexpert-annotated grains across 25 mineral categories. Each annotation consists\nof the mineral class, spatial coordinates, and expert-defined major and minor\naxes represented as intersecting vector paths, capturing grain geometry and\norientation. We evaluate multiple deep learning techniques for mineral\nclassification in LITHOS and propose a dual-encoder transformer architecture\nthat integrates both polarization modalities as a strong baseline for future\nreference. Our method consistently outperforms single-polarization models,\ndemonstrating the value of polarization synergy in mineral classification. We\nhave made the LITHOS Benchmark publicly available, comprising our dataset,\ncode, and pretrained models, to foster reproducibility and further research in\nautomated petrographic analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LITHOS\u2014\u2014\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u5ca9\u77f3\u5b66\u5206\u6790\u7684\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u5305\u542b211,604\u4e2a\u9ad8\u5206\u8fa8\u7387RGB\u504f\u632f\u5149\u56fe\u50cf\u5757\u548c105,802\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u77ff\u7269\u9897\u7c92\uff0c\u6db5\u76d625\u79cd\u77ff\u7269\u7c7b\u522b\u3002", "motivation": "\u5ca9\u77f3\u5b66\u5206\u6790\u662f\u5730\u8d28\u5b66\u4e2d\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u4efb\u52a1\uff0c\u9700\u8981\u4e13\u5bb6\u901a\u8fc7\u5149\u5b66\u504f\u632f\u663e\u5fae\u955c\u8fdb\u884c\u8be6\u7ec6\u89c6\u89c9\u68c0\u67e5\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u7f16\u7801\u5668transformer\u67b6\u6784\uff0c\u6574\u5408\u4e24\u79cd\u504f\u632f\u6a21\u6001\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728LITHOS\u6570\u636e\u96c6\u4e0a\u7684\u77ff\u7269\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u77ff\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u5355\u504f\u632f\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u504f\u632f\u534f\u540c\u5728\u77ff\u7269\u5206\u7c7b\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "LITHOS\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08\u5305\u62ec\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u52a8\u5316\u5ca9\u77f3\u5b66\u5206\u6790\u7684\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.01276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01276", "abs": "https://arxiv.org/abs/2511.01276", "authors": ["Yiyao Ma", "Kai Chen", "Kexin Zheng", "Qi Dou"], "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation", "comment": null, "summary": "Dexterous grasp generation is a fundamental challenge in robotics, requiring\nboth grasp stability and adaptability across diverse objects and tasks.\nAnalytical methods ensure stable grasps but are inefficient and lack task\nadaptability, while generative approaches improve efficiency and task\nintegration but generalize poorly to unseen objects and tasks due to data\nlimitations. In this paper, we propose a transfer-based framework for dexterous\ngrasp generation, leveraging a conditional diffusion model to transfer\nhigh-quality grasps from shape templates to novel objects within the same\ncategory. Specifically, we reformulate the grasp transfer problem as the\ngeneration of an object contact map, incorporating object shape similarity and\ntask specifications into the diffusion process. To handle complex shape\nvariations, we introduce a dual mapping mechanism, capturing intricate\ngeometric relationship between shape templates and novel objects. Beyond the\ncontact map, we derive two additional object-centric maps, the part map and\ndirection map, to encode finer contact details for more stable grasps. We then\ndevelop a cascaded conditional diffusion model framework to jointly transfer\nthese three maps, ensuring their intra-consistency. Finally, we introduce a\nrobust grasp recovery mechanism, identifying reliable contact points and\noptimizing grasp configurations efficiently. Extensive experiments demonstrate\nthe superiority of our proposed method. Our approach effectively balances grasp\nquality, generation efficiency, and generalization performance across various\ntasks. Project homepage: https://cmtdiffusion.github.io/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u7075\u5de7\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63a5\u89e6\u56fe\u3001\u90e8\u4ef6\u56fe\u548c\u65b9\u5411\u56fe\u7684\u8054\u5408\u8fc1\u79fb\uff0c\u5b9e\u73b0\u4ece\u5f62\u72b6\u6a21\u677f\u5230\u65b0\u7269\u4f53\u7684\u9ad8\u8d28\u91cf\u6293\u53d6\u8f6c\u79fb\uff0c\u5e73\u8861\u4e86\u6293\u53d6\u8d28\u91cf\u3001\u751f\u6210\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u7075\u5de7\u6293\u53d6\u751f\u6210\u9762\u4e34\u7a33\u5b9a\u6027\u4e0e\u4efb\u52a1\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\uff0c\u5206\u6790\u65b9\u6cd5\u7a33\u5b9a\u4f46\u6548\u7387\u4f4e\u4e14\u7f3a\u4e4f\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u751f\u6210\u65b9\u6cd5\u6548\u7387\u9ad8\u4f46\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u8fc1\u79fb\u9ad8\u8d28\u91cf\u6293\u53d6\u5230\u65b0\u7269\u4f53\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5c06\u6293\u53d6\u8f6c\u79fb\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7269\u4f53\u63a5\u89e6\u56fe\u751f\u6210\uff0c\u5f15\u5165\u53cc\u6620\u5c04\u673a\u5236\u5904\u7406\u590d\u6742\u5f62\u72b6\u53d8\u5316\uff0c\u8054\u5408\u8fc1\u79fb\u63a5\u89e6\u56fe\u3001\u90e8\u4ef6\u56fe\u548c\u65b9\u5411\u56fe\uff0c\u5e76\u5f00\u53d1\u7ea7\u8054\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6846\u67b6\u548c\u9c81\u68d2\u7684\u6293\u53d6\u6062\u590d\u673a\u5236\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u6293\u53d6\u8d28\u91cf\u3001\u751f\u6210\u6548\u7387\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6293\u53d6\u8f6c\u79fb\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u7a33\u5b9a\u4e14\u9002\u5e94\u4efb\u52a1\u7684\u7075\u5de7\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.00335", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00335", "abs": "https://arxiv.org/abs/2511.00335", "authors": ["Weidong Zhang", "Pak Lun Kevin Ding", "Huan Liu"], "title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models", "comment": "10 pages, 5 tables, 1 figure, 3 equations, 11 mobile models, 7\n  datasets", "summary": "Lightweight vision classification models such as MobileNet, ShuffleNet, and\nEfficientNet are increasingly deployed in mobile and embedded systems, yet\ntheir performance has been predominantly benchmarked on ImageNet. This raises\ncritical questions: Do models that excel on ImageNet also generalize across\nother domains? How can cross-dataset robustness be systematically quantified?\nAnd which architectural elements consistently drive generalization under tight\nresource constraints? Here, we present the first systematic evaluation of 11\nlightweight vision models (2.5M parameters), trained under a fixed 100-epoch\nschedule across 7 diverse datasets. We introduce the Cross-Dataset Score\n(xScore), a unified metric that quantifies the consistency and robustness of\nmodel performance across diverse visual domains. Our results show that (1)\nImageNet accuracy does not reliably predict performance on fine-grained or\nmedical datasets, (2) xScore provides a scalable predictor of mobile model\nperformance that can be estimated from just four datasets, and (3) certain\narchitectural components--such as isotropic convolutions with higher spatial\nresolution and channel-wise attention--promote broader generalization, while\nTransformer-based blocks yield little additional benefit, despite incurring\nhigher parameter overhead. This study provides a reproducible framework for\nevaluating lightweight vision models beyond ImageNet, highlights key design\nprinciples for mobile-friendly architectures, and guides the development of\nfuture models that generalize robustly across diverse application domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8611\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86Cross-Dataset Score (xScore)\u6307\u6807\uff0c\u53d1\u73b0ImageNet\u51c6\u786e\u6027\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u7ec6\u7c92\u5ea6\u6216\u533b\u5b66\u6570\u636e\u96c6\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u4e86\u4fc3\u8fdb\u6cdb\u5316\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\u3002", "motivation": "\u5f53\u524d\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u4e3b\u8981\u5728ImageNet\u4e0a\u8bc4\u4f30\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u7684\u7cfb\u7edf\u7814\u7a76\u3002\u9700\u8981\u91cf\u5316\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89c9\u9886\u57df\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u9a71\u52a8\u6cdb\u5316\u7684\u67b6\u6784\u8981\u7d20\u3002", "method": "\u57287\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0c\u4ee5\u56fa\u5b9a100\u8f6e\u8bad\u7ec3\u8ba1\u5212\u8bad\u7ec311\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b(250\u4e07\u53c2\u6570)\uff0c\u5f15\u5165Cross-Dataset Score (xScore)\u4f5c\u4e3a\u7edf\u4e00\u6307\u6807\u6765\u91cf\u5316\u6a21\u578b\u6027\u80fd\u7684\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "ImageNet\u51c6\u786e\u6027\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u7ec6\u7c92\u5ea6\u6216\u533b\u5b66\u6570\u636e\u96c6\u6027\u80fd\uff1bxScore\u4ec5\u9700\u56db\u4e2a\u6570\u636e\u96c6\u5373\u53ef\u4f30\u8ba1\u79fb\u52a8\u6a21\u578b\u6027\u80fd\uff1b\u5404\u5411\u540c\u6027\u5377\u79ef\u914d\u5408\u66f4\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u901a\u9053\u6ce8\u610f\u529b\u4fc3\u8fdb\u6cdb\u5316\uff0c\u800cTransformer\u6a21\u5757\u5e26\u6765\u989d\u5916\u53c2\u6570\u5f00\u9500\u4f46\u589e\u76ca\u6709\u9650\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5728ImageNet\u4e4b\u5916\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86\u79fb\u52a8\u53cb\u597d\u67b6\u6784\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff0c\u4e3a\u5f00\u53d1\u5728\u591a\u6837\u5316\u5e94\u7528\u9886\u57df\u4e2d\u9c81\u68d2\u6cdb\u5316\u7684\u672a\u6765\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.01182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01182", "abs": "https://arxiv.org/abs/2511.01182", "authors": ["Cuong Van Duc", "Thai Tran Quoc", "Minh Nguyen Dinh Tuan", "Tam Vu Duc", "Son Nguyen Van", "Hanh Nguyen Thi"], "title": "MiRAGE: Misconception Detection with Retrieval-Guided Multi-Stage Reasoning and Ensemble Fusion", "comment": null, "summary": "Detecting student misconceptions in open-ended responses is a longstanding\nchallenge, demanding semantic precision and logical reasoning. We propose\nMiRAGE - Misconception Detection with Retrieval-Guided Multi-Stage Reasoning\nand Ensemble Fusion, a novel framework for automated misconception detection in\nmathematics. MiRAGE operates in three stages: (1) a Retrieval module narrows a\nlarge candidate pool to a semantically relevant subset; (2) a Reasoning module\nemploys chain-of-thought generation to expose logical inconsistencies in\nstudent solutions; and (3) a Reranking module refines predictions by aligning\nthem with the reasoning. These components are unified through an\nensemble-fusion strategy that enhances robustness and interpretability. On\nmathematics datasets, MiRAGE achieves Mean Average Precision scores of\n0.82/0.92/0.93 at levels 1/3/5, consistently outperforming individual modules.\nBy coupling retrieval guidance with multi-stage reasoning, MiRAGE reduces\ndependence on large-scale language models while delivering a scalable and\neffective solution for educational assessment.", "AI": {"tldr": "MiRAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u6570\u5b66\u9886\u57df\u5b66\u751f\u8bef\u89e3\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5f15\u5bfc\u7684\u591a\u9636\u6bb5\u63a8\u7406\u548c\u96c6\u6210\u878d\u5408\uff0c\u5728\u5f00\u653e\u7b54\u6848\u4e2d\u8bc6\u522b\u5b66\u751f\u8bef\u89e3\u3002", "motivation": "\u68c0\u6d4b\u5b66\u751f\u5f00\u653e\u7b54\u6848\u4e2d\u7684\u8bef\u89e3\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u9700\u8981\u8bed\u4e49\u7cbe\u786e\u6027\u548c\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "method": "MiRAGE\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u68c0\u7d22\u6a21\u5757\u7f29\u5c0f\u5019\u9009\u8bef\u89e3\u6c60\uff0c\u63a8\u7406\u6a21\u5757\u4f7f\u7528\u601d\u7ef4\u94fe\u751f\u6210\u66b4\u9732\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\uff0c\u91cd\u6392\u5e8f\u6a21\u5757\u901a\u8fc7\u5bf9\u9f50\u63a8\u7406\u6765\u4f18\u5316\u9884\u6d4b\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u901a\u8fc7\u96c6\u6210\u878d\u5408\u7b56\u7565\u7edf\u4e00\u3002", "result": "\u5728\u6570\u5b66\u6570\u636e\u96c6\u4e0a\uff0cMiRAGE\u57281/3/5\u7ea7\u522b\u5206\u522b\u83b7\u5f970.82/0.92/0.93\u7684\u5e73\u5747\u7cbe\u5ea6\u5206\u6570\uff0c\u6301\u7eed\u4f18\u4e8e\u5404\u5355\u72ec\u6a21\u5757\u3002", "conclusion": "\u901a\u8fc7\u5c06\u68c0\u7d22\u5f15\u5bfc\u4e0e\u591a\u9636\u6bb5\u63a8\u7406\u76f8\u7ed3\u5408\uff0cMiRAGE\u51cf\u5c11\u4e86\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4e3a\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01288", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2511.01288", "abs": "https://arxiv.org/abs/2511.01288", "authors": ["Bixuan Zhang", "Fengqi Zhang", "Haojie Chen", "You Wang", "Jie Hao", "Zhiyuan Luo", "Guang Li"], "title": "A High-Speed Capable Spherical Robot", "comment": "5 pages", "summary": "This paper designs a new spherical robot structure capable of supporting\nhigh-speed motion at up to 10 m/s. Building upon a single-pendulum-driven\nspherical robot, the design incorporates a momentum wheel with an axis aligned\nwith the secondary pendulum, creating a novel spherical robot structure.\nPractical experiments with the physical prototype have demonstrated that this\nnew spherical robot can achieve stable high-speed motion through simple\ndecoupled control, which was unattainable with the original structure. The\nspherical robot designed for high-speed motion not only increases speed but\nalso significantly enhances obstacle-crossing performance and terrain\nrobustness.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\uff0c\u80fd\u591f\u652f\u6301\u9ad8\u8fbe10 m/s\u7684\u9ad8\u901f\u8fd0\u52a8\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u91cf\u8f6e\u548c\u4e8c\u6b21\u6446\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u7b80\u5355\u89e3\u8026\u63a7\u5236\u4e0b\u7684\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\u3002", "motivation": "\u4f20\u7edf\u5355\u6446\u9a71\u52a8\u7403\u5f62\u673a\u5668\u4eba\u65e0\u6cd5\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\uff0c\u9700\u8981\u8bbe\u8ba1\u65b0\u7684\u7ed3\u6784\u6765\u7a81\u7834\u901f\u5ea6\u9650\u5236\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5728\u5355\u6446\u9a71\u52a8\u7403\u5f62\u673a\u5668\u4eba\u57fa\u7840\u4e0a\uff0c\u589e\u52a0\u4e0e\u4e8c\u6b21\u6446\u8f74\u5bf9\u9f50\u7684\u52a8\u91cf\u8f6e\uff0c\u521b\u5efa\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\u3002", "result": "\u7269\u7406\u539f\u578b\u5b9e\u9a8c\u8bc1\u660e\uff0c\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u7b80\u5355\u89e3\u8026\u63a7\u5236\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u901f\u8fd0\u52a8\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8d8a\u969c\u6027\u80fd\u548c\u5730\u5f62\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b0\u578b\u7403\u5f62\u673a\u5668\u4eba\u7ed3\u6784\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u901f\u8fd0\u52a8\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u539f\u6709\u7ed3\u6784\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u65b9\u9762\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6027\u80fd\u3002"}}
{"id": "2511.00960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e865\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57287\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\u7684\u63a8\u7406\u548c\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u521d\u59cb\u51c6\u786e\u7387\u4e0e\u8bc6\u522b\u81ea\u8eab\u9519\u8bef\u7684\u80fd\u529b\u5448\u8d1f\u76f8\u5173\uff0c\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u53cd\u800c\u6700\u8fc7\u5ea6\u81ea\u4fe1\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u7279\u522b\u662f\u5370\u5ea6\u8bed\u8a00\uff09\u4e0a\u8fdb\u884c\u6587\u5316\u57fa\u7840\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u6a21\u578b\u7684\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u521b\u5efa\u591a\u8bed\u8a00\u8c1c\u8bed\u6570\u636e\u96c6\uff0c\u5305\u542b\u4f20\u7edf\u8c1c\u8bed\u548c\u4e0a\u4e0b\u6587\u91cd\u6784\u53d8\u4f53\uff0c\u8bc4\u4f305\u4e2aLLM\u57287\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u91c7\u75287\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5206\u4e24\u9636\u6bb5\u6d4b\u8bd5\u8c1c\u8bed\u89e3\u7b54\u80fd\u529b\u548c\u81ea\u6211\u8bc4\u4f30\u4e00\u81f4\u6027\u3002", "result": "Gemini 2.5 Pro\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5c11\u6837\u672c\u65b9\u6cd5\u4ec5\u5e26\u6765\u8fb9\u9645\u6536\u76ca\uff0c\u51c6\u786e\u7387\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5dee\u5f02\u663e\u8457\u3002\u6a21\u578b\u521d\u59cb\u51c6\u786e\u7387\u4e0e\u8bc6\u522b\u81ea\u8eab\u9519\u8bef\u80fd\u529b\u5448\u8d1f\u76f8\u5173\uff1aGemini 2.5 Pro\u8fc7\u5ea6\u81ea\u4fe1\uff08\u771f\u8d1f\u73874.34%\uff09\uff0c\u800cLLaMA 4 Scout\u66f4\u5177\u81ea\u6211\u610f\u8bc6\uff08\u771f\u8d1f\u738742.09%\uff09\u3002", "conclusion": "\u591a\u8bed\u8a00\u63a8\u7406\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u6709\u6548\u63a8\u7406\u53c8\u80fd\u8bc6\u522b\u81ea\u8eab\u5c40\u9650\u7684\u6a21\u578b\u3002"}}
{"id": "2511.00338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00338", "abs": "https://arxiv.org/abs/2511.00338", "authors": ["Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction", "comment": null, "summary": "This work presents a novel hybrid approach that integrates Deep Operator\nNetworks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex\ninverse problem. The method effectively addresses tasks such as source\nlocalization governed by the Navier-Stokes equations and image reconstruction,\novercoming challenges related to nonlinearity, sparsity, and noisy data. By\nincorporating physics-informed constraints and task-specific regularization\ninto the loss function, the framework ensures solutions that are both\nphysically consistent and accurate. Validation on diverse synthetic and real\ndatasets demonstrates its robustness, scalability, and precision, showcasing\nits broad potential applications in computational physics and imaging sciences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DeepONet\u548cNTK\u7684\u6df7\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u9006\u95ee\u9898\uff0c\u5305\u62ecNavier-Stokes\u65b9\u7a0b\u63a7\u5236\u7684\u6e90\u5b9a\u4f4d\u548c\u56fe\u50cf\u91cd\u5efa\uff0c\u514b\u670d\u4e86\u975e\u7ebf\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\u6570\u636e\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u9006\u95ee\u9898\u4e2d\u7684\u975e\u7ebf\u6027\u3001\u7a00\u758f\u6027\u548c\u566a\u58f0\u6570\u636e\u6311\u6218\uff0c\u786e\u4fdd\u89e3\u5177\u6709\u7269\u7406\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u5c06Deep Operator Networks (DeepONet)\u4e0eNeural Tangent Kernel (NTK)\u7ed3\u5408\uff0c\u5728\u635f\u5931\u51fd\u6570\u4e2d\u878d\u5165\u7269\u7406\u7ea6\u675f\u548c\u4efb\u52a1\u7279\u5b9a\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u79cd\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u7269\u7406\u548c\u6210\u50cf\u79d1\u5b66\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.01183", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01183", "abs": "https://arxiv.org/abs/2511.01183", "authors": ["Hainan Fang", "Yuanbo Wen", "Jun Bi", "Yihan Wang", "Tonghui He", "Yanlin Tang", "Di Huang", "Jiaming Guo", "Rui Zhang", "Qi Guo", "Yunji Chen"], "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code", "comment": "Accepted at NeurIPS 2025", "summary": "Compilers, while essential, are notoriously complex systems that demand\nprohibitively expensive human expertise to develop and maintain. The recent\nadvancements in Large Language Models (LLMs) offer a compelling new paradigm:\nNeural Compilation, which could potentially simplify compiler development for\nnew architectures and facilitate the discovery of innovative optimization\ntechniques. However, several critical obstacles impede its practical adoption.\nFirstly, a significant lack of dedicated benchmarks and robust evaluation\nmethodologies hinders objective assessment and tracking of progress in the\nfield. Secondly, systematically enhancing the reliability and performance of\nLLM-generated assembly remains a critical challenge. Addressing these\nchallenges, this paper introduces NeuComBack, a novel benchmark dataset\nspecifically designed for IR-to-assembly compilation. Leveraging this dataset,\nwe first define a foundational Neural Compilation workflow and conduct a\ncomprehensive evaluation of the capabilities of recent frontier LLMs on Neural\nCompilation, establishing new performance baselines. We further propose a\nself-evolving prompt optimization method that enables LLMs to iteratively\nevolve their internal prompt strategies by extracting insights from prior\nself-debugging traces, thereby enhancing their neural compilation capabilities.\nExperiments demonstrate that our method significantly improves both the\nfunctional correctness and the performance of LLM-generated assembly code.\nCompared to baseline prompts, the functional correctness rates improved from\n44% to 64% on x86_64 and from 36% to 58% on aarch64, respectively. More\nsignificantly, among the 16 correctly generated x86_64 programs using our\nmethod, 14 (87.5%) surpassed clang-O3 performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NeuComBack\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8eIR\u5230\u6c47\u7f16\u7684\u795e\u7ecf\u7f16\u8bd1\uff0c\u5e76\u63d0\u51fa\u81ea\u8fdb\u5316\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u6c47\u7f16\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u7f16\u8bd1\u5668\u5f00\u53d1\u590d\u6742\u4e14\u6602\u8d35\uff0cLLM\u4e3a\u795e\u7ecf\u7f16\u8bd1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u4e13\u7528\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e14LLM\u751f\u6210\u6c47\u7f16\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u9700\u8981\u7cfb\u7edf\u63d0\u5347\u3002", "method": "\u63d0\u51faNeuComBack\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u795e\u7ecf\u7f16\u8bd1\u5de5\u4f5c\u6d41\uff0c\u5e76\u8bbe\u8ba1\u81ea\u8fdb\u5316\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u8ba9LLM\u4ece\u81ea\u8c03\u8bd5\u8f68\u8ff9\u4e2d\u63d0\u53d6\u89c1\u89e3\u8fed\u4ee3\u8fdb\u5316\u63d0\u793a\u7b56\u7565\u3002", "result": "\u529f\u80fd\u6b63\u786e\u7387\u5728x86_64\u4e0a\u4ece44%\u63d0\u5347\u523064%\uff0c\u5728aarch64\u4e0a\u4ece36%\u63d0\u5347\u523058%\uff1b\u5728\u6b63\u786e\u751f\u6210\u7684x86_64\u7a0b\u5e8f\u4e2d\uff0c87.5%\u8d85\u8fc7clang-O3\u6027\u80fd\u3002", "conclusion": "NeuComBack\u57fa\u51c6\u548c\u81ea\u8fdb\u5316\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7f16\u8bd1\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u6c47\u7f16\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u6027\u80fd\u3002"}}
{"id": "2511.01294", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01294", "abs": "https://arxiv.org/abs/2511.01294", "authors": ["Jiawei Wang", "Dingyou Wang", "Jiaming Hu", "Qixuan Zhang", "Jingyi Yu", "Lan Xu"], "title": "Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects", "comment": null, "summary": "A deep understanding of kinematic structures and movable components is\nessential for enabling robots to manipulate objects and model their own\narticulated forms. Such understanding is captured through articulated objects,\nwhich are essential for tasks such as physical simulation, motion planning, and\npolicy learning. However, creating these models, particularly for complex\nsystems like robots or objects with high degrees of freedom (DoF), remains a\nsignificant challenge. Existing methods typically rely on motion sequences or\nstrong assumptions from hand-curated datasets, which hinders scalability. In\nthis paper, we introduce Kinematify, an automated framework that synthesizes\narticulated objects directly from arbitrary RGB images or text prompts. Our\nmethod addresses two core challenges: (i) inferring kinematic topologies for\nhigh-DoF objects and (ii) estimating joint parameters from static geometry. To\nachieve this, we combine MCTS search for structural inference with\ngeometry-driven optimization for joint reasoning, producing physically\nconsistent and functionally valid descriptions. We evaluate Kinematify on\ndiverse inputs from both synthetic and real-world environments, demonstrating\nimprovements in registration and kinematic topology accuracy over prior work.", "AI": {"tldr": "Kinematify\u662f\u4e00\u4e2a\u4eceRGB\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u81ea\u52a8\u5408\u6210\u5173\u8282\u5bf9\u8c61\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9ad8\u81ea\u7531\u5ea6\u5bf9\u8c61\u7684\u8fd0\u52a8\u5b66\u62d3\u6251\u63a8\u65ad\u548c\u5173\u8282\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u5173\u8282\u5bf9\u8c61\u5bf9\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u7269\u7406\u4eff\u771f\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fd0\u52a8\u5e8f\u5217\u6216\u624b\u5de5\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u590d\u6742\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408MCTS\u641c\u7d22\u8fdb\u884c\u7ed3\u6784\u63a8\u65ad\u548c\u57fa\u4e8e\u51e0\u4f55\u7684\u4f18\u5316\u8fdb\u884c\u5173\u8282\u63a8\u7406\uff0c\u751f\u6210\u7269\u7406\u4e00\u81f4\u4e14\u529f\u80fd\u6709\u6548\u7684\u63cf\u8ff0\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u8f93\u5165\u4e0a\u8bc4\u4f30\uff0c\u5728\u914d\u51c6\u548c\u8fd0\u52a8\u5b66\u62d3\u6251\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "Kinematify\u80fd\u591f\u4ece\u9759\u6001\u51e0\u4f55\u81ea\u52a8\u751f\u6210\u5173\u8282\u5bf9\u8c61\u6a21\u578b\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6613\u5230\u96be\u7684\u589e\u5f3a\u6846\u67b6\u6765\u89e3\u51b3\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u901a\u8fc7\u4f7f\u7528\u9488\u5bf9\u8f83\u957f\u6587\u672c\u7684\u7b80\u5355\u76d1\u7763\u5668\u6765\u589e\u5f3a\u66f4\u5177\u6311\u6218\u6027\u7684\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5728\u5404\u79cd\u5b9e\u9645\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5047\u8bbe\u6807\u7b7e\u4e3a\"\u9ec4\u91d1\u6807\u51c6\"\uff0c\u4f46\u5b58\u5728\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u4f20\u7edf\u8bad\u7ec3\u8303\u5f0f\u4e0d\u7cbe\u786e\u3002\u4eba\u7c7b\u8ba4\u77e5\u5c40\u9650\u548c\u68c0\u6d4b\u5668\u8d85\u667a\u80fd\u4f7f\u4e0d\u7cbe\u786e\u5b66\u4e60\u666e\u904d\u4e14\u4e0d\u53ef\u907f\u514d\u3002", "method": "\u63d0\u51fa\u4ece\u6613\u5230\u96be\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u4f7f\u7528\u9488\u5bf9\u76f8\u5bf9\u7b80\u5355\u7684\u8f83\u957f\u6587\u672c\u68c0\u6d4b\u4efb\u52a1\u7684\u7b80\u5355\u76d1\u7763\u5668\uff08\u5c3d\u7ba1\u80fd\u529b\u8f83\u5f31\uff09\u6765\u589e\u5f3a\u66f4\u5177\u6311\u6218\u6027\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u3002\u901a\u8fc7\u5c06\u68c0\u6d4b\u5668\u7ed3\u6784\u6027\u5730\u6574\u5408\u5230\u76d1\u7763\u5668\u4e2d\uff0c\u7406\u8bba\u4e0a\u5c06\u76d1\u7763\u5668\u5efa\u6a21\u4e3a\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u4e0b\u754c\u3002", "result": "\u5728\u8de8LLM\u3001\u8de8\u9886\u57df\u3001\u6df7\u5408\u6587\u672c\u548c\u6539\u5199\u653b\u51fb\u7b49\u591a\u79cd\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5177\u6709\u663e\u8457\u7684\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4ece\u6613\u5230\u96be\u7684\u76d1\u7763\u589e\u5f3a\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u4e0d\u7cbe\u786e\u5b66\u4e60\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u76d1\u7763\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00344", "abs": "https://arxiv.org/abs/2511.00344", "authors": ["Xihang Qiu", "Jiarong Cheng", "Yuhao Fang", "Wanpeng Zhang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities", "comment": null, "summary": "Multimodal Emotion Recognition in Conversations (MERC) enhances emotional\nunderstanding through the fusion of multimodal signals. However, unpredictable\nmodality absence in real-world scenarios significantly degrades the performance\nof existing methods. Conventional missing-modality recovery approaches, which\ndepend on training with complete multimodal data, often suffer from semantic\ndistortion under extreme data distributions, such as fixed-modality absence. To\naddress this, we propose the Federated Dialogue-guided and Semantic-Consistent\nDiffusion (FedDISC) framework, pioneering the integration of federated learning\ninto missing-modality recovery. By federated aggregation of modality-specific\ndiffusion models trained on clients and broadcasting them to clients missing\ncorresponding modalities, FedDISC overcomes single-client reliance on modality\ncompleteness. Additionally, the DISC-Diffusion module ensures consistency in\ncontext, speaker identity, and semantics between recovered and available\nmodalities, using a Dialogue Graph Network to capture conversational\ndependencies and a Semantic Conditioning Network to enforce semantic alignment.\nWe further introduce a novel Alternating Frozen Aggregation strategy, which\ncyclically freezes recovery and classifier modules to facilitate collaborative\noptimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI\ndatasets demonstrate that FedDISC achieves superior emotion classification\nperformance across diverse missing modality patterns, outperforming existing\napproaches.", "AI": {"tldr": "FedDISC\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6062\u590d\u7f3a\u5931\u6a21\u6001\u5e76\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u591a\u79cd\u7f3a\u5931\u6a21\u5f0f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6a21\u6001\u6570\u636e\u7ecf\u5e38\u5b58\u5728\u4e0d\u53ef\u9884\u6d4b\u7684\u6a21\u6001\u7f3a\u5931\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u6781\u7aef\u6570\u636e\u5206\u5e03\u4e0b\u4f1a\u51fa\u73b0\u8bed\u4e49\u5931\u771f\u95ee\u9898\u3002", "method": "\u63d0\u51faFedDISC\u6846\u67b6\uff1a1\uff09\u8054\u90a6\u805a\u5408\u5ba2\u6237\u7aef\u8bad\u7ec3\u7684\u6a21\u6001\u7279\u5b9a\u6269\u6563\u6a21\u578b\uff1b2\uff09DISC-Diffusion\u6a21\u5757\u786e\u4fdd\u6062\u590d\u6a21\u6001\u4e0e\u53ef\u7528\u6a21\u6001\u5728\u4e0a\u4e0b\u6587\u3001\u8bf4\u8bdd\u4eba\u8eab\u4efd\u548c\u8bed\u4e49\u4e0a\u7684\u4e00\u81f4\u6027\uff1b3\uff09\u4ea4\u66ff\u51bb\u7ed3\u805a\u5408\u7b56\u7565\u5faa\u73af\u51bb\u7ed3\u6062\u590d\u548c\u5206\u7c7b\u5668\u6a21\u5757\u4ee5\u4fc3\u8fdb\u534f\u4f5c\u4f18\u5316\u3002", "result": "\u5728IEMOCAP\u3001CMUMOSI\u548cCMUMOSEI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedDISC\u5728\u591a\u79cd\u7f3a\u5931\u6a21\u6001\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u60c5\u611f\u5206\u7c7b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedDISC\u6210\u529f\u5c06\u8054\u90a6\u5b66\u4e60\u96c6\u6210\u5230\u7f3a\u5931\u6a21\u6001\u6062\u590d\u4e2d\uff0c\u514b\u670d\u4e86\u5355\u5ba2\u6237\u7aef\u5bf9\u6a21\u6001\u5b8c\u6574\u6027\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01258", "abs": "https://arxiv.org/abs/2511.01258", "authors": ["Chuyue Lou", "M. Amine Atoui"], "title": "Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems", "comment": null, "summary": "Recently, fault diagnosis methods for marine machinery systems based on deep\nlearning models have attracted considerable attention in the shipping industry.\nMost existing studies assume fault classes are consistent and known between the\ntraining and test datasets, and these methods perform well under controlled\nenvironment. In practice, however, previously unseen or unknown fault types\n(i.e., out-of-distribution or open-set observations not present during\ntraining) can occur, causing such methods to fail and posing a significant\nchallenge to their widespread industrial deployment. To address this challenge,\nthis paper proposes a semi-supervised open-set fault diagnosis (SOFD) framework\nthat enhances and extends the applicability of deep learning models in open-set\nfault diagnosis scenarios. The framework includes a reliability subset\nconstruction process, which uses a multi-layer fusion feature representation\nextracted by a supervised feature learning model to select an unlabeled test\nsubset. The labeled training set and pseudo-labeled test subset are then fed\ninto a semi-supervised diagnosis model to learn discriminative features for\neach class, enabling accurate classification of known faults and effective\ndetection of unknown samples. Experimental results on a public maritime\nbenchmark dataset demonstrate the effectiveness and superiority of the proposed\nSOFD framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5f00\u653e\u96c6\u6545\u969c\u8bca\u65ad\uff08SOFD\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8239\u8236\u673a\u68b0\u7cfb\u7edf\u4e2d\u672a\u77e5\u6545\u969c\u7c7b\u578b\u7684\u8bca\u65ad\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u9760\u6027\u5b50\u96c6\u6784\u5efa\u548c\u534a\u76d1\u7763\u5b66\u4e60\u6765\u51c6\u786e\u5206\u7c7b\u5df2\u77e5\u6545\u969c\u5e76\u68c0\u6d4b\u672a\u77e5\u6837\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8239\u8236\u673a\u68b0\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6545\u969c\u7c7b\u522b\u4e00\u81f4\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f1a\u51fa\u73b0\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u672a\u77e5\u6545\u969c\u7c7b\u578b\uff0c\u5bfc\u81f4\u8fd9\u4e9b\u65b9\u6cd5\u5931\u6548\uff0c\u9650\u5236\u4e86\u5176\u5de5\u4e1a\u90e8\u7f72\u3002", "method": "\u63d0\u51faSOFD\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u591a\u5c42\u878d\u5408\u7279\u5f81\u8868\u793a\u6765\u6784\u5efa\u53ef\u9760\u6027\u5b50\u96c6\uff1b2\uff09\u5c06\u6807\u8bb0\u8bad\u7ec3\u96c6\u548c\u4f2a\u6807\u8bb0\u6d4b\u8bd5\u5b50\u96c6\u8f93\u5165\u534a\u76d1\u7763\u8bca\u65ad\u6a21\u578b\uff0c\u5b66\u4e60\u6bcf\u7c7b\u7684\u5224\u522b\u7279\u5f81\u3002", "result": "\u5728\u516c\u5171\u6d77\u4e8b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SOFD\u6846\u67b6\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "SOFD\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u548c\u6269\u5c55\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5f00\u653e\u96c6\u6545\u969c\u8bca\u65ad\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5b9e\u73b0\u5df2\u77e5\u6545\u969c\u7684\u51c6\u786e\u5206\u7c7b\u548c\u672a\u77e5\u6837\u672c\u7684\u6709\u6548\u68c0\u6d4b\u3002"}}
{"id": "2511.01331", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01331", "abs": "https://arxiv.org/abs/2511.01331", "authors": ["Hongyin Zhang", "Shuo Zhang", "Junxi Jin", "Qixin Zeng", "Runze Li", "Donglin Wang"], "title": "RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as powerful\ngeneral-purpose policies for robotic manipulation, benefiting from large-scale\nmulti-modal pre-training. However, they often fail to generalize reliably in\nout-of-distribution deployments, where unavoidable disturbances such as\nobservation noise, sensor errors, or actuation perturbations become prevalent.\nWhile recent Reinforcement Learning (RL)-based post-training provides a\npractical means to adapt pre-trained VLA models, existing methods mainly\nemphasize reward maximization and overlook robustness to environmental\nuncertainty. In this work, we introduce RobustVLA, a lightweight online RL\npost-training method designed to explicitly enhance the resilience of VLA\nmodels. Through a systematic robustness analysis, we identify two key\nregularizations: Jacobian regularization, which mitigates sensitivity to\nobservation noise, and smoothness regularization, which stabilizes policies\nunder action perturbations. Extensive experiments across diverse robotic\nenvironments demonstrate that RobustVLA significantly outperforms prior\nstate-of-the-art methods in robustness and reliability. Our results highlight\nthe importance of principled robustness-aware RL post-training as a key step\ntoward improving the reliability and robustness of VLA models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RobustVLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u589e\u5f3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u5176\u5728\u5206\u5e03\u5916\u90e8\u7f72\u4e2d\u5bf9\u89c2\u6d4b\u566a\u58f0\u548c\u52a8\u4f5c\u6270\u52a8\u7684\u654f\u611f\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5206\u5e03\u5916\u90e8\u7f72\u65f6\u5bf9\u89c2\u6d4b\u566a\u58f0\u3001\u4f20\u611f\u5668\u8bef\u5dee\u548c\u52a8\u4f5c\u6270\u52a8\u7b49\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u800c\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5956\u52b1\u6700\u5927\u5316\uff0c\u5ffd\u89c6\u4e86\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faRobustVLA\u8f7b\u91cf\u7ea7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u6b63\u5219\u5316\u51cf\u5c11\u5bf9\u89c2\u6d4b\u566a\u58f0\u7684\u654f\u611f\u6027\uff0c\u901a\u8fc7\u5e73\u6ed1\u6b63\u5219\u5316\u5728\u52a8\u4f5c\u6270\u52a8\u4e0b\u7a33\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRobustVLA\u5728\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u539f\u5219\u6027\u7684\u9c81\u68d2\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u662f\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2511.01311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01311", "abs": "https://arxiv.org/abs/2511.01311", "authors": ["Filip Naudot", "Tobias Sundqvist", "Timotheus Kampik"], "title": "llmSHAP: A Principled Approach to LLM Explainability", "comment": null, "summary": "Feature attribution methods help make machine learning-based inference\nexplainable by determining how much one or several features have contributed to\na model's output. A particularly popular attribution method is based on the\nShapley value from cooperative game theory, a measure that guarantees the\nsatisfaction of several desirable principles, assuming deterministic inference.\nWe apply the Shapley value to feature attribution in large language model\n(LLM)-based decision support systems, where inference is, by design, stochastic\n(non-deterministic). We then demonstrate when we can and cannot guarantee\nShapley value principle satisfaction across different implementation variants\napplied to LLM-based decision support, and analyze how the stochastic nature of\nLLMs affects these guarantees. We also highlight trade-offs between explainable\ninference speed, agreement with exact Shapley value attributions, and principle\nattainment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u968f\u673a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u5e94\u7528Shapley\u503c\u8fdb\u884c\u7279\u5f81\u5f52\u56e0\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u5b9e\u73b0\u53d8\u4f53\u4e0bShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u60c5\u51b5\uff0c\u5e76\u63a2\u8ba8\u4e86LLM\u968f\u673a\u6027\u5bf9\u8fd9\u4e9b\u4fdd\u8bc1\u7684\u5f71\u54cd\u3002", "motivation": "\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u4f7f\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u63a8\u7406\u53ef\u89e3\u91ca\uff0c\u4f46\u6d41\u884c\u7684Shapley\u503c\u65b9\u6cd5\u5047\u8bbe\u786e\u5b9a\u6027\u63a8\u7406\uff0c\u800cLLM\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u672c\u8d28\u4e0a\u662f\u968f\u673a\u7684\uff0c\u9700\u8981\u7814\u7a76\u5728\u8fd9\u79cd\u968f\u673a\u73af\u5883\u4e0bShapley\u503c\u539f\u5219\u7684\u9002\u7528\u6027\u3002", "method": "\u5c06Shapley\u503c\u5e94\u7528\u4e8eLLM\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u7279\u5f81\u5f52\u56e0\uff0c\u5206\u6790\u4e0d\u540c\u5b9e\u73b0\u53d8\u4f53\u4e0bShapley\u503c\u539f\u5219\u7684\u6ee1\u8db3\u60c5\u51b5\uff0c\u5e76\u7814\u7a76LLM\u968f\u673a\u6027\u5bf9\u8fd9\u4e9b\u4fdd\u8bc1\u7684\u5f71\u54cd\u3002", "result": "\u786e\u5b9a\u4e86\u5728\u4e0d\u540c\u5b9e\u73b0\u53d8\u4f53\u4e0b\u53ef\u4ee5\u6216\u4e0d\u80fd\u4fdd\u8bc1Shapley\u503c\u539f\u5219\u6ee1\u8db3\u7684\u6761\u4ef6\uff0c\u63ed\u793a\u4e86LLM\u968f\u673a\u6027\u5bf9\u539f\u5219\u4fdd\u8bc1\u7684\u5f71\u54cd\uff0c\u5e76\u7a81\u51fa\u4e86\u53ef\u89e3\u91ca\u63a8\u7406\u901f\u5ea6\u3001\u4e0e\u7cbe\u786eShapley\u503c\u5f52\u56e0\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u539f\u5219\u8fbe\u6210\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u5728LLM\u968f\u673a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u5e94\u7528Shapley\u503c\u8fdb\u884c\u7279\u5f81\u5f52\u56e0\u65f6\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u5b9e\u73b0\u53d8\u4f53\u9009\u62e9\uff0c\u56e0\u4e3a\u4e0d\u540c\u53d8\u4f53\u5728\u539f\u5219\u4fdd\u8bc1\u3001\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u91cd\u8981\u6743\u8861\u3002"}}
{"id": "2511.01014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines.", "AI": {"tldr": "\u63d0\u51faIF-CRITIC\u6a21\u578b\uff0c\u901a\u8fc7\u7ea6\u675f\u6e05\u5355\u751f\u6210\u548c\u591a\u9636\u6bb5\u8fc7\u6ee4\u673a\u5236\uff0c\u4e3a\u6307\u4ee4\u8ddf\u968f\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u8ddf\u968f\u8bc4\u4f30\u6a21\u578b\u5b58\u5728\u6210\u672c\u9ad8\u3001\u8bc4\u4f30\u4e0d\u53ef\u9760\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u7ea6\u675f\u6e05\u5355\u751f\u6210\u5668\u5206\u89e3\u6307\u4ee4\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8fc7\u6ee4\u673a\u5236\u6536\u96c6\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u91c7\u7528\u7ea6\u675f\u7ea7\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u8bad\u7ec3IF-CRITIC\u3002", "result": "IF-CRITIC\u8bc4\u4f30\u6027\u80fd\u8d85\u8d8aDeepseek-R1\u548co4-mini\u7b49\u5f3a\u57fa\u7ebf\uff0c\u5728\u8f83\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e0b\u663e\u8457\u63d0\u5347LLM\u6307\u4ee4\u8ddf\u968f\u4f18\u5316\u6027\u80fd\u3002", "conclusion": "IF-CRITIC\u4e3a\u6307\u4ee4\u8ddf\u968f\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u662f\u63d0\u5347LLM\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00352", "abs": "https://arxiv.org/abs/2511.00352", "authors": ["Mohd Ruhul Ameen", "Akif Islam"], "title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach", "comment": "6 pages, 8 figures, 4 Tables, submitted to ICECTE 2026", "summary": "The rapid rise of generative diffusion models has made distinguishing\nauthentic visual content from synthetic imagery increasingly challenging.\nTraditional deepfake detection methods, which rely on frequency or pixel-level\nartifacts, fail against modern text-to-image systems such as Stable Diffusion\nand DALL-E that produce photorealistic and artifact-free results. This paper\nintroduces a diffusion-based forensic framework that leverages multi-strength\nimage reconstruction dynamics, termed diffusion snap-back, to identify\nAI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and\nPSNR) evolve across varying noise strengths, we extract interpretable\nmanifold-based features that differentiate real and synthetic images. Evaluated\non a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under\ncross-validation and remains robust to common distortions such as compression\nand noise. Despite using limited data and a single diffusion backbone (Stable\nDiffusion v1.5), the proposed method demonstrates strong generalization and\ninterpretability, offering a foundation for scalable, model-agnostic synthetic\nmedia forensics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u91cd\u5efa\u52a8\u6001\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u566a\u58f0\u5f3a\u5ea6\u4e0b\u56fe\u50cf\u91cd\u5efa\u6307\u6807\u7684\u53d8\u5316\u6765\u533a\u5206\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\uff08\u5982Stable Diffusion\u548cDALL-E\uff09\u751f\u6210\u7684\u65e0\u4f2a\u5f71\u903c\u771f\u56fe\u50cf\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u5229\u7528\u591a\u5f3a\u5ea6\u56fe\u50cf\u91cd\u5efa\u52a8\u6001\uff08\u6269\u6563\u56de\u5f39\uff09\u5206\u6790\u91cd\u5efa\u6307\u6807\uff08LPIPS\u3001SSIM\u3001PSNR\uff09\u968f\u566a\u58f0\u5f3a\u5ea6\u7684\u53d8\u5316\uff0c\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u6d41\u5f62\u7279\u5f81\u6765\u533a\u5206\u771f\u5b9e\u548c\u5408\u6210\u56fe\u50cf\u3002", "result": "\u57284000\u5f20\u56fe\u50cf\u7684\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e0b\u8fbe\u52300.993 AUROC\uff0c\u5bf9\u538b\u7f29\u548c\u566a\u58f0\u7b49\u5e38\u89c1\u5931\u771f\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u5408\u6210\u5a92\u4f53\u53d6\u8bc1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.01320", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01320", "abs": "https://arxiv.org/abs/2511.01320", "authors": ["Ziqi Wang", "Hailiang Zhao", "Yuhao Yang", "Daojiang Hu", "Cheng Bao", "Mingyi Liu", "Kai Di", "Schahram Dustdar", "Zhongjie Wang", "Shuiguang Deng"], "title": "OmniFuser: Adaptive Multimodal Fusion for Service-Oriented Predictive Maintenance", "comment": null, "summary": "Accurate and timely prediction of tool conditions is critical for intelligent\nmanufacturing systems, where unplanned tool failures can lead to quality\ndegradation and production downtime. In modern industrial environments,\npredictive maintenance is increasingly implemented as an intelligent service\nthat integrates sensing, analysis, and decision support across production\nprocesses. To meet the demand for reliable and service-oriented operation, we\npresent OmniFuser, a multimodal learning framework for predictive maintenance\nof milling tools that leverages both visual and sensor data. It performs\nparallel feature extraction from high-resolution tool images and cutting-force\nsignals, capturing complementary spatiotemporal patterns across modalities. To\neffectively integrate heterogeneous features, OmniFuser employs a\ncontamination-free cross-modal fusion mechanism that disentangles shared and\nmodality-specific components, allowing for efficient cross-modal interaction.\nFurthermore, a recursive refinement pathway functions as an anchor mechanism,\nconsistently retaining residual information to stabilize fusion dynamics. The\nlearned representations can be encapsulated as reusable maintenance service\nmodules, supporting both tool-state classification (e.g., Sharp, Used, Dulled)\nand multi-step force signal forecasting. Experiments on real-world milling\ndatasets demonstrate that OmniFuser consistently outperforms state-of-the-art\nbaselines, providing a dependable foundation for building intelligent\nindustrial maintenance services.", "AI": {"tldr": "\u63d0\u51faOmniFuser\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u4f20\u611f\u5668\u6570\u636e\u5b9e\u73b0\u94e3\u524a\u5200\u5177\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u5728\u5200\u5177\u72b6\u6001\u5206\u7c7b\u548c\u529b\u4fe1\u53f7\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u667a\u80fd\u5236\u9020\u7cfb\u7edf\u4e2d\u51c6\u786e\u53ca\u65f6\u7684\u5200\u5177\u72b6\u6001\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u975e\u8ba1\u5212\u6027\u5200\u5177\u6545\u969c\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u548c\u751f\u4ea7\u505c\u673a\u3002\u9700\u8981\u53ef\u9760\u7684\u670d\u52a1\u5bfc\u5411\u578b\u9884\u6d4b\u7ef4\u62a4\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5e76\u884c\u7279\u5f81\u63d0\u53d6\u4ece\u9ad8\u5206\u8fa8\u7387\u5200\u5177\u56fe\u50cf\u548c\u5207\u524a\u529b\u4fe1\u53f7\u4e2d\u6355\u83b7\u4e92\u8865\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u4f7f\u7528\u65e0\u6c61\u67d3\u7684\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\u5206\u79bb\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u9012\u5f52\u7cbe\u5316\u8def\u5f84\u4fdd\u7559\u6b8b\u5dee\u4fe1\u606f\u7a33\u5b9a\u878d\u5408\u52a8\u6001\u3002", "result": "\u5728\u771f\u5b9e\u94e3\u524a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOmniFuser\u5728\u5200\u5177\u72b6\u6001\u5206\u7c7b\u548c\u529b\u4fe1\u53f7\u9884\u6d4b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "OmniFuser\u4e3a\u6784\u5efa\u667a\u80fd\u5de5\u4e1a\u7ef4\u62a4\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u5b66\u4e60\u5230\u7684\u8868\u793a\u53ef\u5c01\u88c5\u4e3a\u53ef\u91cd\u7528\u7684\u7ef4\u62a4\u670d\u52a1\u6a21\u5757\u3002"}}
{"id": "2511.01346", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2511.01346", "abs": "https://arxiv.org/abs/2511.01346", "authors": ["Shun Yoshida", "Qingchuan Song", "Bastian E. Rapp", "Thomas Speck", "Falk J. Tauber"], "title": "Thermo-responsive closing and reopening artificial Venus Flytrap utilizing shape memory elastomers", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Despite their often perceived static and slow nature, some plants can move\nfaster than the blink of an eye. The rapid snap closure motion of the Venus\nflytrap (Dionaea muscipula) has long captivated the interest of researchers and\nengineers alike, serving as a model for plant-inspired soft machines and\nrobots. The translation of the fast snapping closure has inspired the\ndevelopment of various artificial Venus flytrap (AVF) systems. However,\ntranslating both the closing and reopening motion of D. muscipula into an\nautonomous plant inspired soft machine has yet to be achieved. In this study,\nwe present an AVF that autonomously closes and reopens, utilizing novel\nthermo-responsive UV-curable shape memory materials for soft robotic systems.\nThe life-sized thermo-responsive AVF exhibits closing and reopening motions\ntriggered in a naturally occurring temperature range. The doubly curved trap\nlobes, built from shape memory polymers, close at 38{\\deg}C, while reopening\ninitiates around 45{\\deg}C, employing shape memory elastomer strips as\nantagonistic actuators to facilitate lobe reopening. This work represents the\nfirst demonstration of thermo-responsive closing and reopening in an AVF with\nprogrammed sequential motion in response to increasing temperature. This\napproach marks the next step toward autonomously bidirectional moving soft\nmachines/robots.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u81ea\u4e3b\u95ed\u5408\u548c\u91cd\u65b0\u6253\u5f00\u7684\u4eff\u751f\u6355\u8747\u8349\u8f6f\u673a\u5668\u4eba\uff0c\u5229\u7528\u65b0\u578b\u70ed\u54cd\u5e94\u5f62\u72b6\u8bb0\u5fc6\u6750\u6599\u5b9e\u73b0\u4e86\u5728\u81ea\u7136\u6e29\u5ea6\u8303\u56f4\u5185\u7684\u53cc\u5411\u8fd0\u52a8\u3002", "motivation": "\u867d\u7136\u6355\u8747\u8349\u7684\u5feb\u901f\u95ed\u5408\u8fd0\u52a8\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\u5e76\u5e94\u7528\u4e8e\u8f6f\u673a\u5668\u4eba\u5f00\u53d1\uff0c\u4f46\u5c06\u5176\u95ed\u5408\u548c\u91cd\u65b0\u6253\u5f00\u7684\u53cc\u5411\u8fd0\u52a8\u8f6c\u5316\u4e3a\u81ea\u4e3b\u7684\u690d\u7269\u542f\u53d1\u8f6f\u673a\u5668\u7cfb\u7edf\u5c1a\u672a\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u65b0\u578b\u70ed\u54cd\u5e94\u7d2b\u5916\u56fa\u5316\u5f62\u72b6\u8bb0\u5fc6\u6750\u6599\u6784\u5efa\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f7f\u7528\u5f62\u72b6\u8bb0\u5fc6\u805a\u5408\u7269\u5236\u4f5c\u53cc\u66f2\u74e3\u7247\u5b9e\u73b0\u95ed\u5408\uff0c\u5229\u7528\u5f62\u72b6\u8bb0\u5fc6\u5f39\u6027\u4f53\u6761\u4f5c\u4e3a\u62ee\u6297\u9a71\u52a8\u5668\u4fc3\u8fdb\u74e3\u7247\u91cd\u65b0\u6253\u5f00\u3002", "result": "\u5f00\u53d1\u51fa\u4e0e\u771f\u5b9e\u6355\u8747\u8349\u5c3a\u5bf8\u76f8\u5f53\u7684\u4eff\u751f\u7cfb\u7edf\uff0c\u572838\u00b0C\u65f6\u95ed\u5408\uff0c\u572845\u00b0C\u5de6\u53f3\u5f00\u59cb\u91cd\u65b0\u6253\u5f00\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6e29\u5ea6\u5347\u9ad8\u54cd\u5e94\u7684\u7a0b\u5e8f\u5316\u987a\u5e8f\u8fd0\u52a8\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u4eff\u751f\u6355\u8747\u8349\u7cfb\u7edf\u4e2d\u5c55\u793a\u70ed\u54cd\u5e94\u95ed\u5408\u548c\u91cd\u65b0\u6253\u5f00\u529f\u80fd\uff0c\u6807\u5fd7\u7740\u5411\u81ea\u4e3b\u53cc\u5411\u79fb\u52a8\u8f6f\u673a\u5668\u53d1\u5c55\u7684\u4e0b\u4e00\u6b65\u8fdb\u5c55\u3002"}}
{"id": "2511.01016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1.", "AI": {"tldr": "Prompt-R1\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u89c4\u6a21LLM\u4e0e\u5927\u89c4\u6a21LLM\u534f\u4f5c\uff0c\u901a\u8fc7\u591a\u8f6e\u63d0\u793a\u4ea4\u4e92\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7528\u6237\u5f80\u5f80\u65e0\u6cd5\u4e3a\u590d\u6742\u95ee\u9898\u63d0\u4f9b\u51c6\u786e\u6709\u6548\u7684\u63d0\u793a\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u53d1\u6325\u3002", "method": "\u91c7\u7528\u5c0f\u89c4\u6a21LLM\u601d\u8003\u5e76\u751f\u6210\u63d0\u793a\uff0c\u5927\u89c4\u6a21LLM\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u53cc\u6a21\u578b\u534f\u4f5c\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u53cc\u91cd\u7ea6\u675f\u5956\u52b1\u673a\u5236\u6765\u4f18\u5316\u6b63\u786e\u6027\u3001\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPrompt-R1\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "Prompt-R1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u652f\u6301\u4e0e\u5404\u79cd\u5927\u89c4\u6a21LLM\u7684\u63a8\u7406\u548c\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7528\u6237\u63d0\u793a\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2511.00357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00357", "abs": "https://arxiv.org/abs/2511.00357", "authors": ["Niklas W\u00f6lki", "Lukas Kondmann", "Christian Molli\u00e8re", "Martin Langer", "Julia Gottfriedsen", "Martin Werner"], "title": "Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation", "comment": "This work was presented at the TerraBytes Workshop at the 42nd\n  International Conference on Machine Learning. This version is not part of the\n  official ICML proceedings", "summary": "Onboard cloud segmentation is a critical yet underexplored task in thermal\nEarth observation (EO), particularly for CubeSat missions constrained by\nlimited hardware and spectral information. CubeSats often rely on a single\nthermal band and lack sufficient labeled data, making conventional cloud\nmasking techniques infeasible. This work addresses these challenges by applying\ntransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using\na UNet with a lightweight MobileNet encoder. We pretrain the model on the\npublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small\nset of mission-specific samples in a joint-training setup, improving the macro\nF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a\nTensorRT engine and demonstrate full-image inference in under 5 seconds on an\nNVIDIA Jetson Nano. These results show that leveraging public datasets and\nlightweight architectures can enable accurate, efficient thermal-only cloud\nmasking on-orbit, supporting real-time decision-making in data-limited EO\nmissions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9CubeSat\u70ed\u5730\u7403\u89c2\u6d4b\u4e2d\u7684\u4e91\u5206\u5272\u4efb\u52a1\uff0c\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7UNet\u67b6\u6784\uff0c\u5728NVIDIA Jetson Nano\u4e0a\u5b9e\u73b05\u79d2\u5185\u5168\u56fe\u50cf\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e91\u5206\u5272\u6027\u80fd\u3002", "motivation": "CubeSat\u4efb\u52a1\u53d7\u9650\u4e8e\u786c\u4ef6\u548c\u5149\u8c31\u4fe1\u606f\uff0c\u901a\u5e38\u53ea\u6709\u5355\u4e00\u70ed\u6ce2\u6bb5\u4e14\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u4f20\u7edf\u4e91\u63a9\u7801\u6280\u672f\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u70ed\u4e91\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528UNet\u67b6\u6784\u914d\u5408\u8f7b\u91cf\u7ea7MobileNet\u7f16\u7801\u5668\uff0c\u5728Landsat-7\u4e91\u8986\u76d6\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u65b9\u5f0f\u5728\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u6837\u672c\u4e0a\u5fae\u8c03\uff0c\u5e76\u5c06\u6a21\u578b\u8f6c\u6362\u4e3aTensorRT\u5f15\u64ce\u3002", "result": "\u4e0e\u4ec5\u4f7f\u7528FOREST-2\u6570\u636e\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b8fF1\u5206\u6570\u4ece0.850\u63d0\u5347\u52300.877\uff0c\u5728NVIDIA Jetson Nano\u4e0a\u5b9e\u73b05\u79d2\u5185\u5168\u56fe\u50cf\u63a8\u7406\u3002", "conclusion": "\u5229\u7528\u516c\u5171\u6570\u636e\u96c6\u548c\u8f7b\u91cf\u7ea7\u67b6\u6784\u53ef\u4ee5\u5728\u8f68\u9053\u4e0a\u5b9e\u73b0\u51c6\u786e\u9ad8\u6548\u7684\u70ed\u4e91\u63a9\u7801\uff0c\u652f\u6301\u6570\u636e\u6709\u9650\u7684\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u3002"}}
{"id": "2511.01347", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01347", "abs": "https://arxiv.org/abs/2511.01347", "authors": ["Riddhi Das", "Joscha Teichmann", "Thomas Speck", "Falk J. Tauber"], "title": "Design and development of an electronics-free earthworm robot", "comment": "Conference Proceedings Paper Living Machines 2025", "summary": "Soft robotic systems have gained widespread attention due to their inherent\nflexibility, adaptability, and safety, making them well-suited for varied\napplications. Among bioinspired designs, earthworm locomotion has been\nextensively studied for its efficient peristaltic motion, enabling movement in\nconfined and unstructured environments. Existing earthworm-inspired robots\nprimarily utilize pneumatic actuation due to its high force-to-weight ratio and\nease of implementation. However, these systems often rely on bulky,\npower-intensive electronic control units, limiting their practicality. In this\nwork, we present an electronics-free, earthworm-inspired pneumatic robot\nutilizing a modified Pneumatic Logic Gate (PLG) design. By integrating\npreconfigured PLG units with bellow actuators, we achieved a plug-and-play\nstyle modular system capable of peristaltic locomotion without external\nelectronic components. The proposed design reduces system complexity while\nmaintaining efficient actuation. We characterize the bellow actuators under\ndifferent operating conditions and evaluate the robots locomotion performance.\nOur findings demonstrate that the modified PLG-based control system effectively\ngenerates peristaltic wave propagation, achieving autonomous motion with\nminimal deviation. This study serves as a proof of concept for the development\nof electronics-free, peristaltic soft robots. The proposed system has potential\nfor applications in hazardous environments, where untethered, adaptable\nlocomotion is critical. Future work will focus on further optimizing the robot\ndesign and exploring untethered operation using onboard compressed air sources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u6c14\u52a8\u903b\u8f91\u95e8\u7684\u65e0\u7535\u5b50\u5143\u4ef6\u4eff\u86af\u8693\u6c14\u52a8\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5916\u90e8\u7535\u5b50\u63a7\u5236\u7684\u8815\u52a8\u8fd0\u52a8\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u6709\u4eff\u86af\u8693\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u6c14\u52a8\u9a71\u52a8\uff0c\u4f46\u901a\u5e38\u9700\u8981\u7b28\u91cd\u3001\u9ad8\u529f\u8017\u7684\u7535\u5b50\u63a7\u5236\u5355\u5143\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5c06\u9884\u914d\u7f6e\u7684\u6c14\u52a8\u903b\u8f91\u95e8\u5355\u5143\u4e0e\u6ce2\u7eb9\u7ba1\u6267\u884c\u5668\u96c6\u6210\uff0c\u6784\u5efa\u4e86\u5373\u63d2\u5373\u7528\u5f0f\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u65e0\u9700\u5916\u90e8\u7535\u5b50\u7ec4\u4ef6\u5373\u53ef\u5b9e\u73b0\u8815\u52a8\u8fd0\u52a8\u3002", "result": "\u6539\u8fdb\u7684\u6c14\u52a8\u903b\u8f91\u95e8\u63a7\u5236\u7cfb\u7edf\u6709\u6548\u4ea7\u751f\u8815\u52a8\u6ce2\u4f20\u64ad\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u8fd0\u52a8\u4e14\u504f\u5dee\u6700\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u65e0\u7535\u5b50\u5143\u4ef6\u7684\u8815\u52a8\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5728\u5371\u9669\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u5c06\u4f18\u5316\u8bbe\u8ba1\u5e76\u63a2\u7d22\u4f7f\u7528\u673a\u8f7d\u538b\u7f29\u7a7a\u6c14\u6e90\u7684\u65e0\u7f06\u64cd\u4f5c\u3002"}}
{"id": "2511.01019", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz.", "AI": {"tldr": "OceanAI\u662f\u4e00\u4e2a\u5bf9\u8bdd\u5f0f\u5e73\u53f0\uff0c\u5c06\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u6d41\u7545\u6027\u4e0eNOAA\u6743\u5a01\u6d77\u6d0b\u6570\u636e\u7684\u5b9e\u65f6\u53c2\u6570\u5316\u8bbf\u95ee\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7API\u8c03\u7528\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u548c\u6570\u636e\u53ef\u89c6\u5316\uff0c\u63d0\u9ad8\u79d1\u5b66AI\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u5bf9\u8bddAI\u7cfb\u7edf\u751f\u6210\u672a\u7ecf\u9a8c\u8bc1\u7684'\u5e7b\u89c9'\u5185\u5bb9\u3001\u7834\u574f\u79d1\u5b66\u4e25\u8c28\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u6d77\u6d0b\u79d1\u5b66\u63d0\u4f9b\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u53ef\u4fe1AI\u51b3\u7b56\u652f\u6301\u3002", "method": "\u96c6\u6210\u5f00\u6e90LLM\u7684\u81ea\u7136\u8bed\u8a00\u80fd\u529b\u4e0eNOAA\u5b9e\u65f6\u6570\u636e\u6d41\uff0c\u901a\u8fc7API\u8c03\u7528\u8bc6\u522b\u3001\u89e3\u6790\u548c\u5408\u6210\u76f8\u5173\u6570\u636e\u96c6\uff0c\u751f\u6210\u53ef\u91cd\u590d\u7684\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u548c\u6570\u636e\u53ef\u89c6\u5316\u3002", "result": "\u5728\u76f2\u6d4b\u6bd4\u8f83\u4e2d\uff0c\u53ea\u6709OceanAI\u80fd\u63d0\u4f9bNOAA\u6765\u6e90\u7684\u6570\u636e\u503c\u548c\u539f\u59cb\u6570\u636e\u5f15\u7528\uff0c\u5176\u4ed6AI\u4ea7\u54c1\u8981\u4e48\u62d2\u7edd\u56de\u7b54\uff0c\u8981\u4e48\u63d0\u4f9b\u65e0\u652f\u6301\u7684\u7ed3\u679c\u3002", "conclusion": "OceanAI\u901a\u8fc7\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u89c2\u6d4b\u7684\u8f93\u51fa\uff0c\u63a8\u8fdb\u4e86\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u4fe1\u4efb\u5ea6\uff0c\u4e3a\u6d77\u6d0b\u9886\u57df\u7684AI\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2511.00362", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations.", "AI": {"tldr": "Oitijjo-3D\u662f\u4e00\u4e2a\u514d\u8d39\u7684\u751f\u6210\u5f0fAI\u6846\u67b6\uff0c\u5229\u7528Google\u8857\u666f\u56fe\u50cf\u91cd\u5efa\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u9057\u4ea7\u76843D\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u6570\u5b57\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6280\u672f\u8981\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u9057\u4ea7\u4fee\u590d\u9762\u4e34\u8d44\u6e90\u6709\u9650\u548c\u6280\u672f\u4e13\u5bb6\u7a00\u7f3a\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4f20\u7edf3D\u6570\u5b57\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u548c\u64cd\u4f5c\u4eba\u5458\uff0c\u5bfc\u81f4\u8bb8\u591a\u5efa\u7b51\u7470\u5b9d\u65e0\u6cd5\u5f97\u5230\u6570\u5b57\u5316\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u4f7f\u7528Gemini 2.5 Flash Image\u8fdb\u884c\u591a\u6a21\u6001\u89c6\u89c9\u63a8\u7406\u5b9e\u73b0\u7ed3\u6784-\u7eb9\u7406\u5408\u6210\uff0c\u7136\u540e\u901a\u8fc7Hexagen\u8fdb\u884c\u795e\u7ecf\u56fe\u50cf\u52303D\u7684\u51e0\u4f55\u6062\u590d\u3002", "result": "\u7cfb\u7edf\u5728\u51e0\u79d2\u949f\u5185\u751f\u6210\u903c\u771f\u3001\u5ea6\u91cf\u4e00\u81f4\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u8fd0\u52a8\u7ed3\u6784\u6062\u590d\u6d41\u7a0b\u663e\u8457\u63d0\u901f\uff0c\u4e14\u65e0\u9700\u4e13\u4e1a\u786c\u4ef6\u6216\u4e13\u5bb6\u76d1\u7763\u3002\u5728Ahsan Manzil\u3001Choto Sona Mosque\u548cPaharpur\u7b49\u6807\u5fd7\u6027\u5efa\u7b51\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u5176\u4fdd\u6301\u4e86\u89c6\u89c9\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f00\u653e\u56fe\u50cf\u8f6c\u5316\u4e3a\u6570\u5b57\u9057\u4ea7\uff0c\u8fd9\u9879\u5de5\u4f5c\u5c06\u4fdd\u62a4\u91cd\u65b0\u5b9a\u4e49\u4e3a\u793e\u533a\u9a71\u52a8\u3001AI\u8f85\u52a9\u7684\u6587\u5316\u5ef6\u7eed\u884c\u4e3a\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u56fd\u5bb6\u5927\u5e45\u964d\u4f4e\u4e86\u7ecf\u6d4e\u548c\u6280\u672f\u95e8\u69db\u3002"}}
{"id": "2511.01350", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01350", "abs": "https://arxiv.org/abs/2511.01350", "authors": ["Maartje H. M. Wermelink", "Renate Sachse", "Sebastian Kruppert", "Thomas Speck", "Falk J. Tauber"], "title": "Model to Model: Understanding the Venus Flytrap Snapping Mechanism and Transferring it to a 3D-printed Bistable Soft Robotic Demonstrator", "comment": "Conference Proceedings Paper Living machines 2025", "summary": "The Venus flytrap (Dionaea muscipula) does not only serve as the textbook\nmodel for a carnivorous plant, but also has long intrigued both botanists and\nengineers with its rapidly closing leaf trap. The trap closure is triggered by\ntwo consecutive touches of a potential prey, after which the lobes rapidly\nswitch from their concave open-state to their convex close-state and catch the\nprey within 100-500 ms after being triggered. This transformation from concave\nto convex is initiated by changes in turgor pressure and the release of stored\nelastic energy from prestresses in the concave state, which accelerate this\nmovement, leading to inversion of the lobes bi-axial curvature. Possessing two\nlow-energy states, the leaves can be characterized as bistable systems. With\nour research, we seek to deepen the understanding of Venus flytrap motion\nmechanics and apply its principles to the design of an artificial bistable lobe\nactuator. We identified geometrical characteristics, such as dimensional ratios\nand the thickness gradient in the lobe, and transferred these to two 3D-printed\nbistable actuator models. One actuator parallels the simulated geometry of a\nVenus flytrap leaf, the other is a lobe model designed with CAD. Both models\ndisplay concave-convex bi-stability and snap close. These demonstrators are the\nfirst step in the development of an artificial Venus flytrap that mimics the\nmechanical behavior of the biological model and can be used as a soft fast\ngripper.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u6355\u8747\u8349\u7684\u5feb\u901f\u95ed\u5408\u673a\u5236\uff0c\u5e76\u5c06\u5176\u53cc\u7a33\u6001\u7279\u6027\u5e94\u7528\u4e8e\u8bbe\u8ba1\u4eba\u5de5\u53cc\u7a33\u6001\u53f6\u7247\u6267\u884c\u5668\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd3D\u6253\u5370\u6a21\u578b\u6765\u6a21\u62df\u6355\u8747\u8349\u7684\u673a\u68b0\u884c\u4e3a\u3002", "motivation": "\u52a0\u6df1\u5bf9\u6355\u8747\u8349\u8fd0\u52a8\u529b\u5b66\u7684\u7406\u89e3\uff0c\u5e76\u5c06\u5176\u539f\u7406\u5e94\u7528\u4e8e\u4eba\u5de5\u53cc\u7a33\u6001\u6267\u884c\u5668\u7684\u8bbe\u8ba1\uff0c\u5f00\u53d1\u53ef\u7528\u4e8e\u8f6f\u5feb\u901f\u6293\u53d6\u5668\u7684\u4eba\u5de5\u6355\u8747\u8349\u3002", "method": "\u8bc6\u522b\u6355\u8747\u8349\u53f6\u7247\u7684\u51e0\u4f55\u7279\u5f81\uff08\u5c3a\u5bf8\u6bd4\u4f8b\u548c\u539a\u5ea6\u68af\u5ea6\uff09\uff0c\u5e76\u5c06\u5176\u8f6c\u79fb\u5230\u4e24\u79cd3D\u6253\u5370\u7684\u53cc\u7a33\u6001\u6267\u884c\u5668\u6a21\u578b\uff1a\u4e00\u79cd\u6a21\u62df\u6355\u8747\u8349\u53f6\u7247\u51e0\u4f55\u5f62\u72b6\uff0c\u53e6\u4e00\u79cd\u4f7f\u7528CAD\u8bbe\u8ba1\u7684\u53f6\u7247\u6a21\u578b\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u51f9-\u51f8\u53cc\u7a33\u6001\u7279\u6027\uff0c\u5e76\u80fd\u5feb\u901f\u95ed\u5408\uff0c\u6210\u529f\u5c55\u793a\u4e86\u53cc\u7a33\u6001\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u4e9b\u6f14\u793a\u5668\u662f\u5f00\u53d1\u4eba\u5de5\u6355\u8747\u8349\u7684\u7b2c\u4e00\u6b65\uff0c\u80fd\u591f\u6a21\u62df\u751f\u7269\u6a21\u578b\u7684\u673a\u68b0\u884c\u4e3a\uff0c\u53ef\u7528\u4f5c\u8f6f\u5feb\u901f\u6293\u53d6\u5668\u3002"}}
{"id": "2511.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4.", "AI": {"tldr": "VayuChat\u662f\u4e00\u4e2a\u5bf9\u8bdd\u5f0f\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u5e2e\u52a9\u7528\u6237\u5206\u6790\u7a7a\u6c14\u8d28\u91cf\u3001\u6c14\u8c61\u548c\u653f\u7b56\u9879\u76ee\u6570\u636e\uff0c\u751f\u6210Python\u4ee3\u7801\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u4f7f\u73af\u5883\u6570\u636e\u5206\u6790\u5bf9\u51b3\u7b56\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u516c\u6c11\u66f4\u52a0\u6613\u7528\u3002", "motivation": "\u5370\u5ea6\u6bcf\u5e74\u56e0\u7a7a\u6c14\u6c61\u67d3\u5bfc\u81f4\u7ea6160\u4e07\u4eba\u8fc7\u65e9\u6b7b\u4ea1\uff0c\u4f46\u51b3\u7b56\u8005\u96be\u4ee5\u5c06\u5206\u6563\u7684\u6570\u636e\u8f6c\u5316\u4e3a\u6709\u6548\u51b3\u7b56\u3002\u73b0\u6709\u5de5\u5177\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u63d0\u4f9b\u9759\u6001\u4eea\u8868\u677f\uff0c\u65e0\u6cd5\u89e3\u51b3\u5173\u952e\u653f\u7b56\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u7cfb\u7edfVayuChat\uff0c\u6574\u5408\u4e2d\u592e\u6c61\u67d3\u63a7\u5236\u59d4\u5458\u4f1a\u76d1\u6d4b\u7ad9\u6570\u636e\u3001\u5dde\u7ea7\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u548c\u56fd\u5bb6\u6e05\u6d01\u7a7a\u6c14\u8ba1\u5212\u8d44\u91d1\u8bb0\u5f55\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u754c\u9762\u63d0\u4f9b\u6570\u636e\u5206\u6790\u80fd\u529b\u3002", "result": "VayuChat\u80fd\u591f\u56de\u7b54\u5173\u4e8e\u7a7a\u6c14\u8d28\u91cf\u3001\u6c14\u8c61\u548c\u653f\u7b56\u9879\u76ee\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684Python\u4ee3\u7801\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u4f7f\u590d\u6742\u7684\u73af\u5883\u5206\u6790\u53d8\u5f97\u7b80\u5355\u6613\u7528\u3002", "conclusion": "VayuChat\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u754c\u9762\u4f7f\u73af\u5883\u6570\u636e\u79d1\u5b66\u5bf9\u653f\u7b56\u5236\u5b9a\u8005\u3001\u7814\u7a76\u4eba\u5458\u548c\u516c\u6c11\u66f4\u52a0\u53ef\u8bbf\u95ee\uff0c\u5df2\u5728Hugging Face\u5e73\u53f0\u516c\u5f00\u90e8\u7f72\u3002"}}
{"id": "2511.00370", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00370", "abs": "https://arxiv.org/abs/2511.00370", "authors": ["Chaochen Wu", "Guan Luo", "Meiyun Zuo", "Zhitao Fan"], "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict", "comment": null, "summary": "Video moment retrieval uses a text query to locate a moment from a given\nuntrimmed video reference. Locating corresponding video moments with text\nqueries helps people interact with videos efficiently. Current solutions for\nthis task have not considered conflict within location results from different\nmodels, so various models cannot integrate correctly to produce better results.\nThis study introduces a reinforcement learning-based video moment retrieval\nmodel that can scan the whole video once to find the moment's boundary while\nproducing its locational evidence. Moreover, we proposed a multi-agent system\nframework that can use evidential learning to resolve conflicts between agents'\nlocalization output. As a side product of observing and dealing with conflicts\nbetween agents, we can decide whether a query has no corresponding moment in a\nvideo (out-of-scope) without additional training, which is suitable for\nreal-world applications. Extensive experiments on benchmark datasets show the\neffectiveness of our proposed methods compared with state-of-the-art\napproaches. Furthermore, the results of our study reveal that modeling\ncompetition and conflict of the multi-agent system is an effective way to\nimprove RL performance in moment retrieval and show the new role of evidential\nlearning in the multi-agent framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u8bc1\u636e\u5b66\u4e60\u89e3\u51b3\u4e0d\u540c\u6a21\u578b\u5b9a\u4f4d\u7ed3\u679c\u7684\u51b2\u7a81\uff0c\u5e76\u80fd\u8bc6\u522b\u65e0\u5bf9\u5e94\u65f6\u523b\u7684\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\u672a\u8003\u8651\u4e0d\u540c\u6a21\u578b\u5b9a\u4f4d\u7ed3\u679c\u95f4\u7684\u51b2\u7a81\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u6574\u5408\u3002\u9700\u8981\u89e3\u51b3\u51b2\u7a81\u95ee\u9898\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u626b\u63cf\u6574\u4e2a\u89c6\u9891\u5b9a\u4f4d\u65f6\u523b\u8fb9\u754c\u5e76\u751f\u6210\u4f4d\u7f6e\u8bc1\u636e\uff0c\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u5229\u7528\u8bc1\u636e\u5b66\u4e60\u89e3\u51b3\u667a\u80fd\u4f53\u5b9a\u4f4d\u8f93\u51fa\u7684\u51b2\u7a81\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u65e0\u5bf9\u5e94\u65f6\u523b\u7684\u67e5\u8be2\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ade\u4e89\u548c\u51b2\u7a81\u5efa\u6a21\u662f\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u5728\u65f6\u523b\u68c0\u7d22\u4e2d\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u8bc1\u636e\u5b66\u4e60\u5728\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u53d1\u6325\u4e86\u65b0\u4f5c\u7528\u3002"}}
{"id": "2511.01375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01375", "abs": "https://arxiv.org/abs/2511.01375", "authors": ["Hamin Koo", "Minseon Kim", "Jaehyung Kim"], "title": "Align to Misalign: Automatic LLM Jailbreak with Meta-Optimized LLM Judges", "comment": "under review, 28 pages", "summary": "Identifying the vulnerabilities of large language models (LLMs) is crucial\nfor improving their safety by addressing inherent weaknesses. Jailbreaks, in\nwhich adversaries bypass safeguards with crafted input prompts, play a central\nrole in red-teaming by probing LLMs to elicit unintended or unsafe behaviors.\nRecent optimization-based jailbreak approaches iteratively refine attack\nprompts by leveraging LLMs. However, they often rely heavily on either binary\nattack success rate (ASR) signals, which are sparse, or manually crafted\nscoring templates, which introduce human bias and uncertainty in the scoring\noutcomes. To address these limitations, we introduce AMIS (Align to MISalign),\na meta-optimization framework that jointly evolves jailbreak prompts and\nscoring templates through a bi-level structure. In the inner loop, prompts are\nrefined using fine-grained and dense feedback using a fixed scoring template.\nIn the outer loop, the template is optimized using an ASR alignment score,\ngradually evolving to better reflect true attack outcomes across queries. This\nco-optimization process yields progressively stronger jailbreak prompts and\nmore calibrated scoring signals. Evaluations on AdvBench and JBB-Behaviors\ndemonstrate that AMIS achieves state-of-the-art performance, including 88.0%\nASR on Claude-3.5-Haiku and 100.0% ASR on Claude-4-Sonnet, outperforming\nexisting baselines by substantial margins.", "AI": {"tldr": "AMIS\u662f\u4e00\u4e2a\u5143\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u7ed3\u6784\u8054\u5408\u4f18\u5316\u8d8a\u72f1\u63d0\u793a\u548c\u8bc4\u5206\u6a21\u677f\uff0c\u5728AdvBench\u548cClaude\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u8d8a\u72f1\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7a00\u758f\u7684\u4e8c\u8fdb\u5236\u653b\u51fb\u6210\u529f\u7387\u4fe1\u53f7\uff0c\u8981\u4e48\u4f9d\u8d56\u5f15\u5165\u4eba\u4e3a\u504f\u89c1\u7684\u624b\u5de5\u8bc4\u5206\u6a21\u677f\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u7ed3\u6784\uff1a\u5185\u5c42\u5faa\u73af\u4f7f\u7528\u56fa\u5b9a\u8bc4\u5206\u6a21\u677f\u901a\u8fc7\u7ec6\u7c92\u5ea6\u53cd\u9988\u4f18\u5316\u63d0\u793a\uff0c\u5916\u5c42\u5faa\u73af\u4f7f\u7528ASR\u5bf9\u9f50\u5206\u6570\u4f18\u5316\u8bc4\u5206\u6a21\u677f\uff0c\u5b9e\u73b0\u63d0\u793a\u548c\u6a21\u677f\u7684\u534f\u540c\u8fdb\u5316\u3002", "result": "\u5728AdvBench\u548cJBB-Behaviors\u4e0a\u8bc4\u4f30\uff0cAMIS\u5728Claude-3.5-Haiku\u4e0a\u8fbe\u523088.0% ASR\uff0c\u5728Claude-4-Sonnet\u4e0a\u8fbe\u5230100.0% ASR\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AMIS\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63d0\u793a\u548c\u8bc4\u5206\u6a21\u677f\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5f3a\u7684\u8d8a\u72f1\u63d0\u793a\u548c\u66f4\u51c6\u786e\u7684\u8bc4\u5206\u4fe1\u53f7\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u6d4b\u8bd5\u80fd\u529b\u3002"}}
{"id": "2511.01369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01369", "abs": "https://arxiv.org/abs/2511.01369", "authors": ["Luis Diener", "Jens Kalkkuhl", "Markus Enzweiler"], "title": "Lateral Velocity Model for Vehicle Parking Applications", "comment": "This manuscript has been submitted to Vehicle System Dynamics for\n  possible publication", "summary": "Automated parking requires accurate localization for quick and precise\nmaneuvering in tight spaces. While the longitudinal velocity can be measured\nusing wheel encoders, the estimation of the lateral velocity remains a key\nchallenge due to the absence of dedicated sensors in consumer-grade vehicles.\nExisting approaches often rely on simplified vehicle models, such as the\nzero-slip model, which assumes no lateral velocity at the rear axle. It is well\nestablished that this assumption does not hold during low-speed driving and\nresearchers thus introduce additional heuristics to account for differences. In\nthis work, we analyze real-world data from parking scenarios and identify a\nsystematic deviation from the zero-slip assumption. We provide explanations for\nthe observed effects and then propose a lateral velocity model that better\ncaptures the lateral dynamics of the vehicle during parking. The model improves\nestimation accuracy, while relying on only two parameters, making it\nwell-suited for integration into consumer-grade applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u6d88\u8d39\u7ea7\u8f66\u8f86\u5728\u81ea\u52a8\u6cca\u8f66\u573a\u666f\u4e2d\u4fa7\u5411\u901f\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u8be5\u6a21\u578b\u4ec5\u9700\u4e24\u4e2a\u53c2\u6570\u4e14\u80fd\u66f4\u51c6\u786e\u5730\u6355\u6349\u8f66\u8f86\u5728\u6cca\u8f66\u8fc7\u7a0b\u4e2d\u7684\u4fa7\u5411\u52a8\u529b\u5b66\u7279\u6027\u3002", "motivation": "\u81ea\u52a8\u6cca\u8f66\u9700\u8981\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u4f46\u6d88\u8d39\u7ea7\u8f66\u8f86\u7f3a\u4e4f\u4e13\u7528\u4f20\u611f\u5668\u6765\u6d4b\u91cf\u4fa7\u5411\u901f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u96f6\u6ed1\u79fb\u5047\u8bbe\u7684\u7b80\u5316\u8f66\u8f86\u6a21\u578b\uff0c\u4f46\u5728\u4f4e\u901f\u9a7e\u9a76\u65f6\u8be5\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002", "method": "\u5206\u6790\u771f\u5b9e\u6cca\u8f66\u573a\u666f\u6570\u636e\uff0c\u8bc6\u522b\u96f6\u6ed1\u79fb\u5047\u8bbe\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u63d0\u51fa\u4e00\u4e2a\u4ec5\u9700\u4e24\u4e2a\u53c2\u6570\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\u6765\u66f4\u597d\u5730\u6355\u6349\u8f66\u8f86\u5728\u6cca\u8f66\u8fc7\u7a0b\u4e2d\u7684\u4fa7\u5411\u52a8\u529b\u5b66\u7279\u6027\u3002", "result": "\u63d0\u51fa\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u4ec5\u4f9d\u8d56\u4e24\u4e2a\u53c2\u6570\uff0c\u9002\u5408\u96c6\u6210\u5230\u6d88\u8d39\u7ea7\u5e94\u7528\u4e2d\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u4fa7\u5411\u901f\u5ea6\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u8f66\u8f86\u5728\u6cca\u8f66\u8fc7\u7a0b\u4e2d\u7684\u4fa7\u5411\u901f\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u96f6\u6ed1\u79fb\u5047\u8bbe\u5728\u4f4e\u901f\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6d88\u8d39\u7ea7\u81ea\u52a8\u6cca\u8f66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u6307\u5357\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u57fa\u4e8e\u6307\u5357\u7684\u4e34\u5e8a\u63a8\u7406\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u7528GPT\u5e2e\u52a9\u521b\u5efa\u4ece\u591a\u4e2a\u8bca\u65ad\u7684\u516c\u5f00\u6307\u5357\u4e2d\u63d0\u53d6\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u60a3\u8005\u573a\u666f\u548c\u4e34\u5e8a\u95ee\u9898\uff0c\u5e76\u5bf9\u4e00\u7cfb\u5217\u6d41\u884c\u7684LLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30LLM\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u6307\u5357\u4f9d\u4ece\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30LLM\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u6307\u5357\u4f9d\u4ece\u6027\u7684\u6846\u67b6\uff0c\u586b\u8865\u4e86\u533b\u7597\u9886\u57dfLLM\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.01396", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2511.01396", "abs": "https://arxiv.org/abs/2511.01396", "authors": ["Cl\u00e9ment Yvernes", "Emilie Devijver", "Ad\u00e8le H. Ribeiro", "Marianne Clausel--Lesourd", "\u00c9ric Gaussier"], "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering", "comment": "Accepted at The Thirty-ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS2025)", "summary": "Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes\nrepresent clusters of variables, and edges encode both cluster-level causal\nrelationships and dependencies arisen from unobserved confounding. C-DAGs\ndefine an equivalence class of acyclic causal graphs that agree on\ncluster-level relationships, enabling causal reasoning at a higher level of\nabstraction. However, when the chosen clustering induces cycles in the\nresulting C-DAG, the partition is deemed inadmissible under conventional C-DAG\nsemantics. In this work, we extend the C-DAG framework to support arbitrary\nvariable clusterings by relaxing the partition admissibility constraint,\nthereby allowing cyclic C-DAG representations. We extend the notions of\nd-separation and causal calculus to this setting, significantly broadening the\nscope of causal reasoning across clusters and enabling the application of\nC-DAGs in previously intractable scenarios. Our calculus is both sound and\natomically complete with respect to the do-calculus: all valid interventional\nqueries at the cluster level can be derived using our rules, each corresponding\nto a primitive do-calculus step.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86C-DAG\u6846\u67b6\uff0c\u652f\u6301\u4efb\u610f\u53d8\u91cf\u805a\u7c7b\uff0c\u5141\u8bb8\u5faa\u73afC-DAG\u8868\u793a\uff0c\u5e76\u6269\u5c55\u4e86d-\u5206\u79bb\u548c\u56e0\u679c\u6f14\u7b97\u6982\u5ff5\u3002", "motivation": "\u4f20\u7edfC-DAG\u6846\u67b6\u8981\u6c42\u805a\u7c7b\u5206\u533a\u5fc5\u987b\u662f\u65e0\u73af\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e00\u7ea6\u675f\uff0c\u4f7fC-DAG\u80fd\u591f\u5904\u7406\u4efb\u610f\u805a\u7c7b\u5206\u533a\u3002", "method": "\u901a\u8fc7\u653e\u5bbd\u5206\u533a\u53ef\u5bb9\u8bb8\u6027\u7ea6\u675f\uff0c\u5141\u8bb8\u5faa\u73afC-DAG\u8868\u793a\uff0c\u5e76\u6269\u5c55d-\u5206\u79bb\u548c\u56e0\u679c\u6f14\u7b97\u6982\u5ff5\u5230\u8fd9\u4e00\u65b0\u8bbe\u7f6e\u4e2d\u3002", "result": "\u63d0\u51fa\u7684\u6f14\u7b97\u5728do-\u6f14\u7b97\u65b9\u9762\u662f\u5065\u5168\u4e14\u539f\u5b50\u5b8c\u5907\u7684\uff1a\u6240\u6709\u6709\u6548\u7684\u96c6\u7fa4\u7ea7\u5e72\u9884\u67e5\u8be2\u90fd\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u7684\u89c4\u5219\u63a8\u5bfc\u51fa\u6765\uff0c\u6bcf\u4e2a\u89c4\u5219\u5bf9\u5e94\u4e00\u4e2a\u539f\u59cb\u7684do-\u6f14\u7b97\u6b65\u9aa4\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u663e\u8457\u6269\u5c55\u4e86C-DAG\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u4ee5\u524d\u96be\u4ee5\u5904\u7406\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u3002"}}
{"id": "2511.01379", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01379", "abs": "https://arxiv.org/abs/2511.01379", "authors": ["Kun Hu", "Menggang Li", "Zhiwen Jin", "Chaoquan Tang", "Eryi Hu", "Gongbo Zhou"], "title": "CM-LIUW-Odometry: Robust and High-Precision LiDAR-Inertial-UWB-Wheel Odometry for Extreme Degradation Coal Mine Tunnels", "comment": "Accepted by IROS 2025", "summary": "Simultaneous Localization and Mapping (SLAM) in large-scale, complex, and\nGPS-denied underground coal mine environments presents significant challenges.\nSensors must contend with abnormal operating conditions: GPS unavailability\nimpedes scene reconstruction and absolute geographic referencing, uneven or\nslippery terrain degrades wheel odometer accuracy, and long, feature-poor\ntunnels reduce LiDAR effectiveness. To address these issues, we propose\nCoalMine-LiDAR-IMU-UWB-Wheel-Odometry (CM-LIUW-Odometry), a multimodal SLAM\nframework based on the Iterated Error-State Kalman Filter (IESKF). First,\nLiDAR-inertial odometry is tightly fused with UWB absolute positioning\nconstraints to align the SLAM system with a global coordinate. Next, wheel\nodometer is integrated through tight coupling, enhanced by nonholonomic\nconstraints (NHC) and vehicle lever arm compensation, to address performance\ndegradation in areas beyond UWB measurement range. Finally, an adaptive motion\nmode switching mechanism dynamically adjusts the robot's motion mode based on\nUWB measurement range and environmental degradation levels. Experimental\nresults validate that our method achieves superior accuracy and robustness in\nreal-world underground coal mine scenarios, outperforming state-of-the-art\napproaches. We open source our code of this work on Github to benefit the\nrobotics community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5730\u4e0b\u7164\u77ff\u73af\u5883\u7684CM-LIUW-Odometry\u591a\u6a21\u6001SLAM\u6846\u67b6\uff0c\u7ed3\u5408LiDAR\u3001IMU\u3001UWB\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u901a\u8fc7IESKF\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002", "motivation": "\u89e3\u51b3\u5730\u4e0b\u7164\u77ff\u73af\u5883\u4e2dGPS\u4e0d\u53ef\u7528\u3001\u5730\u5f62\u6076\u52a3\u3001\u7279\u5f81\u7a00\u5c11\u7b49\u6311\u6218\uff0c\u63d0\u9ad8SLAM\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u57fa\u4e8eIESKF\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff1a1\uff09LiDAR-IMU\u91cc\u7a0b\u8ba1\u4e0eUWB\u7edd\u5bf9\u5b9a\u4f4d\u7ea6\u675f\u7d27\u8026\u5408\uff1b2\uff09\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u901a\u8fc7\u975e\u5b8c\u6574\u7ea6\u675f\u548c\u8f66\u8f86\u6760\u6746\u81c2\u8865\u507f\u96c6\u6210\uff1b3\uff09\u81ea\u9002\u5e94\u8fd0\u52a8\u6a21\u5f0f\u5207\u6362\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u5728\u771f\u5b9e\u5730\u4e0b\u7164\u77ff\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CM-LIUW-Odometry\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5730\u4e0b\u7164\u77ff\u73af\u5883\u4e2d\u7684SLAM\u6311\u6218\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u673a\u5668\u4eba\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2511.00389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00389", "abs": "https://arxiv.org/abs/2511.00389", "authors": ["Fan Zhang", "Haoxuan Li", "Shengju Qian", "Xin Wang", "Zheng Lian", "Hao Wu", "Zhihong Zhu", "Yuan Gao", "Qiankun Li", "Yefeng Zheng", "Zhouchen Lin", "Pheng-Ann Heng"], "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized numerous\nresearch fields, including computer vision and affective computing. As a\npivotal challenge in this interdisciplinary domain, facial expression\nrecognition (FER) has evolved from separate, domain-specific models to more\nunified approaches. One promising avenue to unify FER tasks is converting\nconventional FER datasets into visual question-answering (VQA) formats,\nenabling the direct application of powerful generalist MLLMs for inference.\nHowever, despite the success of cutting-edge MLLMs in various tasks, their\nperformance on FER tasks remains largely unexplored. To address this gap, we\nprovide FERBench, a systematic benchmark that incorporates 20 state-of-the-art\nMLLMs across four widely used FER datasets. Our results reveal that, while\nMLLMs exhibit good classification performance, they still face significant\nlimitations in reasoning and interpretability. To this end, we introduce\npost-training strategies aimed at enhancing the facial expression reasoning\ncapabilities of MLLMs. Specifically, we curate two high-quality and large-scale\ndatasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K\nfor reinforcement learning with verifiable rewards (RLVR), respectively.\nBuilding upon them, we develop a unified and interpretable FER foundation model\ntermed UniFER-7B, which outperforms many open-sourced and closed-source\ngeneralist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FERBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8620\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86UniFER-7B\u6a21\u578b\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8868\u60c5\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaFERBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d64\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684FER\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u4f7f\u7528UniFER-CoT-230K\u8fdb\u884c\u51b7\u542f\u52a8\u521d\u59cb\u5316\u548cUniFER-RLVR-360K\u8fdb\u884c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff1b\u6700\u7ec8\u6784\u5efa\u7edf\u4e00\u7684UniFER-7B\u57fa\u7840\u6a21\u578b\u3002", "result": "MLLMs\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff1bUniFER-7B\u6a21\u578b\u8d85\u8d8a\u4e86\u591a\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u901a\u7528MLLMs\uff0c\u5305\u62ecGemini-2.5-Pro\u548cQwen2.5-VL-72B\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0cUniFER-7B\u6a21\u578b\u5728\u8be5\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.01415", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01415", "abs": "https://arxiv.org/abs/2511.01415", "authors": ["Amrapali Pednekar", "\u00c1lvaro Garrido-P\u00e9rez", "Yara Khaluf", "Pieter Simoens"], "title": "Modulation of temporal decision-making in a deep reinforcement learning agent under the dual-task paradigm", "comment": "Accepted at CogInterp workshop @ NeurIPS 2025", "summary": "This study explores the interference in temporal processing within a\ndual-task paradigm from an artificial intelligence (AI) perspective. In this\ncontext, the dual-task setup is implemented as a simplified version of the\nOvercooked environment with two variations, single task (T) and dual task\n(T+N). Both variations involve an embedded time production task, but the dual\ntask (T+N) additionally involves a concurrent number comparison task. Two deep\nreinforcement learning (DRL) agents were separately trained for each of these\ntasks. These agents exhibited emergent behavior consistent with human timing\nresearch. Specifically, the dual task (T+N) agent exhibited significant\noverproduction of time relative to its single task (T) counterpart. This result\nwas consistent across four target durations. Preliminary analysis of neural\ndynamics in the agents' LSTM layers did not reveal any clear evidence of a\ndedicated or intrinsic timer. Hence, further investigation is needed to better\nunderstand the underlying time-keeping mechanisms of the agents and to provide\ninsights into the observed behavioral patterns. This study is a small step\ntowards exploring parallels between emergent DRL behavior and behavior observed\nin biological systems in order to facilitate a better understanding of both.", "AI": {"tldr": "\u672c\u7814\u7a76\u4eceAI\u89d2\u5ea6\u63a2\u7d22\u53cc\u4efb\u52a1\u8303\u5f0f\u4e2d\u7684\u65f6\u95f4\u5904\u7406\u5e72\u6270\uff0c\u4f7f\u7528\u7b80\u5316\u7684Overcooked\u73af\u5883\u8bad\u7ec3DRL\u667a\u80fd\u4f53\uff0c\u53d1\u73b0\u53cc\u4efb\u52a1\u667a\u80fd\u4f53\u76f8\u5bf9\u4e8e\u5355\u4efb\u52a1\u667a\u80fd\u4f53\u663e\u8457\u9ad8\u4f30\u65f6\u95f4\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8ba1\u65f6\u7814\u7a76\u4e00\u81f4\uff0c\u4f46\u672a\u53d1\u73b0\u660e\u786e\u7684\u5185\u90e8\u8ba1\u65f6\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u53cc\u4efb\u52a1\u8303\u5f0f\u4e2d\u7684\u65f6\u95f4\u5904\u7406\u884c\u4e3a\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8ba1\u65f6\u7814\u7a76\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u4fc3\u8fdb\u5bf9\u751f\u7269\u7cfb\u7edf\u548cAI\u7cfb\u7edf\u884c\u4e3a\u7684\u66f4\u597d\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u7b80\u5316\u7684Overcooked\u73af\u5883\u5b9e\u73b0\u53cc\u4efb\u52a1\u8303\u5f0f\uff0c\u5305\u62ec\u5355\u4efb\u52a1(T)\u548c\u53cc\u4efb\u52a1(T+N)\u4e24\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u8bad\u7ec3\u4e24\u4e2aDRL\u667a\u80fd\u4f53\uff0c\u53cc\u4efb\u52a1\u5305\u542b\u65f6\u95f4\u4ea7\u751f\u548c\u6570\u5b57\u6bd4\u8f83\u4e24\u4e2a\u5e76\u53d1\u4efb\u52a1\u3002", "result": "\u53cc\u4efb\u52a1(T+N)\u667a\u80fd\u4f53\u76f8\u5bf9\u4e8e\u5355\u4efb\u52a1(T)\u667a\u80fd\u4f53\u5728\u56db\u4e2a\u76ee\u6807\u6301\u7eed\u65f6\u95f4\u4e0a\u90fd\u663e\u8457\u9ad8\u4f30\u65f6\u95f4\uff0c\u4e0e\u4eba\u7c7b\u8ba1\u65f6\u7814\u7a76\u4e00\u81f4\uff1b\u5bf9LSTM\u5c42\u795e\u7ecf\u52a8\u529b\u5b66\u7684\u521d\u6b65\u5206\u6790\u672a\u53d1\u73b0\u660e\u786e\u7684\u4e13\u7528\u8ba1\u65f6\u5668\u8bc1\u636e\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u667a\u80fd\u4f53\u7684\u57fa\u7840\u8ba1\u65f6\u673a\u5236\u4ee5\u7406\u89e3\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u8fd9\u662f\u63a2\u7d22DRL\u6d8c\u73b0\u884c\u4e3a\u4e0e\u751f\u7269\u7cfb\u7edf\u884c\u4e3a\u76f8\u4f3c\u6027\u7684\u521d\u6b65\u5c1d\u8bd5\u3002"}}
{"id": "2511.01383", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01383", "abs": "https://arxiv.org/abs/2511.01383", "authors": ["Landson Guo", "Andres M. Diaz Aguilar", "William Talbot", "Turcan Tuna", "Marco Hutter", "Cesar Cadena"], "title": "CaRLi-V: Camera-RADAR-LiDAR Point-Wise 3D Velocity Estimation", "comment": null, "summary": "Accurate point-wise velocity estimation in 3D is crucial for robot\ninteraction with non-rigid, dynamic agents, such as humans, enabling robust\nperformance in path planning, collision avoidance, and object manipulation in\ndynamic environments. To this end, this paper proposes a novel RADAR, LiDAR,\nand camera fusion pipeline for point-wise 3D velocity estimation named CaRLi-V.\nThis pipeline leverages raw RADAR measurements to create a novel RADAR\nrepresentation, the velocity cube, which densely represents radial velocities\nwithin the RADAR's field-of-view. By combining the velocity cube for radial\nvelocity extraction, optical flow for tangential velocity estimation, and LiDAR\nfor point-wise range measurements through a closed-form solution, our approach\ncan produce 3D velocity estimates for a dense array of points. Developed as an\nopen-source ROS2 package, CaRLi-V has been field-tested against a custom\ndataset and proven to produce low velocity error metrics relative to ground\ntruth, enabling point-wise velocity estimation for robotic applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCaRLi-V\u7684\u65b0\u578b\u96f7\u8fbe\u3001\u6fc0\u5149\u96f7\u8fbe\u548c\u76f8\u673a\u878d\u5408\u7ba1\u9053\uff0c\u7528\u4e8e\u70b9\u7ea73D\u901f\u5ea6\u4f30\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u901f\u5ea6\u7acb\u65b9\u4f53\u3001\u5149\u6d41\u548c\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u80fd\u591f\u4e3a\u5bc6\u96c6\u70b9\u9635\u751f\u62103D\u901f\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u57283D\u4e2d\u51c6\u786e\u4f30\u8ba1\u70b9\u7ea7\u901f\u5ea6\u5bf9\u4e8e\u673a\u5668\u4eba\u4e0e\u975e\u521a\u6027\u52a8\u6001\u667a\u80fd\u4f53\uff08\u5982\u4eba\u7c7b\uff09\u7684\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u8def\u5f84\u89c4\u5212\u3001\u78b0\u649e\u907f\u514d\u548c\u7269\u4f53\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u5229\u7528\u539f\u59cb\u96f7\u8fbe\u6d4b\u91cf\u521b\u5efa\u901f\u5ea6\u7acb\u65b9\u4f53\u8868\u793a\u5f84\u5411\u901f\u5ea6\uff0c\u7ed3\u5408\u5149\u6d41\u4f30\u8ba1\u5207\u5411\u901f\u5ea6\uff0c\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u83b7\u53d6\u70b9\u7ea7\u8ddd\u79bb\u6d4b\u91cf\uff0c\u91c7\u7528\u95ed\u5f0f\u89e3\u65b9\u6cd5\u878d\u5408\u8fd9\u4e9b\u6570\u636e\u6e90\u3002", "result": "CaRLi-V\u5df2\u4f5c\u4e3a\u5f00\u6e90ROS2\u5305\u5f00\u53d1\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u73b0\u573a\u6d4b\u8bd5\uff0c\u76f8\u5bf9\u4e8e\u5730\u9762\u771f\u5b9e\u503c\u4ea7\u751f\u4e86\u8f83\u4f4e\u7684\u901f\u5ea6\u8bef\u5dee\u6307\u6807\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u5b9e\u73b0\u70b9\u7ea7\u901f\u5ea6\u4f30\u8ba1\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u51c6\u786e\u7684\u901f\u5ea6\u4fe1\u606f\u3002"}}
{"id": "2511.01090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u7279\u5f81\u548c\u8986\u76d6\u8303\u56f4\uff0c\u5e76\u4e0e\u82f1\u8bed\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u5728\u591a\u4efb\u52a1\u6a21\u578b\u4e0a\u8fdb\u884c\u8f7b\u91cf\u7ea7\u8bad\u7ec3\uff0c\u4f7f\u7528LLM\u6807\u6ce8\u7684\u7f57\u9a6c\u5c3c\u4e9a\u6587\u672c\u8fdb\u884c\u591a\u7ea7\u8fc7\u6ee4\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u663e\u793a\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u548c\u82f1\u8bed\u6570\u636e\u5728\u4e3b\u9898\u5206\u5e03\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u6570\u636e\u8fc7\u6ee4\u80fd\u6709\u6548\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u5173\u952e\u56e0\u7d20\u4e4b\u4e00\u662f\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u7b5b\u9009\u3002\u5bf9\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u5982\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff0c\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5176\u9884\u8bad\u7ec3\u8bed\u6599\u7279\u5f81\u5e76\u5f00\u53d1\u6709\u6548\u7684\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u3002", "method": "\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u4f7f\u7528LLM\u6807\u6ce8\u7684\u7f57\u9a6c\u5c3c\u4e9a\u6587\u672c\u8fdb\u884c\u591a\u7ea7\u8fc7\u6ee4\uff08\u5982\u6559\u80b2\u4ef7\u503c\u3001\u4e3b\u9898\u3001\u683c\u5f0f\uff09\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u548c\u82f1\u8bed\u6570\u636e\u5728\u4e3b\u9898\u5206\u5e03\u4e0a\u5b58\u5728\u663e\u8457\u8d8b\u52bf\u5dee\u5f02\u3002\u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2dLLM\u9884\u8bad\u7ec3\u6027\u80fd\u5f97\u5230\u6539\u5584\uff0c\u8bc1\u660e\u4e86\u8fc7\u6ee4\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u7684\u6570\u636e\u7279\u5f81\u5206\u6790\u548c\u591a\u7ea7\u8fc7\u6ee4\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u5c24\u5176\u5bf9\u4e8e\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584LLM\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00391", "abs": "https://arxiv.org/abs/2511.00391", "authors": ["Xuanle Zhao", "Deyang Jiang", "Zhixiong Zeng", "Lei Chen", "Haibo Qiu", "Jing Huang", "Yufeng Zhong", "Liming Zheng", "Yilin Cao", "Lin Ma"], "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning", "comment": "Preprint Version, Work in Progress", "summary": "Multimodal code generation has garnered significant interest within the\nresearch community. Despite the notable success of recent vision-language\nmodels (VLMs) on specialized tasks like Chart-to-code generation, their\nreliance on single-task training regimens fosters a narrow paradigm that\nhinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode\n\\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a\nunified multimodal code generation model that addresses this limitation via a\ntwo-stage training framework. We begin by constructing a large-scale Supervised\nFinetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving\ndirect code generation and visual-based code refinement. Subsequently, we\nintroduce a Visual Reinforcement Learning (ViRL) strategy, which employs a\ncoarse-to-fine reward mechanism to improve visual fidelity by calculating\nvisual similarity across local and global image patches. Extensive experiments\non various multimodal code generation benchmarks demonstrate that VinciCoder\nachieves state-of-the-art performance, underscoring the effectiveness of our\ncoarse-to-fine ViRL strategy. The code and model will be available at\nhttps://github.com/DocTron-hub/VinciCoder.", "AI": {"tldr": "VinciCoder\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4f9d\u8d56\u5355\u4efb\u52a1\u8bad\u7ec3\uff0c\u5f62\u6210\u4e86\u72ed\u9698\u7684\u8303\u5f0f\uff0c\u963b\u788d\u4e86\u901a\u7528\u89c6\u89c9\u4ee3\u7801\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u9996\u5148\u6784\u5efa\u5305\u542b160\u4e07\u56fe\u50cf-\u4ee3\u7801\u5bf9\u7684\u76d1\u7763\u5fae\u8c03\u8bed\u6599\u5e93\uff0c\u7136\u540e\u5f15\u5165\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u5956\u52b1\u673a\u5236\u8ba1\u7b97\u5c40\u90e8\u548c\u5168\u5c40\u56fe\u50cf\u5757\u7684\u89c6\u89c9\u76f8\u4f3c\u5ea6\u3002", "result": "\u5728\u5404\u79cd\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVinciCoder\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "VinciCoder\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u548c\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u901a\u7528\u89c6\u89c9\u4ee3\u7801\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01407", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01407", "abs": "https://arxiv.org/abs/2511.01407", "authors": ["Paolo Rabino", "Gabriele Tiboni", "Tatiana Tommasi"], "title": "FoldPath: End-to-End Object-Centric Motion Generation via Modulated Implicit Paths", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Object-Centric Motion Generation (OCMG) is instrumental in advancing\nautomated manufacturing processes, particularly in domains requiring\nhigh-precision expert robotic motions, such as spray painting and welding. To\nrealize effective automation, robust algorithms are essential for generating\nextended, object-aware trajectories across intricate 3D geometries. However,\ncontemporary OCMG techniques are either based on ad-hoc heuristics or employ\nlearning-based pipelines that are still reliant on sensitive post-processing\nsteps to generate executable paths. We introduce FoldPath, a novel, end-to-end,\nneural field based method for OCMG. Unlike prior deep learning approaches that\npredict discrete sequences of end-effector waypoints, FoldPath learns the robot\nmotion as a continuous function, thus implicitly encoding smooth output paths.\nThis paradigm shift eliminates the need for brittle post-processing steps that\nconcatenate and order the predicted discrete waypoints. Particularly, our\napproach demonstrates superior predictive performance compared to recently\nproposed learning-based methods, and attains generalization capabilities even\nin real industrial settings, where only a limited amount of 70 expert samples\nare provided. We validate FoldPath through comprehensive experiments in a\nrealistic simulation environment and introduce new, rigorous metrics designed\nto comprehensively evaluate long-horizon robotic paths, thus advancing the OCMG\ntask towards practical maturity.", "AI": {"tldr": "FoldPath\u662f\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u7684\u7aef\u5230\u7aef\u5bf9\u8c61\u4e2d\u5fc3\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\u6765\u751f\u6210\u5e73\u6ed1\u8def\u5f84\uff0c\u65e0\u9700\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u4ec5\u9700\u5c11\u91cf\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u826f\u597d\u6cdb\u5316\u3002", "motivation": "\u5f53\u524dOCMG\u6280\u672f\u8981\u4e48\u57fa\u4e8e\u4e34\u65f6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8981\u4e48\u4f9d\u8d56\u5b66\u4e60\u4f46\u9700\u8981\u654f\u611f\u7684\u540e\u5904\u7406\u6b65\u9aa4\u6765\u751f\u6210\u53ef\u6267\u884c\u8def\u5f84\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u7b97\u6cd5\u6765\u751f\u6210\u590d\u67423D\u51e0\u4f55\u4e0a\u7684\u5bf9\u8c61\u611f\u77e5\u8f68\u8ff9\u3002", "method": "\u63d0\u51faFoldPath\u65b9\u6cd5\uff0c\u57fa\u4e8e\u795e\u7ecf\u573a\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u4f5c\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u9690\u5f0f\u7f16\u7801\u5e73\u6ed1\u8f93\u51fa\u8def\u5f84\uff0c\u800c\u975e\u9884\u6d4b\u79bb\u6563\u7684\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u70b9\u5e8f\u5217\u3002", "result": "\u76f8\u6bd4\u6700\u8fd1\u63d0\u51fa\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0cFoldPath\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u4ec5\u970070\u4e2a\u4e13\u5bb6\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u771f\u5b9e\u4eff\u771f\u73af\u5883\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "FoldPath\u901a\u8fc7\u5c06\u673a\u5668\u4eba\u8fd0\u52a8\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u6d88\u9664\u4e86\u8106\u5f31\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u63a8\u8fdb\u4e86OCMG\u4efb\u52a1\u5411\u5b9e\u9645\u5e94\u7528\u7684\u6210\u719f\u53d1\u5c55\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u4e25\u683c\u6307\u6807\u6765\u5168\u9762\u8bc4\u4f30\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u8def\u5f84\u3002"}}
{"id": "2511.01101", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications.", "AI": {"tldr": "TSVer\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u65f6\u95f4\u5e8f\u5217\u8bc1\u636e\u8fdb\u884c\u65f6\u5e8f\u548c\u6570\u503c\u63a8\u7406\u7684\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b287\u4e2a\u771f\u5b9e\u4e16\u754c\u58f0\u660e\u548c400\u4e2a\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u591a\u6b65\u9aa4\u6807\u6ce8\u8fc7\u7a0b\u63d0\u9ad8\u6807\u6ce8\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5728\u8bc4\u4f30\u65f6\u5e8f\u548c\u6570\u503c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d7\u5230\u6570\u636e\u96c6\u9650\u5236\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u5f80\u5f80\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bc1\u636e\u3001\u5bf9\u88c1\u51b3\u63d0\u4f9b\u4e0d\u8db3\u7684\u8bba\u8bc1\u6216\u4f9d\u8d56\u5408\u6210\u58f0\u660e\u3002", "method": "\u4ece38\u4e2a\u4e8b\u5b9e\u6838\u67e5\u7ec4\u7ec7\u6536\u96c6287\u4e2a\u771f\u5b9e\u4e16\u754c\u58f0\u660e\uff0c\u6784\u5efa\u5305\u542b400\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u6570\u636e\u5e93\uff0c\u91c7\u7528LLM\u8f85\u52a9\u7684\u591a\u6b65\u9aa4\u6807\u6ce8\u8fc7\u7a0b\uff0c\u4e3a\u6bcf\u4e2a\u58f0\u660e\u6807\u6ce8\u65f6\u95f4\u6846\u67b6\u3001\u88c1\u51b3\u548c\u8bc1\u636e\u4f7f\u7528\u8bba\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e86kappa=0.745\u7684\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\u5373\u4f7f\u662fGemini-2.5-Pro\u7b49\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9a8c\u8bc1\u4e0a\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u88c1\u51b3\u51c6\u786e\u7387\u4e3a63.37%\uff0c\u8bba\u8bc1Ev2R\u5f97\u5206\u4e3a48.63%\u3002", "conclusion": "TSVer\u6570\u636e\u96c6\u586b\u8865\u4e86\u65f6\u5e8f\u548c\u6570\u503c\u63a8\u7406\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u8bc1\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2511.00396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00396", "abs": "https://arxiv.org/abs/2511.00396", "authors": ["Long Li", "Shuichen Ji", "Ziyang Luo", "Nian Liu", "Dingwen Zhang", "Junwei Han"], "title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks", "comment": "14 pages,10 figures", "summary": "We present the first unified framework that jointly handles three\noperationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting\neach as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model\n(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT\nquality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a\nlightweight single-sample algorithm that leverages the discrepancy between\nreward and model confidence as a per-sample advantage signal. This design\nnaturally focuses updates on informative responses while eliminating group\nsampling, thereby addressing GRPO's key limitations: confidence-agnostic\nlearning, signal dilution, and prohibitive computational overhead. We also\nintroduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data\nthat ensures logical consistency with ground-truth masks. Experiments show our\nmodel matches or outperforms specialized SOTA methods and strong closed-source\nVLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for\nCoSOD, surpassing the prior best by 8.0 percentage points, despite using far\nless training data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06SOD\u3001CoSOD\u548cSIS\u4e09\u4e2a\u5f02\u6784\u663e\u8457\u6027\u4efb\u52a1\u8f6c\u5316\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u5f02\u8d28\u6027\u95ee\u9898\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u6765\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e3a\u6bcf\u4e2a\u663e\u8457\u6027\u4efb\u52a1\u8bbe\u8ba1\u4e13\u95e8\u6a21\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u5904\u7406\u5f02\u6784\u4efb\u52a1\u7684\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u8fc7\u7a0b\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7edf\u4e00\u5904\u7406\u4e09\u4e2a\u64cd\u4f5c\u4e0a\u5f02\u6784\u7684\u663e\u8457\u6027\u4efb\u52a1\u3002", "method": "1. \u5c06\u4e09\u4e2a\u663e\u8457\u6027\u4efb\u52a1\u8f6c\u5316\u4e3a\u601d\u7ef4\u94fe\u63a8\u7406\u8fc7\u7a0b\uff1b2. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff1b3. \u63d0\u51fa\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7b56\u7565\u4f18\u5316\uff08CGPO\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u5956\u52b1\u4e0e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u4f5c\u4e3a\u4f18\u52bf\u4fe1\u53f7\uff1b4. \u5f15\u5165\"\u8f93\u51fa\u5230\u63a8\u7406\"\u7b56\u7565\u6784\u5efa\u9ad8\u8d28\u91cfSFT\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u4f18\u4e8e\u4e13\u95e8\u7684SOTA\u65b9\u6cd5\u548c\u5f3a\u95ed\u6e90VLM\uff0c\u7279\u522b\u662f\u5728CoSOD\u4efb\u52a1\u4e0a\uff0cCoCA\u7684S-measure\u8fbe\u52300.899\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53478.0\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u6709\u6548\u5904\u7406\u5f02\u6784\u663e\u8457\u6027\u4efb\u52a1\uff0cCGPO\u7b97\u6cd5\u89e3\u51b3\u4e86GRPO\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2511.01444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01444", "abs": "https://arxiv.org/abs/2511.01444", "authors": ["Huiting Huang", "Tieliang Gong", "Kai He", "Jialun Wu", "Erik Cambria", "Mengling Feng"], "title": "Robust Multimodal Sentiment Analysis via Double Information Bottleneck", "comment": null, "summary": "Multimodal sentiment analysis has received significant attention across\ndiverse research domains. Despite advancements in algorithm design, existing\napproaches suffer from two critical limitations: insufficient learning of\nnoise-contaminated unimodal data, leading to corrupted cross-modal\ninteractions, and inadequate fusion of multimodal representations, resulting in\ndiscarding discriminative unimodal information while retaining multimodal\nredundant information. To address these challenges, this paper proposes a\nDouble Information Bottleneck (DIB) strategy to obtain a powerful, unified\ncompact multimodal representation. Implemented within the framework of low-rank\nRenyi's entropy functional, DIB offers enhanced robustness against diverse\nnoise sources and computational tractability for high-dimensional data, as\ncompared to the conventional Shannon entropy-based methods. The DIB comprises\ntwo key modules: 1) learning a sufficient and compressed representation of\nindividual unimodal data by maximizing the task-relevant information and\ndiscarding the superfluous information, and 2) ensuring the discriminative\nability of multimodal representation through a novel attention bottleneck\nfusion mechanism. Consequently, DIB yields a multimodal representation that\neffectively filters out noisy information from unimodal data while capturing\ninter-modal complementarity. Extensive experiments on CMU-MOSI, CMU-MOSEI,\nCH-SIMS, and MVSA-Single validate the effectiveness of our method. The model\nachieves 47.4% accuracy under the Acc-7 metric on CMU-MOSI and 81.63% F1-score\non CH-SIMS, outperforming the second-best baseline by 1.19%. Under noise, it\nshows only 0.36% and 0.29% performance degradation on CMU-MOSI and CMU-MOSEI\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u4fe1\u606f\u74f6\u9888\uff08DIB\uff09\u7b56\u7565\u6765\u89e3\u51b3\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u566a\u58f0\u6c61\u67d3\u7684\u5355\u6a21\u6001\u6570\u636e\u5b66\u4e60\u4e0d\u8db3\u548c\u8de8\u6a21\u6001\u8868\u793a\u878d\u5408\u4e0d\u5145\u5206\u3002DIB\u901a\u8fc7\u6700\u5927\u5316\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u4e22\u5f03\u5197\u4f59\u4fe1\u606f\uff0c\u751f\u6210\u7edf\u4e00\u7d27\u51d1\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u5bf9\u566a\u58f0\u6c61\u67d3\u7684\u5355\u6a21\u6001\u6570\u636e\u5b66\u4e60\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u4ea4\u4e92\u53d7\u635f\uff1b\u591a\u6a21\u6001\u8868\u793a\u878d\u5408\u4e0d\u5145\u5206\uff0c\u5bfc\u81f4\u4e22\u5f03\u5224\u522b\u6027\u5355\u6a21\u6001\u4fe1\u606f\u800c\u4fdd\u7559\u5197\u4f59\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u53cc\u4fe1\u606f\u74f6\u9888\uff08DIB\uff09\u7b56\u7565\uff0c\u5728\u4f4e\u79e9Renyi\u71b5\u51fd\u6570\u6846\u67b6\u4e0b\u5b9e\u73b0\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1\uff09\u901a\u8fc7\u6700\u5927\u5316\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u4e22\u5f03\u5197\u4f59\u4fe1\u606f\uff0c\u5b66\u4e60\u5145\u5206\u538b\u7f29\u7684\u5355\u6a21\u6001\u6570\u636e\u8868\u793a\uff1b2\uff09\u901a\u8fc7\u65b0\u9896\u7684\u6ce8\u610f\u529b\u74f6\u9888\u878d\u5408\u673a\u5236\u786e\u4fdd\u591a\u6a21\u6001\u8868\u793a\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728CMU-MOSI\u3001CMU-MOSEI\u3001CH-SIMS\u548cMVSA-Single\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5728CMU-MOSI\u4e0a\u8fbe\u523047.4%\u7684Acc-7\u51c6\u786e\u7387\uff0c\u5728CH-SIMS\u4e0a\u8fbe\u523081.63%\u7684F1\u5206\u6570\uff0c\u6bd4\u6b21\u4f18\u57fa\u7ebf\u63d0\u9ad81.19%\u3002\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\uff0cCMU-MOSI\u548cCMU-MOSEI\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u4ec5\u4e3a0.36%\u548c0.29%\u3002", "conclusion": "DIB\u7b56\u7565\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u5355\u6a21\u6001\u6570\u636e\u4e2d\u7684\u566a\u58f0\u4fe1\u606f\uff0c\u540c\u65f6\u6355\u6349\u6a21\u6001\u95f4\u7684\u4e92\u8865\u6027\uff0c\u751f\u6210\u5f3a\u5927\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u9999\u519c\u71b5\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2511.01166", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MicroRemed\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7aef\u5230\u7aef\u5fae\u670d\u52a1\u4fee\u590d\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86ThinkRemed\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u6a21\u62dfSRE\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u7684\u63d0\u793a\uff0cLLM\u4ec5\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u3002\u5fae\u670d\u52a1\u4fee\u590d\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u65b9\u5411\u3002", "method": "\u5f15\u5165MicroRemed\u57fa\u51c6\u8bc4\u4f30LLM\u751f\u6210\u53ef\u6267\u884cAnsible\u5267\u672c\u7684\u80fd\u529b\uff1b\u63d0\u51faThinkRemed\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6a21\u62dfSRE\u7684\u53cd\u601d\u548c\u611f\u77e5\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMicroRemed\u5bf9\u5f53\u524dLLM\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u800cThinkRemed\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u7cfb\u7edf\u53cd\u601d\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u4fee\u590d\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5728\u5fae\u670d\u52a1\u4fee\u590d\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9996\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u6709\u6548\u7684\u591a\u667a\u80fd\u4f53\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.00419", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00419", "abs": "https://arxiv.org/abs/2511.00419", "authors": ["Thanh Hieu Cao", "Trung Khang Tran", "Gia Thinh Pham", "Tuong Nghiem Diep", "Thanh Binh Nguyen"], "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion", "comment": "15 pages, 5 figures, to appear in SoICT 2025", "summary": "Recent advancements in large-scale pretraining in natural language processing\nhave enabled pretrained vision-language models such as CLIP to effectively\nalign images and text, significantly improving performance in zero-shot image\nclassification tasks. Subsequent studies have further demonstrated that\ncropping images into smaller regions and using large language models to\ngenerate multiple descriptions for each caption can further enhance model\nperformance. However, due to the inherent sensitivity of CLIP, random image\ncrops can introduce misinformation and bias, as many images share similar\nfeatures at small scales. To address this issue, we propose\nLocalized-Globalized Cross-Alignment (LGCA), a framework that first captures\nthe local features of an image and then repeatedly selects the most salient\nregions and expands them. The similarity score is designed to incorporate both\nthe original and expanded images, enabling the model to capture both local and\nglobal features while minimizing misinformation. Additionally, we provide a\ntheoretical analysis demonstrating that the time complexity of LGCA remains the\nsame as that of the original model prior to the repeated expansion process,\nhighlighting its efficiency and scalability. Extensive experiments demonstrate\nthat our method substantially improves zero-shot performance across diverse\ndatasets, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faLGCA\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u6355\u83b7\u548c\u663e\u8457\u533a\u57df\u6269\u5c55\u6765\u89e3\u51b3CLIP\u6a21\u578b\u5728\u56fe\u50cf\u88c1\u526a\u65f6\u5f15\u5165\u9519\u8bef\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u63d0\u5347\u96f6\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "CLIP\u7b49\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u968f\u673a\u56fe\u50cf\u88c1\u526a\u4f1a\u5f15\u5165\u9519\u8bef\u4fe1\u606f\u548c\u504f\u89c1\uff0c\u56e0\u4e3a\u5c0f\u5c3a\u5ea6\u4e0b\u8bb8\u591a\u56fe\u50cf\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u5c40\u90e8-\u5168\u5c40\u4ea4\u53c9\u5bf9\u9f50\uff08LGCA\uff09\u6846\u67b6\uff1a\u5148\u6355\u83b7\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\uff0c\u7136\u540e\u91cd\u590d\u9009\u62e9\u6700\u663e\u8457\u533a\u57df\u5e76\u6269\u5c55\uff0c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u548c\u6269\u5c55\u56fe\u50cf\uff0c\u540c\u65f6\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LGCA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CLIP\u6a21\u578b\u5728\u56fe\u50cf\u88c1\u526a\u4e2d\u7684\u9519\u8bef\u4fe1\u606f\u95ee\u9898\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e\u539f\u6a21\u578b\u76f8\u540c\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.01472", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01472", "abs": "https://arxiv.org/abs/2511.01472", "authors": ["Sarthak Mishra", "Rishabh Dev Yadav", "Avirup Das", "Saksham Gupta", "Wei Pan", "Spandan Roy"], "title": "AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models", "comment": null, "summary": "The rapid progress of vision--language models (VLMs) has sparked growing\ninterest in robotic control, where natural language can express the operation\ngoals while visual feedback links perception to action. However, directly\ndeploying VLM-driven policies on aerial manipulators remains unsafe and\nunreliable since the generated actions are often inconsistent,\nhallucination-prone, and dynamically infeasible for flight. In this work, we\npresent AERMANI-VLM, the first framework to adapt pretrained VLMs for aerial\nmanipulation by separating high-level reasoning from low-level control, without\nany task-specific fine-tuning. Our framework encodes natural language\ninstructions, task context, and safety constraints into a structured prompt\nthat guides the model to generate a step-by-step reasoning trace in natural\nlanguage. This reasoning output is used to select from a predefined library of\ndiscrete, flight-safe skills, ensuring interpretable and temporally consistent\nexecution. By decoupling symbolic reasoning from physical action, AERMANI-VLM\nmitigates hallucinated commands and prevents unsafe behavior, enabling robust\ntask completion. We validate the framework in both simulation and hardware on\ndiverse multi-step pick-and-place tasks, demonstrating strong generalization to\npreviously unseen commands, objects, and environments.", "AI": {"tldr": "AERMANI-VLM\u662f\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u63a7\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u5c42\u63a8\u7406\u548c\u4f4e\u5c42\u63a7\u5236\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u53ef\u9760\u7684\u64cd\u4f5c\u3002", "motivation": "\u76f4\u63a5\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u7b56\u7565\u5230\u7a7a\u4e2d\u673a\u68b0\u81c2\u4e0a\u5b58\u5728\u4e0d\u5b89\u5168\u548c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u751f\u6210\u7684\u52a8\u4f5c\u5f80\u5f80\u4e0d\u4e00\u81f4\u3001\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4e14\u5bf9\u98de\u884c\u6765\u8bf4\u52a8\u6001\u4e0d\u53ef\u884c\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7f16\u7801\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\uff0c\u7136\u540e\u4ece\u9884\u5b9a\u4e49\u7684\u98de\u884c\u5b89\u5168\u6280\u80fd\u5e93\u4e2d\u9009\u62e9\u79bb\u6563\u6280\u80fd\u3002", "result": "\u5728\u6a21\u62df\u548c\u786c\u4ef6\u4e0a\u7684\u591a\u6837\u5316\u591a\u6b65\u9aa4\u62fe\u653e\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5bf9\u672a\u89c1\u8fc7\u7684\u6307\u4ee4\u3001\u7269\u4f53\u548c\u73af\u5883\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u7b26\u53f7\u63a8\u7406\u548c\u7269\u7406\u52a8\u4f5c\uff0cAERMANI-VLM\u51cf\u8f7b\u4e86\u5e7b\u89c9\u547d\u4ee4\u5e76\u9632\u6b62\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2511.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00427", "abs": "https://arxiv.org/abs/2511.00427", "authors": ["Daichi Zhang", "Tong Zhang", "Jianmin Bao", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "comment": null, "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u5ea6\u7684\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5ITEM\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u68c0\u6d4b\u771f\u5b9e\u56fe\u50cf\u4e0e\u751f\u6210\u56fe\u50cf\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u5bfc\u81f4\u7684\u6cdb\u5316\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u68c0\u6d4b\u751f\u6210\u7684\u865a\u5047\u56fe\u50cf\u4ee5\u9632\u6b62\u6076\u610f\u4f7f\u7528\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bfc\u81f4\u8bad\u7ec3\u51fa\u7684\u68c0\u6d4b\u5668\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u56fe\u50cf\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51faITEM\u68c0\u6d4b\u5668\uff0c\u5229\u7528\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u6d4b\u91cf\u56fe\u50cf\u4e0e\u6807\u9898\u7684\u5bf9\u9f50\u5ea6\u5dee\u5f02\uff0c\u7136\u540e\u8bad\u7ec3MLP\u5934\u90e8\u8fdb\u884c\u5206\u7c7b\u3002\u91c7\u7528\u5206\u5c42\u5bf9\u9f50\u65b9\u6848\uff0c\u5148\u5173\u6ce8\u6574\u5f20\u56fe\u50cf\uff0c\u518d\u5173\u6ce8\u6807\u9898\u4e2d\u63cf\u8ff0\u7684\u6bcf\u4e2a\u8bed\u4e49\u5bf9\u8c61\uff0c\u63a2\u7d22\u5168\u5c40\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u8bed\u4e49\u5bf9\u9f50\u5dee\u5f02\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6700\u65b0\u751f\u6210\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u89c6\u89d2\u5229\u7528\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u5dee\u5f02\u4f5c\u4e3a\u5224\u522b\u7ebf\u7d22\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01527", "abs": "https://arxiv.org/abs/2511.01527", "authors": ["Hanwen Xu", "Xuyao Huang", "Yuzhe Liu", "Kai Yu", "Zhijie Deng"], "title": "TPS-Bench: Evaluating AI Agents' Tool Planning \\& Scheduling Abilities in Compounding Tasks", "comment": null, "summary": "Large language model (LLM) agents have exhibited strong problem-solving\ncompetence across domains like research and coding. Yet, it remains\nunderexplored whether LLM agents can tackle compounding real-world problems\nthat require a diverse set of tools to complete. Given a broad, heterogeneous\ntool repository, LLM agents must not only select appropriate tools based on\ntask planning analysis but also strategically schedule the execution order to\nensure efficiency. This paper introduces TPS-Bench to benchmark the ability of\nLLM agents in solving such problems that demand Tool Planning and Scheduling.\nTPS-Bench collects 200 compounding tasks of two difficulty levels, based on a\ntool repository containing hundreds of model context protocol (MCP) tools. In\nparticular, each task is composed of multiple subtasks, such as web search, map\nnavigation, calendar checking, etc., and each subtask can be completed by a\nbasic tool. Our evaluation emphasizes both task completion rate and efficiency.\nThe empirical studies on popular closed-source and open-source LLMs indicate\nthat most models can perform reasonable tool planning, but differ in\nscheduling. For example, GLM-4.5 achieves an outperforming task completion rate\nof 64.72% with extensive sequential tool calls, hence suffering from\nsignificantly long execution time. By contrast, GPT-4o prioritizes parallel\ntool calls but achieves only a 45.08% completion rate. Considering\nreinforcement learning (RL) can be a viable way to improve the scheduling\nefficiency without compromising performance, we perform an initial study on\nQwen3-1.7B and witness a 14% reduction in execution time alongside a 6% gain in\ntask completion rate based on rarely 100 RL training samples. Our code is\navailable https://github.com/hanwenxu1/mcp-agent.", "AI": {"tldr": "TPS-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u9700\u8981\u5de5\u5177\u89c4\u5212\u548c\u8c03\u5ea6\u7684\u590d\u5408\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b200\u4e2a\u57fa\u4e8e\u6570\u767e\u4e2aMCP\u5de5\u5177\u7684\u590d\u5408\u4efb\u52a1\uff0c\u5f3a\u8c03\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6548\u7387\u3002", "motivation": "\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u662f\u5426\u80fd\u591f\u5904\u7406\u9700\u8981\u591a\u6837\u5316\u5de5\u5177\u7ec4\u5408\u7684\u590d\u5408\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e0d\u4ec5\u9700\u8981\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\uff0c\u8fd8\u9700\u8981\u6218\u7565\u6027\u5730\u8c03\u5ea6\u6267\u884c\u987a\u5e8f\u4ee5\u786e\u4fdd\u6548\u7387\u3002", "method": "\u6784\u5efaTPS-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6536\u96c6200\u4e2a\u57fa\u4e8e\u6570\u767e\u4e2aMCP\u5de5\u5177\u7684\u590d\u5408\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7531\u591a\u4e2a\u5b50\u4efb\u52a1\u7ec4\u6210\uff0c\u8bc4\u4f30\u6d41\u884c\u7684\u95ed\u6e90\u548c\u5f00\u6e90LLM\u5728\u5de5\u5177\u89c4\u5212\u548c\u8c03\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u8c03\u5ea6\u6548\u7387\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u5408\u7406\u7684\u5de5\u5177\u89c4\u5212\uff0c\u4f46\u5728\u8c03\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u540c\u3002GLM-4.5\u8fbe\u523064.72%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u4f46\u6267\u884c\u65f6\u95f4\u957f\uff0cGPT-4o\u4f18\u5148\u5e76\u884c\u5de5\u5177\u8c03\u7528\u4f46\u5b8c\u6210\u7387\u4ec545.08%\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728Qwen3-1.7B\u4e0a\u5b9e\u73b0\u4e8614%\u7684\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u548c6%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u5347\u3002", "conclusion": "LLM\u667a\u80fd\u4f53\u5728\u5de5\u5177\u89c4\u5212\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8c03\u5ea6\u6548\u7387\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u6539\u8fdb\u8c03\u5ea6\u6548\u7387\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u7684\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2511.01476", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01476", "abs": "https://arxiv.org/abs/2511.01476", "authors": ["Cankut Bora Tuncer", "Marc Toussaint", "Ozgur S. Oguz"], "title": "MO-SeGMan: Rearrangement Planning Framework for Multi Objective Sequential and Guided Manipulation in Constrained Environments", "comment": "8 pages, 8 figures, website:https://sites.google.com/view/mo-segman/", "summary": "In this work, we introduce MO-SeGMan, a Multi-Objective Sequential and Guided\nManipulation planner for highly constrained rearrangement problems. MO-SeGMan\ngenerates object placement sequences that minimize both replanning per object\nand robot travel distance while preserving critical dependency structures with\na lazy evaluation method. To address highly cluttered, non-monotone scenarios,\nwe propose a Selective Guided Forward Search (SGFS) that efficiently relocates\nonly critical obstacles and to feasible relocation points. Furthermore, we\nadopt a refinement method for adaptive subgoal selection to eliminate\nunnecessary pick-and-place actions, thereby improving overall solution quality.\nExtensive evaluations on nine benchmark rearrangement tasks demonstrate that\nMO-SeGMan generates feasible motion plans in all cases, consistently achieving\nfaster solution times and superior solution quality compared to the baselines.\nThese results highlight the robustness and scalability of the proposed\nframework for complex rearrangement planning problems.", "AI": {"tldr": "MO-SeGMan\u662f\u4e00\u4e2a\u591a\u76ee\u6807\u987a\u5e8f\u5f15\u5bfc\u64cd\u4f5c\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5ea6\u53d7\u9650\u7684\u91cd\u6392\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u6700\u5c0f\u5316\u91cd\u89c4\u5212\u548c\u673a\u5668\u4eba\u79fb\u52a8\u8ddd\u79bb\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u4f9d\u8d56\u7ed3\u6784\uff0c\u5728\u975e\u5355\u8c03\u573a\u666f\u4e2d\u9ad8\u6548\u91cd\u5b9a\u4f4d\u5173\u952e\u969c\u788d\u7269\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5b50\u76ee\u6807\u9009\u62e9\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u64cd\u4f5c\u52a8\u4f5c\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5ea6\u53d7\u9650\u3001\u975e\u5355\u8c03\u7684\u91cd\u6392\u89c4\u5212\u95ee\u9898\uff0c\u9700\u8981\u5728\u4fdd\u6301\u5173\u952e\u4f9d\u8d56\u7ed3\u6784\u7684\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u76ee\u6807\uff08\u91cd\u89c4\u5212\u6b21\u6570\u548c\u79fb\u52a8\u8ddd\u79bb\uff09\uff0c\u5e76\u5904\u7406\u9ad8\u5ea6\u6742\u4e71\u7684\u573a\u666f\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u987a\u5e8f\u5f15\u5bfc\u64cd\u4f5c\u89c4\u5212\uff0c\u7ed3\u5408\u60f0\u6027\u8bc4\u4f30\u65b9\u6cd5\u4fdd\u6301\u4f9d\u8d56\u7ed3\u6784\uff1b\u63d0\u51fa\u9009\u62e9\u6027\u5f15\u5bfc\u524d\u5411\u641c\u7d22\uff08SGFS\uff09\u9ad8\u6548\u91cd\u5b9a\u4f4d\u5173\u952e\u969c\u788d\u7269\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u5b50\u76ee\u6807\u9009\u62e9\u7ec6\u5316\u65b9\u6cd5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u62fe\u653e\u52a8\u4f5c\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u91cd\u6392\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cMO-SeGMan\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u80fd\u751f\u6210\u53ef\u884c\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u5feb\u7684\u6c42\u89e3\u65f6\u95f4\u548c\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "MO-SeGMan\u6846\u67b6\u5728\u590d\u6742\u91cd\u6392\u89c4\u5212\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u5ea6\u53d7\u9650\u7684\u573a\u666f\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01187", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment.", "AI": {"tldr": "DebateBias-8K\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u8fa9\u8bba\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f0f\u573a\u666f\u4e2d\u7684\u53d9\u4e8b\u504f\u89c1\uff0c\u6db5\u76d64\u4e2a\u654f\u611f\u9886\u57df\u548c7\u79cd\u8bed\u8a00\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u523b\u677f\u5370\u8c61\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\u5206\u7c7b\u4efb\u52a1\uff0c\u65e0\u6cd5\u63ed\u793a\u771f\u5b9e\u751f\u6210\u573a\u666f\u4e2d\u7684\u53d9\u4e8b\u504f\u89c1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8d34\u8fd1\u5b9e\u9645\u5e94\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u5305\u542b8,400\u4e2a\u7ed3\u6784\u5316\u8fa9\u8bba\u63d0\u793a\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d64\u4e2a\u654f\u611f\u9886\u57df\u548c7\u79cd\u8bed\u8a00\uff0c\u4f7f\u75284\u4e2a\u4e3b\u6d41\u6a21\u578b\u751f\u6210\u8d85\u8fc710\u4e07\u6761\u54cd\u5e94\u5e76\u8fdb\u884c\u81ea\u52a8\u5206\u7c7b\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u91cd\u73b0\u4e86\u6839\u6df1\u8482\u56fa\u7684\u523b\u677f\u5370\u8c61\uff1a\u963f\u62c9\u4f2f\u4eba\u4e0e\u6050\u6016\u4e3b\u4e49\u548c\u5b97\u6559\u9ad8\u5ea6\u5173\u8054(\u226595%)\uff0c\u975e\u6d32\u4eba\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u201c\u843d\u540e\u201d\u5173\u8054(\u9ad8\u8fbe77%)\uff0c\u897f\u65b9\u7fa4\u4f53\u88ab\u4e00\u81f4\u63cf\u8ff0\u4e3a\u73b0\u4ee3\u6216\u8fdb\u6b65\u3002\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u504f\u89c1\u66f4\u52a0\u660e\u663e\u3002", "conclusion": "\u5f53\u524d\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u7684\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u5728\u5168\u7403\u8303\u56f4\u5185\u6cdb\u5316\uff0c\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u663e\u6027\u6bd2\u6027\uff0c\u4f46\u65e0\u6cd5\u9632\u6b62\u5f00\u653e\u4e0a\u4e0b\u6587\u4e2d\u7684\u504f\u89c1\u8f93\u51fa\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u6587\u5316\u5305\u5bb9\u7684\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2511.00429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00429", "abs": "https://arxiv.org/abs/2511.00429", "authors": ["Daichi Zhang", "Tong Zhang", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection", "comment": null, "summary": "Diffusion models have achieved remarkable success in image synthesis, but the\ngenerated high-quality images raise concerns about potential malicious use.\nExisting detectors often struggle to capture discriminative clues across\ndifferent models and settings, limiting their generalization to unseen\ndiffusion models and robustness to various perturbations. To address this\nissue, we observe that diffusion-generated images exhibit progressively larger\ndifferences from natural real images across low- to high-frequency bands. Based\non this insight, we propose a simple yet effective representation by enhancing\nthe Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we\nintroduce a frequency-selective function which serves as a weighted filter to\nthe Fourier spectrum, suppressing less discriminative bands while enhancing\nmore informative ones. This approach, grounded in a comprehensive analysis of\nfrequency-based differences between natural real and diffusion-generated\nimages, enables general detection of images from unseen diffusion models and\nprovides robust resilience to various perturbations. Extensive experiments on\nvarious diffusion-generated image datasets demonstrate that our method\noutperforms state-of-the-art detectors with superior generalization and\nrobustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u4f2a\u9020\u7ebf\u7d22\uff08F^2C\uff09\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u6240\u6709\u9891\u6bb5\u7684\u9891\u7387\u5dee\u5f02\u7279\u5f81\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u5f88\u9ad8\uff0c\u4f46\u53ef\u80fd\u88ab\u6076\u610f\u4f7f\u7528\u3002\u73b0\u6709\u68c0\u6d4b\u5668\u96be\u4ee5\u6355\u6349\u4e0d\u540c\u6a21\u578b\u548c\u8bbe\u7f6e\u4e0b\u7684\u5224\u522b\u6027\u7ebf\u7d22\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u672a\u89c1\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u5404\u79cd\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "method": "\u89c2\u5bdf\u5230\u6269\u6563\u751f\u6210\u56fe\u50cf\u4e0e\u81ea\u7136\u771f\u5b9e\u56fe\u50cf\u5728\u4f4e\u9891\u5230\u9ad8\u9891\u6ce2\u6bb5\u5b58\u5728\u6e10\u8fdb\u6027\u5dee\u5f02\uff0c\u63d0\u51fa\u9891\u7387\u9009\u62e9\u6027\u51fd\u6570\u4f5c\u4e3a\u52a0\u6743\u6ee4\u6ce2\u5668\u6765\u6291\u5236\u5224\u522b\u6027\u8f83\u5f31\u7684\u9891\u6bb5\u5e76\u589e\u5f3a\u4fe1\u606f\u91cf\u66f4\u5927\u7684\u9891\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u6269\u6563\u751f\u6210\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u5668\u3002", "conclusion": "\u57fa\u4e8e\u9891\u7387\u4f2a\u9020\u7ebf\u7d22\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6765\u81ea\u672a\u89c1\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\uff0c\u5e76\u5bf9\u5404\u79cd\u6270\u52a8\u5177\u6709\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.01550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01550", "abs": "https://arxiv.org/abs/2511.01550", "authors": ["Ujjwal Sharma", "Stevan Rudinac", "Ana Mi\u0107kovi\u0107", "Willemijn van Dolen", "Marcel Worring"], "title": "Analyzing Sustainability Messaging in Large-Scale Corporate Social Media", "comment": null, "summary": "In this work, we introduce a multimodal analysis pipeline that leverages\nlarge foundation models in vision and language to analyze corporate social\nmedia content, with a focus on sustainability-related communication. Addressing\nthe challenges of evolving, multimodal, and often ambiguous corporate messaging\non platforms such as X (formerly Twitter), we employ an ensemble of large\nlanguage models (LLMs) to annotate a large corpus of corporate tweets on their\ntopical alignment with the 17 Sustainable Development Goals (SDGs). This\napproach avoids the need for costly, task-specific annotations and explores the\npotential of such models as ad-hoc annotators for social media data that can\nefficiently capture both explicit and implicit references to sustainability\nthemes in a scalable manner. Complementing this textual analysis, we utilize\nvision-language models (VLMs), within a visual understanding framework that\nuses semantic clusters to uncover patterns in visual sustainability\ncommunication. This integrated approach reveals sectoral differences in SDG\nengagement, temporal trends, and associations between corporate messaging,\nenvironmental, social, governance (ESG) risks, and consumer engagement. Our\nmethods-automatic label generation and semantic visual clustering-are broadly\napplicable to other domains and offer a flexible framework for large-scale\nsocial media analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5206\u6790\u7ba1\u9053\uff0c\u5229\u7528\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5206\u6790\u4f01\u4e1a\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u6301\u7eed\u6027\u76f8\u5173\u6c9f\u901a\u3002\u901a\u8fc7LLM\u96c6\u6210\u6807\u6ce8\u4f01\u4e1a\u63a8\u6587\u4e0e17\u4e2a\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u7684\u4e3b\u9898\u5bf9\u9f50\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u53ef\u6301\u7eed\u6027\u6c9f\u901a\u6a21\u5f0f\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5728\u6f14\u5316\u3001\u591a\u6a21\u6001\u548c\u6a21\u7cca\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u4e34\u65f6\u6807\u6ce8\u5668\u7684\u6f5c\u529b\uff0c\u907f\u514d\u6602\u8d35\u7684\u4efb\u52a1\u7279\u5b9a\u6807\u6ce8\u9700\u6c42\u3002", "method": "\u4f7f\u7528LLM\u96c6\u6210\u6807\u6ce8\u4f01\u4e1a\u63a8\u6587\u4e0eSDG\u5bf9\u9f50\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u4e49\u805a\u7c7b\u8fdb\u884c\u89c6\u89c9\u53ef\u6301\u7eed\u6027\u6c9f\u901a\u6a21\u5f0f\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u884c\u4e1a\u5728SDG\u53c2\u4e0e\u5ea6\u3001\u65f6\u95f4\u8d8b\u52bf\u4ee5\u53ca\u4f01\u4e1a\u4fe1\u606f\u4e0eESG\u98ce\u9669\u3001\u6d88\u8d39\u8005\u53c2\u4e0e\u5ea6\u4e4b\u95f4\u7684\u5173\u8054\u6027\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u6807\u7b7e\u751f\u6210\u548c\u8bed\u4e49\u89c6\u89c9\u805a\u7c7b\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u5206\u6790\u63d0\u4f9b\u4e86\u7075\u6d3b\u6846\u67b6\u3002"}}
{"id": "2511.01493", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01493", "abs": "https://arxiv.org/abs/2511.01493", "authors": ["Wei Huang", "Jiaxin Li", "Zang Wan", "Huijun Di", "Wei Liang", "Zhu Yang"], "title": "Floor Plan-Guided Visual Navigation Incorporating Depth and Directional Cues", "comment": null, "summary": "Guiding an agent to a specific target in indoor environments based solely on\nRGB inputs and a floor plan is a promising yet challenging problem. Although\nexisting methods have made significant progress, two challenges remain\nunresolved. First, the modality gap between egocentric RGB observations and the\nfloor plan hinders the integration of visual and spatial information for both\nlocal obstacle avoidance and global planning. Second, accurate localization is\ncritical for navigation performance, but remains challenging at deployment in\nunseen environments due to the lack of explicit geometric alignment between RGB\ninputs and floor plans. We propose a novel diffusion-based policy, denoted as\nGlocDiff, which integrates global path planning from the floor plan with local\ndepth-aware features derived from RGB observations. The floor plan offers\nexplicit global guidance, while the depth features provide implicit geometric\ncues, collectively enabling precise prediction of optimal navigation directions\nand robust obstacle avoidance. Moreover, GlocDiff introduces noise perturbation\nduring training to enhance robustness against pose estimation errors, and we\nfind that combining this with a relatively stable VO module during inference\nresults in significantly improved navigation performance. Extensive experiments\non the FloNa benchmark demonstrate GlocDiff's efficiency and effectiveness in\nachieving superior navigation performance, and the success of real-world\ndeployments also highlights its potential for widespread practical\napplications.", "AI": {"tldr": "\u63d0\u51faGlocDiff\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u697c\u5c42\u5e73\u9762\u56fe\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cRGB\u89c2\u6d4b\u7684\u5c40\u90e8\u6df1\u5ea6\u7279\u5f81\uff0c\u89e3\u51b3\u5ba4\u5185\u5bfc\u822a\u4e2d\u6a21\u6001\u5dee\u5f02\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u5bfc\u822a\u4e2d\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1aRGB\u89c2\u6d4b\u4e0e\u697c\u5c42\u5e73\u9762\u56fe\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u963b\u788d\u4e86\u89c6\u89c9\u548c\u7a7a\u95f4\u4fe1\u606f\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7531\u4e8e\u7f3a\u4e4fRGB\u8f93\u5165\u4e0e\u5e73\u9762\u56fe\u7684\u663e\u5f0f\u51e0\u4f55\u5bf9\u9f50\u800c\u5bfc\u81f4\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faGlocDiff\u6269\u6563\u7b56\u7565\uff0c\u96c6\u6210\u6765\u81ea\u697c\u5c42\u5e73\u9762\u56fe\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u6765\u81eaRGB\u89c2\u6d4b\u7684\u5c40\u90e8\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u3002\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u566a\u58f0\u6270\u52a8\u589e\u5f3a\u5bf9\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u5728\u63a8\u7406\u65f6\u7ed3\u5408\u76f8\u5bf9\u7a33\u5b9a\u7684VO\u6a21\u5757\u3002", "result": "\u5728FloNa\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660eGlocDiff\u5728\u5b9e\u73b0\u4f18\u8d8a\u5bfc\u822a\u6027\u80fd\u65b9\u9762\u7684\u6548\u7387\u548c\u6709\u6548\u6027\uff0c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6210\u529f\u4e5f\u7a81\u663e\u5176\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "GlocDiff\u901a\u8fc7\u878d\u5408\u5168\u5c40\u5e73\u9762\u56fe\u6307\u5bfc\u548c\u5c40\u90e8\u6df1\u5ea6\u7279\u5f81\uff0c\u80fd\u591f\u7cbe\u786e\u9884\u6d4b\u6700\u4f18\u5bfc\u822a\u65b9\u5411\u5e76\u5b9e\u73b0\u9c81\u68d2\u7684\u969c\u788d\u7269\u907f\u8ba9\uff0c\u5728\u5ba4\u5185\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.01520", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01520", "abs": "https://arxiv.org/abs/2511.01520", "authors": ["Shipeng Lyu", "Lijie Sheng", "Fangyuan Wang", "Wenyao Zhang", "Weiwei Lin", "Zhenzhong Jia", "David Navarro-Alarcon", "Guodong Guo"], "title": "Phy-Tac: Toward Human-Like Grasping via Physics-Conditioned Tactile Goals", "comment": "9 papges, 10 figures, 3 tables", "summary": "Humans naturally grasp objects with minimal level required force for\nstability, whereas robots often rely on rigid, over-squeezing control. To\nnarrow this gap, we propose a human-inspired physics-conditioned tactile method\n(Phy-Tac) for force-optimal stable grasping (FOSG) that unifies pose selection,\ntactile prediction, and force regulation. A physics-based pose selector first\nidentifies feasible contact regions with optimal force distribution based on\nsurface geometry. Then, a physics-conditioned latent diffusion model (Phy-LDM)\npredicts the tactile imprint under FOSG target. Last, a latent-space LQR\ncontroller drives the gripper toward this tactile imprint with minimal\nactuation, preventing unnecessary compression. Trained on a physics-conditioned\ntactile dataset covering diverse objects and contact conditions, the proposed\nPhy-LDM achieves superior tactile prediction accuracy, while the Phy-Tac\noutperforms fixed-force and GraspNet-based baselines in grasp stability and\nforce efficiency. Experiments on classical robotic platforms demonstrate\nforce-efficient and adaptive manipulation that bridges the gap between robotic\nand human grasping.", "AI": {"tldr": "\u63d0\u51faPhy-Tac\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u6761\u4ef6\u89e6\u89c9\u6280\u672f\u5b9e\u73b0\u529b\u6700\u4f18\u7a33\u5b9a\u6293\u53d6\uff0c\u7ed3\u5408\u59ff\u6001\u9009\u62e9\u3001\u89e6\u89c9\u9884\u6d4b\u548c\u529b\u8c03\u8282\uff0c\u4f7f\u673a\u5668\u4eba\u50cf\u4eba\u7c7b\u4e00\u6837\u7528\u6700\u5c0f\u5fc5\u8981\u529b\u7a33\u5b9a\u6293\u53d6\u7269\u4f53\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u65f6\u8fc7\u5ea6\u7528\u529b\u7684\u95ee\u9898\uff0c\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u81ea\u7136\u6293\u53d6\uff08\u4f7f\u7528\u6700\u5c0f\u5fc5\u8981\u529b\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "1. \u57fa\u4e8e\u7269\u7406\u7684\u59ff\u6001\u9009\u62e9\u5668\u8bc6\u522b\u6700\u4f18\u529b\u5206\u5e03\u7684\u63a5\u89e6\u533a\u57df\uff1b2. \u7269\u7406\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u9884\u6d4b\u76ee\u6807\u89e6\u89c9\u5370\u8bb0\uff1b3. \u6f5c\u5728\u7a7a\u95f4LQR\u63a7\u5236\u5668\u9a71\u52a8\u5939\u722a\u4ee5\u6700\u5c0f\u9a71\u52a8\u8fbe\u5230\u76ee\u6807\u89e6\u89c9\u3002", "result": "Phy-LDM\u5728\u89e6\u89c9\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cPhy-Tac\u5728\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u529b\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u56fa\u5b9a\u529b\u548cGraspNet\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u529b\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u64cd\u4f5c\uff0c\u7f29\u5c0f\u4e86\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u6293\u53d6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.01191", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability.", "AI": {"tldr": "Self-Harmony\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u539f\u59cb\u95ee\u9898\u4e0e\u5176\u6539\u5199\u7248\u672c\u4e4b\u95f4\u7b54\u6848\u7684\u4e00\u81f4\u6027\u6765\u6784\u5efa\u53ef\u9760\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u907f\u514d\u4e86\u591a\u6570\u6295\u7968\u65b9\u6cd5\u503e\u5411\u4e8e\u865a\u5047\u4f46\u6d41\u884c\u7b54\u6848\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u591a\u6570\u6295\u7968\u7b49\u673a\u5236\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5bb9\u6613\u504f\u5411\u865a\u5047\u4f46\u6d41\u884c\u7684\u7b54\u6848\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u5b66\u4e60\u4fe1\u53f7\u6784\u5efa\u65b9\u6cd5\uff0c\u5229\u7528\u95ee\u9898\u6539\u5199\u524d\u540e\u7684\u7b54\u6848\u7a33\u5b9a\u6027\u6765\u8bc6\u522b\u6b63\u786e\u89e3\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u626e\u6f14\u4e24\u4e2a\u4e92\u8865\u89d2\u8272\uff1a\u6c42\u89e3\u5668\u751f\u6210\u7b54\u6848\uff0c\u91cd\u6784\u5668\u6539\u5199\u8f93\u5165\u3002\u7136\u540e\u63d0\u51fa\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u4f7f\u7528\u8c03\u548c\u5e73\u5747\u800c\u975e\u591a\u6570\u6295\u7968\u6765\u805a\u5408\u539f\u59cb\u89c6\u56fe\u548c\u91cd\u6784\u89c6\u56fe\u7684\u7b54\u6848\u9891\u7387\uff0c\u9009\u62e9\u5728\u91cd\u6784\u4e0b\u4fdd\u6301\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSelf-Harmony\u5728\u65e0\u6807\u7b7e\u6d4b\u8bd5\u65f6\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u572830\u4e2a\u8bbe\u7f6e\u4e2d\u768428\u4e2a\u6392\u540d\u7b2c\u4e00\u3002\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u9c81\u68d2\u6027\uff0c\u6240\u6709\u5b9e\u9a8c\u4e2d\u96f6\u8bad\u7ec3\u5931\u8d25\u3002", "conclusion": "Self-Harmony\u901a\u8fc7\u5229\u7528\u95ee\u9898\u6539\u5199\u7a33\u5b9a\u6027\u6784\u5efa\u53ef\u9760\u5b66\u4e60\u4fe1\u53f7\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u5728\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u91cd\u6784\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.00456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": null, "summary": "This study proposes a weakly supervised deep learning framework for pneumonia\nclassification and localization from chest X-rays, utilizing Grad-CAM\nexplanations. Instead of costly pixel-level annotations, our approach utilizes\nimage-level labels to generate clinically meaningful heatmaps that highlight\nregions affected by pneumonia. We evaluate seven ImageNet-pretrained\narchitectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and\nViT-B16 under identical training conditions with focal loss and patient-wise\nsplits to prevent data leakage. Experimental results on the Kermany CXR dataset\ndemonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test\naccuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides\nan optimal trade-off between accuracy and computational cost. Grad-CAM\nvisualizations confirm that the proposed models focus on clinically relevant\nlung regions, supporting the use of interpretable AI for radiological\ndiagnostics. This work highlights the potential of weakly supervised\nexplainable models that enhance pneumonia screening transparency, and clinical\ntrust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f31\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528Grad-CAM\u89e3\u91ca\u8fdb\u884c\u80ba\u708e\u5206\u7c7b\u548c\u5b9a\u4f4d\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\uff0c\u4ec5\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u751f\u6210\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u70ed\u529b\u56fe\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u80ba\u708e\u8bca\u65ad\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u50cf\u7d20\u7ea7\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u7ed3\u679c\u7684\u5f31\u76d1\u7763\u6a21\u578b\uff0c\u589e\u5f3aAI\u8f85\u52a9\u533b\u5b66\u5f71\u50cf\u7684\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u3002", "method": "\u4f7f\u7528\u4e03\u79cdImageNet\u9884\u8bad\u7ec3\u67b6\u6784\uff08ResNet-18/50\u3001DenseNet-121\u3001EfficientNet-B0\u3001MobileNet-V2/V3\u548cViT-B16\uff09\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u91c7\u7528\u7126\u70b9\u635f\u5931\u548c\u60a3\u8005\u7ea7\u6570\u636e\u5206\u5272\uff0c\u5229\u7528Grad-CAM\u751f\u6210\u70ed\u529b\u56fe\u3002", "result": "\u5728Kermany CXR\u6570\u636e\u96c6\u4e0a\uff0cResNet-18\u548cEfficientNet-B0\u8fbe\u5230\u6700\u4f73\u6d4b\u8bd5\u51c6\u786e\u738798%\uff0cROC-AUC=0.997\uff0cF1=0.987\uff1bMobileNet-V2\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95f4\u63d0\u4f9b\u6700\u4f73\u5e73\u8861\u3002", "conclusion": "\u5f31\u76d1\u7763\u53ef\u89e3\u91ca\u6a21\u578b\u80fd\u591f\u6709\u6548\u8bc6\u522b\u80ba\u708e\u76f8\u5173\u80ba\u533a\u57df\uff0c\u589e\u5f3a\u80ba\u708e\u7b5b\u67e5\u7684\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u4fe1\u4efb\uff0c\u5c55\u793a\u4e86\u53ef\u89e3\u91caAI\u5728\u653e\u5c04\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.01594", "categories": ["cs.RO", "cs.CV", "I.2.9; I.2.11; I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2511.01594", "abs": "https://arxiv.org/abs/2511.01594", "authors": ["Renjun Gao", "Peiyan Zhong"], "title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence", "comment": "3 figures, 1 table; under review at Multimedia Systems (Springer)", "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nin cross-modal understanding and reasoning, offering new opportunities for\nintelligent assistive systems, yet existing systems still struggle with\nrisk-aware planning, user personalization, and grounding language plans into\nexecutable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic\nSystem powered by MLLMs for assistive intelligence and designed for smart home\nrobots supporting people with disabilities. The system integrates four agents:\na visual perception agent for extracting semantic and spatial features from\nenvironment images, a risk assessment agent for identifying and prioritizing\nhazards, a planning agent for generating executable action sequences, and an\nevaluation agent for iterative optimization. By combining multimodal perception\nwith hierarchical multi-agent decision-making, the framework enables adaptive,\nrisk-aware, and personalized assistance in dynamic indoor environments.\nExperiments on multiple datasets demonstrate the superior overall performance\nof the proposed system in risk-aware planning and coordinated multi-agent\nexecution compared with state-of-the-art multimodal models. The proposed\napproach also highlights the potential of collaborative AI for practical\nassistive scenarios and provides a generalizable methodology for deploying\nMLLM-enabled multi-agent systems in real-world environments.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4e13\u4e3a\u667a\u80fd\u5bb6\u5c45\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u4e3a\u6b8b\u969c\u4eba\u58eb\u63d0\u4f9b\u8f85\u52a9\u667a\u80fd\u670d\u52a1\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u56db\u4e2a\u667a\u80fd\u4f53\u7684\u534f\u540c\u5de5\u4f5c\u5b9e\u73b0\u98ce\u9669\u611f\u77e5\u3001\u4e2a\u6027\u5316\u89c4\u5212\u548c\u53ef\u6267\u884c\u6280\u80fd\u843d\u5730\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u98ce\u9669\u611f\u77e5\u89c4\u5212\u3001\u7528\u6237\u4e2a\u6027\u5316\u548c\u5c06\u8bed\u8a00\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u6280\u80fd\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6742\u4e71\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u52a8\u6001\u5ba4\u5185\u73af\u5883\u7684\u81ea\u9002\u5e94\u3001\u98ce\u9669\u611f\u77e5\u548c\u4e2a\u6027\u5316\u8f85\u52a9\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u56db\u4e2a\u667a\u80fd\u4f53\uff1a\u89c6\u89c9\u611f\u77e5\u667a\u80fd\u4f53\u4ece\u73af\u5883\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e49\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u98ce\u9669\u8bc4\u4f30\u667a\u80fd\u4f53\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u5371\u9669\uff0c\u89c4\u5212\u667a\u80fd\u4f53\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u5206\u5c42\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u98ce\u9669\u611f\u77e5\u89c4\u5212\u548c\u534f\u8c03\u591a\u667a\u80fd\u4f53\u6267\u884c\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u534f\u4f5cAI\u5728\u5b9e\u9645\u8f85\u52a9\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eMLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.01192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86DEER\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u8026\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u6765\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u6709\u6548\u5904\u7406\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5728\u9886\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u540c\u65f6\u6355\u83b7\u9886\u57df\u7279\u5b9a\u548c\u9886\u57df\u901a\u7528\u7684MGT\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u8026\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff1a1\uff09\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u5b66\u4e60\u7ec6\u7c92\u5ea6\u533a\u5206\uff0c\u5171\u4eab\u4e13\u5bb6\u63d0\u53d6\u8de8\u9886\u57df\u7279\u5f81\uff1b2\uff09\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u7531\u673a\u5236\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\uff0c\u89e3\u51b3\u63a8\u7406\u65f6\u9886\u57df\u6807\u7b7e\u4e0d\u53ef\u7528\u7684\u95ee\u9898\u3002", "result": "\u57285\u4e2a\u9886\u57df\u5185\u548c5\u4e2a\u9886\u57df\u5916\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDEER\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9886\u57df\u5185F1\u5206\u6570\u5e73\u5747\u63d0\u53471.39%\uff0c\u9886\u57df\u5916\u63d0\u53475.32%\uff1b\u51c6\u786e\u7387\u5206\u522b\u63d0\u53471.35%\u548c3.61%\u3002", "conclusion": "\u89e3\u8026\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u81ea\u9002\u5e94\u8def\u7531\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cDEER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u9886\u57df\u504f\u79fb\u6311\u6218\u3002"}}
{"id": "2511.00468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00468", "abs": "https://arxiv.org/abs/2511.00468", "authors": ["Panwang Pan", "Tingting Shen", "Chenxin Li", "Yunlong Lin", "Kairun Wen", "Jingjing Zhao", "Yixuan Yuan"], "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation", "comment": "Accepted to NeurIPS 2025; Project page: [this\n  URL](https://paulpanwang.github.io/HumanCrafter)", "summary": "Recent advances in generative models have achieved high-fidelity in 3D human\nreconstruction, yet their utility for specific tasks (e.g., human 3D\nsegmentation) remains constrained. We propose HumanCrafter, a unified framework\nthat enables the joint modeling of appearance and human-part semantics from a\nsingle image in a feed-forward manner. Specifically, we integrate human\ngeometric priors in the reconstruction stage and self-supervised semantic\npriors in the segmentation stage. To address labeled 3D human datasets\nscarcity, we further develop an interactive annotation procedure for generating\nhigh-quality data-label pairs. Our pixel-aligned aggregation enables cross-task\nsynergy, while the multi-task objective simultaneously optimizes texture\nmodeling fidelity and semantic consistency. Extensive experiments demonstrate\nthat HumanCrafter surpasses existing state-of-the-art methods in both 3D\nhuman-part segmentation and 3D human reconstruction from a single image.", "AI": {"tldr": "HumanCrafter\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u8054\u5408\u5efa\u6a21\u5916\u89c2\u548c\u4eba\u4f53\u90e8\u4f4d\u8bed\u4e49\uff0c\u901a\u8fc7\u96c6\u6210\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\u548c\u81ea\u76d1\u7763\u8bed\u4e49\u5148\u9a8c\uff0c\u57283D\u4eba\u4f53\u91cd\u5efa\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u57283D\u4eba\u4f53\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u4eba\u4f533D\u5206\u5272\uff09\u4e2d\u7684\u5b9e\u7528\u6027\u4ecd\u7136\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u5efa\u6a21\uff0c\u4e14\u6807\u8bb0\u76843D\u4eba\u4f53\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u63d0\u51faHumanCrafter\u6846\u67b6\uff0c\u5728\u91cd\u5efa\u9636\u6bb5\u96c6\u6210\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\uff0c\u5728\u5206\u5272\u9636\u6bb5\u96c6\u6210\u81ea\u76d1\u7763\u8bed\u4e49\u5148\u9a8c\u3002\u5f00\u53d1\u4ea4\u4e92\u5f0f\u6807\u6ce8\u7a0b\u5e8f\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u6807\u7b7e\u5bf9\uff0c\u901a\u8fc7\u50cf\u7d20\u5bf9\u9f50\u805a\u5408\u5b9e\u73b0\u8de8\u4efb\u52a1\u534f\u540c\uff0c\u591a\u4efb\u52a1\u76ee\u6807\u540c\u65f6\u4f18\u5316\u7eb9\u7406\u5efa\u6a21\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHumanCrafter\u5728\u5355\u56fe\u50cf3D\u4eba\u4f53\u90e8\u4f4d\u5206\u5272\u548c3D\u4eba\u4f53\u91cd\u5efa\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HumanCrafter\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u5916\u89c2\u548c\u8bed\u4e49\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u4eba\u4f53\u91cd\u5efa\u548c\u5206\u5272\u7684\u534f\u540c\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.01265", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article--headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9886\u57df\u7279\u5f02\u6027\u5bf9\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u6587\u672c\u6458\u8981\u751f\u6210\u7684\u5f71\u54cd\uff0c\u6784\u5efa\u4e86\u6700\u5927\u7684\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u65b0\u95fb\u6570\u636e\u96c6AraFinNews\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u9886\u57df\u9002\u5e94\u7684\u6a21\u578b\u80fd\u751f\u6210\u66f4\u51c6\u786e\u3001\u8fde\u8d2f\u7684\u6458\u8981\u3002", "motivation": "\u7814\u7a76\u9886\u57df\u7279\u5f02\u6027\u5728\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u6587\u672c\u6458\u8981\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u586b\u8865\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u8bc4\u4f30\u9886\u57df\u9002\u5e94\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6570\u503c\u53ef\u9760\u6027\u548c\u98ce\u683c\u5bf9\u9f50\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efaAraFinNews\u6570\u636e\u96c6\uff0821.25\u4e07\u7bc7\u6587\u7ae0-\u6807\u9898\u5bf9\uff09\uff0c\u8bc4\u4f30\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff08mT5\u3001AraT5\u548c\u9886\u57df\u9002\u5e94\u7684FinAraT5\uff09\uff0c\u5206\u6790\u91d1\u878d\u9886\u57df\u9884\u8bad\u7ec3\u5bf9\u6458\u8981\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u57df\u9002\u5e94\u7684\u6a21\u578b\u80fd\u751f\u6210\u66f4\u5fe0\u5b9e\u3001\u8fde\u8d2f\u7684\u6458\u8981\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5b9a\u91cf\u4fe1\u606f\u548c\u5b9e\u4f53\u4e2d\u5fc3\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u9002\u5e94\u5bf9\u4e8e\u63d0\u9ad8\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u6458\u8981\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u53d9\u8ff0\u6d41\u7545\u6027\u81f3\u5173\u91cd\u8981\uff0cAraFinNews\u6570\u636e\u96c6\u4e3a\u963f\u62c9\u4f2f\u8bed\u91d1\u878d\u6587\u672c\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2511.00472", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00472", "abs": "https://arxiv.org/abs/2511.00472", "authors": ["Navodini Wijethilake", "Marina Ivory", "Oscar MacCormac", "Siddhant Kumar", "Aaron Kujawa", "Lorena Garcia-Foncillas Macias", "Rebecca Burger", "Amanda Hitchings", "Suki Thomson", "Sinan Barazi", "Eleni Maratos", "Rupert Obholzer", "Dan Jiang", "Fiona McClenaghan", "Kazumi Chia", "Omar Al-Salihi", "Nick Thomas", "Steve Connor", "Tom Vercauteren", "Jonathan Shapey"], "title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations", "comment": null, "summary": "Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance\nImaging (MRI) is essential for patient management but often requires\ntime-intensive manual annotations by experts. While recent advances in deep\nlearning (DL) have facilitated automated segmentation, challenges remain in\nachieving robust performance across diverse datasets and complex clinical\ncases. We present an annotated dataset stemming from a bootstrapped DL-based\nframework for iterative segmentation and quality refinement of VS in MRI. We\ncombine data from multiple centres and rely on expert consensus for\ntrustworthiness of the annotations. We show that our approach enables effective\nand resource-efficient generalisation of automated segmentation models to a\ntarget data distribution. The framework achieved a significant improvement in\nsegmentation accuracy with a Dice Similarity Coefficient (DSC) increase from\n0.9125 to 0.9670 on our target internal validation dataset, while maintaining\nstable performance on representative external datasets. Expert evaluation on\n143 scans further highlighted areas for model refinement, revealing nuanced\ncases where segmentation required expert intervention. The proposed approach is\nestimated to enhance efficiency by approximately 37.4% compared to the\nconventional manual annotation process. Overall, our human-in-the-loop model\ntraining approach achieved high segmentation accuracy, highlighting its\npotential as a clinically adaptable and generalisable strategy for automated VS\nsegmentation in diverse clinical settings. The dataset includes 190 patients,\nwith tumour annotations available for 534 longitudinal contrast-enhanced\nT1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans\nfrom 6 patients. This dataset is publicly accessible on The Cancer Imaging\nArchive (TCIA) (https://doi.org/10.7937/bq0z-xa62).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fed\u4ee3\u5206\u5272\u548c\u8d28\u91cf\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8eMRI\u4e2d\u524d\u5ead\u795e\u7ecf\u9798\u7624\u7684\u81ea\u52a8\u5206\u5272\uff0c\u901a\u8fc7\u591a\u4e2d\u5fc3\u6570\u636e\u548c\u4e13\u5bb6\u5171\u8bc6\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u5e76\u63d0\u5347\u6548\u7387\u7ea637.4%\u3002", "motivation": "MRI\u4e2d\u524d\u5ead\u795e\u7ecf\u9798\u7624\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u60a3\u8005\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8de8\u6570\u636e\u96c6\u548c\u590d\u6742\u4e34\u5e8a\u75c5\u4f8b\u4e2d\u7684\u9c81\u68d2\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u5f15\u5bfc\u5f0f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8fed\u4ee3\u5206\u5272\u548c\u8d28\u91cf\u4f18\u5316\uff0c\u7ed3\u5408\u591a\u4e2d\u5fc3\u6570\u636e\u5e76\u4f9d\u8d56\u4e13\u5bb6\u5171\u8bc6\u786e\u4fdd\u6807\u6ce8\u53ef\u4fe1\u5ea6\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u76ee\u6807\u5185\u90e8\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\uff0cDice\u76f8\u4f3c\u7cfb\u6570\u4ece0.9125\u663e\u8457\u63d0\u9ad8\u52300.9670\uff0c\u5728\u4ee3\u8868\u6027\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002\u4e13\u5bb6\u8bc4\u4f30143\u4e2a\u626b\u63cf\u53d1\u73b0\u9700\u8981\u4e13\u5bb6\u5e72\u9884\u7684\u590d\u6742\u75c5\u4f8b\u3002", "conclusion": "\u8be5\u4eba\u673a\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u5206\u5272\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u4f5c\u4e3a\u4e34\u5e8a\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u63a8\u5e7f\u7684\u81ea\u52a8VS\u5206\u5272\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.01770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01770", "abs": "https://arxiv.org/abs/2511.01770", "authors": ["Liudi Yang", "Yang Bai", "Yuhao Wang", "Ibrahim Alsarraj", "Gitta Kutyniok", "Zhanchi Wang", "Ke Wu"], "title": "Lightweight Learning from Actuation-Space Demonstrations via Flow Matching for Whole-Body Soft Robotic Grasping", "comment": null, "summary": "Robotic grasping under uncertainty remains a fundamental challenge due to its\nuncertain and contact-rich nature. Traditional rigid robotic hands, with\nlimited degrees of freedom and compliance, rely on complex model-based and\nheavy feedback controllers to manage such interactions. Soft robots, by\ncontrast, exhibit embodied mechanical intelligence: their underactuated\nstructures and passive flexibility of their whole body, naturally accommodate\nuncertain contacts and enable adaptive behaviors. To harness this capability,\nwe propose a lightweight actuation-space learning framework that infers\ndistributional control representations for whole-body soft robotic grasping,\ndirectly from deterministic demonstrations using a flow matching model\n(Rectified Flow),without requiring dense sensing or heavy control loops. Using\nonly 30 demonstrations (less than 8% of the reachable workspace), the learned\npolicy achieves a 97.5% grasp success rate across the whole workspace,\ngeneralizes to grasped-object size variations of +-33%, and maintains stable\nperformance when the robot's dynamic response is directly adjusted by scaling\nthe execution time from 20% to 200%. These results demonstrate that\nactuation-space learning, by leveraging its passive redundant DOFs and\nflexibility, converts the body's mechanics into functional control intelligence\nand substantially reduces the burden on central controllers for this\nuncertain-rich task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9a71\u52a8\u7a7a\u95f4\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6d41\u5339\u914d\u6a21\u578b\u4ece\u786e\u5b9a\u6027\u6f14\u793a\u4e2d\u5b66\u4e60\u8f6f\u4f53\u673a\u5668\u4eba\u6293\u53d6\u7684\u63a7\u5236\u8868\u793a\uff0c\u4ec5\u970030\u4e2a\u6f14\u793a\u5373\u53ef\u5728\u5b8c\u6574\u5de5\u4f5c\u7a7a\u95f4\u5b9e\u73b097.5%\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u673a\u5668\u4eba\u624b\u5728\u4e0d\u786e\u5b9a\u63a5\u89e6\u4e0b\u9700\u8981\u590d\u6742\u6a21\u578b\u548c\u91cd\u53cd\u9988\u63a7\u5236\uff0c\u800c\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u673a\u68b0\u667a\u80fd\u7279\u6027\uff0c\u5176\u6b20\u9a71\u52a8\u7ed3\u6784\u548c\u5168\u8eab\u67d4\u987a\u6027\u80fd\u591f\u81ea\u7136\u9002\u5e94\u4e0d\u786e\u5b9a\u63a5\u89e6\u3002", "method": "\u4f7f\u7528\u6d41\u5339\u914d\u6a21\u578b\u4ece\u786e\u5b9a\u6027\u6f14\u793a\u4e2d\u5b66\u4e60\u5206\u5e03\u5f0f\u7684\u63a7\u5236\u8868\u793a\uff0c\u65e0\u9700\u5bc6\u96c6\u4f20\u611f\u6216\u91cd\u63a7\u5236\u5faa\u73af\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u6570\u636e\u3002", "result": "\u4ec5\u752830\u4e2a\u6f14\u793a\uff08\u4e0d\u5230\u53ef\u8fbe\u5de5\u4f5c\u7a7a\u95f4\u76848%\uff09\u5373\u53ef\u5728\u6574\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u5b9e\u73b097.5%\u7684\u6293\u53d6\u6210\u529f\u7387\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u00b133%\u7684\u6293\u53d6\u7269\u4f53\u5c3a\u5bf8\u53d8\u5316\uff0c\u5e76\u5728\u6267\u884c\u65f6\u95f4\u7f29\u653e20%-200%\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u9a71\u52a8\u7a7a\u95f4\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u88ab\u52a8\u5197\u4f59\u81ea\u7531\u5ea6\u548c\u67d4\u987a\u6027\uff0c\u5c06\u8eab\u4f53\u529b\u5b66\u8f6c\u5316\u4e3a\u529f\u80fd\u63a7\u5236\u667a\u80fd\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u4e2d\u592e\u63a7\u5236\u5668\u5728\u8fd9\u79cd\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u7684\u8d1f\u62c5\u3002"}}
{"id": "2511.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality.", "AI": {"tldr": "ReSpec\u662f\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u51b3\u7b56\u673a\u5236\u53d6\u4ee3\u542f\u53d1\u5f0f\u5207\u6362\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5982EAGLE-2\u51c6\u786e\u4f46\u6210\u672c\u9ad8\uff0c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5982SAM-Decoding\u4f9d\u8d56\u542f\u53d1\u5f0f\u5207\u6362\u7b56\u7565\uff0c\u7ecf\u5e38\u89e6\u53d1\u4e0d\u5fc5\u8981\u7684\u68c0\u7d22\u3002\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\u6548\u7387\u3002", "method": "\u63d0\u51faReSpec\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u71b5\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u89e6\u53d1\u5668\uff0c\u4ec5\u5728\u4e0d\u786e\u5b9a\u6027\u4f4e\u65f6\u542f\u52a8\u68c0\u7d22\uff1b2\uff09\u53cd\u9988\u9a71\u52a8\u7684\u5019\u9009\u9009\u62e9\uff0c\u5229\u7528\u5386\u53f2\u53cd\u9988\u7ec4\u7ec7\u9ad8\u8d28\u91cf\u5019\u9009\u8fdb\u884c\u5e76\u884c\u9a8c\u8bc1\uff1b3\uff09\u6e90\u611f\u77e5\u7684\u5bbd\u677e\u9a8c\u8bc1\u7b56\u7565\uff0c\u5bf9\u6a21\u578b\u751f\u6210\u8349\u7a3f\u4e25\u683c\u68c0\u67e5\uff0c\u5bf9\u68c0\u7d22\u8349\u7a3f\u5bbd\u677e\u9a8c\u8bc1\u3002", "result": "\u5728Spec-Bench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cReSpec\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u52a0\u901f\u6548\u679c\uff0c\u5206\u522b\u6bd4EAGLE-2\u548cSAM-Decoding\u5feb33%\u548c25%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "ReSpec\u901a\u8fc7\u5c06\u542f\u53d1\u5f0f\u8349\u7a3f\u5207\u6362\u8f6c\u5316\u4e3a\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u52a0\u901f\u6027\u80fd\u548c\u8f93\u51fa\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2511.00480", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00480", "abs": "https://arxiv.org/abs/2511.00480", "authors": ["Weihao Bo", "Yanpeng Sun", "Yu Wang", "Xinyu Zhang", "Zechao Li"], "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts", "comment": null, "summary": "In this paper, we introduce FedMGP, a new paradigm for personalized federated\nprompt learning in vision-language models. FedMGP equips each client with\nmultiple groups of paired textual and visual prompts, enabling the model to\ncapture diverse, fine-grained semantic and instance-level cues. A diversity\nloss is introduced to drive each prompt group to specialize in distinct and\ncomplementary semantic aspects, ensuring that the groups collectively cover a\nbroader range of local characteristics. During communication, FedMGP employs a\ndynamic prompt aggregation strategy based on similarity-guided probabilistic\nsampling: each client computes the cosine similarity between its prompt groups\nand the global prompts from the previous round, then samples s groups via a\nsoftmax-weighted distribution. This soft selection mechanism preferentially\naggregates semantically aligned knowledge while still enabling exploration of\nunderrepresented patterns effectively balancing the preservation of common\nknowledge with client-specific features. Notably, FedMGP maintains parameter\nefficiency by redistributing a fixed prompt capacity across multiple groups,\nachieving state-of-the-art performance with the lowest communication parameters\namong all federated prompt learning methods. Theoretical analysis shows that\nour dynamic aggregation strategy promotes robust global representation learning\nby reinforcing shared semantics while suppressing client-specific noise.\nExtensive experiments demonstrate that FedMGP consistently outperforms prior\napproaches in both personalization and domain generalization across diverse\nfederated vision-language benchmarks. The code will be released on\nhttps://github.com/weihao-bo/FedMGP.git.", "AI": {"tldr": "FedMGP\u662f\u4e00\u79cd\u65b0\u7684\u4e2a\u6027\u5316\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u914d\u5907\u591a\u7ec4\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u901a\u8fc7\u591a\u6837\u6027\u635f\u5931\u548c\u52a8\u6001\u63d0\u793a\u805a\u5408\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6355\u83b7\u591a\u6837\u5316\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u5b9e\u4f8b\u7ea7\u7ebf\u7d22\u7684\u4e2a\u6027\u5316\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u914d\u5907\u591a\u7ec4\u914d\u5bf9\u6587\u672c\u548c\u89c6\u89c9\u63d0\u793a\uff0c\u5f15\u5165\u591a\u6837\u6027\u635f\u5931\u4f7f\u5404\u7ec4\u4e13\u6ce8\u4e8e\u4e0d\u540c\u8bed\u4e49\u65b9\u9762\uff0c\u91c7\u7528\u57fa\u4e8e\u76f8\u4f3c\u6027\u5f15\u5bfc\u6982\u7387\u91c7\u6837\u7684\u52a8\u6001\u63d0\u793a\u805a\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u8f6f\u9009\u62e9\u673a\u5236\u5e73\u8861\u5171\u540c\u77e5\u8bc6\u548c\u5ba2\u6237\u7aef\u7279\u5b9a\u7279\u5f81\u3002", "result": "FedMGP\u5728\u6240\u6709\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f4e\u901a\u4fe1\u53c2\u6570\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5728\u4e2a\u6027\u5316\u548c\u9886\u57df\u6cdb\u5316\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u52a8\u6001\u805a\u5408\u7b56\u7565\u80fd\u4fc3\u8fdb\u7a33\u5065\u7684\u5168\u5c40\u8868\u793a\u5b66\u4e60\u3002", "conclusion": "FedMGP\u901a\u8fc7\u591a\u7ec4\u63d0\u793a\u548c\u52a8\u6001\u805a\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.01774", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.01774", "abs": "https://arxiv.org/abs/2511.01774", "authors": ["Alexander Schperberg", "Yusuke Tanaka", "Stefano Di Cairano", "Dennis Hong"], "title": "MOBIUS: A Multi-Modal Bipedal Robot that can Walk, Crawl, Climb, and Roll", "comment": "23 pages, 20 figures. Collaborative work between the Robotics and\n  Mechanisms Laboratory (RoMeLa) and Mitsubishi Electric Research Laboratories\n  (MERL)", "summary": "This article presents a Multi-Modal Bipedal Intelligent Urban Scout robot\n(MOBIUS) capable of walking, crawling, climbing, and rolling. MOBIUS features\nfour limbs--two 6-DoF arms with two-finger grippers for manipulation and\nclimbing, and two 4-DoF legs for locomotion--enabling smooth transitions across\ndiverse terrains without reconfiguration. A hybrid control architecture\ncombines reinforcement learning-based locomotion with model-based predictive\nand admittance control enhanced for safety by a Reference Governor toward\ncompliant contact interactions. A high-level MIQCP planner autonomously selects\nlocomotion modes to balance stability and energy efficiency. Hardware\nexperiments demonstrate robust gait transitions, dynamic climbing, and\nfull-body load support via pinch grasp. Overall, MOBIUS demonstrates the\nimportance of tight integration between morphology, high-level planning, and\ncontrol to enable mobile loco-manipulation and grasping, substantially\nexpanding its interaction capabilities, workspace, and traversability.", "AI": {"tldr": "MOBIUS\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u53cc\u8db3\u667a\u80fd\u57ce\u5e02\u4fa6\u5bdf\u673a\u5668\u4eba\uff0c\u80fd\u591f\u884c\u8d70\u3001\u722c\u884c\u3001\u6500\u722c\u548c\u6eda\u52a8\uff0c\u901a\u8fc7\u6df7\u5408\u63a7\u5236\u67b6\u6784\u548c\u9ad8\u7ea7\u89c4\u5212\u5668\u5b9e\u73b0\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\u7684\u5e73\u6ed1\u8f6c\u6362\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u591a\u6837\u5316\u5730\u5f62\u4e2d\u65e0\u7f1d\u8f6c\u6362\u8fd0\u52a8\u6a21\u5f0f\u7684\u673a\u5668\u4eba\uff0c\u6269\u5c55\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u80fd\u529b\u3001\u5de5\u4f5c\u7a7a\u95f4\u548c\u53ef\u7a7f\u8d8a\u6027\u3002", "method": "\u91c7\u7528\u56db\u80a2\u7ed3\u6784\uff08\u4e24\u4e2a6\u81ea\u7531\u5ea6\u624b\u81c2\u548c\u4e24\u4e2a4\u81ea\u7531\u5ea6\u817f\u90e8\uff09\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u63a7\u5236\u4e0e\u6a21\u578b\u9884\u6d4b\u548c\u5bfc\u7eb3\u63a7\u5236\uff0c\u4f7f\u7528MIQCP\u89c4\u5212\u5668\u81ea\u4e3b\u9009\u62e9\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\u5c55\u793a\u4e86\u7a33\u5065\u7684\u6b65\u6001\u8f6c\u6362\u3001\u52a8\u6001\u6500\u722c\u548c\u901a\u8fc7\u5939\u6301\u6293\u63e1\u5b9e\u73b0\u7684\u5168\u8eab\u4f53\u91cd\u652f\u6491\u3002", "conclusion": "MOBIUS\u8bc1\u660e\u4e86\u5f62\u6001\u5b66\u3001\u9ad8\u7ea7\u89c4\u5212\u548c\u63a7\u5236\u4e4b\u95f4\u7684\u7d27\u5bc6\u96c6\u6210\u5bf9\u4e8e\u5b9e\u73b0\u79fb\u52a8\u5b9a\u4f4d\u64cd\u4f5c\u548c\u6293\u53d6\u7684\u91cd\u8981\u6027\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5176\u4ea4\u4e92\u80fd\u529b\u3002"}}
{"id": "2511.01287", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u79d1\u5b66\u8bba\u6587\u4e2d\u9690\u85cf\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5bf9AI\u5ba1\u7a3f\u4eba\u7684\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u9759\u6001\u548c\u8fed\u4ee3\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u68c0\u6d4b\u9632\u5fa1\u673a\u5236\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5728\u79d1\u5b66\u8bba\u6587\u8bc4\u5ba1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u53d1\u73b0\u5b58\u5728\u901a\u8fc7\u9690\u85cf\u63d0\u793a\u64cd\u7eb5AI\u5ba1\u7a3f\u4eba\u7ed9\u51fa\u8fc7\u9ad8\u8bc4\u4ef7\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u5bf9\u6b64\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u653b\u51fb\u65b9\u6cd5\uff1a\u9759\u6001\u653b\u51fb\u4f7f\u7528\u56fa\u5b9a\u6ce8\u5165\u63d0\u793a\uff0c\u8fed\u4ee3\u653b\u51fb\u901a\u8fc7\u6a21\u62df\u5ba1\u7a3f\u4eba\u6a21\u578b\u4f18\u5316\u6ce8\u5165\u63d0\u793a\u4ee5\u6700\u5927\u5316\u6548\u679c\uff1b\u540c\u65f6\u63a2\u7d22\u57fa\u4e8e\u68c0\u6d4b\u7684\u9632\u5fa1\u673a\u5236\u3002", "result": "\u4e24\u79cd\u653b\u51fb\u65b9\u6cd5\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u7ecf\u5e38\u80fd\u8bf1\u5bfc\u524d\u6cbfAI\u5ba1\u7a3f\u4eba\u7ed9\u51fa\u6ee1\u5206\u8bc4\u4ef7\uff1b\u653b\u51fb\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u90fd\u5177\u6709\u9c81\u68d2\u6027\uff1b\u68c0\u6d4b\u9632\u5fa1\u80fd\u5927\u5e45\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u81ea\u9002\u5e94\u653b\u51fb\u8005\u53ef\u4ee5\u90e8\u5206\u89c4\u907f\u9632\u5fa1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728AI\u8f85\u52a9\u540c\u884c\u8bc4\u5ba1\u4e2d\u9700\u8981\u66f4\u591a\u5173\u6ce8\u548c\u4e25\u683c\u9632\u62a4\u63aa\u65bd\u6765\u5e94\u5bf9\u63d0\u793a\u6ce8\u5165\u5a01\u80c1\u3002"}}
{"id": "2511.00503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00503", "abs": "https://arxiv.org/abs/2511.00503", "authors": ["Panwang Pan", "Chenguo Lin", "Jingjing Zhao", "Chenxin Li", "Yuchen Lin", "Haopeng Li", "Honglei Yan", "Kairun Wen", "Yunlong Lin", "Yixuan Yuan", "Yadong Mu"], "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models", "comment": null, "summary": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable\nand explicit 4D scenes from a single image. Our approach unifies the generative\npriors of video diffusion models with geometry and motion constraints learned\nfrom large-scale 4D datasets. Given a single input image, a camera trajectory,\nand an optional text prompt, Diff4Splat directly predicts a deformable 3D\nGaussian field that encodes appearance, geometry, and motion, all in a single\nforward pass, without test-time optimization or post-hoc refinement. At the\ncore of our framework lies a video latent transformer, which augments video\ndiffusion models to jointly capture spatio-temporal dependencies and predict\ntime-varying 3D Gaussian primitives. Training is guided by objectives on\nappearance fidelity, geometric accuracy, and motion consistency, enabling\nDiff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate\nthe effectiveness of Diff4Splatacross video generation, novel view synthesis,\nand geometry extraction, where it matches or surpasses optimization-based\nmethods for dynamic scene synthesis while being significantly more efficient.", "AI": {"tldr": "Diff4Splat\u662f\u4e00\u79cd\u524d\u9988\u65b9\u6cd5\uff0c\u53ef\u4ece\u5355\u5f20\u56fe\u50cf\u5408\u6210\u53ef\u63a7\u7684\u663e\u5f0f4D\u573a\u666f\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u4e0e4D\u6570\u636e\u96c6\u5b66\u4e60\u5230\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u7ea6\u675f\uff0c\u76f4\u63a5\u9884\u6d4b\u53ef\u53d8\u5f623D\u9ad8\u65af\u573a\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf4D\u573a\u666f\u7684\u6311\u6218\uff0c\u907f\u514d\u6d4b\u8bd5\u65f6\u4f18\u5316\u548c\u540e\u5904\u7406\uff0c\u63d0\u9ad8\u5408\u6210\u6548\u7387\u3002", "method": "\u4f7f\u7528\u89c6\u9891\u6f5c\u5728\u53d8\u6362\u5668\u589e\u5f3a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u8054\u5408\u6355\u83b7\u65f6\u7a7a\u4f9d\u8d56\u6027\u5e76\u9884\u6d4b\u65f6\u53d83D\u9ad8\u65af\u57fa\u5143\uff0c\u901a\u8fc7\u5916\u89c2\u4fdd\u771f\u5ea6\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u572830\u79d2\u5185\u5408\u6210\u9ad8\u8d28\u91cf4D\u573a\u666f\uff0c\u5728\u89c6\u9891\u751f\u6210\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u51e0\u4f55\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u4e14\u6548\u7387\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "Diff4Splat\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b21\u524d\u9988\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u53ef\u63a7\u76844D\u573a\u666f\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2511.01289", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA.", "AI": {"tldr": "\u63d0\u51fa\u4e86FirstAidQA\u6570\u636e\u96c6\uff0c\u5305\u542b5500\u4e2a\u9ad8\u8d28\u91cf\u6025\u6551\u95ee\u7b54\u5bf9\uff0c\u65e8\u5728\u652f\u6301\u5728\u4f4e\u8fde\u63a5\u73af\u5883\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5c0f\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\uff0c\u4ee5\u6539\u5584\u6025\u6551\u54cd\u5e94\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u8f7b\u91cf\u7ea7\u6025\u6551\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5f53\u524d\u6a21\u578b\u8ba1\u7b97\u5bc6\u96c6\u4e14\u4e0d\u9002\u5408\u7b2c\u4e00\u54cd\u5e94\u8005\u4f7f\u7528\u7684\u4f4e\u7aef\u8bbe\u5907\u3002", "method": "\u4f7f\u7528ChatGPT-4o-mini\u901a\u8fc7\u63d0\u793a\u5f0f\u4e0a\u4e0b\u6587\u5b66\u4e60\u751f\u6210\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u300aVital First Aid Book (2019)\u300b\u6587\u672c\uff0c\u7ecf\u8fc7\u6587\u672c\u6e05\u6d17\u3001\u4e0a\u4e0b\u6587\u5206\u5757\u3001\u8fc7\u6ee4\u7b49\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5e76\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u51c6\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b5500\u4e2a\u9ad8\u8d28\u91cf\u6025\u6551\u95ee\u7b54\u5bf9\u7684FirstAidQA\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u6025\u6551\u548c\u5e94\u6025\u54cd\u5e94\u573a\u666f\u3002", "conclusion": "FirstAidQA\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\uff0c\u5c06\u63a8\u52a8\u6025\u6551\u548c\u5e94\u6025\u54cd\u5e94\u4e2d\u5b89\u5168\u5173\u952e\u548c\u8d44\u6e90\u53d7\u9650AI\u5e94\u7528\u7684\u7814\u7a76\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u4e14\u5177\u5907\u79bb\u7ebf\u80fd\u529b\u7684\u7cfb\u7edf\u3002"}}
{"id": "2511.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00504", "abs": "https://arxiv.org/abs/2511.00504", "authors": ["Hai-Dang Nguyen", "Ha-Hieu Pham", "Hao T. Nguyen", "Huy-Hieu Pham"], "title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning", "comment": "ISBI submission. Contains 5 pages, 2 figures, and 6 tables. Code &\n  data: https://huggingface.co/datasets/Dangindev/VinDR-CXR-VQA", "summary": "We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable\nMedical Visual Question Answering (Med-VQA) with spatial grounding. The dataset\ncontains 17,597 question-answer pairs across 4,394 images, each annotated with\nradiologist-verified bounding boxes and clinical reasoning explanations. Our\nquestion taxonomy spans six diagnostic types-Where, What, Is there, How many,\nWhich, and Yes/No-capturing diverse clinical intents. To improve reliability,\nwe construct a balanced distribution of 41.7% positive and 58.3% negative\nsamples, mitigating hallucinations in normal cases. Benchmarking with\nMedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over\nbaseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance\nreproducible and clinically grounded Med-VQA research. The dataset and\nevaluation tools are publicly available at\nhuggingface.co/datasets/Dangindev/VinDR-CXR-VQA.", "AI": {"tldr": "VinDr-CXR-VQA\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u53ef\u89e3\u91ca\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b17,597\u4e2a\u95ee\u7b54\u5bf9\u548c4,394\u5f20\u80f8\u7247\u56fe\u50cf\uff0c\u5e26\u6709\u653e\u5c04\u79d1\u533b\u751f\u9a8c\u8bc1\u7684\u8fb9\u754c\u6846\u548c\u4e34\u5e8a\u63a8\u7406\u89e3\u91ca\uff0c\u65e8\u5728\u63a8\u8fdb\u53ef\u91cd\u73b0\u7684\u533b\u5b66VQA\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6b63\u5e38\u75c5\u4f8b\u4e2d\uff0c\u7f3a\u4e4f\u53ef\u9760\u7684\u7a7a\u95f4\u5b9a\u4f4d\u548c\u4e34\u5e8a\u89e3\u91ca\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u516d\u79cd\u8bca\u65ad\u7c7b\u578b\uff08\u4f4d\u7f6e\u3001\u5185\u5bb9\u3001\u5b58\u5728\u6027\u3001\u6570\u91cf\u3001\u9009\u62e9\u548c\u662f/\u975e\uff09\u7684\u95ee\u9898\u5206\u7c7b\u6cd5\uff0c\u5e76\u5e73\u8861\u6b63\u8d1f\u6837\u672c\u5206\u5e03\uff0841.7%\u9633\u6027 vs 58.3%\u9634\u6027\uff09\uff0c\u4f7f\u7528MedGemma-4B-it\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08F1=0.624\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad811.8%\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u75c5\u53d8\u5b9a\u4f4d\u529f\u80fd\u3002", "conclusion": "VinDr-CXR-VQA\u6570\u636e\u96c6\u4e3a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\uff0c\u652f\u6301\u53ef\u91cd\u73b0\u548c\u4e34\u5e8a\u57fa\u7840\u7684\u7814\u7a76\uff0c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2511.01305", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards.", "AI": {"tldr": "DeepSpecs\u662f\u4e00\u4e2a\u589e\u5f3a\u7684RAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u65f6\u5e8f\u63a8\u7406\u6765\u89e3\u51b35G\u6807\u51c6\u6587\u6863\u4e2d\u7684\u4ea4\u53c9\u5f15\u7528\u548c\u89c4\u8303\u6f14\u8fdb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u7ea7\u95ee\u7b54\u7684\u8d28\u91cf\u3002", "motivation": "5G\u6280\u672f\u4e3a\u6570\u5341\u4ebf\u7528\u6237\u63d0\u4f9b\u79fb\u52a8\u4e92\u8054\u7f51\u63a5\u5165\uff0c\u4f46\u56de\u7b54\u5173\u4e8e5G\u89c4\u8303\u7684\u4e13\u5bb6\u7ea7\u95ee\u9898\u9700\u8981\u6d4f\u89c8\u6570\u5343\u9875\u4ea4\u53c9\u5f15\u7528\u7684\u6807\u51c6\u6587\u6863\uff0c\u8fd9\u4e9b\u6587\u6863\u5728\u4e0d\u540c\u7248\u672c\u95f4\u4e0d\u65ad\u6f14\u8fdb\u3002\u73b0\u6709\u7684RAG\u6846\u67b6\u65e0\u6cd5\u53ef\u9760\u5730\u89e3\u6790\u4ea4\u53c9\u5f15\u7528\u6216\u63a8\u7406\u89c4\u8303\u6f14\u8fdb\u3002", "method": "DeepSpecs\u901a\u8fc7\u4e09\u4e2a\u5143\u6570\u636e\u4e30\u5bcc\u7684\u6570\u636e\u5e93\u589e\u5f3aRAG\u7cfb\u7edf\uff1aSpecDB\uff08\u6761\u6b3e\u5bf9\u9f50\u7684\u89c4\u8303\u6587\u672c\uff09\u3001ChangeDB\uff08\u884c\u7ea7\u7248\u672c\u5dee\u5f02\uff09\u548cTDocDB\uff08\u6807\u51c6\u5316\u4f1a\u8bae\u6587\u6863\uff09\u3002\u7cfb\u7edf\u901a\u8fc7\u5143\u6570\u636e\u67e5\u627e\u9012\u5f52\u68c0\u7d22\u5f15\u7528\u6761\u6b3e\u6765\u663e\u5f0f\u89e3\u6790\u4ea4\u53c9\u5f15\u7528\uff0c\u5e76\u901a\u8fc7\u6316\u6398\u53d8\u66f4\u5e76\u5c06\u5176\u94fe\u63a5\u5230\u8bb0\u5f55\u8bbe\u8ba1\u539f\u7406\u7684\u53d8\u66f4\u8bf7\u6c42\u6765\u8ffd\u8e2a\u89c4\u8303\u6f14\u8fdb\u3002", "result": "\u5728\u591a\u4e2aLLM\u540e\u7aef\u4e0a\uff0cDeepSpecs\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u7535\u4fe1RAG\u7cfb\u7edf\uff1b\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u663e\u5f0f\u4ea4\u53c9\u5f15\u7528\u89e3\u6790\u548c\u6f14\u8fdb\u611f\u77e5\u68c0\u7d22\u663e\u8457\u63d0\u9ad8\u4e86\u7b54\u6848\u8d28\u91cf\u3002", "conclusion": "\u5efa\u6a215G\u6807\u51c6\u7684\u7ed3\u6784\u548c\u65f6\u5e8f\u7279\u6027\u5bf9\u4e8e\u53ef\u9760\u56de\u7b54\u4e13\u5bb6\u7ea7\u95ee\u9898\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cDeepSpecs\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u65f6\u5e8f\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RAG\u7cfb\u7edf\u57285G\u6807\u51c6\u6587\u6863\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.01323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness.", "AI": {"tldr": "DeepAmbigQA\u662f\u4e00\u4e2a\u65b0\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b3600\u4e2a\u9700\u8981\u591a\u8df3\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5176\u4e2d\u4e00\u534a\u6d89\u53ca\u663e\u5f0f\u540d\u79f0\u6d88\u6b67\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u95ee\u7b54\u57fa\u51c6\u5f88\u5c11\u540c\u65f6\u8bc4\u4f30\u540d\u79f0\u6b67\u4e49\u548c\u591a\u6b65\u63a8\u7406\u4e24\u4e2a\u6311\u6218\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56de\u7b54\u590d\u6742\u95ee\u9898\u65f6\u5f80\u5f80\u65e0\u6cd5\u4ea7\u751f\u5b8c\u6574\u7684\u7b54\u6848\u96c6\u3002", "method": "\u5f00\u53d1\u4e86DeepAmbigQAGen\u81ea\u52a8\u6570\u636e\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8e\u6587\u672c\u8bed\u6599\u5e93\u548c\u94fe\u63a5\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u95ee\u7b54\u4efb\u52a1\uff0c\u751f\u6210\u81ea\u7136\u4e14\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u7cfb\u7edf\u6027\u5730\u5d4c\u5165\u540d\u79f0\u6b67\u4e49\u548c\u591a\u6b65\u63a8\u7406\u3002", "result": "\u6784\u5efa\u4e86DeepAmbigQA\u6570\u636e\u96c6\uff0c\u5305\u542b3600\u4e2a\u95ee\u9898\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662fGPT-5\u8fd9\u6837\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728\u6b67\u4e49\u95ee\u9898\u4e0a\u7cbe\u786e\u5339\u914d\u7387\u4ec5\u4e3a0.13\uff0c\u5728\u975e\u6b67\u4e49\u95ee\u9898\u4e0a\u4e3a0.21\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u4fe1\u606f\u6536\u96c6\u548c\u7b54\u6848\u5b8c\u6574\u6027\u7684\u6311\u6218\u3002"}}
{"id": "2511.00511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00511", "abs": "https://arxiv.org/abs/2511.00511", "authors": ["Panwang Pan", "Jingjing Zhao", "Yuchen Lin", "Chenguo Lin", "Chenxin Li", "Haopeng Li", "Honglei Yan", "Tingting Shen", "Yadong Mu"], "title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "comment": null, "summary": "Video generative models pretrained on large-scale datasets can produce\nhigh-quality videos, but are often conditioned on text or a single image,\nlimiting controllability and applicability. We introduce ID-Composer, a novel\nframework that addresses this gap by tackling multi-subject video generation\nfrom a text prompt and reference images. This task is challenging as it\nrequires preserving subject identities, integrating semantics across subjects\nand modalities, and maintaining temporal consistency. To faithfully preserve\nthe subject consistency and textual information in synthesized videos,\nID-Composer designs a \\textbf{hierarchical identity-preserving attention\nmechanism}, which effectively aggregates features within and across subjects\nand modalities. To effectively allow for the semantic following of user\nintention, we introduce \\textbf{semantic understanding via pretrained\nvision-language model (VLM)}, leveraging VLM's superior semantic understanding\nto provide fine-grained guidance and capture complex interactions between\nmultiple subjects. Considering that standard diffusion loss often fails in\naligning the critical concepts like subject ID, we employ an \\textbf{online\nreinforcement learning phase} to drive the overall training objective of\nID-Composer into RLVR. Extensive experiments demonstrate that our model\nsurpasses existing methods in identity preservation, temporal consistency, and\nvideo quality.", "AI": {"tldr": "ID-Composer\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u548c\u53c2\u8003\u56fe\u50cf\u5b9e\u73b0\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u53ef\u63a7\u6027\u548c\u9002\u7528\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u57fa\u4e8e\u6587\u672c\u6216\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u751f\u6210\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u548c\u5e94\u7528\u8303\u56f4\u3002\u9700\u8981\u89e3\u51b3\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u8de8\u4e3b\u4f53\u548c\u6a21\u6001\u8bed\u4e49\u6574\u5408\u4ee5\u53ca\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u5c42\u8eab\u4efd\u4fdd\u6301\u6ce8\u610f\u529b\u673a\u5236\u6765\u805a\u5408\u4e3b\u4f53\u5185\u548c\u8de8\u4e3b\u4f53\u7684\u7279\u5f81\uff1b\u5f15\u5165\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\u4ee5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\uff1b\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u6765\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ID-Composer\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u3001\u8bed\u4e49\u7406\u89e3\u6280\u672f\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u53ef\u63a7\u6027\u3002"}}
{"id": "2511.01354", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86DistilQwen\u6a21\u578b\u7cfb\u5217\uff0c\u9488\u5bf9\u5de5\u4e1a\u9700\u6c42\u5f00\u53d1\u4e86\u56db\u79cd\u6a21\u578b\uff1a\u6162\u601d\u8003\u6a21\u578b\uff08\u9ad8\u7cbe\u5ea6\u63a8\u7406\uff09\u3001\u81ea\u9002\u5e94\u601d\u8003\u6a21\u578b\uff08\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff09\u548c\u84b8\u998f\u5956\u52b1\u6a21\u578b\uff08\u652f\u6301\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u9ad8\u6548\u63a8\u7406\u548c\u5f3a\u5927\u6027\u80fd\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u73b0\u5b9e\u5e94\u7528\u5bf9\u5c0f\u578b\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u7684\u9700\u6c42\uff0c\u5e73\u8861\u63a8\u7406\u6027\u80fd\u4e0e\u63a8\u7406\u901f\u5ea6\uff0c\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u5de5\u4e1a\u9700\u6c42\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u3002", "method": "\u57fa\u4e8eQwen\u6a21\u578b\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5f00\u53d1\u56db\u79cd\u6a21\u578b\u7cfb\u5217\uff1a\u6162\u601d\u8003\u6a21\u578b\u3001\u81ea\u9002\u5e94\u601d\u8003\u6a21\u578b\uff08\u4e24\u4e2a\u7cfb\u5217\uff09\u548c\u84b8\u998f\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u5728\u963f\u91cc\u4e91PAI\u5e73\u53f0\u4e0a\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u529f\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fd9\u4e9b\u6a21\u578b\u5c55\u73b0\u51fa\u9ad8\u63a8\u7406\u6548\u7387\u548c\u5f3a\u5927\u7684\u63a8\u7406\u6027\u80fd\uff0c\u84b8\u998f\u5956\u52b1\u6a21\u578b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u5f00\u53d1\u7684DistilQwen\u6a21\u578b\u7cfb\u5217\u6210\u529f\u6ee1\u8db3\u4e86\u5de5\u4e1a\u5e94\u7528\u5bf9\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u7684\u9700\u6c42\uff0c\u5728\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u884c\u4e1a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00523", "abs": "https://arxiv.org/abs/2511.00523", "authors": ["Fangyu Wu", "Yujun Cai"], "title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation", "comment": null, "summary": "Vision language models such as CLIP have shown remarkable performance in zero\nshot classification, but remain susceptible to spurious correlations, where\nirrelevant visual features influence predictions. Existing debiasing methods\noften require access to training data and explicit group labels to perform\nfine-tuning or adjust embeddings, which limits their practicality in real-world\nsettings. Test-time methods attempt to avoid this constraint, but many still\ndepend on prior knowledge of dataset specific biases, limiting their\ngeneralizability in open set settings. In this work, we propose a test-time\ndebiasing method for ViT based CLIP models that requires no additional training\nor assumptions of bias annotations. Our approach uses a pretrained segmentation\nmodel to isolate the target visual attribute, then adjusts the non target\nregions so that their embeddings are uniformly similar to all class specific\ntext prompts. This procedure removes unintended bias signals from confounding\nvisual regions while preserving the target attribute. Experiments on Waterbirds\nand CelebA show that our method outperforms existing test-time debiasing\napproaches in both group robustness metrics and Attention IoU. These results\ndemonstrate the effectiveness of segmentation guided interventions for scalable\nand annotation free bias mitigation in vision language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u504f\u5dee\u6807\u6ce8\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u9694\u79bb\u76ee\u6807\u89c6\u89c9\u5c5e\u6027\uff0c\u8c03\u6574\u975e\u76ee\u6807\u533a\u57df\u4f7f\u5176\u5d4c\u5165\u4e0e\u6240\u6709\u7c7b\u522b\u6587\u672c\u63d0\u793a\u5747\u5300\u76f8\u4f3c\uff0c\u4ece\u800c\u6d88\u9664\u6df7\u6742\u89c6\u89c9\u533a\u57df\u7684\u610f\u5916\u504f\u5dee\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u8bad\u7ec3\u6570\u636e\u548c\u663e\u5f0f\u7ec4\u6807\u7b7e\u8fdb\u884c\u5fae\u8c03\u6216\u8c03\u6574\u5d4c\u5165\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u867d\u7136\u907f\u514d\u8fd9\u4e00\u7ea6\u675f\uff0c\u4f46\u8bb8\u591a\u4ecd\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u504f\u5dee\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5728\u5f00\u653e\u96c6\u8bbe\u7f6e\u4e2d\u7684\u6cdb\u5316\u6027\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u9694\u79bb\u76ee\u6807\u89c6\u89c9\u5c5e\u6027\uff0c\u8c03\u6574\u975e\u76ee\u6807\u533a\u57df\u4f7f\u5176\u5d4c\u5165\u4e0e\u6240\u6709\u7c7b\u522b\u7279\u5b9a\u6587\u672c\u63d0\u793a\u5747\u5300\u76f8\u4f3c\uff0c\u5728\u4fdd\u7559\u76ee\u6807\u5c5e\u6027\u7684\u540c\u65f6\u6d88\u9664\u6df7\u6742\u89c6\u89c9\u533a\u57df\u7684\u504f\u5dee\u4fe1\u53f7\u3002", "result": "\u5728Waterbirds\u548cCelebA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7ec4\u9c81\u68d2\u6027\u6307\u6807\u548cAttention IoU\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5272\u5f15\u5bfc\u7684\u5e72\u9884\u63aa\u65bd\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u504f\u5dee\u7f13\u89e3\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.01359", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684NLI\u6a21\u578bMiniTruePrefixes\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u6587\u672c\u524d\u7f00\u4e2d\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u53d7\u63a7\u89e3\u7801\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u62bd\u8c61\u6458\u8981\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684NLI\u6a21\u578b\u867d\u7136\u80fd\u68c0\u6d4b\u5b8c\u6574\u53e5\u5b50\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u81ea\u56de\u5f52\u751f\u6210\u67b6\u6784\u4e2d\uff0c\u51b3\u7b56\u662f\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u9488\u5bf9\u6bcf\u4e2a\u4e0d\u65ad\u6f14\u5316\u7684\u6587\u672c\u524d\u7f00\u8fdb\u884c\u7684\u3002\u56e0\u6b64\u9700\u8981\u5c06\u8574\u6db5\u68c0\u6d4b\u4efb\u52a1\u63a8\u5e7f\u5230\u4efb\u610f\u6587\u672c\u524d\u7f00\uff0c\u4ee5\u6539\u8fdb\u751f\u6210\u5185\u5bb9\u7684\u5fe0\u5b9e\u5ea6\u3002", "method": "1. \u5c06\u8574\u6db5\u68c0\u6d4b\u4efb\u52a1\u63a8\u5e7f\u5230\u6587\u672c\u524d\u7f00\u7ea7\u522b\uff1b2. \u63d0\u4f9b\u76f8\u5e94\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u6570\u636e\u96c6\uff1b3. \u8bad\u7ec3\u4e13\u95e8\u7684MiniTruePrefixes\u6a21\u578b\uff1b4. \u5c06\u8be5\u6a21\u578b\u96c6\u6210\u5230\u53d7\u63a7\u89e3\u7801\u6846\u67b6\u4e2d\u3002", "result": "1. MiniTruePrefixes\u5728\u524d\u7f00\u7ea7\u522b\u8574\u6db5\u68c0\u6d4b\u4e0a\u6bd4\u57fa\u7ebfNLI\u6a21\u578b\u9ad8\u51fa5-14\u4e2aF1\u5206\u6570\uff1b2. \u5728\u62bd\u8c61\u6458\u8981\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528MiniTruePrefixes\u6307\u5bfc\u7684LLaMA-3.2-3B-Instruct\u6a21\u578b\u5728\u5fe0\u5b9e\u5ea6\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u4e0e\u540c\u7cfb\u5217\u76848B\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u4ec5\u4f7f\u7528\u4e00\u534a\u5185\u5b58\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9\u6587\u672c\u524d\u7f00\u8bad\u7ec3\u7684NLI\u6a21\u578b\u5e76\u5c06\u5176\u96c6\u6210\u5230\u89e3\u7801\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8LLM\u751f\u6210\u5185\u5bb9\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.00524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00524", "abs": "https://arxiv.org/abs/2511.00524", "authors": ["Jihao Gu", "Kun Li", "He Wang", "Kaan Ak\u015fit"], "title": "Text-guided Fine-Grained Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "AI": {"tldr": "\u63d0\u51faT-VAD\u6846\u67b6\uff0c\u57fa\u4e8e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u7684\u7ec6\u7c92\u5ea6\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5f02\u5e38\u70ed\u56fe\u89e3\u7801\u5668\u548c\u533a\u57df\u611f\u77e5\u5f02\u5e38\u7f16\u7801\u5668\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u4ea4\u4e92\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u534a\u81ea\u52a8\u5316\uff0c\u9700\u8981\u4eba\u5de5\u8bc4\u4f30\uff0c\u4e14\u8f93\u51fa\u4ec5\u9650\u4e8e\u6b63\u5e38\u6216\u5f02\u5e38\u4e24\u7c7b\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u548c\u4ea4\u4e92\u6027\u3002", "method": "\u6784\u5efaT-VAD\u6846\u67b6\uff0c\u5305\u542b\u5f02\u5e38\u70ed\u56fe\u89e3\u7801\u5668\u8fdb\u884c\u50cf\u7d20\u7ea7\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u751f\u6210\u70ed\u56fe\uff0c\u4ee5\u53ca\u533a\u57df\u611f\u77e5\u5f02\u5e38\u7f16\u7801\u5668\u5c06\u70ed\u56fe\u8f6c\u6362\u4e3a\u53ef\u5b66\u4e60\u6587\u672c\u5d4c\u5165\uff0c\u6307\u5bfc\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u51c6\u786e\u5b9a\u4f4d\u5f02\u5e38\u3002", "result": "\u5728UBnormal\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.8% AUC\u548c67.8%/76.7%\u70ed\u56fe\u7cbe\u5ea6\uff1b\u5728ShanghaiTech\u6570\u636e\u96c6\u4e0aBLEU-4\u8fbe62.67/88.84\uff0cYes/No\u51c6\u786e\u738797.67%\uff1b\u5728UBnormal\u6570\u636e\u96c6\u4e0aBLEU-4\u8fbe50.32/78.10\uff0cYes/No\u51c6\u786e\u738789.73%\u3002", "conclusion": "T-VAD\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u7c92\u5ea6\u548c\u4ea4\u4e92\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2511.01360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aCancer-Myth-Indic\u7684\u5370\u5ea6\u8bed\u8a00\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u591a\u8bed\u8a00\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u533b\u5b66\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u82f1\u8bed\u7684\u7a7a\u767d\u3002", "motivation": "\u968f\u7740\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u533b\u7597\u5065\u5eb7\u5efa\u8bae\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u56de\u7b54\u7684\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u3002\u73b0\u6709\u533b\u5b66\u57fa\u51c6\u51e0\u4e4e\u90fd\u662f\u82f1\u6587\u7684\uff0c\u5bfc\u81f4\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5c06Cancer-Myth\u57fa\u51c6\u7684500\u4e2a\u9879\u76ee\u5b50\u96c6\u7ffb\u8bd1\u62105\u79cd\u5370\u5ea6\u6b21\u5927\u9646\u5e38\u7528\u4f46\u670d\u52a1\u4e0d\u8db3\u7684\u8bed\u8a00\uff08\u6bcf\u79cd\u8bed\u8a00500\u9879\uff0c\u51712500\u9879\uff09\uff0c\u7531\u6bcd\u8bed\u8bd1\u8005\u6309\u7167\u98ce\u683c\u6307\u5357\u8fdb\u884c\u7ffb\u8bd1\u4ee5\u4fdd\u7559\u9690\u542b\u9884\u8bbe\uff0c\u8fd9\u4e9b\u9879\u76ee\u5305\u542b\u4e0e\u764c\u75c7\u76f8\u5173\u7684\u9519\u8bef\u9884\u8bbe\u3002", "result": "\u5728\u9884\u8bbe\u538b\u529b\u4e0b\u8bc4\u4f30\u4e86\u51e0\u4e2a\u6d41\u884c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u591a\u8bed\u8a00\u533b\u7597\u57fa\u51c6\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u975e\u82f1\u8bed\u533b\u7597\u54a8\u8be2\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u586b\u8865\u591a\u8bed\u8a00LLM\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.00540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00540", "abs": "https://arxiv.org/abs/2511.00540", "authors": ["Wenbing Zhu", "Chengjie Wang", "Bin-Bin Gao", "Jiangning Zhang", "Guannan Jiang", "Jie Hu", "Zhenye Gan", "Lidong Wang", "Ziqing Zhou", "Linjie Cheng", "Yurui Pan", "Bo Peng", "Mingmin Chi", "Lizhuang Ma"], "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era", "comment": "13 pages, 4 figures and 5 tables", "summary": "Industrial Anomaly Detection (IAD) is critical for enhancing operational\nsafety, ensuring product quality, and optimizing manufacturing efficiency\nacross global industries. However, the IAD algorithms are severely constrained\nby the limitations of existing public benchmarks. Current datasets exhibit\nrestricted category diversity and insufficient scale, frequently resulting in\nmetric saturation and limited model transferability to real-world scenarios. To\naddress this gap, we introduce Real-IAD Variety, the largest and most diverse\nIAD benchmark, comprising 198,960 high-resolution images across 160 distinct\nobject categories. Its diversity is ensured through comprehensive coverage of\n28 industries, 24 material types, and 22 color variations. Our comprehensive\nexperimental analysis validates the benchmark's substantial challenge:\nstate-of-the-art multi-class unsupervised anomaly detection methods experience\nsignificant performance degradation when scaled from 30 to 160 categories.\nCrucially, we demonstrate that vision-language models exhibit remarkable\nrobustness to category scale-up, with minimal performance variation across\ndifferent category counts, significantly enhancing generalization capabilities\nin diverse industrial contexts. The unprecedented scale and complexity of\nReal-IAD Variety position it as an essential resource for training and\nevaluating next-generation foundation models for anomaly detection. By\nproviding this comprehensive benchmark with rigorous evaluation protocols\nacross multi-class unsupervised, multi-view, and zero-/few-shot settings, we\naim to accelerate research beyond domain-specific constraints, enabling the\ndevelopment of scalable, general-purpose anomaly detection systems. Real-IAD\nVariety will be made publicly available to facilitate innovation in this\ncritical field.", "AI": {"tldr": "\u63d0\u51fa\u4e86Real-IAD Variety\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u6700\u5927\u6700\u5168\u9762\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542b198,960\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u6db5\u76d6160\u4e2a\u5bf9\u8c61\u7c7b\u522b\u300128\u4e2a\u884c\u4e1a\u300124\u79cd\u6750\u6599\u548c22\u79cd\u989c\u8272\u53d8\u5316\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7c7b\u522b\u591a\u6837\u6027\u4e0d\u8db3\u548c\u89c4\u6a21\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u53d7\u9650\u4e8e\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b58\u5728\u7c7b\u522b\u591a\u6837\u6027\u4e0d\u8db3\u3001\u89c4\u6a21\u6709\u9650\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6307\u6807\u9971\u548c\u548c\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u6784\u5efa\u4e86Real-IAD Variety\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8986\u76d628\u4e2a\u884c\u4e1a\u300124\u79cd\u6750\u6599\u7c7b\u578b\u548c22\u79cd\u989c\u8272\u53d8\u5316\u6765\u786e\u4fdd\u591a\u6837\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u7c7b\u65e0\u76d1\u7763\u3001\u591a\u89c6\u56fe\u548c\u96f6/\u5c11\u6837\u672c\u8bbe\u7f6e\u7684\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u57fa\u51c6\u7684\u6311\u6218\u6027\uff1a\u6700\u5148\u8fdb\u7684\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u7c7b\u522b\u4ece30\u6269\u5c55\u5230160\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u5bf9\u7c7b\u522b\u6269\u5c55\u7684\u663e\u8457\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u53d8\u5316\u6700\u5c0f\u3002", "conclusion": "Real-IAD Variety\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u4f7f\u5176\u6210\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e0b\u4e00\u4ee3\u5f02\u5e38\u68c0\u6d4b\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u671b\u52a0\u901f\u8d85\u8d8a\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u7684\u7814\u7a76\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2511.01223", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01223", "abs": "https://arxiv.org/abs/2511.01223", "authors": ["Zahra Mehraban", "Sebastien Glaser", "Michael Milford", "Ronald Schroeter"], "title": "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering", "comment": null, "summary": "Domain adaptation is required for automated driving models to generalize well\nacross diverse road conditions. This paper explores a training method for\ndomain adaptation to adapt PilotNet, an end-to-end deep learning-based model,\nfor left-hand driving conditions using real-world Australian highway data. Four\ntraining methods were evaluated: (1) a baseline model trained on U.S.\nright-hand driving data, (2) a model trained on flipped U.S. data, (3) a model\npretrained on U.S. data and then fine-tuned on Australian highways, and (4) a\nmodel pretrained on flipped U.S. data and then finetuned on Australian\nhighways. This setup examines whether incorporating flipped data enhances the\nmodel adaptation by providing an initial left-hand driving alignment. The paper\ncompares model performance regarding steering prediction accuracy and\nattention, using saliency-based analysis to measure attention shifts across\nsignificant road regions. Results show that pretraining on flipped data alone\nworsens prediction stability due to misaligned feature representations, but\nsignificantly improves adaptation when followed by fine-tuning, leading to\nlower prediction error and stronger focus on left-side cues. To validate this\napproach across different architectures, the same experiments were done on\nResNet, which confirmed similar adaptation trends. These findings emphasize the\nimportance of preprocessing techniques, such as flipped-data pretraining,\nfollowed by fine-tuning to improve model adaptation with minimal retraining\nrequirements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ffb\u8f6c\u7f8e\u56fd\u53f3\u9a7e\u6570\u636e\u9884\u8bad\u7ec3\u518d\u5fae\u8c03\u6fb3\u5927\u5229\u4e9a\u5de6\u9a7e\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86PilotNet\u6a21\u578b\u5728\u5de6\u9a7e\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u9700\u8981\u826f\u597d\u7684\u9886\u57df\u81ea\u9002\u5e94\u80fd\u529b\u6765\u9002\u5e94\u4e0d\u540c\u7684\u9053\u8def\u6761\u4ef6\uff0c\u7279\u522b\u662f\u5de6\u53f3\u9a7e\u9a76\u4e60\u60ef\u7684\u5dee\u5f02\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6709\u6548\u9002\u5e94PilotNet\u6a21\u578b\u5230\u5de6\u9a7e\u6761\u4ef6\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a1) \u7f8e\u56fd\u53f3\u9a7e\u6570\u636e\u57fa\u7ebf\u6a21\u578b\uff1b2) \u7ffb\u8f6c\u7f8e\u56fd\u6570\u636e\u8bad\u7ec3\uff1b3) \u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3+\u6fb3\u5927\u5229\u4e9a\u9ad8\u901f\u5fae\u8c03\uff1b4) \u7ffb\u8f6c\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3+\u6fb3\u5927\u5229\u4e9a\u9ad8\u901f\u5fae\u8c03\u3002\u4f7f\u7528\u663e\u8457\u6027\u5206\u6790\u8bc4\u4f30\u6ce8\u610f\u529b\u53d8\u5316\u3002", "result": "\u4ec5\u4f7f\u7528\u7ffb\u8f6c\u6570\u636e\u9884\u8bad\u7ec3\u4f1a\u964d\u4f4e\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u4f46\u7ffb\u8f6c\u9884\u8bad\u7ec3\u540e\u5fae\u8c03\u663e\u8457\u6539\u5584\u4e86\u9002\u5e94\u6548\u679c\uff0c\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u5e76\u589e\u5f3a\u4e86\u5bf9\u5de6\u4fa7\u7ebf\u7d22\u7684\u5173\u6ce8\u3002\u5728ResNet\u4e0a\u7684\u9a8c\u8bc1\u4e5f\u663e\u793a\u4e86\u76f8\u4f3c\u8d8b\u52bf\u3002", "conclusion": "\u7ffb\u8f6c\u6570\u636e\u9884\u8bad\u7ec3\u7ed3\u5408\u5fae\u8c03\u662f\u4e00\u79cd\u6709\u6548\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u518d\u8bad\u7ec3\u9700\u6c42\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2511.01365", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["\u0130brahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u8d85\u8d8a\u57fa\u51c6\u662f\u5426\u771f\u6b63\u4f53\u73b0\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5728\u8ffd\u8e2a\u4e0e\u58f0\u79f0\u80fd\u529b\u8131\u8282\u7684\u6570\u5b57\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u57fa\u51c6\u6d4b\u8bd5\u4e5f\u5728\u5feb\u901f\u589e\u52a0\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\u548c\u8bad\u7ec3\u6570\u636e\u7684\u5305\u542b\uff0c\u7ed3\u679c\u8d8b\u4e8e\u9971\u548c\uff0c\u9700\u8981\u4e0d\u65ad\u5bfb\u627e\u65b0\u7684\u66f4\u5177\u6311\u6218\u6027\u7684\u66ff\u4ee3\u57fa\u51c6\u3002", "method": "\u7814\u7a76\u805a\u7126\u4e8eOpenAI\u3001Anthropic\u548cGoogle\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\uff0c\u5206\u6790\u5b83\u4eec\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a8\u7406\u80fd\u529b\u968f\u65f6\u95f4\u7684\u6f14\u53d8\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u63a8\u7406\u4efb\u52a1\u7684\u5168\u9762\u6982\u8ff0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u7684\u73b0\u72b6\u548c\u5269\u4f59\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u4e3a\u672a\u6765\u63a8\u7406\u8bc4\u4f30\u548c\u6a21\u578b\u5f00\u53d1\u7814\u7a76\u63d0\u4f9b\u9996\u4e2a\u53c2\u8003\u57fa\u7840\uff0c\u5e2e\u52a9\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u7684\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.00542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00542", "abs": "https://arxiv.org/abs/2511.00542", "authors": ["Kailun Su", "Ziqi He", "Xi Wang", "Yang Zhou"], "title": "MIFO: Learning and Synthesizing Multi-Instance from One Image", "comment": "17 pages, 30 figures", "summary": "This paper proposes a method for precise learning and synthesizing\nmulti-instance semantics from a single image. The difficulty of this problem\nlies in the limited training data, and it becomes even more challenging when\nthe instances to be learned have similar semantics or appearance. To address\nthis, we propose a penalty-based attention optimization to disentangle similar\nsemantics during the learning stage. Then, in the synthesis, we introduce and\noptimize box control in attention layers to further mitigate semantic leakage\nwhile precisely controlling the output layout. Experimental results demonstrate\nthat our method achieves disentangled and high-quality semantic learning and\nsynthesis, strikingly balancing editability and instance consistency. Our\nmethod remains robust when dealing with semantically or visually similar\ninstances or rare-seen objects. The code is publicly available at\nhttps://github.com/Kareneveve/MIFO", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u7cbe\u786e\u5b66\u4e60\u548c\u5408\u6210\u591a\u5b9e\u4f8b\u8bed\u4e49\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u60e9\u7f5a\u7684\u6ce8\u610f\u529b\u4f18\u5316\u6765\u89e3\u8026\u76f8\u4f3c\u8bed\u4e49\uff0c\u5e76\u5728\u5408\u6210\u9636\u6bb5\u5f15\u5165\u6ce8\u610f\u529b\u5c42\u7684\u6846\u63a7\u5236\u6765\u7f13\u89e3\u8bed\u4e49\u6cc4\u6f0f\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u5f20\u56fe\u50cf\u4e2d\u5b66\u4e60\u591a\u5b9e\u4f8b\u8bed\u4e49\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b9e\u4f8b\u5177\u6709\u76f8\u4f3c\u8bed\u4e49\u6216\u5916\u89c2\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u8bed\u4e49\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u60e9\u7f5a\u7684\u6ce8\u610f\u529b\u4f18\u5316\u6765\u5728\u8bad\u7ec3\u9636\u6bb5\u89e3\u8026\u76f8\u4f3c\u8bed\u4e49\uff0c\u5e76\u5728\u5408\u6210\u9636\u6bb5\u5f15\u5165\u548c\u4f18\u5316\u6ce8\u610f\u529b\u5c42\u7684\u6846\u63a7\u5236\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u8026\u8bed\u4e49\u5b66\u4e60\u548c\u5408\u6210\uff0c\u5728\u53ef\u7f16\u8f91\u6027\u548c\u5b9e\u4f8b\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5bf9\u8bed\u4e49\u6216\u89c6\u89c9\u76f8\u4f3c\u5b9e\u4f8b\u4ee5\u53ca\u7f55\u89c1\u7269\u4f53\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u5b9e\u4f8b\u8bed\u4e49\u5b66\u4e60\u548c\u5408\u6210\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u76f8\u4f3c\u8bed\u4e49\u6216\u7f55\u89c1\u7269\u4f53\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.01381", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01381", "abs": "https://arxiv.org/abs/2511.01381", "authors": ["Hitesh Kyatham", "Arjun Suresh", "Aadi Palnitkar", "Yiannis Aloimonos"], "title": "EREBUS: End-to-end Robust Event Based Underwater Simulation", "comment": "Accepted to ICRA AQUA2SIM Workshop 2025, 6 pages, 3 figures,\n  conference paper", "summary": "The underwater domain presents a vast array of challenges for roboticists and\ncomputer vision researchers alike, such as poor lighting conditions and high\ndynamic range scenes. In these adverse conditions, traditional vision\ntechniques struggle to adapt and lead to suboptimal performance. Event-based\ncameras present an attractive solution to this problem, mitigating the issues\nof traditional cameras by tracking changes in the footage on a frame-by-frame\nbasis. In this paper, we introduce a pipeline which can be used to generate\nrealistic synthetic data of an event-based camera mounted to an AUV (Autonomous\nUnderwater Vehicle) in an underwater environment for training vision models. We\ndemonstrate the effectiveness of our pipeline using the task of rock detection\nwith poor visibility and suspended particulate matter, but the approach can be\ngeneralized to other underwater tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u6c34\u4e0b\u73af\u5883\u4e2d\u5b89\u88c5\u5728AUV\u4e0a\u7684\u4e8b\u4ef6\u76f8\u673a\u771f\u5b9e\u5408\u6210\u6570\u636e\u7684\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u6076\u52a3\u6c34\u4e0b\u6761\u4ef6\uff08\u5982\u80fd\u89c1\u5ea6\u5dee\u548c\u60ac\u6d6e\u9897\u7c92\u7269\uff09\u6765\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u7279\u522b\u4ee5\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e3a\u4f8b\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u6280\u672f\u5728\u6c34\u4e0b\u6076\u52a3\u6761\u4ef6\uff08\u5982\u5149\u7167\u4e0d\u8db3\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4e8b\u4ef6\u76f8\u673a\u901a\u8fc7\u9010\u5e27\u8ddf\u8e2a\u53d8\u5316\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002\u9700\u8981\u5408\u6210\u6570\u636e\u6765\u8bad\u7ec3\u6c34\u4e0b\u89c6\u89c9\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6d41\u6c34\u7ebf\u6765\u751f\u6210\u5b89\u88c5\u5728AUV\u4e0a\u7684\u4e8b\u4ef6\u76f8\u673a\u7684\u771f\u5b9e\u5408\u6210\u6570\u636e\uff0c\u6a21\u62df\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6076\u52a3\u6761\u4ef6\uff0c\u5305\u62ec\u80fd\u89c1\u5ea6\u5dee\u548c\u60ac\u6d6e\u9897\u7c92\u7269\u3002", "result": "\u8be5\u6d41\u6c34\u7ebf\u5728\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u6076\u52a3\u7684\u6c34\u4e0b\u89c6\u89c9\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bad\u7ec3\u6c34\u4e0b\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u6c34\u4e0b\u4efb\u52a1\u3002"}}
{"id": "2511.00560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00560", "abs": "https://arxiv.org/abs/2511.00560", "authors": ["Chun-Tin Wu", "Jun-Cheng Chen"], "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting", "comment": "10 pages, 7 figures", "summary": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel\nview synthesis, extending it to dynamic scenes still results in substantial\nmemory overhead from replicating Gaussians across frames. To address this\nchallenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines\nvoxel-based representations with neural Gaussian splatting for efficient\ndynamic scene modeling. Instead of generating separate Gaussian sets per\ntimestamp, our method employs a compact set of neural voxels with learned\ndeformation fields to model temporal dynamics. The design greatly reduces\nmemory consumption and accelerates training while preserving high image\nquality. We further introduce a novel view refinement stage that selectively\nimproves challenging viewpoints through targeted optimization, maintaining\nglobal efficiency while enhancing rendering quality for difficult viewing\nangles. Experiments demonstrate that our method outperforms state-of-the-art\napproaches with significant memory reduction and faster training, enabling\nreal-time rendering with superior visual fidelity.", "AI": {"tldr": "4D-NVS\u7ed3\u5408\u4f53\u7d20\u8868\u793a\u4e0e\u795e\u7ecf\u9ad8\u65af\u6cfc\u6e85\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u795e\u7ecf\u4f53\u7d20\u548c\u53d8\u5f62\u573a\u5efa\u6a21\u52a8\u6001\u573a\u666f\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u5e76\u52a0\u901f\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u52a8\u6001\u573a\u666f\u4e2d\u56e0\u8de8\u5e27\u590d\u5236\u9ad8\u65af\u51fd\u6570\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u52a8\u6001\u573a\u666f\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u7684\u795e\u7ecf\u4f53\u7d20\u96c6\u5408\u548c\u5b66\u4e60\u7684\u53d8\u5f62\u573a\u6765\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6233\u751f\u6210\u5355\u72ec\u7684\u9ad8\u65af\u96c6\u5408\uff1b\u5f15\u5165\u89c6\u89d2\u7ec6\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4f18\u5316\u6539\u8fdb\u6311\u6218\u6027\u89c6\u89d2\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u51cf\u5c11\u5185\u5b58\u548c\u52a0\u901f\u8bad\u7ec3\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u548c\u4f18\u8d8a\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "4D-NVS\u901a\u8fc7\u4f53\u7d20\u8868\u793a\u548c\u795e\u7ecf\u9ad8\u65af\u6cfc\u6e85\u7684\u7ed3\u5408\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u5185\u5b58\u9ad8\u6548\u4e14\u8bad\u7ec3\u5feb\u901f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2511.01386", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevin\u00e7", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization.", "AI": {"tldr": "RAGSmith\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u9057\u4f20\u641c\u7d22\u572846,080\u4e2a\u53ef\u884c\u7ba1\u9053\u914d\u7f6e\u4e2d\u4f18\u5316RAG\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u5728\u516d\u4e2a\u9886\u57df\u5e73\u5747\u63d0\u53473.8%\u6027\u80fd\u3002", "motivation": "RAG\u8d28\u91cf\u53d7\u591a\u4e2a\u76f8\u4e92\u5f71\u54cd\u7684\u6a21\u5757\u9009\u62e9\u5f71\u54cd\uff0c\u5b64\u7acb\u4f18\u5316\u6a21\u5757\u5b58\u5728\u8106\u5f31\u6027\uff0c\u9700\u8981\u7aef\u5230\u7aef\u7684\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u641c\u7d22\u57289\u4e2a\u6280\u672f\u5bb6\u65cf\u548c46,080\u4e2a\u53ef\u884c\u7ba1\u9053\u914d\u7f6e\u4e2d\u8fdb\u884c\u7aef\u5230\u7aef\u67b6\u6784\u641c\u7d22\uff0c\u4f18\u5316\u7ed3\u5408\u68c0\u7d22\u6307\u6807\u548c\u751f\u6210\u6307\u6807\u7684\u6807\u91cf\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u516d\u4e2a\u7ef4\u57fa\u767e\u79d1\u9886\u57df\u6d4b\u8bd5\u4e2d\uff0cRAGSmith\u627e\u5230\u7684\u914d\u7f6e\u5e73\u5747\u6bd4\u6734\u7d20RAG\u57fa\u7ebf\u63d0\u53473.8%\uff08\u8303\u56f41.2%-6.9%\uff09\uff0c\u68c0\u7d22\u548c\u751f\u6210\u5206\u522b\u63d0\u5347\u8fbe12.5%\u548c7.5%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u8fdb\u5316\u641c\u7d22\u5728\u5168\u7ba1\u9053\u4f18\u5316\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u53d1\u73b0\u4e86\u7a33\u5065\u7684\u9aa8\u5e72\u67b6\u6784\uff08\u5411\u91cf\u68c0\u7d22\u52a0\u540e\u751f\u6210\u53cd\u601d/\u4fee\u8ba2\uff09\uff0c\u5e76\u4e3a\u6784\u5efa\u6709\u6548RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9886\u57df\u611f\u77e5\u6307\u5bfc\u3002"}}
{"id": "2511.00573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00573", "abs": "https://arxiv.org/abs/2511.00573", "authors": ["Wei Feng", "Zongyuan Ge"], "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective", "comment": "29 pages, 5 figures", "summary": "Generalized Category Discovery (GCD) aims to leverage labeled samples from\nknown categories to cluster unlabeled data that may include both known and\nunknown categories. While existing methods have achieved impressive results\nunder standard conditions, their performance often deteriorates in the presence\nof distribution shifts. In this paper, we explore a more realistic task:\nDomain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled\ndata includes not only unknown categories but also samples from unknown\ndomains. To tackle this challenge, we propose a\n\\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized\nCat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE)\nthat enhances the model's ability to discover categories under distributional\nshift by leveraging frequency-domain information. Specifically, we first\npropose a frequency-based domain separation strategy that partitions samples\ninto known and unknown domains by measuring their amplitude differences. We\nthen propose two types of frequency-domain perturbation strategies: a\ncross-domain strategy, which adapts to new distributions by exchanging\namplitude components across domains, and an intra-domain strategy, which\nenhances robustness to intra-domain variations within the unknown domain.\nFurthermore, we extend the self-supervised contrastive objective and semantic\nclustering loss to better guide the training process. Finally, we introduce a\nclustering-difficulty-aware resampling technique to adaptively focus on\nharder-to-cluster categories, further enhancing model performance. Extensive\nexperiments demonstrate that our method effectively mitigates the impact of\ndistributional shifts across various benchmark datasets and achieves superior\nperformance in discovering both known and unknown categories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9891\u7387\u5f15\u5bfc\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u6846\u67b6\uff08FREE\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u9886\u57df\u504f\u79fb\u4e0b\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u95ee\u9898\uff0c\u901a\u8fc7\u9891\u57df\u4fe1\u606f\u589e\u5f3a\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u53d1\u73b0\u7c7b\u522b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u65b9\u6cd5\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u65f6\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u4e2a\u66f4\u73b0\u5b9e\u7684\u4efb\u52a1\uff1a\u9886\u57df\u504f\u79fb\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08DS_GCD\uff09\uff0c\u5176\u4e2d\u672a\u6807\u8bb0\u6570\u636e\u4e0d\u4ec5\u5305\u542b\u672a\u77e5\u7c7b\u522b\uff0c\u8fd8\u5305\u542b\u6765\u81ea\u672a\u77e5\u9886\u57df\u7684\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86FREE\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u9891\u7387\u7684\u9886\u57df\u5206\u79bb\u7b56\u7565\uff0c\u901a\u8fc7\u6d4b\u91cf\u5e45\u5ea6\u5dee\u5f02\u5c06\u6837\u672c\u5212\u5206\u4e3a\u5df2\u77e5\u548c\u672a\u77e5\u9886\u57df\uff1b2\uff09\u8de8\u9886\u57df\u548c\u9886\u57df\u5185\u9891\u7387\u6270\u52a8\u7b56\u7565\uff1b3\uff09\u6269\u5c55\u7684\u81ea\u76d1\u7763\u5bf9\u6bd4\u76ee\u6807\u548c\u8bed\u4e49\u805a\u7c7b\u635f\u5931\uff1b4\uff09\u805a\u7c7b\u96be\u5ea6\u611f\u77e5\u7684\u91cd\u91c7\u6837\u6280\u672f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u8f7b\u4e86\u5206\u5e03\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u66f4\u597d\u5730\u53d1\u73b0\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u3002", "conclusion": "FREE\u6846\u67b6\u901a\u8fc7\u5229\u7528\u9891\u57df\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u6761\u4ef6\u4e0b\u53d1\u73b0\u7c7b\u522b\u7684\u80fd\u529b\uff0c\u4e3a\u9886\u57df\u504f\u79fb\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01502", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01502", "abs": "https://arxiv.org/abs/2511.01502", "authors": ["Mengtan Zhang", "Zizhan Guo", "Hongbo Zhao", "Yi Feng", "Zuyi Xiong", "Yue Wang", "Shaoyi Du", "Hanli Wang", "Rui Fan"], "title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning", "comment": "18 pages, 14 figures", "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u548c\u81ea\u8fd0\u52a8\u8054\u5408\u5b66\u4e60\u6846\u67b6DiMoDE\uff0c\u901a\u8fc7\u5bf9\u8fd0\u52a8\u5206\u91cf\u8fdb\u884c\u533a\u5206\u5904\u7406\uff0c\u5229\u7528\u5404\u81ea\u7684\u521a\u6027\u6d41\u51e0\u4f55\u89c4\u5f8b\u6765\u6539\u8fdb\u6df1\u5ea6\u548c\u81ea\u8fd0\u52a8\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u81ea\u8fd0\u52a8\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\uff0c\u8981\u4e48\u6df7\u5408\u6240\u6709\u8fd0\u52a8\u7c7b\u578b\uff0c\u8981\u4e48\u6392\u9664\u4e0e\u6df1\u5ea6\u65e0\u5173\u7684\u65cb\u8f6c\u8fd0\u52a8\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u9650\u5236\u4e86\u5f3a\u51e0\u4f55\u7ea6\u675f\u7684\u5f15\u5165\uff0c\u964d\u4f4e\u4e86\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u9996\u5148\u5bf9\u9f50\u6e90\u76f8\u673a\u548c\u76ee\u6807\u76f8\u673a\u7684\u5149\u8f74\u548c\u6210\u50cf\u5e73\u9762\uff0c\u5c06\u5e27\u95f4\u5149\u6d41\u901a\u8fc7\u8fd9\u4e9b\u5bf9\u9f50\u8fdb\u884c\u53d8\u6362\uff0c\u5e76\u91cf\u5316\u504f\u5dee\u4ee5\u5bf9\u6bcf\u4e2a\u81ea\u8fd0\u52a8\u5206\u91cf\u5355\u72ec\u65bd\u52a0\u51e0\u4f55\u7ea6\u675f\u3002\u8fd9\u4e9b\u5bf9\u9f50\u8fdb\u4e00\u6b65\u5c06\u8054\u5408\u5b66\u4e60\u8fc7\u7a0b\u91cd\u65b0\u8868\u8ff0\u4e3a\u540c\u8f74\u548c\u5171\u9762\u5f62\u5f0f\uff0c\u5176\u4e2d\u6df1\u5ea6\u548c\u6bcf\u4e2a\u5e73\u79fb\u5206\u91cf\u53ef\u4ee5\u901a\u8fc7\u95ed\u5f0f\u51e0\u4f55\u5173\u7cfb\u76f8\u4e92\u63a8\u5bfc\u3002", "result": "DiMoDE\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u65b0\u6536\u96c6\u7684\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u901a\u8fc7\u533a\u5206\u5904\u7406\u8fd0\u52a8\u5206\u91cf\u5e76\u5f15\u5165\u4e92\u8865\u7684\u51e0\u4f55\u7ea6\u675f\uff0cDiMoDE\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u548c\u81ea\u8fd0\u52a8\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.01409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge.", "AI": {"tldr": "LiveSearchBench\u662f\u4e00\u4e2a\u81ea\u52a8\u6784\u5efa\u68c0\u7d22\u4f9d\u8d56\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u5bf9\u6bd4Wikidata\u5feb\u7167\u53d8\u5316\u751f\u6210\u57fa\u4e8e\u6700\u65b0\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u8bc4\u4f30LLMs\u5728\u52a8\u6001\u77e5\u8bc6\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5956\u52b1\u8bb0\u5fc6\u800c\u975e\u68c0\u7d22\uff0c\u65e0\u6cd5\u6355\u6349\u4e16\u754c\u77e5\u8bc6\u7684\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30LLMs\u5728\u6700\u65b0\u77e5\u8bc6\u4e0b\u8868\u73b0\u7684\u52a8\u6001\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97Wikidata\u8fde\u7eed\u5feb\u7167\u7684\u5dee\u5f02\uff0c\u7b5b\u9009\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\uff0c\u751f\u6210\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u5e76\u4f7f\u7528SPARQL\u9a8c\u8bc1\u786e\u4fdd\u7b54\u6848\u552f\u4e00\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u9762\u5bf9\u9884\u8bad\u7ec3\u540e\u65b0\u4e8b\u5b9e\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u591a\u8df3\u67e5\u8be2\u5dee\u8ddd\u6700\u660e\u663e\u3002\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u548c\u66f4\u5927\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u53ea\u80fd\u90e8\u5206\u7f13\u89e3\u4f46\u65e0\u6cd5\u6d88\u9664\u8fd9\u79cd\u65f6\u6548\u6027\u5dee\u8ddd\u3002", "conclusion": "LiveSearchBench\u5c06\u8bc4\u4f30\u4ece\u9759\u6001\u8bb0\u5fc6\u8f6c\u5411\u9700\u8981\u6700\u65b0\u68c0\u7d22\u548c\u63a8\u7406\u7684\u4efb\u52a1\uff0c\u4e3a\u7cfb\u7edf\u5316\u957f\u671f\u8bc4\u4f30LLMs\u5728\u6f14\u5316\u77e5\u8bc6\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.00580", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.5.4; I.4.8; C.3"], "pdf": "https://arxiv.org/pdf/2511.00580", "abs": "https://arxiv.org/abs/2511.00580", "authors": ["Yousuf Ahmed Siddiqui", "Sufiyaan Usmani", "Umer Tariq", "Jawwad Ahmed Shamsi", "Muhammad Burhan Khan"], "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "comment": "10 pages, 5 figures", "summary": "Video anomalies often depend on contextual information available and temporal\nevolution. Non-anomalous action in one context can be anomalous in some other\ncontext. Most anomaly detectors, however, do not notice this type of context,\nwhich seriously limits their capability to generalize to new, real-life\nsituations. Our work addresses the context-aware zero-shot anomaly detection\nchallenge, in which systems need to learn adaptively to detect new events by\ncorrelating temporal and appearance features with textual traces of memory in\nreal time. Our approach defines a memory-augmented pipeline, correlating\ntemporal signals with visual embeddings using cross-attention, and real-time\nzero-shot anomaly classification by contextual similarity scoring. We achieve\n90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art\namong zero-shot models. Our model achieves real-time inference with high\nprecision and explainability for deployment. We show that, by fusing\ncross-attention temporal fusion and contextual memory, we achieve high fidelity\nanomaly detection, a step towards the applicability of zero-shot models in\nreal-world surveillance and infrastructure monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u8de8\u6ce8\u610f\u529b\u65f6\u5e8f\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u5728UCF-Crime\u548cXD-Violence\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u5f02\u5e38\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u65f6\u5e8f\u6f14\u5316\uff0c\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u5668\u5927\u591a\u672a\u80fd\u6ce8\u610f\u5230\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8bb0\u5fc6\u589e\u5f3a\u7ba1\u9053\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u5c06\u65f6\u5e8f\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5d4c\u5165\u76f8\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u6027\u8bc4\u5206\u8fdb\u884c\u5b9e\u65f6\u96f6\u6837\u672c\u5f02\u5e38\u5206\u7c7b\u3002", "result": "\u5728UCF-Crime\u4e0a\u8fbe\u523090.4% AUC\uff0c\u5728XD-Violence\u4e0a\u8fbe\u523083.67% AP\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\u5e76\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u8de8\u6ce8\u610f\u529b\u65f6\u5e8f\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u96f6\u6837\u672c\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u76d1\u63a7\u548c\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.01571", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01571", "abs": "https://arxiv.org/abs/2511.01571", "authors": ["Wenqi Liang", "Gan Sun", "Yao He", "Jiahua Dong", "Suyan Dai", "Ivan Laptev", "Salman Khan", "Yang Cong"], "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model", "comment": "17pages,7 figures, 5 tabels", "summary": "Vision-Language-Action models (VLAs) are emerging as powerful tools for\nlearning generalizable visuomotor control policies. However, current VLAs are\nmostly trained on large-scale image-text-action data and remain limited in two\nkey ways: (i) they struggle with pixel-level scene understanding, and (ii) they\nrely heavily on textual prompts, which reduces their flexibility in real-world\nsettings. To address these challenges, we introduce PixelVLA, the first VLA\nmodel designed to support both pixel-level reasoning and multimodal prompting\nwith text and visual inputs. Our approach is built on a new visuomotor\ninstruction tuning framework that integrates a multiscale pixel-aware encoder\nwith a visual prompting encoder. To train PixelVLA effectively, we further\npropose a two-stage automated annotation pipeline that generates Pixel-160K, a\nlarge-scale dataset with pixel-level annotations derived from existing robot\ndata. Experiments on three standard VLA benchmarks and two VLA model variants\nshow that PixelVLA improves manipulation success rates by 10.1%-17.8% over\nOpenVLA, while requiring only 1.5% of its pretraining cost. These results\ndemonstrate that PixelVLA can be integrated into existing VLAs to enable more\naccurate, efficient, and versatile robot control in complex environments. The\ndataset and code will be released as open source.", "AI": {"tldr": "PixelVLA\u662f\u9996\u4e2a\u652f\u6301\u50cf\u7d20\u7ea7\u63a8\u7406\u548c\u591a\u6a21\u6001\u63d0\u793a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u751f\u6210Pixel-160K\u6570\u636e\u96c6\uff0c\u5728\u4e09\u4e2a\u6807\u51c6VLA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4OpenVLA\u5c06\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u5347\u4e8610.1%-17.8%\uff0c\u540c\u65f6\u4ec5\u9700\u51761.5%\u7684\u9884\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(i)\u96be\u4ee5\u8fdb\u884c\u50cf\u7d20\u7ea7\u573a\u666f\u7406\u89e3\uff0c(ii)\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u89c6\u89c9\u8fd0\u52a8\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u96c6\u6210\u591a\u5c3a\u5ea6\u50cf\u7d20\u611f\u77e5\u7f16\u7801\u5668\u548c\u89c6\u89c9\u63d0\u793a\u7f16\u7801\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u751f\u6210Pixel-160K\u5927\u89c4\u6a21\u50cf\u7d20\u7ea7\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6VLA\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2aVLA\u6a21\u578b\u53d8\u4f53\u4e0a\uff0cPixelVLA\u76f8\u6bd4OpenVLA\u5c06\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u5347\u4e8610.1%-17.8%\uff0c\u540c\u65f6\u9884\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u540e\u8005\u76841.5%\u3002", "conclusion": "PixelVLA\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709VLA\u4e2d\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u63a7\u5236\u3002"}}
{"id": "2511.01454", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u590d\u73b0\u7684\u57fa\u4e8e\u8349\u7a3f\u4f18\u5316\u7684\u7ffb\u8bd1\u6d41\u7a0b\uff0c\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u62c9\u4e01\u8bed\uff09\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u9876\u7ea7\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u7ffb\u8bd1\u5f62\u6001\u4e30\u5bcc\u3001\u8d44\u6e90\u7a00\u7f3a\u7684\u8bed\u8a00\u5982\u62c9\u4e01\u8bed\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e0e\u4e13\u6709\u7cfb\u7edf\u7ade\u4e89\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684NLLB-1.3B\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u6784\u5fe0\u5b9e\u8349\u7a3f\uff0c\u7136\u540e\u901a\u8fc7\u96f6\u6837\u672cLLM\uff08Llama-3.3\u6216Qwen3\uff09\u8fdb\u884c\u4f18\u5316\uff0c\u53ef\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a33\u5065\uff1a\u6807\u51c6\u57df\u5185\u6d4b\u8bd5\u96c6\u548c\u65b0\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u57df\u591612\u4e16\u7eaa\u62c9\u4e01\u8bed\u4fe1\u4ef6\u96c6\u3002\u5f00\u6e90RAG\u7cfb\u7edf\u6027\u80fd\u4e0eGPT-5\u57fa\u7ebf\u7edf\u8ba1\u76f8\u5f53\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684LLM\u5fae\u8c03\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f00\u6e90RAG\u7cfb\u7edf\u5728\u62c9\u4e01\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u9876\u7ea7\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00613", "abs": "https://arxiv.org/abs/2511.00613", "authors": ["Yating Yu", "Congqi Cao", "Zhaoying Wang", "Weihua Meng", "Jie Li", "Yuxin Li", "Zihao Wei", "Zhongpei Shen", "Jiajun Zhang"], "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "comment": null, "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.", "AI": {"tldr": "CueBench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u4e2d\u5fc3\u5316\u5206\u5c42\u5206\u7c7b\u6cd5\u5b9a\u4e49\u4e8614\u79cd\u6761\u4ef6\u5f02\u5e38\u548c18\u79cd\u7edd\u5bf9\u5f02\u5e38\u4e8b\u4ef6\uff0c\u6db5\u76d6174\u4e2a\u573a\u666f\u548c198\u4e2a\u5c5e\u6027\uff0c\u7edf\u4e00\u4e86\u8bc6\u522b\u3001\u65f6\u5e8f\u5b9a\u4f4d\u3001\u68c0\u6d4b\u548c\u9884\u6d4b\u7b49\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5f02\u5e38\u7684\u7406\u89e3\u4ecd\u505c\u7559\u5728\u8868\u9762\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u539f\u7406\u548c\u5fae\u5999\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u6cd5\u533a\u5206\u5982\u5e26\u5b89\u5168\u88c5\u5907\u4e0e\u4e0d\u5e26\u5b89\u5168\u88c5\u5907\u6500\u5ca9\u7b49\u7ec6\u5fae\u5dee\u5f02\u3002", "method": "\u63d0\u51faCueBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5efa\u7acb\u4e8b\u4ef6\u4e2d\u5fc3\u5316\u5206\u5c42\u5206\u7c7b\u6cd5\uff1b\u5f00\u53d1Cue-R1\u6a21\u578b\uff0c\u57fa\u4e8eR1\u98ce\u683c\u5f3a\u5316\u5fae\u8c03\uff0c\u91c7\u7528\u53ef\u9a8c\u8bc1\u3001\u4efb\u52a1\u5bf9\u9f50\u548c\u5c42\u6b21\u7ec6\u5316\u7684\u5956\u52b1\u673a\u5236\u8fdb\u884c\u7edf\u4e00\u751f\u6210\u5f0f\u8bad\u7ec3\u3002", "result": "\u5728CueBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7406\u89e3\u4ecd\u4e0d\u4ee4\u4eba\u6ee1\u610f\uff0c\u800cCue-R1\u6a21\u578b\u5e73\u5747\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u8d85\u8fc724%\u3002", "conclusion": "\u6df1\u5ea6\u6a21\u578b\u8ddd\u79bb\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4ecd\u6709\u5f88\u5927\u5dee\u8ddd\uff0cCueBench\u4e3a\u8bc4\u4f30\u4e0a\u4e0b\u6587\u611f\u77e5\u5f02\u5e38\u7406\u89e3\u63d0\u4f9b\u4e86\u4e25\u8c28\u6846\u67b6\uff0cCue-R1\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.01755", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01755", "abs": "https://arxiv.org/abs/2511.01755", "authors": ["Rong Li", "Yuhao Dong", "Tianshuai Hu", "Ao Liang", "Youquan Liu", "Dongyue Lu", "Liang Pan", "Lingdong Kong", "Junwei Liang", "Ziwei Liu"], "title": "3EED: Ground Everything Everywhere in 3D", "comment": "NeurIPS 2025 DB Track; 29 pages, 17 figures, 10 tables; Project Page\n  at https://project-3eed.github.io/", "summary": "Visual grounding in 3D is the key for embodied agents to localize\nlanguage-referred objects in open-world environments. However, existing\nbenchmarks are limited to indoor focus, single-platform constraints, and small\nscale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark\nfeaturing RGB and LiDAR data from vehicle, drone, and quadruped platforms. We\nprovide over 128,000 objects and 22,000 validated referring expressions across\ndiverse outdoor scenes -- 10x larger than existing datasets. We develop a\nscalable annotation pipeline combining vision-language model prompting with\nhuman verification to ensure high-quality spatial grounding. To support\ncross-platform learning, we propose platform-aware normalization and\ncross-modal alignment techniques, and establish benchmark protocols for\nin-domain and cross-platform evaluations. Our findings reveal significant\nperformance gaps, highlighting the challenges and opportunities of\ngeneralizable 3D grounding. The 3EED dataset and benchmark toolkit are released\nto advance future research in language-driven 3D embodied perception.", "AI": {"tldr": "3EED\u662f\u4e00\u4e2a\u591a\u5e73\u53f0\u3001\u591a\u6a21\u6001\u76843D\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u7684RGB\u548cLiDAR\u6570\u636e\uff0c\u63d0\u4f9b\u8d85\u8fc7128,000\u4e2a\u5bf9\u8c61\u548c22,000\u4e2a\u9a8c\u8bc1\u8fc7\u7684\u6307\u4ee3\u8868\u8fbe\u5f0f\uff0c\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u592710\u500d\u3002", "motivation": "\u73b0\u6709\u76843D\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u5c40\u9650\u4e8e\u5ba4\u5185\u73af\u5883\u3001\u5355\u4e00\u5e73\u53f0\u7ea6\u675f\u548c\u5c0f\u89c4\u6a21\uff0c\u9700\u8981\u6784\u5efa\u5927\u89c4\u6a21\u3001\u591a\u5e73\u53f0\u7684\u5ba4\u59163D\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6765\u63a8\u52a8\u8bed\u8a00\u9a71\u52a8\u76843D\u5177\u8eab\u611f\u77e5\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u548c\u4eba\u5de5\u9a8c\u8bc1\u6765\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u7a7a\u95f4\u5b9a\u4f4d\uff1b\u63d0\u51fa\u4e86\u5e73\u53f0\u611f\u77e5\u5f52\u4e00\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6280\u672f\u6765\u652f\u6301\u8de8\u5e73\u53f0\u5b66\u4e60\uff1b\u5efa\u7acb\u4e86\u57df\u5185\u548c\u8de8\u5e73\u53f0\u8bc4\u4f30\u7684\u57fa\u51c6\u534f\u8bae\u3002", "result": "\u6784\u5efa\u4e86\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u5ba4\u59163D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u53ef\u6cdb\u53163D\u89c6\u89c9\u5b9a\u4f4d\u7684\u6311\u6218\u548c\u673a\u9047\u3002", "conclusion": "3EED\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5de5\u5177\u5305\u7684\u53d1\u5e03\u5c06\u63a8\u52a8\u8bed\u8a00\u9a71\u52a8\u76843D\u5177\u8eab\u611f\u77e5\u7684\u672a\u6765\u7814\u7a76\uff0c\u4e3a\u8de8\u5e73\u53f03D\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.01470", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets.", "AI": {"tldr": "BARD\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08SFT\u548cRL\uff09\u5c06\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\uff0c\u540c\u65f6\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u9884\u7b97\u4fe1\u53f7\u7cbe\u786e\u63a7\u5236\u63a8\u7406\u957f\u5ea6\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCoT\u84b8\u998f\u4e2d\u63a8\u7406\u8fc7\u7a0b\u5197\u4f59\u4e14\u8ba1\u7b97\u9884\u7b97\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u3002", "method": "\u63d0\u51faBARD\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u6559\u5e08\u751f\u6210\u7684\u957fCoT\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u540c\u65f6\u8003\u8651\u63a8\u7406\u6027\u80fd\u548c\u9884\u7b97\u4fdd\u771f\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c8B\u5b66\u751f\u6a21\u578b\u5728AIME24\u3001AIME25\u3001GPQA\u7b49\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u8de8\u5e7f\u6cdb\u9884\u7b97\u8303\u56f4\u7cbe\u786e\u81ea\u9002\u5e94\u63a7\u5236\u63a8\u7406\u957f\u5ea6\u3002", "conclusion": "BARD\u6210\u529f\u5b9e\u73b0\u4e86\u63a8\u7406\u80fd\u529b\u84b8\u998f\u4e0e\u63a8\u7406\u957f\u5ea6\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u7edf\u4e00\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.00643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00643", "abs": "https://arxiv.org/abs/2511.00643", "authors": ["Oluwatosin Alabi", "Meng Wei", "Charlie Budd", "Tom Vercauteren", "Miaojing Shi"], "title": "Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach", "comment": null, "summary": "Understanding surgical instrument-tissue interactions requires not only\nidentifying which instrument performs which action on which anatomical target,\nbut also grounding these interactions spatially within the surgical scene.\nExisting surgical action triplet recognition methods are limited to learning\nfrom frame-level classification, failing to reliably link actions to specific\ninstrument instances.Previous attempts at spatial grounding have primarily\nrelied on class activation maps, which lack the precision and robustness\nrequired for detailed instrument-tissue interaction analysis.To address this\ngap, we propose grounding surgical action triplets with instrument instance\nsegmentation, or triplet segmentation for short, a new unified task which\nproduces spatially grounded <instrument, verb, target> outputs.We start by\npresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000\nannotated frames, linking instrument instance masks with action verb and\nanatomical target annotations, and establishing the first benchmark for\nstrongly supervised, instance-level triplet grounding and evaluation.To learn\ntriplet segmentation, we propose TargetFusionNet, a novel architecture that\nextends Mask2Former with a target-aware fusion mechanism to address the\nchallenge of accurate anatomical target prediction by fusing weak anatomy\npriors with instrument instance queries.Evaluated across recognition,\ndetection, and triplet segmentation metrics, TargetFusionNet consistently\nimproves performance over existing baselines, demonstrating that strong\ninstance supervision combined with weak target priors significantly enhances\nthe accuracy and robustness of surgical action understanding.Triplet\nsegmentation establishes a unified framework for spatially grounding surgical\naction triplets. The proposed benchmark and architecture pave the way for more\ninterpretable, surgical scene understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u4eea\u5668\u5b9e\u4f8b\u5206\u5272\u5b9e\u73b0\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5e76\u5f00\u53d1\u4e86TargetFusionNet\u67b6\u6784\u548cCholecTriplet-Seg\u6570\u636e\u96c6\u6765\u63d0\u5347\u624b\u672f\u52a8\u4f5c\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u8bc6\u522b\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5e27\u7ea7\u5206\u7c7b\uff0c\u65e0\u6cd5\u53ef\u9760\u5730\u5c06\u52a8\u4f5c\u4e0e\u7279\u5b9a\u4eea\u5668\u5b9e\u4f8b\u5173\u8054\uff0c\u4e14\u7a7a\u95f4\u5b9a\u4f4d\u4e3b\u8981\u4f9d\u8d56\u7c7b\u6fc0\u6d3b\u56fe\uff0c\u7f3a\u4e4f\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u5143\u7ec4\u5206\u5272\u4efb\u52a1\uff0c\u521b\u5efaCholecTriplet-Seg\u6570\u636e\u96c6\uff0830,000+\u6807\u6ce8\u5e27\uff09\uff0c\u5f00\u53d1TargetFusionNet\u67b6\u6784\uff0c\u6269\u5c55Mask2Former\u5e76\u5f15\u5165\u76ee\u6807\u611f\u77e5\u878d\u5408\u673a\u5236\uff0c\u5c06\u5f31\u89e3\u5256\u5b66\u5148\u9a8c\u4e0e\u4eea\u5668\u5b9e\u4f8b\u67e5\u8be2\u878d\u5408\u3002", "result": "TargetFusionNet\u5728\u8bc6\u522b\u3001\u68c0\u6d4b\u548c\u4e09\u5143\u7ec4\u5206\u5272\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u8868\u660e\u5f3a\u5b9e\u4f8b\u76d1\u7763\u4e0e\u5f31\u76ee\u6807\u5148\u9a8c\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u52a8\u4f5c\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e09\u5143\u7ec4\u5206\u5272\u4e3a\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u7a7a\u95f4\u5b9a\u4f4d\u5efa\u7acb\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u67b6\u6784\u4e3a\u66f4\u53ef\u89e3\u91ca\u7684\u624b\u672f\u573a\u666f\u7406\u89e3\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.01482", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01482", "abs": "https://arxiv.org/abs/2511.01482", "authors": ["Neha Sharma", "Navneet Agarwal", "Kairit Sirts"], "title": "Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation", "comment": null, "summary": "Text-based automated Cognitive Distortion detection is a challenging task due\nto its subjective nature, with low agreement scores observed even among expert\nhuman annotators, leading to unreliable annotations. We explore the use of\nLarge Language Models (LLMs) as consistent and reliable annotators, and propose\nthat multiple independent LLM runs can reveal stable labeling patterns despite\nthe inherent subjectivity of the task. Furthermore, to fairly compare models\ntrained on datasets with different characteristics, we introduce a\ndataset-agnostic evaluation framework using Cohen's kappa as an effect size\nmeasure. This methodology allows for fair cross-dataset and cross-study\ncomparisons where traditional metrics like F1 score fall short. Our results\nshow that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),\nresulting in improved test set performance for models trained on these\nannotations compared to those trained on human-labeled data. Our findings\nsuggest that LLMs can offer a scalable and internally consistent alternative\nfor generating training data that supports strong downstream performance in\nsubjective NLP tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u7684\u6807\u6ce8\u5de5\u5177\uff0c\u63d0\u51fa\u591a\u8f6e\u72ec\u7acbLLM\u6807\u6ce8\u53ef\u4ea7\u751f\u7a33\u5b9a\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u6570\u636e\u96c6\u65e0\u5173\u7684\u8bc4\u4f30\u6846\u67b6\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u89e3\u51b3\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4eba\u7c7b\u6807\u6ce8\u8005\u4e3b\u89c2\u6027\u5f3a\u3001\u4e00\u81f4\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u4f5c\u4e3a\u53ef\u9760\u6807\u6ce8\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u72ec\u7acbLLM\u8fd0\u884c\u8fdb\u884c\u6807\u6ce8\uff0c\u5f15\u5165\u57fa\u4e8eCohen's kappa\u7684\u6570\u636e\u96c6\u65e0\u5173\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "GPT-4\u80fd\u4ea7\u751f\u4e00\u81f4\u6027\u9ad8\u7684\u6807\u6ce8\uff08Fleiss's Kappa = 0.78\uff09\uff0c\u57fa\u4e8eLLM\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u751f\u6210\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u6269\u5c55\u4e14\u5185\u90e8\u4e00\u81f4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u5f3a\u5927\u7684\u4e0b\u6e38\u6027\u80fd\u3002"}}
{"id": "2511.00653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00653", "abs": "https://arxiv.org/abs/2511.00653", "authors": ["Lassi Ruoppa", "Tarmo Hietala", "Verneri Sepp\u00e4nen", "Josef Taher", "Teemu Hakala", "Xiaowei Yu", "Antero Kukko", "Harri Kaartinen", "Juha Hyypp\u00e4"], "title": "Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset", "comment": "39 pages, 9 figures", "summary": "Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for\napplications such as forest inventory, carbon monitoring and biodiversity\nassessment. Traditionally, ITS has been achieved with unsupervised\ngeometry-based algorithms, while more recent advances have shifted toward\nsupervised deep learning (DL). In the past, progress in method development was\nhindered by the lack of large-scale benchmark datasets, and the availability of\nnovel data formats, particularly multispectral (MS) LiDAR, remains limited to\nthis day, despite evidence that MS reflectance can improve the accuracy of ITS.\nThis study introduces FGI-EMIT, the first large-scale MS airborne laser\nscanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550\nnm, the dataset consists of 1,561 manually annotated trees, with a particular\nfocus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked\nfour conventional unsupervised algorithms and four supervised DL approaches.\nHyperparameters of unsupervised methods were optimized using a Bayesian\napproach, while DL models were trained from scratch. Among the unsupervised\nmethods, Treeiso achieved the highest test set F1-score of 52.7%. The DL\napproaches performed significantly better overall, with the best model,\nForestFormer3D, attaining an F1-score of 73.3%. The most significant difference\nwas observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9\npercentage points. An ablation study demonstrated that current DL-based\napproaches generally fail to leverage MS reflectance information when it is\nprovided as additional input features, although single channel reflectance can\nimprove accuracy marginally, especially for understory trees. A performance\nanalysis across point densities further showed that DL methods consistently\nremain superior to unsupervised algorithms, even at densities as low as 10\npoints/m$^2$.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u5149\u8c31\u6fc0\u5149\u96f7\u8fbe\u57fa\u51c6\u6570\u636e\u96c6FGI-EMIT\uff0c\u7528\u4e8e\u5355\u6728\u5206\u5272\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4f20\u7edf\u65e0\u76d1\u7763\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5355\u6728\u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u591a\u5149\u8c31\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\uff0c\u800c\u591a\u5149\u8c31\u53cd\u5c04\u7387\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528FGI-EMIT\u6570\u636e\u96c6\uff0c\u5bf94\u79cd\u4f20\u7edf\u65e0\u76d1\u7763\u7b97\u6cd5\u548c4\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u5934\u8bad\u7ec3\u3002", "result": "\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2dTreeiso\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u657052.7%\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6574\u4f53\u8868\u73b0\u66f4\u597d\uff0cForestFormer3D\u8fbe\u523073.3%\u7684F1\u5206\u6570\uff0c\u5728\u4e0b\u5c42\u6811\u6728\u5206\u5272\u4e2d\u4f18\u52bf\u5c24\u5176\u660e\u663e\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u6728\u5206\u5272\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u5149\u8c31\u53cd\u5c04\u7387\u4fe1\u606f\uff0c\u4e14\u5373\u4f7f\u5728\u4f4e\u70b9\u5bc6\u5ea6\u4e0b\u4e5f\u4fdd\u6301\u4f18\u52bf\u3002"}}
{"id": "2511.01490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01490", "abs": "https://arxiv.org/abs/2511.01490", "authors": ["Max Schaffelder", "Albert Gatt"], "title": "Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning", "comment": null, "summary": "As synthetic data becomes widely used in language model development,\nunderstanding its impact on model behavior is crucial. This paper investigates\nthe impact of the diversity of sources of synthetic data on fine-tuned large\nlanguage models. We focus on three key dimensions: distribution collapse,\nadversarial robustness, and self-preference bias. Our findings reveal that\nfine-tuning models on synthetic data from diverse sources can mitigate\ndistribution collapse, preserving the breadth of the output distribution and\nthe diversity of the output text. Furthermore, while both human and synthetic\nfine-tuning data can remove safeguards, the latter preserves higher output\nquality, thus making outputs potentially more usable and dangerous. Finally,\nfine-tuning reduces self-preference bias, with human data being the most\neffective, followed by multi-source synthetic data.", "AI": {"tldr": "\u7814\u7a76\u5408\u6210\u6570\u636e\u6765\u6e90\u591a\u6837\u6027\u5bf9\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u5e03\u5d29\u6e83\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u81ea\u504f\u597d\u504f\u5dee\u4e09\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u968f\u7740\u5408\u6210\u6570\u636e\u5728\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5176\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6bd4\u8f83\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u5408\u6210\u6570\u636e\u5728\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff1a\u5206\u5e03\u5d29\u6e83\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u81ea\u504f\u597d\u504f\u5dee\u3002", "result": "\u4f7f\u7528\u591a\u6765\u6e90\u5408\u6210\u6570\u636e\u5fae\u8c03\u53ef\u4ee5\u7f13\u89e3\u5206\u5e03\u5d29\u6e83\uff0c\u4fdd\u6301\u8f93\u51fa\u5206\u5e03\u7684\u5e7f\u5ea6\u548c\u6587\u672c\u591a\u6837\u6027\uff1b\u5408\u6210\u6570\u636e\u5fae\u8c03\u5728\u79fb\u9664\u5b89\u5168\u9632\u62a4\u65f6\u80fd\u4fdd\u6301\u66f4\u9ad8\u7684\u8f93\u51fa\u8d28\u91cf\uff1b\u5fae\u8c03\u80fd\u51cf\u5c11\u81ea\u504f\u597d\u504f\u5dee\uff0c\u4eba\u7c7b\u6570\u636e\u6548\u679c\u6700\u597d\uff0c\u591a\u6765\u6e90\u5408\u6210\u6570\u636e\u6b21\u4e4b\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u6765\u6e90\u7684\u591a\u6837\u6027\u5bf9\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u591a\u6765\u6e90\u5408\u6210\u6570\u636e\u5728\u7f13\u89e3\u5206\u5e03\u5d29\u6e83\u548c\u51cf\u5c11\u81ea\u504f\u597d\u504f\u5dee\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u6ce8\u610f\u5176\u53ef\u80fd\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2511.00681", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00681", "abs": "https://arxiv.org/abs/2511.00681", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control", "comment": null, "summary": "Magnetic Resonance Imaging suffers from substantial data heterogeneity and\nthe absence of standardized contrast labels across scanners, protocols, and\ninstitutions, which severely limits large-scale automated analysis. A unified\nrepresentation of MRI contrast would enable a wide range of downstream\nutilities, from automatic sequence recognition to harmonization and quality\ncontrol, without relying on manual annotations. To this end, we introduce\nMR-CLIP, a metadata-guided framework that learns MRI contrast representations\nby aligning volumetric images with their DICOM acquisition parameters. The\nresulting embeddings shows distinct clusters of MRI sequences and outperform\nsupervised 3D baselines under data scarcity in few-shot sequence\nclassification. Moreover, MR-CLIP enables unsupervised data quality control by\nidentifying corrupted or inconsistent metadata through image-metadata embedding\ndistances. By transforming routinely available acquisition metadata into a\nsupervisory signal, MR-CLIP provides a scalable foundation for label-efficient\nMRI analysis across diverse clinical datasets.", "AI": {"tldr": "MR-CLIP\u662f\u4e00\u4e2a\u5143\u6570\u636e\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MRI\u4f53\u79ef\u56fe\u50cf\u4e0e\u5176DICOM\u91c7\u96c6\u53c2\u6570\u5bf9\u9f50\u6765\u5b66\u4e60MRI\u5bf9\u6bd4\u5ea6\u8868\u793a\uff0c\u89e3\u51b3\u4e86MRI\u6570\u636e\u5f02\u8d28\u6027\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u5bf9\u6bd4\u5ea6\u6807\u7b7e\u7684\u95ee\u9898\u3002", "motivation": "MRI\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u5f02\u8d28\u6027\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u5bf9\u6bd4\u5ea6\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5206\u6790\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684MRI\u5bf9\u6bd4\u5ea6\u8868\u793a\u6765\u652f\u6301\u81ea\u52a8\u5e8f\u5217\u8bc6\u522b\u3001\u534f\u8c03\u548c\u8d28\u91cf\u63a7\u5236\u7b49\u4e0b\u6e38\u5e94\u7528\u3002", "method": "\u5f15\u5165MR-CLIP\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4f53\u79ef\u56fe\u50cf\u4e0eDICOM\u91c7\u96c6\u53c2\u6570\u5bf9\u9f50\u6765\u5b66\u4e60MRI\u5bf9\u6bd4\u5ea6\u8868\u793a\uff0c\u5229\u7528\u5e38\u89c4\u53ef\u7528\u7684\u91c7\u96c6\u5143\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u751f\u6210\u7684\u5d4c\u5165\u663e\u793aMRI\u5e8f\u5217\u7684\u660e\u663e\u805a\u7c7b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\uff0c\u5728\u5c11\u6837\u672c\u5e8f\u5217\u5206\u7c7b\u4e2d\u4f18\u4e8e\u76d1\u77633D\u57fa\u7ebf\u3002\u6b64\u5916\uff0cMR-CLIP\u901a\u8fc7\u56fe\u50cf-\u5143\u6570\u636e\u5d4c\u5165\u8ddd\u79bb\u8bc6\u522b\u635f\u574f\u6216\u4e0d\u4e00\u81f4\u7684\u5143\u6570\u636e\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u6570\u636e\u8d28\u91cf\u63a7\u5236\u3002", "conclusion": "MR-CLIP\u901a\u8fc7\u5c06\u5e38\u89c4\u91c7\u96c6\u5143\u6570\u636e\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u4e3a\u8de8\u4e0d\u540c\u4e34\u5e8a\u6570\u636e\u96c6\u7684\u6807\u7b7e\u9ad8\u6548MRI\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2511.01512", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Pareto\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u53bb\u6bd2\u65b0\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b68,041\u4e2a\u6709\u6bd2\u53e5\u5b50\u7684BanglaNirTox\u6570\u636e\u96c6\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u5728\u7ebf\u73af\u5883\u4e2d\u5b58\u5728\u5927\u91cf\u6709\u6bd2\u8bed\u8a00\uff0c\u4f46\u7531\u4e8e\u8d44\u6e90\u6709\u9650\uff0c\u8be5\u8bed\u8a00\u7684\u6587\u672c\u53bb\u6bd2\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528Pareto\u4f18\u5316\u7684LLM\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u751f\u6210\u53bb\u6bd2\u53e5\u5b50\uff0c\u6784\u5efa\u5305\u542b\u6bd2\u6027\u6807\u7b7e\u3001\u63a8\u7406\u548c\u53bb\u6bd2\u6539\u5199\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u3002", "result": "Pareto\u4f18\u5316\u7684LLM\u7ed3\u5408\u601d\u7ef4\u94fe\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u53bb\u6bd2\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6587\u672c\u53bb\u6bd2\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0cBanglaNirTox\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u5b5f\u52a0\u62c9\u8bedNLP\u7814\u7a76\u3002"}}
{"id": "2511.01526", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01526", "abs": "https://arxiv.org/abs/2511.01526", "authors": ["Seokhoon Kang", "Yejin Jeon", "Seonjeong Hwang", "Gary Geunbae Lee"], "title": "Difficulty-Controllable Cloze Question Distractor Generation", "comment": null, "summary": "Multiple-choice cloze questions are commonly used to assess linguistic\nproficiency and comprehension. However, generating high-quality distractors\nremains challenging, as existing methods often lack adaptability and control\nover difficulty levels, and the absence of difficulty-annotated datasets\nfurther hinders progress. To address these issues, we propose a novel framework\nfor generating distractors with controllable difficulty by leveraging both data\naugmentation and a multitask learning strategy. First, to create a\nhigh-quality, difficulty-annotated dataset, we introduce a two-way distractor\ngeneration process in order to produce diverse and plausible distractors. These\ncandidates are subsequently refined through filtering and then categorized by\ndifficulty using an ensemble QA system. Second, this newly created dataset is\nleveraged to train a difficulty-controllable generation model via multitask\nlearning. The framework includes carefully designed auxiliary tasks that\nenhance the model's semantic understanding of distractors and its ability to\nestimate their difficulty. Experimental results demonstrate that our method\ngenerates high-quality distractors across difficulty levels and substantially\noutperforms GPT-4o in aligning distractor difficulty with human perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u751f\u6210\u53ef\u63a7\u96be\u5ea6\u7684\u5e72\u6270\u9879\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u96be\u5ea6\u63a7\u5236\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\u65f6\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u96be\u5ea6\u63a7\u5236\u80fd\u529b\uff0c\u4e14\u7f3a\u4e4f\u96be\u5ea6\u6807\u6ce8\u6570\u636e\u96c6\u963b\u788d\u4e86\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u53cc\u5411\u5e72\u6270\u9879\u751f\u6210\u8fc7\u7a0b\u521b\u5efa\u9ad8\u8d28\u91cf\u96be\u5ea6\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8fc7\u6ee4\u548c\u96c6\u6210QA\u7cfb\u7edf\u5206\u7c7b\u96be\u5ea6\uff0c\u7136\u540e\u5229\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u8bad\u7ec3\u96be\u5ea6\u53ef\u63a7\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u8de8\u96be\u5ea6\u7ea7\u522b\u7684\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\uff0c\u5728\u5e72\u6270\u9879\u96be\u5ea6\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8eGPT-4o\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u5e72\u6270\u9879\u751f\u6210\u4e2d\u7684\u96be\u5ea6\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2511.00686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00686", "abs": "https://arxiv.org/abs/2511.00686", "authors": ["Alex Inch", "Passawis Chaiyapattanaporn", "Yuchen Zhu", "Yuan Lu", "Ting-Wen Ko", "Davide Paglieri"], "title": "Evolve to Inspire: Novelty Search for Diverse Image Generation", "comment": "14 pages, 10 figures, Accepted to Neurips 2025 GenProCC Workshop", "summary": "Text-to-image diffusion models, while proficient at generating high-fidelity\nim- ages, often suffer from limited output diversity, hindering their\napplication in exploratory and ideation tasks. Existing prompt optimization\ntechniques typically target aesthetic fitness or are ill-suited to the creative\nvisual domain. To address this shortcoming, we introduce WANDER, a novelty\nsearch-based approach to generating diverse sets of images from a single input\nprompt. WANDER operates directly on natural language prompts, employing a Large\nLanguage Model (LLM) for semantic evolution of diverse sets of images, and\nusing CLIP embeddings to quantify novelty. We additionally apply emitters to\nguide the search into distinct regions of the prompt space, and demonstrate\nthat they boost the diversity of the generated images. Empirical evaluations\nusing FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that\nWANDER significantly outperforms existing evolutionary prompt optimization\nbaselines in diversity metrics. Ablation studies confirm the efficacy of\nemitters.", "AI": {"tldr": "WANDER\u662f\u4e00\u79cd\u57fa\u4e8e\u65b0\u9896\u6027\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u8bed\u4e49\u6f14\u5316\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u56fe\u50cf\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8f93\u51fa\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u8f93\u51fa\u591a\u6837\u6027\u6709\u9650\uff0c\u9650\u5236\u4e86\u5728\u63a2\u7d22\u6027\u548c\u6784\u601d\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u7f8e\u5b66\u9002\u5e94\u6027\uff0c\u4e0d\u9002\u5408\u521b\u610f\u89c6\u89c9\u9886\u57df\u3002", "method": "WANDER\u76f4\u63a5\u5728\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e0a\u64cd\u4f5c\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u6f14\u5316\uff0c\u5229\u7528CLIP\u5d4c\u5165\u91cf\u5316\u65b0\u9896\u6027\uff0c\u5e76\u5e94\u7528\u53d1\u5c04\u5668\u5f15\u5bfc\u641c\u7d22\u5230\u4e0d\u540c\u7684\u63d0\u793a\u7a7a\u95f4\u533a\u57df\u3002", "result": "\u4f7f\u7528FLUX-DEV\u751f\u6210\u548cGPT-4o-mini\u53d8\u5f02\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cWANDER\u5728\u591a\u6837\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8fdb\u5316\u63d0\u793a\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u53d1\u5c04\u5668\u7684\u6709\u6548\u6027\u3002", "conclusion": "WANDER\u901a\u8fc7\u65b0\u9896\u6027\u641c\u7d22\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8f93\u51fa\u591a\u6837\u6027\uff0c\u53d1\u5c04\u5668\u7684\u5e94\u7528\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2511.01558", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01558", "abs": "https://arxiv.org/abs/2511.01558", "authors": ["Luciana Ciringione", "Emma Franchino", "Simone Reigl", "Isaia D'Onofrio", "Anna Serbati", "Oleksandra Poquet", "Florence Gabriel", "Massimo Stella"], "title": "Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o", "comment": null, "summary": "Math anxiety poses significant challenges for university psychology students,\naffecting their career choices and overall well-being. This study employs a\nframework based on behavioural forma mentis networks (i.e. cognitive models\nthat map how individuals structure their associative knowledge and emotional\nperceptions of concepts) to explore individual and group differences in the\nperception and association of concepts related to math and anxiety. We\nconducted 4 experiments involving psychology undergraduates from 2 samples (n1\n= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;\nGPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network\nfeatures to predict psychometric scores for math anxiety and its facets\n(observational, social and evaluational) from the Math Anxiety Scale.\nExperiment 4 focuses on group-level perceptions extracted from human students,\nGPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive\nvalence ratings and higher network degree for \"anxiety\", together with negative\nratings for \"math\", can predict higher total and evaluative math anxiety. In\ncontrast, these models do not work on GPT-based data because of differences in\nsimulated networks and psychometric scores compared to humans. These results\nwere also reconciled with differences found in the ways that high/low subgroups\nof simulated and real students framed semantically and emotionally STEM\nconcepts. High math-anxiety students collectively framed \"anxiety\" in an\nemotionally polarising way, absent in the negative perception of low\nmath-anxiety students. \"Science\" was rated positively, but contrasted against\nthe negative perception of \"math\". These findings underscore the importance of\nunderstanding concept perception and associations in managing students' math\nanxiety.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00698", "abs": "https://arxiv.org/abs/2511.00698", "authors": ["Taifour Yousra", "Beghdadi Azeddine", "Marie Luong", "Zuheng Ming"], "title": "Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics", "comment": null, "summary": "Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to\nmitigate high exposure side effects, but often suffers from noise and artifacts\nthat affect diagnostic accuracy. To tackle this issue, deep learning models\nhave been developed to enhance LDCT images. Various loss functions have been\nemployed, including classical approaches such as Mean Square Error and\nadversarial losses, as well as customized loss functions(LFs) designed for\nspecific architectures. Although these models achieve remarkable performance in\nterms of PSNR and SSIM, these metrics are limited in their ability to reflect\nperceptual quality, especially for medical images. In this paper, we focus on\none of the most critical elements of DL-based architectures, namely the loss\nfunction. We conduct an objective analysis of the relevance of different loss\nfunctions for LDCT image quality enhancement and their consistency with image\nquality metrics. Our findings reveal inconsistencies between LFs and quality\nmetrics, and highlight the need of consideration of image quality metrics when\ndeveloping a new loss function for image quality enhancement.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4f4e\u5242\u91cfCT\u56fe\u50cf\u589e\u5f3a\u4e2d\u4e0d\u540c\u635f\u5931\u51fd\u6570\u4e0e\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u7684\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u5ba2\u89c2\u5206\u6790\uff0c\u53d1\u73b0\u635f\u5931\u51fd\u6570\u4e0e\u8d28\u91cf\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u5728\u5f00\u53d1\u65b0\u635f\u5931\u51fd\u6570\u65f6\u9700\u8981\u8003\u8651\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u3002", "motivation": "\u4f4e\u5242\u91cfCT\u6210\u50cf\u5e7f\u6cdb\u7528\u4e8e\u51cf\u5c11\u8f90\u5c04\u66b4\u9732\uff0c\u4f46\u5e38\u53d7\u566a\u58f0\u548c\u4f2a\u5f71\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5728\u53cd\u6620\u533b\u5b66\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e2d\u6700\u5173\u952e\u7684\u5143\u7d20\u4e4b\u4e00\u2014\u2014\u635f\u5931\u51fd\u6570\uff0c\u5bf9\u4f4e\u5242\u91cfCT\u56fe\u50cf\u8d28\u91cf\u589e\u5f3a\u4e2d\u4e0d\u540c\u635f\u5931\u51fd\u6570\u7684\u76f8\u5173\u6027\u53ca\u5176\u4e0e\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u7684\u4e00\u81f4\u6027\u8fdb\u884c\u5ba2\u89c2\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u635f\u5931\u51fd\u6570\u4e0e\u8d28\u91cf\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u635f\u5931\u51fd\u6570\u5728\u53cd\u6620\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5728\u5f00\u53d1\u56fe\u50cf\u8d28\u91cf\u589e\u5f3a\u7684\u65b0\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5fc5\u987b\u8003\u8651\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff0c\u4ee5\u786e\u4fdd\u635f\u5931\u51fd\u6570\u80fd\u591f\u66f4\u597d\u5730\u53cd\u6620\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002"}}
{"id": "2511.01568", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01568", "abs": "https://arxiv.org/abs/2511.01568", "authors": ["Seungmin Shin", "Dooyoung Kim", "Youngjoong Ko"], "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation", "comment": "Published at EMNLP 2025 main", "summary": "Controllable Dialogue Generation (CDG) enables chatbots to generate responses\nwith desired attributes, and weighted decoding methods have achieved\nsignificant success in the CDG task. However, using a fixed constant value to\nmanage the bias of attribute probabilities makes it challenging to find an\nideal control strength that satisfies both controllability and fluency. To\naddress this issue, we propose ECO decoding (Entropy-based COntrol), which\ndynamically adjusts the control strength at each generation step according to\nthe model's entropy in both the language model and attribute classifier\nprobability distributions. Experiments on the DailyDialog and MultiWOZ datasets\ndemonstrate that ECO decoding consistently improves controllability while\nmaintaining fluency and grammaticality, outperforming prior decoding methods\nacross various models and settings. Furthermore, ECO decoding alleviates\nprobability interpolation issues in multi-attribute generation and consequently\ndemonstrates strong performance in both single and multi-attribute scenarios.", "AI": {"tldr": "\u63d0\u51faECO\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u52a8\u6001\u8c03\u6574\u63a7\u5236\u5f3a\u5ea6\uff0c\u89e3\u51b3\u53ef\u63a7\u5bf9\u8bdd\u751f\u6210\u4e2d\u56fa\u5b9a\u63a7\u5236\u5f3a\u5ea6\u96be\u4ee5\u5e73\u8861\u53ef\u63a7\u6027\u548c\u6d41\u7545\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u52a0\u6743\u89e3\u7801\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u5e38\u6570\u7ba1\u7406\u5c5e\u6027\u6982\u7387\u504f\u5dee\uff0c\u96be\u4ee5\u627e\u5230\u540c\u65f6\u6ee1\u8db3\u53ef\u63a7\u6027\u548c\u6d41\u7545\u6027\u7684\u7406\u60f3\u63a7\u5236\u5f3a\u5ea6\u3002", "method": "\u63d0\u51faECO\u89e3\u7801\uff0c\u6839\u636e\u8bed\u8a00\u6a21\u578b\u548c\u5c5e\u6027\u5206\u7c7b\u5668\u6982\u7387\u5206\u5e03\u7684\u71b5\uff0c\u5728\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\u52a8\u6001\u8c03\u6574\u63a7\u5236\u5f3a\u5ea6\u3002", "result": "\u5728DailyDialog\u548cMultiWOZ\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cECO\u89e3\u7801\u5728\u4fdd\u6301\u6d41\u7545\u6027\u548c\u8bed\u6cd5\u6027\u7684\u540c\u65f6\uff0c\u6301\u7eed\u63d0\u9ad8\u53ef\u63a7\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u3002", "conclusion": "ECO\u89e3\u7801\u7f13\u89e3\u4e86\u591a\u5c5e\u6027\u751f\u6210\u4e2d\u7684\u6982\u7387\u63d2\u503c\u95ee\u9898\uff0c\u5728\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\u3002"}}
{"id": "2511.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00728", "abs": "https://arxiv.org/abs/2511.00728", "authors": ["Hugo Massaroli", "Hernan Chaves", "Pilar Anania", "Mauricio Farez", "Emmanuel Iarussi", "Viviana Siless"], "title": "Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data", "comment": "7 pages, 2 figures", "summary": "Deep learning models have shown strong performance in diagnosing Alzheimer's\ndisease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with\ntraining datasets largely composed of North American cohorts such as those in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their\ngeneralization to underrepresented populations remains underexplored. In this\nstudy, we benchmark convolutional and Transformer-based models on the ADNI\ndataset and assess their generalization performance on a novel Latin American\nclinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show\nthat while all models achieve high AUCs on ADNI (up to .96, .97), their\nperformance drops substantially on FLENI (down to .82, .80, respectively),\nrevealing a significant domain shift. The tested architectures demonstrated\nsimilar performance, calling into question the supposed advantages of\ntransformers for this specific task. Through ablation studies, we identify\nper-image normalization and a correct sampling selection as key factors for\ngeneralization. Occlusion sensitivity analysis further reveals that models\ntrained on ADNI, generally attend to canonical hypometabolic regions for the AD\nclass, but focus becomes unclear for the other classes and for FLENI scans.\nThese findings highlight the need for population-aware validation of diagnostic\nAI models and motivate future work on domain adaptation and cohort\ndiversification.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728ADNI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u62c9\u4e01\u7f8e\u6d32FLENI\u961f\u5217\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u663e\u8457\u7684\u9886\u57df\u504f\u79fb\u3002Transformer\u67b6\u6784\u5e76\u672a\u663e\u793a\u51fa\u660e\u663e\u4f18\u52bf\uff0c\u56fe\u50cf\u5f52\u4e00\u5316\u548c\u91c7\u6837\u9009\u62e9\u662f\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4ee3\u8868\u6027\u4e0d\u8db3\u4eba\u7fa4\u7684\u9002\u7528\u6027\uff0c\u56e0\u4e3a\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u5317\u7f8e\u961f\u5217\u3002", "method": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5377\u79ef\u548cTransformer\u6a21\u578b\uff0c\u7136\u540e\u5728\u963f\u6839\u5ef7FLENI\u4e34\u5e8a\u961f\u5217\u4e0a\u8fdb\u884c\u6cdb\u5316\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5206\u6790\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u8fdb\u884c\u906e\u6321\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728ADNI\u4e0aAUC\u9ad8\u8fbe0.96-0.97\uff0c\u4f46\u5728FLENI\u4e0a\u964d\u81f30.80-0.82\uff0c\u663e\u793a\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002\u4e0d\u540c\u67b6\u6784\u8868\u73b0\u76f8\u4f3c\uff0c\u56fe\u50cf\u5f52\u4e00\u5316\u548c\u91c7\u6837\u9009\u62e9\u5bf9\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u9700\u8981\u57fa\u4e8e\u4eba\u7fa4\u7684AI\u6a21\u578b\u9a8c\u8bc1\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u5173\u6ce8\u9886\u57df\u9002\u5e94\u548c\u961f\u5217\u591a\u6837\u5316\uff0c\u4ee5\u786e\u4fdd\u8bca\u65ad\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.00738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00738", "abs": "https://arxiv.org/abs/2511.00738", "authors": ["Dmitrii Khizbullin", "Maksim Konoplia"], "title": "Towards classification-based representation learning for place recognition on LiDAR scans", "comment": null, "summary": "Place recognition is a crucial task in autonomous driving, allowing vehicles\nto determine their position using sensor data. While most existing methods rely\non contrastive learning, we explore an alternative approach by framing place\nrecognition as a multi-class classification problem. Our method assigns\ndiscrete location labels to LiDAR scans and trains an encoder-decoder model to\nclassify each scan's position directly. We evaluate this approach on the\nNuScenes dataset and show that it achieves competitive performance compared to\ncontrastive learning-based methods while offering advantages in training\nefficiency and stability.", "AI": {"tldr": "\u5c06\u5730\u70b9\u8bc6\u522b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u7c7b\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u4e3aLiDAR\u626b\u63cf\u5206\u914d\u79bb\u6563\u4f4d\u7f6e\u6807\u7b7e\uff0c\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u76f4\u63a5\u5206\u7c7b\u4f4d\u7f6e\uff0c\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\uff0c\u672c\u6587\u63a2\u7d22\u5c06\u5176\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u7c7b\u5206\u7c7b\u95ee\u9898\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u4e3aLiDAR\u626b\u63cf\u5206\u914d\u79bb\u6563\u4f4d\u7f6e\u6807\u7b7e\uff0c\u8bad\u7ec3\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u76f4\u63a5\u5bf9\u6bcf\u4e2a\u626b\u63cf\u7684\u4f4d\u7f6e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u591a\u7c7b\u5206\u7c7b\u65b9\u6cd5\u5728\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u4f18\u52bf\u3002"}}
{"id": "2511.01615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo L\u00f3pez"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u8de8\u5b66\u79d1\u5206\u6790\u897f\u73ed\u7259\u8bed\u6bcd\u8bed\u8005\u7684\u8bed\u8a00\u9519\u8bef\uff0c\u901a\u8fc7\u7406\u8bba\u8bed\u8a00\u5b66\u3001\u795e\u7ecf\u8bed\u8a00\u5b66\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e09\u4e2a\u89c6\u89d2\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8fd9\u4e9b\u9519\u8bef\u7684\u89e3\u91ca\u548c\u7ea0\u6b63\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u9519\u8bef\u4e0d\u4ec5\u662f\u8bed\u6cd5\u504f\u5dee\uff0c\u66f4\u662f\u7406\u89e3\u8bed\u8a00\u8ba4\u77e5\u67b6\u6784\u548c\u63ed\u793a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5c40\u9650\u6027\u7684\u7a97\u53e3\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u7b26\u5408\u8ba4\u77e5\u7684NLP\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u5305\u542b500+\u897f\u73ed\u7259\u8bed\u6bcd\u8bed\u8005\u771f\u5b9e\u9519\u8bef\u7684\u8bed\u6599\u5e93\uff0c\u4f7f\u7528GPT\u3001Gemini\u7b49AI\u6a21\u578b\u6d4b\u8bd5\u5176\u5bf9\u9519\u8bef\u7684\u89e3\u91ca\u51c6\u786e\u6027\u548c\u5bf9\u4eba\u7c7b\u8bed\u8a00\u884c\u4e3a\u6a21\u5f0f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30AI\u6a21\u578b\u5bf9\u8bed\u8a00\u9519\u8bef\u7684\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u8ba4\u77e5\u5316\u7684NLP\u7cfb\u7edf\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u8be5\u9879\u76ee\u4e0d\u4ec5\u4fc3\u8fdb\u5bf9\u897f\u73ed\u7259\u8bed\u4f5c\u4e3a\u6bcd\u8bed\u7684\u7406\u89e3\uff0c\u8fd8\u63a8\u52a8\u5f00\u53d1\u80fd\u591f\u5904\u7406\u4eba\u7c7b\u8bed\u8a00\u4e0d\u5b8c\u7f8e\u3001\u591a\u53d8\u548c\u6a21\u7cca\u7279\u6027\u7684NLP\u7cfb\u7edf\u3002"}}
{"id": "2511.00749", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00749", "abs": "https://arxiv.org/abs/2511.00749", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Gavin Abercrombie", "Ioannis Konstas"], "title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models", "comment": "This is a preprint under review", "summary": "Social media has exacerbated the promotion of Western beauty norms, leading\nto negative self-image, particularly in women and girls, and causing harm such\nas body dysmorphia. Increasingly content on the internet has been artificially\ngenerated, leading to concerns that these norms are being exaggerated. The aim\nof this work is to study how generative AI models may encode 'beauty' and erase\n'ugliness', and discuss the implications of this for society. To investigate\nthese aims, we create two image generation pipelines: a text-to-image model and\na text-to-language model-to image model. We develop a structured beauty\ntaxonomy which we use to prompt three language models (LMs) and two\ntext-to-image models to cumulatively generate 5984 images using our two\npipelines. We then recruit women and non-binary social media users to evaluate\n1200 of the images through a Likert-scale within-subjects study. Participants\nshow high agreement in their ratings. Our results show that 86.5% of generated\nimages depicted people with lighter skin tones, 22% contained explicit content\ndespite Safe for Work (SFW) training, and 74% were rated as being in a younger\nage demographic. In particular, the images of non-binary individuals were rated\nas both younger and more hypersexualised, indicating troubling intersectional\neffects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such\nas \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings\nregardless of gender. This work sheds light on the pervasive demographic biases\nrelated to beauty standards present in generative AI models -- biases that are\nactively perpetuated by model developers, such as via negative prompting. We\nconclude by discussing the implications of this on society, which include\npollution of the data streams and active erasure of features that do not fall\ninside the stereotype of what is considered beautiful by developers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u751f\u6210\u5f0fAI\u6a21\u578b\u5982\u4f55\u7f16\u7801'\u7f8e'\u5e76\u6d88\u9664'\u4e11'\uff0c\u901a\u8fc7\u4e24\u4e2a\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u751f\u62105984\u5f20\u56fe\u50cf\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u660e\u663e\u7684\u80a4\u8272\u3001\u5e74\u9f84\u548c\u6027\u522b\u504f\u89c1\uff0c86.5%\u56fe\u50cf\u63cf\u7ed8\u6d45\u80a4\u8272\u4eba\u7fa4\uff0c74%\u4e3a\u5e74\u8f7b\u4eba\u7fa4\uff0c\u975e\u4e8c\u5143\u6027\u522b\u4e2a\u4f53\u88ab\u8fc7\u5ea6\u6027\u5316\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u52a0\u5267\u4e86\u897f\u65b9\u5ba1\u7f8e\u6807\u51c6\u7684\u63a8\u5e7f\uff0c\u5bfc\u81f4\u8d1f\u9762\u81ea\u6211\u5f62\u8c61\u548c\u8eab\u4f53\u7578\u5f62\u6050\u60e7\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\u7684\u589e\u52a0\uff0c\u62c5\u5fe7\u8fd9\u4e9b\u5ba1\u7f8e\u6807\u51c6\u88ab\u5938\u5927\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u6a21\u578b\u5982\u4f55\u7f16\u7801'\u7f8e'\u6982\u5ff5\u53ca\u5176\u793e\u4f1a\u5f71\u54cd\u3002", "method": "\u521b\u5efa\u4e24\u4e2a\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\uff1a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u548c\u6587\u672c\u5230\u8bed\u8a00\u6a21\u578b\u5230\u56fe\u50cf\u6a21\u578b\u3002\u5f00\u53d1\u7ed3\u6784\u5316\u5ba1\u7f8e\u5206\u7c7b\u6cd5\uff0c\u4f7f\u7528\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u62105984\u5f20\u56fe\u50cf\u3002\u62db\u52df\u5973\u6027\u548c\u975e\u4e8c\u5143\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u901a\u8fc7\u674e\u514b\u7279\u91cf\u8868\u8bc4\u4f301200\u5f20\u56fe\u50cf\u3002", "result": "86.5%\u751f\u6210\u56fe\u50cf\u63cf\u7ed8\u6d45\u80a4\u8272\u4eba\u7fa4\uff1b22%\u5305\u542b\u660e\u786e\u5185\u5bb9\uff08\u5c3d\u7ba1\u7ecf\u8fc7\u5b89\u5168\u8bad\u7ec3\uff09\uff1b74%\u88ab\u8bc4\u4e3a\u5e74\u8f7b\u5e74\u9f84\u6bb5\uff1b\u975e\u4e8c\u5143\u4e2a\u4f53\u56fe\u50cf\u88ab\u8bc4\u4ef7\u4e3a\u66f4\u5e74\u8f7b\u548c\u8fc7\u5ea6\u6027\u5316\uff1b\u5e26\u6709'\u8d1f\u9762'\u6216'\u4e11\u964b'\u7279\u5f81\u7684\u63d0\u793a\u8bcd\u4ea7\u751f\u66f4\u9ad8NSFW\u8bc4\u5206\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6a21\u578b\u5b58\u5728\u4e0e\u5ba1\u7f8e\u6807\u51c6\u76f8\u5173\u7684\u666e\u904d\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u901a\u8fc7\u6a21\u578b\u5f00\u53d1\u8005\u7684\u884c\u4e3a\uff08\u5982\u8d1f\u9762\u63d0\u793a\uff09\u88ab\u79ef\u6781\u5ef6\u7eed\uff0c\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6d41\u6c61\u67d3\u548c\u5bf9\u4e0d\u7b26\u5408\u5f00\u53d1\u8005\u5ba1\u7f8e\u523b\u677f\u5370\u8c61\u7279\u5f81\u7684\u79ef\u6781\u6d88\u9664\u3002"}}
{"id": "2511.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00777", "abs": "https://arxiv.org/abs/2511.00777", "authors": ["Anis Suttan Shahrir", "Zakiah Ayop", "Syarulnaziah Anawar", "Norulzahrah Mohd Zainudin"], "title": "A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection", "comment": null, "summary": "Durian plantation suffers from animal intrusions that cause crop damage and\nfinancial loss. The traditional farming practices prove ineffective due to the\nunavailability of monitoring without human intervention. The fast growth of\nmachine learning and Internet of Things (IoT) technology has led to new ways to\ndetect animals. However, current systems are limited by dependence on single\nobject detection algorithms, less accessible notification platforms, and\nlimited deterrent mechanisms. This research suggests an IoT-enabled animal\ndetection system for durian crops. The system integrates YOLOv5 and SSD object\ndetection algorithms to improve detection accuracy. The system provides\nreal-time monitoring, with detected intrusions automatically reported to\nfarmers via Telegram notifications for rapid response. An automated sound\nmechanism (e.g., tiger roar) is triggered once the animal is detected. The\nYOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,\n85% and 70%, respectively. The system shows the highest accuracy in daytime and\ndecreases at night, regardless of whether the image is still or a video.\nOverall, this study contributes a comprehensive and practical framework that\ncombines detection, notification, and deterrence, paving the way for future\ninnovations in automated farming solutions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408YOLOv5\u548cSSD\u7b97\u6cd5\u7684\u7269\u8054\u7f51\u52a8\u7269\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u69b4\u83b2\u79cd\u690d\u56ed\uff0c\u901a\u8fc7Telegram\u5b9e\u65f6\u901a\u77e5\u548c\u58f0\u97f3\u5a01\u6151\u673a\u5236\u4fdd\u62a4\u4f5c\u7269\u514d\u53d7\u52a8\u7269\u5165\u4fb5\u3002", "motivation": "\u4f20\u7edf\u519c\u4e1a\u5b9e\u8df5\u56e0\u7f3a\u4e4f\u65e0\u4eba\u76d1\u63a7\u800c\u65e0\u6548\uff0c\u73b0\u6709\u7cfb\u7edf\u53d7\u9650\u4e8e\u5355\u4e00\u68c0\u6d4b\u7b97\u6cd5\u3001\u901a\u77e5\u5e73\u53f0\u4e0d\u53ef\u53ca\u548c\u5a01\u6151\u673a\u5236\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210YOLOv5\u548cSSD\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7ed3\u5408\u7269\u8054\u7f51\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\uff0c\u901a\u8fc7Telegram\u53d1\u9001\u81ea\u52a8\u901a\u77e5\uff0c\u5e76\u89e6\u53d1\u58f0\u97f3\u5a01\u6151\u673a\u5236\uff08\u5982\u864e\u5578\uff09\u3002", "result": "YOLO+SSD\u6a21\u578b\u5bf9\u5927\u8c61\u3001\u91ce\u732a\u548c\u7334\u5b50\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u5206\u522b\u4e3a90%\u300185%\u548c70%\uff0c\u767d\u5929\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u591c\u95f4\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u5408\u68c0\u6d4b\u3001\u901a\u77e5\u548c\u5a01\u6151\u7684\u5168\u9762\u5b9e\u7528\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5316\u519c\u4e1a\u89e3\u51b3\u65b9\u6848\u7684\u672a\u6765\u521b\u65b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.01643", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicol\u00f2 Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u56fe\u57fa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u67b6\u6784\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u80fd\u6548\u95ee\u7b54\uff0c\u901a\u8fc7\u4ece\u80fd\u6e90\u6587\u6863\u81ea\u52a8\u63d0\u53d6\u77e5\u8bc6\u56fe\u8c31\u5e76\u63a8\u7406\u5bfc\u822a\uff0c\u5b9e\u73b0\u4e86\u591a\u8bed\u8a00\u51c6\u786e\u56de\u7b54\uff0c\u9a8c\u8bc1\u663e\u793a75.2%\u7684\u6b63\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u56fe\u57faRAG\u67b6\u6784\u89e3\u51b3\u80fd\u6e90\u6548\u7387\u9886\u57df\u7684\u95ee\u7b54\u9700\u6c42\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u56de\u7b54\u51c6\u786e\u6027\u5e76\u652f\u6301\u591a\u8bed\u8a00\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u4ece\u80fd\u6e90\u6307\u5bfc\u6587\u4ef6\u548c\u6cd5\u89c4\u6587\u6863\u81ea\u52a8\u63d0\u53d6\u77e5\u8bc6\u56fe\u8c31\uff0c\u7136\u540e\u901a\u8fc7\u56fe\u5bfc\u822a\u548c\u63a8\u7406\u4e3a\u7528\u6237\u63d0\u4f9b\u591a\u8bed\u8a00\u51c6\u786e\u7b54\u6848\uff0c\u4f7f\u7528RAGAs\u6846\u67b6\u548c\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u7cfb\u7edf\u5728\u7ea675.2%\u7684\u60c5\u51b5\u4e0b\u80fd\u6b63\u786e\u56de\u7b54\u95ee\u9898\uff0c\u901a\u7528\u80fd\u6548\u95ee\u9898\u51c6\u786e\u7387\u53ef\u8fbe81.0%\uff0c\u591a\u8bed\u8a00\u80fd\u529b\u8868\u73b0\u826f\u597d\uff08\u7ffb\u8bd1\u4ec5\u5bfc\u81f44.4%\u51c6\u786e\u7387\u635f\u5931\uff09\u3002", "conclusion": "\u8be5\u67b6\u6784\u5c55\u73b0\u4e86\u5728\u80fd\u6548\u95ee\u7b54\u9886\u57df\u7684\u6f5c\u529b\uff0c\u8bc1\u5b9e\u4e86\u56fe\u57faRAG\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u7cfb\u7edf\u7684\u4f18\u52bf\u548c\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.00785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00785", "abs": "https://arxiv.org/abs/2511.00785", "authors": ["Juan Wang", "Yasutomo Kawanishi", "Tomo Miyazaki", "Zhijie Wang", "Shinichiro Omachi"], "title": "Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking", "comment": "Under review in Pattern Recognition", "summary": "3D instance segmentation is an important task for real-world applications. To\navoid costly manual annotations, existing methods have explored generating\npseudo labels by transferring 2D masks from foundation models to 3D. However,\nthis approach is often suboptimal since the video frames are processed\nindependently. This causes inconsistent segmentation granularity and\nconflicting 3D pseudo labels, which degrades the accuracy of final\nsegmentation. To address this, we introduce a Granularity-Consistent automatic\n2D Mask Tracking approach that maintains temporal correspondences across\nframes, eliminating conflicting pseudo labels. Combined with a three-stage\ncurriculum learning framework, our approach progressively trains from\nfragmented single-view data to unified multi-view annotations, ultimately\nglobally coherent full-scene supervision. This structured learning pipeline\nenables the model to progressively expose to pseudo-labels of increasing\nconsistency. Thus, we can robustly distill a consistent 3D representation from\ninitially fragmented and contradictory 2D priors. Experimental results\ndemonstrated that our method effectively generated consistent and accurate 3D\nsegmentations. Furthermore, the proposed method achieved state-of-the-art\nresults on standard benchmarks and open-vocabulary ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c92\u5ea6\u4e00\u81f4\u7684\u81ea\u9002\u5e942D\u63a9\u7801\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u6301\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\u6765\u6d88\u9664\u51b2\u7a81\u76843D\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u788e\u7247\u5316\u7684\u5355\u89c6\u56fe\u6570\u636e\u9010\u6b65\u8bad\u7ec3\u5230\u7edf\u4e00\u7684\u591a\u89c6\u56fe\u6807\u6ce8\uff0c\u6700\u7ec8\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u7684\u5b8c\u6574\u573a\u666f\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c06\u57fa\u7840\u6a21\u578b\u76842D\u63a9\u7801\u8f6c\u79fb\u52303D\u6765\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4f46\u7531\u4e8e\u89c6\u9891\u5e27\u88ab\u72ec\u7acb\u5904\u7406\uff0c\u5bfc\u81f4\u5206\u5272\u7c92\u5ea6\u4e0d\u4e00\u81f4\u548c\u51b2\u7a81\u76843D\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u964d\u4f4e\u6700\u7ec8\u5206\u5272\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u7c92\u5ea6\u4e00\u81f4\u7684\u81ea\u9002\u5e942D\u63a9\u7801\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u4fdd\u6301\u8de8\u5e27\u7684\u65f6\u95f4\u5bf9\u5e94\u5173\u7cfb\uff1b\u7ed3\u5408\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u5355\u89c6\u56fe\u788e\u7247\u5316\u6570\u636e\u9010\u6b65\u8bad\u7ec3\u5230\u591a\u89c6\u56fe\u7edf\u4e00\u6807\u6ce8\uff0c\u6700\u7ec8\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u7684\u5b8c\u6574\u573a\u666f\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u4e00\u81f4\u4e14\u51c6\u786e\u76843D\u5206\u5272\u7ed3\u679c\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u5907\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6700\u521d\u788e\u7247\u5316\u548c\u77db\u76fe\u76842D\u5148\u9a8c\u4e2d\u7a33\u5065\u5730\u63d0\u53d6\u51fa\u4e00\u81f4\u76843D\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u5b9e\u4f8b\u5206\u5272\u3002"}}
{"id": "2511.01649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01649", "abs": "https://arxiv.org/abs/2511.01649", "authors": ["Hung-Shin Lee", "Chen-Chi Chang", "Ching-Yuan Chen", "Yun-Hsiang Hsu"], "title": "Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation", "comment": "This paper has been accepted by The Electronic Library, and the full\n  article is now available on Emerald Insight", "summary": "This study proposes a cognitive benchmarking framework to evaluate how large\nlanguage models (LLMs) process and apply culturally specific knowledge. The\nframework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)\nto assess model performance across six hierarchical cognitive domains:\nRemembering, Understanding, Applying, Analyzing, Evaluating, and Creating.\nUsing a curated Taiwanese Hakka digital cultural archive as the primary\ntestbed, the evaluation measures LLM-generated responses' semantic accuracy and\ncultural relevance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8ba4\u77e5\u57fa\u51c6\u6846\u67b6\uff0c\u7ed3\u5408\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6587\u5316\u7279\u5b9a\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u53f0\u6e7e\u5ba2\u5bb6\u6570\u5b57\u6587\u5316\u6863\u6848\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u548c\u5e94\u7528\u6587\u5316\u7279\u5b9a\u77e5\u8bc6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6587\u5316\u654f\u611f\u5185\u5bb9\u7684\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u6574\u5408\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u5728\u516d\u4e2a\u8ba4\u77e5\u9886\u57df\uff08\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u5e94\u7528\u3001\u5206\u6790\u3001\u8bc4\u4f30\u3001\u521b\u9020\uff09\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u7528\u53f0\u6e7e\u5ba2\u5bb6\u6570\u5b57\u6587\u5316\u6863\u6848\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u3002", "result": "\u6d4b\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u54cd\u5e94\u7684\u8bed\u4e49\u51c6\u786e\u6027\u548c\u6587\u5316\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.00795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00795", "abs": "https://arxiv.org/abs/2511.00795", "authors": ["Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru", "Sai Teja Erukude"], "title": "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data", "comment": "Published in IEEE", "summary": "Federated Learning (FL) allows multiple institutions to cooperatively train\nmachine learning models while retaining sensitive data at the source, which has\ngreat utility in privacy-sensitive environments. However, FL systems remain\nvulnerable to membership-inference attacks and data heterogeneity. This paper\npresents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using\nsynthetic oncologic CT scans with tumor annotations. It evaluates segmentation\nperformance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and\nFedAvg with DP-SGD. Results show a distinct trade-off between privacy and\nutility: FedAvg is high performance (Dice around 0.85) with more privacy\nleakage (attack AUC about 0.72), while DP-SGD provides a higher level of\nprivacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx\nand FedBN offer balanced performance under heterogeneous data, especially with\nnon-identical distributed client data. FedOnco-Bench serves as a standardized,\nopen-source platform for benchmarking and developing privacy-preserving FL\nmethods for medical image segmentation.", "AI": {"tldr": "FedOnco-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u91cd\u590d\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u7528\u5408\u6210\u80bf\u7624CT\u626b\u63cf\u6570\u636e\u8bc4\u4f30\u5206\u5272\u6027\u80fd\u548c\u9690\u79c1\u6cc4\u9732\uff0c\u63ed\u793a\u4e86\u9690\u79c1\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u62a4\u9690\u79c1\u654f\u611f\u6570\u636e\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4ecd\u9762\u4e34\u6210\u5458\u63a8\u7406\u653b\u51fb\u548c\u6570\u636e\u5f02\u6784\u6027\u7b49\u6f0f\u6d1e\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30\u9690\u79c1\u4fdd\u62a4FL\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1FedOnco-Bench\u57fa\u51c6\u5e73\u53f0\uff0c\u4f7f\u7528\u5408\u6210\u80bf\u7624CT\u626b\u63cf\u6570\u636e\uff0c\u8bc4\u4f30FedAvg\u3001FedProx\u3001FedBN\u548cFedAvg+DP-SGD\u7b49FL\u65b9\u6cd5\u7684\u5206\u5272\u6027\u80fd\u548c\u9690\u79c1\u6cc4\u9732\u60c5\u51b5\u3002", "result": "FedAvg\u6027\u80fd\u6700\u4f73\uff08Dice\u7ea60.85\uff09\u4f46\u9690\u79c1\u6cc4\u9732\u6700\u591a\uff08\u653b\u51fbAUC\u7ea60.72\uff09\uff0cDP-SGD\u9690\u79c1\u4fdd\u62a4\u6700\u5f3a\uff08AUC\u7ea60.25\uff09\u4f46\u51c6\u786e\u7387\u4e0b\u964d\uff08Dice\u7ea60.79\uff09\uff0cFedProx\u548cFedBN\u5728\u5f02\u6784\u6570\u636e\u4e0b\u8868\u73b0\u5747\u8861\u3002", "conclusion": "FedOnco-Bench\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u9690\u79c1\u4fdd\u62a4FL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u5f00\u6e90\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u65b9\u6cd5\u5f00\u53d1\u548c\u6bd4\u8f83\u3002"}}
{"id": "2511.01650", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors.", "AI": {"tldr": "EngChain\u662f\u4e00\u4e2a\u7528\u4e8e\u9a8c\u8bc1\u591a\u6b65\u9aa4\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b90\u4e2a\u8de83\u4e2a\u5de5\u7a0b\u5206\u652f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7b26\u53f7\u6a21\u677f\u751f\u6210\u4ee5\u786e\u4fdd\u591a\u6837\u6027\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1a\u5b9a\u91cf\u9a8c\u8bc1\u63a8\u7406\u6b65\u9aa4\u7684\u6709\u6548\u6027\uff0c\u7136\u540e\u4f7f\u7528LLM-As-A-Judge\u81ea\u52a8\u5206\u7c7b\u63a8\u7406\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u8bed\u8a00\u7406\u89e3\u3001\u4e8b\u5b9e\u56de\u5fc6\u3001\u6570\u5b66\u6216\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5de5\u7a0b\u9886\u57df\u6240\u9700\u7684\u6574\u5408\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002\u5de5\u7a0b\u95ee\u9898\u9700\u8981\u79d1\u5b66\u539f\u7406\u3001\u5b9a\u91cf\u5efa\u6a21\u548c\u5b9e\u8df5\u7ea6\u675f\u7684\u878d\u5408\u3002", "method": "\u5f00\u53d1EngChain\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b90\u4e2a\u95ee\u9898\uff0c\u6db5\u76d63\u4e2a\u5de5\u7a0b\u5206\u652f\u30019\u4e2a\u9886\u57df\u548c20\u4e2a\u5177\u4f53\u9886\u57df\u3002\u95ee\u9898\u901a\u8fc7\u7b26\u53f7\u6a21\u677f\u751f\u6210\uff0c\u5177\u6709\u9ad8\u5ea6\u968f\u673a\u5316\u4ee5\u786e\u4fdd\u591a\u6837\u6027\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u65b9\u6cd5\uff1a\u5b9a\u91cf\u9a8c\u8bc1\u63a8\u7406\u6b65\u9aa4\uff0c\u7136\u540e\u4f7f\u7528LLM-As-A-Judge\u8fdb\u884c\u5b9a\u6027\u9519\u8bef\u5206\u7c7b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5de5\u7a0b\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u9a8c\u8bc1\u591a\u6b65\u9aa4\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u6709\u6548\u6027\uff0c\u5e76\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u63a8\u7406\u9519\u8bef\u3002", "conclusion": "EngChain\u586b\u8865\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bc4\u4f30\u5de5\u7a0b\u6574\u5408\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u590d\u6742\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.01670", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages.", "AI": {"tldr": "SeaLLMs-Audio\u662f\u9996\u4e2a\u9488\u5bf9\u4e1c\u5357\u4e9a\u8bed\u8a00\uff08\u5370\u5c3c\u8bed\u3001\u6cf0\u8bed\u3001\u8d8a\u5357\u8bed\uff09\u4ee5\u53ca\u82f1\u8bed\u548c\u4e2d\u6587\u7684\u5927\u89c4\u6a21\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\u548c\u591a\u4efb\u52a1\u5904\u7406\uff0c\u5728\u97f3\u9891\u7406\u89e3\u548c\u8bed\u97f3\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e1c\u5357\u4e9a\u5730\u533a\u5f00\u53d1\u4e13\u95e8\u7684\u5927\u89c4\u6a21\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u586b\u8865\u8be5\u5730\u533a\u591a\u8bed\u8a00\u97f3\u9891AI\u6280\u672f\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u533a\u57df\u7814\u7a76\u548c\u4ea7\u4e1a\u53d1\u5c55\u3002", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u97f3\u9891\u8bed\u6599\u5e93\u8bad\u7ec3\uff0c\u652f\u63015\u79cd\u8bed\u8a00\uff08\u5370\u5c3c\u8bed\u3001\u6cf0\u8bed\u3001\u8d8a\u5357\u8bed\u3001\u82f1\u8bed\u3001\u4e2d\u6587\uff09\uff0c\u91c7\u7528\u591a\u6a21\u6001\u67b6\u6784\uff0c\u53ef\u63a5\u53d7\u7eaf\u97f3\u9891\u3001\u7eaf\u6587\u672c\u6216\u97f3\u9891+\u6587\u672c\u7684\u7075\u6d3b\u8f93\u5165\u3002", "result": "\u5728\u591a\u79cd\u97f3\u9891\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u97f3\u9891\u5b57\u5e55\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u8bed\u97f3\u7ffb\u8bd1\u3001\u60c5\u611f\u8bc6\u522b\u3001\u95ee\u7b54\u548c\u6458\u8981\u7b49\uff0c\u5728\u4e1c\u5357\u4e9a\u8bed\u8a00\u4e0a\u4e0e\u5176\u4ed6LALMs\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "SeaLLMs-Audio\u662f\u63a8\u8fdb\u4e1c\u5357\u4e9a\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u540c\u65f6\u63a8\u51fa\u7684SeaBench-Audio\u57fa\u51c6\u6d4b\u8bd5\u5c06\u6709\u52a9\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u9884\u671f\u5c06\u60e0\u53ca\u533a\u57df\u7814\u7a76\u793e\u533a\u548c\u4ea7\u4e1a\u754c\u3002"}}
{"id": "2511.01689", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90\u7684AI\u52a9\u624b\u89d2\u8272\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5baa\u6cd5AI\u548c\u5408\u6210\u81ea\u7701\u6570\u636e\u6765\u5851\u9020\u52a9\u624b\u89d2\u8272\uff0c\u76f8\u6bd4\u7cfb\u7edf\u63d0\u793a\u7ea6\u675f\u548c\u6fc0\u6d3b\u5f15\u5bfc\u7b49\u65b9\u6cd5\u66f4\u6709\u6548\u3001\u53ef\u63a7\u4e14\u9c81\u68d2\u3002", "motivation": "\u73b0\u4ee3\u804a\u5929\u673a\u5668\u4eba\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684AI\u52a9\u624b\u89d2\u8272\u7279\u5f81\u4f1a\u5f71\u54cd\u4ea4\u4e92\u8d28\u91cf\u3001\u611f\u77e5\u667a\u80fd\u4ee5\u53ca\u4e0e\u5f00\u53d1\u8005\u548c\u7528\u6237\u610f\u56fe\u7684\u5bf9\u9f50\uff0c\u4f46\u89d2\u8272\u8bad\u7ec3\u8fd9\u4e00\u5173\u952e\u7ec4\u4ef6\u5728\u5b66\u672f\u754c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5baa\u6cd5AI\u548c\u57fa\u4e8e\u5408\u6210\u81ea\u7701\u6570\u636e\u7684\u65b0\u6570\u636e\u7ba1\u9053\uff0c\u5bf9\u4e09\u4e2a\u6d41\u884c\u7684\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e94\u752811\u4e2a\u793a\u4f8b\u89d2\u8272\uff08\u5982\u5e7d\u9ed8\u3001\u6df1\u5ea6\u5173\u6000\u751a\u81f3\u6076\u610f\uff09\uff0c\u5e76\u901a\u8fc7\u63ed\u793a\u504f\u597d\u5206\u6790\u65b9\u6cd5\u8ddf\u8e2a\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u76f8\u6bd4\u7cfb\u7edf\u63d0\u793a\u7ea6\u675f\u548c\u6fc0\u6d3b\u5f15\u5bfc\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u66f4\u9c81\u68d2\uff0c\u540c\u65f6\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u771f\u5b9e\u7684\u6587\u672c\uff0c\u4e14\u5bf9\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u51e0\u4e4e\u6ca1\u6709\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u5f00\u53d1\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3aAI\u52a9\u624b\u89d2\u8272\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9996\u4e2a\u5f00\u6e90\u5b9e\u73b0\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u548c\u53ef\u63a7\u7684\u89d2\u8272\u5851\u9020\u3002"}}
{"id": "2511.01706", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684rank-2\u6295\u5f71\u5b50\u7a7a\u95f4\u6765\u66f4\u51c6\u786e\u5730\u89e3\u6784\u53c2\u6570\u77e5\u8bc6(PK)\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6(CK)\u7684\u8d21\u732e\uff0c\u5e76\u9996\u6b21\u5bf9\u957f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca(NLE)\u5e8f\u5217\u4e2d\u7684\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53c2\u6570\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u4ea4\u4e92\u5bf9\u4e8e\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u8fd9\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u6b65\u751f\u6210\u4e14\u4ec5\u5c06PK\u548cCK\u4ea4\u4e92\u5efa\u6a21\u4e3arank-1\u5b50\u7a7a\u95f4\u4e2d\u7684\u4e8c\u5143\u9009\u62e9\uff0c\u5ffd\u7565\u4e86\u66f4\u4e30\u5bcc\u7684\u4ea4\u4e92\u5f62\u5f0f\u3002", "method": "\u63d0\u51farank-2\u6295\u5f71\u5b50\u7a7a\u95f4\u6765\u66f4\u51c6\u786e\u5730\u89e3\u6784PK\u548cCK\u8d21\u732e\uff0c\u5e76\u5728\u56db\u4e2aQA\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u5f00\u6e90\u6307\u4ee4\u8c03\u4f18LLM\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9996\u6b21\u8fdb\u884c\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6837\u5316\u7684\u77e5\u8bc6\u4ea4\u4e92\u5728rank-1\u5b50\u7a7a\u95f4\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728rank-2\u516c\u5f0f\u4e2d\u80fd\u6709\u6548\u6355\u6349\uff1b\u591a\u6b65\u5206\u6790\u63ed\u793a\u5e7b\u89c9NLE\u4e0ePK\u65b9\u5411\u5f3a\u5bf9\u9f50\uff0c\u4e0a\u4e0b\u6587\u5fe0\u5b9eNLE\u5e73\u8861PK\u548cCK\uff0c\u800c\u601d\u7ef4\u94fe\u63d0\u793a\u901a\u8fc7\u51cf\u5c11PK\u4f9d\u8d56\u5c06\u751f\u6210\u7684NLE\u5411CK\u65b9\u5411\u504f\u79fb\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u901a\u8fc7\u66f4\u4e30\u5bcc\u7684rank-2\u5b50\u7a7a\u95f4\u89e3\u6784\u6765\u7cfb\u7edf\u7814\u7a76LLM\u4e2d\u7684\u591a\u6b65\u77e5\u8bc6\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9996\u4e2a\u6846\u67b6\u3002"}}
{"id": "2511.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00821", "abs": "https://arxiv.org/abs/2511.00821", "authors": ["Ruoxiang Huang", "Xindian Ma", "Rundong Kong", "Zhen Yuan", "Peng Zhang"], "title": "OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance across\nvarious multimodal tasks, where position encoding plays a vital role in\nmodeling both the sequential structure of textual information and the spatial\nstructure of visual information. However, current VLMs commonly adopt\nmodality-unified 1D or 2D positional indexing strategies, which treat textual\nand visual tokens uniformly without accounting for their distinct structural\nproperties and sequential continuity for text and spatial coherence for vision.\nTo address this limitation, we propose OMEGA, a novel position encoding\nframework that employs Modality-Specific Position Encoding (MSPE) to assign\npositional indices while preserving the inherent structures of each modality\nacross separate coordinate dimensions. Additionally, to align the information\ndensity of multimodal data in the positional index space, OMEGA introduces\nGlobal Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the\nposition encoding step size of visual tokens based on the embedding entropy of\nboth modalities. Experimental results demonstrate that OMEGA consistently\nenhances VLM performance across diverse architectures and VQA benchmarks. On\nvisual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline\nposition encoding strategies on Qwen2.5-VL-3B, with consistent gains observed\nacross larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.", "AI": {"tldr": "OMEGA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4f4d\u7f6e\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u4f4d\u7f6e\u7f16\u7801\u548c\u5168\u5c40\u81ea\u9002\u5e94\u7f16\u7801\u6b65\u957f\u7f29\u653e\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u91c7\u7528\u7edf\u4e00\u76841D\u62162D\u4f4d\u7f6e\u7d22\u5f15\u7b56\u7565\uff0c\u6ca1\u6709\u8003\u8651\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u5728\u7ed3\u6784\u7279\u6027\u548c\u8fde\u7eed\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faOMEGA\u6846\u67b6\uff0c\u5305\u542b\u6a21\u6001\u7279\u5b9a\u4f4d\u7f6e\u7f16\u7801\uff08MSPE\uff09\u6765\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u53ca\u5168\u5c40\u81ea\u9002\u5e94\u7f16\u7801\u6b65\u957f\u7f29\u653e\uff08GAESS\uff09\u6765\u8c03\u6574\u89c6\u89c9token\u7684\u4f4d\u7f6e\u7f16\u7801\u6b65\u957f\u3002", "result": "\u5728Qwen2.5-VL-3B\u6a21\u578b\u4e0a\uff0cOMEGA\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7ebf\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u63d0\u5347\u4e863.43%\uff0c\u5728Qwen2.5-VL-7B\u548cLLaVA-v1.5-7B\u7b49\u66f4\u5927\u6a21\u578b\u4e0a\u4e5f\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "OMEGA\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u67b6\u6784\u548cVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.00831", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00831", "abs": "https://arxiv.org/abs/2511.00831", "authors": ["Xin Liu", "Aoyang Zhou", "Aoyang Zhou"], "title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack", "comment": "Accepted by NAACL2025 findings", "summary": "Visual-Language Pre-training (VLP) models have achieved significant\nperformance across various downstream tasks. However, they remain vulnerable to\nadversarial examples. While prior efforts focus on improving the adversarial\ntransferability of multimodal adversarial examples through cross-modal\ninteractions, these approaches suffer from overfitting issues, due to a lack of\ninput diversity by relying excessively on information from adversarial examples\nin one modality when crafting attacks in another. To address this issue, we\ndraw inspiration from strategies in some adversarial training methods and\npropose a novel attack called Local Shuffle and Sample-based Attack (LSSA).\nLSSA randomly shuffles one of the local image blocks, thus expanding the\noriginal image-text pairs, generating adversarial images, and sampling around\nthem. Then, it utilizes both the original and sampled images to generate the\nadversarial texts. Extensive experiments on multiple models and datasets\ndemonstrate that LSSA significantly enhances the transferability of multimodal\nadversarial examples across diverse VLP models and downstream tasks. Moreover,\nLSSA outperforms other advanced attacks on Large Vision-Language Models.", "AI": {"tldr": "\u63d0\u51faLSSA\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u56fe\u50cf\u5757\u968f\u673a\u6253\u4e71\u548c\u91c7\u6837\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u5728\u591a\u79cdVLP\u6a21\u578b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u6027\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u8f93\u5165\u591a\u6837\u6027\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u6a21\u6001\u7684\u5bf9\u6297\u6837\u672c\u4fe1\u606f\u3002", "method": "LSSA\u65b9\u6cd5\u968f\u673a\u6253\u4e71\u5c40\u90e8\u56fe\u50cf\u5757\u6765\u6269\u5c55\u539f\u59cb\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u751f\u6210\u5bf9\u6297\u56fe\u50cf\u5e76\u5728\u5176\u5468\u56f4\u91c7\u6837\uff0c\u7136\u540e\u5229\u7528\u539f\u59cb\u548c\u91c7\u6837\u56fe\u50cf\u751f\u6210\u5bf9\u6297\u6587\u672c\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSSA\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5bf9\u6297\u6837\u672c\u5728\u4e0d\u540cVLP\u6a21\u578b\u548c\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u6027\uff0c\u5e76\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "LSSA\u901a\u8fc7\u589e\u52a0\u8f93\u5165\u591a\u6837\u6027\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2511.01805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u548c\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u4f1a\u79ef\u7d2f\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u5176\u4fe1\u5ff5\u914d\u7f6e\u6587\u4ef6\u53d1\u751f\u663e\u8457\u53d8\u5316\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u884c\u4e3a\u548c\u54cd\u5e94\u4e00\u81f4\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u4e0a\u4e0b\u6587\u79ef\u7d2f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4fe1\u5ff5\u65e0\u58f0\u53d8\u5316\uff0c\u5e26\u6765\u7528\u6237\u4f53\u9a8c\u4e0d\u4e00\u81f4\u548c\u504f\u79bb\u539f\u59cb\u5bf9\u9f50\u7684\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u591a\u8f6e\u8ba8\u8bba\u9053\u5fb7\u56f0\u5883\u548c\u5b89\u5168\u67e5\u8be2\u6d4b\u8bd5GPT-5\u7684\u4fe1\u5ff5\u53d8\u5316\uff0c\u901a\u8fc7\u9605\u8bfb\u5bf9\u7acb\u7acb\u573a\u6587\u672c\u6d4b\u8bd5Grok 4\u7684\u653f\u6cbb\u7acb\u573a\u53d8\u5316\uff0c\u5e76\u8bbe\u8ba1\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u6765\u89c2\u5bdf\u884c\u4e3a\u53d8\u5316\u3002", "result": "GPT-5\u572810\u8f6e\u8ba8\u8bba\u540e\u4fe1\u5ff5\u53d8\u5316\u8fbe54.7%\uff0cGrok 4\u5728\u9605\u8bfb\u5bf9\u7acb\u6587\u672c\u540e\u653f\u6cbb\u7acb\u573a\u53d8\u531627.2%\uff0c\u884c\u4e3a\u53d8\u5316\u4e0e\u4fe1\u5ff5\u53d8\u5316\u4e00\u81f4\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u95f4\u5bf9\u8bdd\u6216\u9605\u8bfb\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4fe1\u5ff5\u6f02\u79fb\u7684\u9690\u85cf\u98ce\u9669\uff0c\u5bfc\u81f4\u5176\u89c2\u70b9\u548c\u884c\u4e3a\u4e0d\u53ef\u9760\u3002"}}
{"id": "2511.00833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00833", "abs": "https://arxiv.org/abs/2511.00833", "authors": ["Yifan Pu", "Jixuan Ying", "Qixiu Li", "Tianzhu Ye", "Dongchen Han", "Xiaochen Wang", "Ziyi Wang", "Xinyu Shao", "Gao Huang", "Xiu Li"], "title": "Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials", "comment": "NeurIPS 2025", "summary": "Vision Transformers (ViTs) have become a universal backbone for both image\nrecognition and image generation. Yet their Multi-Head Self-Attention (MHSA)\nlayer still performs a quadratic query-key interaction for every token pair,\nspending the bulk of computation on visually weak or redundant correlations. We\nintroduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that\ninjects an explicit notion of discrimination while reducing the theoretical\ncomplexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's\ndense query field into a handful of spatially pooled visual-contrast tokens,\nthen splits them into a learnable positive and negative stream whose\ndifferential interaction highlights what truly separates one region from\nanother. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,\nrequires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA\nlifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and\nimproves three strong hierarchical ViTs by up to 3.1%, while in\nclass-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points\nacross both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm\nthat (i) spatial pooling supplies low-variance global cues, (ii) dual\npositional embeddings are indispensable for contrastive reasoning, and (iii)\ncombining the two in both stages yields the strongest synergy. VCA therefore\noffers a simple path towards faster and sharper Vision Transformers. The source\ncode is available at https://github.com/LeapLabTHU/LinearDiff.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89c6\u89c9\u5bf9\u6bd4\u6ce8\u610f\u529b\uff08VCA\uff09\u4f5c\u4e3aMHSA\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u5224\u522b\u673a\u5236\u5c06\u7406\u8bba\u590d\u6742\u5ea6\u4eceO(NNC)\u964d\u81f3O(NnC)\uff0c\u5728\u56fe\u50cf\u8bc6\u522b\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfVision Transformers\u7684MHSA\u5c42\u5bf9\u6240\u6709token\u5bf9\u8fdb\u884c\u4e8c\u6b21\u67e5\u8be2-\u952e\u4ea4\u4e92\uff0c\u5c06\u5927\u91cf\u8ba1\u7b97\u82b1\u8d39\u5728\u89c6\u89c9\u4e0a\u5f31\u6216\u5197\u4f59\u7684\u76f8\u5173\u6027\u4e0a\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "VCA\u9996\u5148\u5c06\u6bcf\u4e2a\u5934\u7684\u5bc6\u96c6\u67e5\u8be2\u573a\u84b8\u998f\u4e3a\u5c11\u91cf\u7a7a\u95f4\u6c60\u5316\u7684\u89c6\u89c9\u5bf9\u6bd4token\uff0c\u7136\u540e\u5c06\u5176\u5206\u4e3a\u53ef\u5b66\u4e60\u7684\u6b63\u8d1f\u6d41\uff0c\u901a\u8fc7\u5dee\u5206\u4ea4\u4e92\u7a81\u51fa\u533a\u57df\u95f4\u7684\u771f\u6b63\u533a\u522b\u3002", "result": "VCA\u5c06DeiT-Tiny\u5728ImageNet-1K\u4e0a\u7684top-1\u51c6\u786e\u7387\u4ece72.2%\u63d0\u5347\u81f375.6%\uff08+3.4%\uff09\uff0c\u5728\u4e09\u79cd\u5f3a\u5c42\u6b21\u5316ViT\u4e0a\u63d0\u5347\u8fbe3.1%\uff0c\u5728\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4e2d\u5c06FID-50K\u964d\u4f4e2.1-5.2\u70b9\u3002", "conclusion": "VCA\u901a\u8fc7\u7a7a\u95f4\u6c60\u5316\u63d0\u4f9b\u4f4e\u65b9\u5dee\u5168\u5c40\u7ebf\u7d22\uff0c\u53cc\u4f4d\u7f6e\u5d4c\u5165\u5bf9\u5bf9\u6bd4\u63a8\u7406\u4e0d\u53ef\u6216\u7f3a\uff0c\u4e24\u8005\u7ed3\u5408\u4ea7\u751f\u6700\u5f3a\u534f\u540c\u6548\u5e94\uff0c\u4e3a\u66f4\u5feb\u66f4\u9510\u5229\u7684Vision Transformers\u63d0\u4f9b\u4e86\u7b80\u5355\u8def\u5f84\u3002"}}
{"id": "2511.01807", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5f15\u5bfc\u5b9e\u73b0\u7cbe\u786e\u7684\u957f\u5ea6\u63a7\u5236\uff0c\u5728\u6587\u6863\u6458\u8981\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5ea6\u9075\u5faa\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e2d\u77ed\u957f\u5ea6\u7ea6\u675f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u957f\u5ea6\u63a7\u5236\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u590d\u6742\u7684\u63a8\u7406\u65f6\u5de5\u5177\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5f15\u5bfc\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5728\u63d0\u793a\u4e2d\u5b9e\u73b0\u7cbe\u5fc3\u7684\u89c4\u5212\u548c\u5b57\u6570\u7edf\u8ba1\u673a\u5236\uff0c\u9f13\u52b1\u6a21\u578b\u4ed4\u7ec6\u8ddf\u8e2a\u5e76\u9075\u5b88\u6307\u5b9a\u7684\u957f\u5ea6\u7ea6\u675f\u3002", "result": "\u5728\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u5ea6\u9075\u5faa\u5ea6\uff0c\u67d0\u4e9b\u6a21\u578b\u63d0\u5347\u4e8637.6%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9700\u8981\u7cbe\u786e\u957f\u5ea6\u63a7\u5236\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7acb\u5373\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\u6216\u6210\u672c\u8fc7\u9ad8\u7684\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2511.00846", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00846", "abs": "https://arxiv.org/abs/2511.00846", "authors": ["Zhihao Peng", "Cheng Wang", "Shengyuan Liu", "Zhiying Liang", "Yixuan Yuan"], "title": "OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks", "comment": null, "summary": "Brain imaging analysis is vital for diagnosing and treating brain disorders,\nand multimodal large language models (MLLMs) are increasingly assisting in that\nanalysis. However, current brain-oriented visual question-answering (VQA)\nbenchmarks either cover a few imaging modalities or are limited to\ncoarse-grained pathological descriptions, hindering a comprehensive assessment\nof MLLMs throughout the full clinical continuum. To address these, we introduce\nOmniBrainBench, the first comprehensive multimodal VQA benchmark specifically\ndesigned to assess the multimodal comprehension capabilities of MLLMs in brain\nimaging analysis.OmniBrainBench consists of 15 distinct brain imaging\nmodalities collected from 30 verified medical sources, yielding 9,527 validated\nVQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15\nmulti-stage clinical tasks rigorously validated by a professional radiologist.\nEvaluation of 24 state-of-the-art models, including open-source, medical, and\nproprietary MLLMs, highlights the substantial challenges posed by\nOmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)\nbeat open-source and medical models but lag physicians; (2) medical MLLMs vary\nwidely in performance; (3) open-source MLLMs trail overall but excel in\nspecific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,\nrevealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new\nstandard for evaluating and advancing MLLMs in brain imaging analysis,\nhighlighting gaps compared to expert clinical reasoning. We release it at\nbenchmark \\& code.", "AI": {"tldr": "OmniBrainBench\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8111\u6210\u50cf\u5206\u6790\u4e2d\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b15\u79cd\u8111\u6210\u50cf\u6a21\u6001\u30019,527\u4e2a\u9a8c\u8bc1\u95ee\u7b54\u5bf9\u548c31,706\u5f20\u56fe\u50cf\uff0c\u6db5\u76d615\u4e2a\u591a\u9636\u6bb5\u4e34\u5e8a\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u9762\u5411\u8111\u90e8\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u8981\u4e48\u8986\u76d6\u7684\u6210\u50cf\u6a21\u6001\u6709\u9650\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u7684\u75c5\u7406\u63cf\u8ff0\uff0c\u963b\u788d\u4e86\u5bf9MLLMs\u5728\u6574\u4e2a\u4e34\u5e8a\u8fde\u7eed\u4f53\u4e2d\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b15\u79cd\u4e0d\u540c\u8111\u6210\u50cf\u6a21\u6001\u7684\u7efc\u5408\u591a\u6a21\u6001VQA\u57fa\u51c6\uff0c\u6a21\u62df\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6db5\u76d615\u4e2a\u591a\u9636\u6bb5\u4e34\u5e8a\u4efb\u52a1\uff0c\u5e76\u7531\u4e13\u4e1a\u653e\u5c04\u79d1\u533b\u751f\u4e25\u683c\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f3024\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u663e\u793a\uff1a(1)\u4e13\u6709MLLMs\u4f18\u4e8e\u5f00\u6e90\u548c\u533b\u7597\u6a21\u578b\u4f46\u843d\u540e\u4e8e\u533b\u751f\uff1b(2)\u533b\u7597MLLMs\u6027\u80fd\u5dee\u5f02\u5927\uff1b(3)\u5f00\u6e90MLLMs\u6574\u4f53\u843d\u540e\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff1b(4)MLLMs\u5728\u590d\u6742\u672f\u524d\u4efb\u52a1\u4e2d\u8868\u73b0\u660e\u663e\u4e0d\u4f73\uff0c\u63ed\u793a\u89c6\u89c9\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u5dee\u8ddd\u3002", "conclusion": "OmniBrainBench\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdbMLLMs\u5728\u8111\u6210\u50cf\u5206\u6790\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u7a81\u663e\u4e86\u4e0e\u4e13\u5bb6\u4e34\u5e8a\u63a8\u7406\u76f8\u6bd4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.01846", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/.", "AI": {"tldr": "IMO-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u7ade\u8d5b(IMO)\u7ea7\u522b\u7684\u9ad8\u7ea7\u63a8\u7406\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542bIMO-AnswerBench\u548cIMO-Proof Bench\u4e24\u4e2a\u90e8\u5206\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728Gemini Deep Think\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u5386\u53f2\u6027\u7684\u91d1\u724c\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u7b80\u5355\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u83b7\u53d6\u6b63\u786e\u7b54\u6848\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9ad8\u7ea7\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u8bc4\u4f30\u6807\u51c6\uff0c\u7279\u522b\u662fIMO\u7ea7\u522b\u7684\u590d\u6742\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86IMO-Bench\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u62ec400\u4e2a\u591a\u6837\u5316\u7684\u5965\u6797\u5339\u514b\u95ee\u9898\u7684IMO-AnswerBench\u548c\u9488\u5bf9\u8bc1\u660e\u5199\u4f5c\u80fd\u529b\u7684IMO-Proof Bench\uff0c\u5e76\u5efa\u7acb\u4e86\u8be6\u7ec6\u7684\u8bc4\u5206\u6307\u5357\u4ee5\u652f\u6301\u81ea\u52a8\u8bc4\u5206\u3002", "result": "Gemini Deep Think\u6a21\u578b\u5728IMO-AnswerBench\u4e0a\u8fbe\u523080.0%\uff0c\u5728\u9ad8\u7ea7IMO-Proof Bench\u4e0a\u8fbe\u523065.7%\uff0c\u5206\u522b\u6bd4\u6700\u4f73\u975eGemini\u6a21\u578b\u9ad8\u51fa6.9%\u548c42.4%\u3002", "conclusion": "IMO-Bench\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u65b9\u9762\u7684\u8fdb\u6b65\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u8bc4\u5206\u5668\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u826f\u597d\u76f8\u5173\u6027\u3002"}}
{"id": "2511.00858", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00858", "abs": "https://arxiv.org/abs/2511.00858", "authors": ["Yu Liu", "Zhijie Liu", "Zedong Yang", "You-Fu Li", "He Kong"], "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction", "comment": "This manuscript has been accepted to the IEEE Transactions on\n  Intelligent Transportation Systems as a regular paper", "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of\nmobile robots and intelligent vehicles. Although recent deep learning-based\nmodels have shown significant success in forecasting intentions, few consider\nincomplete observation under occlusion scenarios. To tackle this challenge, we\npropose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded\nmotion patterns and leverages them to guide future intention prediction. During\nthe denoising stage, we introduce an occlusion-aware diffusion transformer\narchitecture to estimate noise features associated with occluded patterns,\nthereby enhancing the model's ability to capture contextual relationships in\noccluded semantic scenarios. Furthermore, an occlusion mask-guided reverse\nprocess is introduced to effectively utilize observation information, reducing\nthe accumulation of prediction errors and enhancing the accuracy of\nreconstructed motion features. The performance of the proposed method under\nvarious occlusion scenarios is comprehensively evaluated and compared with\nexisting methods on popular benchmarks, namely PIE and JAAD. Extensive\nexperimental results demonstrate that the proposed method achieves more robust\nperformance than existing methods in the literature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u906e\u6321\u611f\u77e5\u6269\u6563\u6a21\u578b\uff08ODM\uff09\uff0c\u7528\u4e8e\u5728\u906e\u6321\u573a\u666f\u4e0b\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\uff0c\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u6321\u7684\u8fd0\u52a8\u6a21\u5f0f\u6765\u6307\u5bfc\u672a\u6765\u610f\u56fe\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5f88\u5c11\u8003\u8651\u906e\u6321\u573a\u666f\u4e0b\u7684\u4e0d\u5b8c\u6574\u89c2\u5bdf\u95ee\u9898\u3002", "method": "\u91c7\u7528\u906e\u6321\u611f\u77e5\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5728\u53bb\u566a\u9636\u6bb5\u4f30\u8ba1\u4e0e\u906e\u6321\u6a21\u5f0f\u76f8\u5173\u7684\u566a\u58f0\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u906e\u6321\u63a9\u7801\u5f15\u5bfc\u7684\u53cd\u5411\u8fc7\u7a0b\u6765\u6709\u6548\u5229\u7528\u89c2\u5bdf\u4fe1\u606f\u3002", "result": "\u5728PIE\u548cJAAD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u906e\u6321\u573a\u666f\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u906e\u6321\u611f\u77e5\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u906e\u6321\u573a\u666f\u4e0b\u7684\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2511.00916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00916", "abs": "https://arxiv.org/abs/2511.00916", "authors": ["Yan Shu", "Chi Liu", "Robin Chen", "Derek Li", "Bryan Dai"], "title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\neffectiveness in various general-domain scenarios, such as visual question\nanswering and image captioning. Recently, researchers have increasingly focused\non empowering MLLMs with medical conversational abilities, which hold\nsignificant promise for clinical applications. However, medical data presents\nunique challenges due to its heterogeneous nature -- encompassing diverse\nmodalities including 2D images, 3D volumetric scans, and temporal video\nsequences. The substantial domain gap and data format inconsistencies across\nthese modalities have hindered the development of unified medical MLLMs. To\naddress these challenges, we propose Fleming-VL, a unified end-to-end framework\nfor comprehensive medical visual understanding across heterogeneous modalities.\nFleming-VL tackles this problem from a data-centric perspective through three\nkey strategies: (1) scaling up pretraining by integrating long-context data\nfrom both natural and medical-specific domains; (2) complementing fine-tuning\nwith rare medical data, including holistic video analysis and underrepresented\n2D modalities such as ultrasound and dermoscopy images; (3) extending existing\nevaluation frameworks to incorporate 3D volumetric and video understanding\nbenchmarks. Through supervised fine-tuning (SFT) and group relative policy\noptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive\nexperiments demonstrate that Fleming-VL achieves state-of-the-art performance\nacross multiple benchmarks, including medical VQA, video QA, and 3D medical\nimage understanding. We publicly release Fleming-VL to promote transparent,\nreproducible, and auditable progress in medical AI.", "AI": {"tldr": "Fleming-VL\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5f02\u6784\u533b\u5b66\u6570\u636e\uff08\u5305\u62ec2D\u56fe\u50cf\u30013D\u4f53\u79ef\u626b\u63cf\u548c\u65f6\u5e8f\u89c6\u9891\u5e8f\u5217\uff09\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u533b\u5b66\u6570\u636e\u5177\u6709\u5f02\u6784\u6027\uff0c\u5305\u542b2D\u56fe\u50cf\u30013D\u4f53\u79ef\u626b\u63cf\u548c\u65f6\u5e8f\u89c6\u9891\u5e8f\u5217\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u8fd9\u4e9b\u6a21\u6001\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\u548c\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u963b\u788d\u4e86\u7edf\u4e00\u533b\u5b66MLLMs\u7684\u53d1\u5c55\u3002", "method": "\u4ece\u6570\u636e\u4e2d\u5fc3\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u91c7\u7528\u4e09\u4e2a\u5173\u952e\u7b56\u7565\uff1a(1) \u6574\u5408\u81ea\u7136\u548c\u533b\u5b66\u7279\u5b9a\u9886\u57df\u7684\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u6269\u5c55\uff1b(2) \u4f7f\u7528\u7f55\u89c1\u533b\u5b66\u6570\u636e\uff08\u5305\u62ec\u6574\u4f53\u89c6\u9891\u5206\u6790\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u76842D\u6a21\u6001\uff09\u8fdb\u884c\u5fae\u8c03\u8865\u5145\uff1b(3) \u6269\u5c55\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4ee5\u7eb3\u51653D\u4f53\u79ef\u548c\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u3002\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u5f00\u53d1\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u3002", "result": "Fleming-VL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u533b\u5b66VQA\u3001\u89c6\u9891QA\u548c3D\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u3002", "conclusion": "Fleming-VL\u516c\u5f00\u53d1\u5e03\uff0c\u65e8\u5728\u4fc3\u8fdb\u533b\u5b66AI\u9886\u57df\u7684\u900f\u660e\u3001\u53ef\u590d\u73b0\u548c\u53ef\u5ba1\u8ba1\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.00956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00956", "abs": "https://arxiv.org/abs/2511.00956", "authors": ["Liuzhuozheng Li", "Yue Gong", "Shanyuan Liu", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Dengyang Jiang", "Zanyi Wang", "Dawei Leng", "Yuhui Yin"], "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference", "comment": null, "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.", "AI": {"tldr": "EVTAR\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\uff0c\u4ec5\u9700\u6e90\u56fe\u50cf\u548c\u76ee\u6807\u670d\u88c5\u4f5c\u4e3a\u8f93\u5165\uff0c\u65e0\u9700\u590d\u6742\u9884\u5904\u7406\uff0c\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u589e\u5f3a\u8bd5\u7a7f\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u8f93\u5165\uff08\u5982\u4e0d\u53ef\u77e5\u4eba\u7269\u56fe\u50cf\u3001\u4eba\u4f53\u59ff\u6001\u7b49\uff09\uff0c\u5bfc\u81f4\u52b3\u52a8\u5bc6\u96c6\u4e14\u4e0d\u5b9e\u7528\u3002EVTAR\u65e8\u5728\u7b80\u5316\u8f93\u5165\u8981\u6c42\uff0c\u63d0\u9ad8\u73b0\u5b9e\u5e94\u7528\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528\u989d\u5916\u53c2\u8003\u56fe\u50cf\uff08\u4e0d\u540c\u4eba\u7a7f\u7740\u76f8\u540c\u670d\u88c5\uff09\u6765\u4fdd\u6301\u670d\u88c5\u7eb9\u7406\u548c\u7ec6\u8282\uff0c\u6a21\u62df\u4eba\u7c7b\u9009\u62e9\u670d\u88c5\u65f6\u7684\u53c2\u8003\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u4e00\u81f4\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "EVTAR\u901a\u8fc7\u7b80\u5316\u8f93\u5165\u8981\u6c42\u548c\u5229\u7528\u53c2\u8003\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u66f4\u73b0\u5b9e\u548c\u9ad8\u8d28\u91cf\u7684\u865a\u62df\u8bd5\u7a7f\u6548\u679c\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.00962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00962", "abs": "https://arxiv.org/abs/2511.00962", "authors": ["Dongheng Lin", "Mengxue Qu", "Kunyang Han", "Jianbo Jiao", "Xiaojie Jin", "Yunchao Wei"], "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis", "comment": "NeurIPS 2025 poster", "summary": "Most video-anomaly research stops at frame-wise detection, offering little\ninsight into why an event is abnormal, typically outputting only frame-wise\nanomaly scores without spatial or semantic context. Recent video anomaly\nlocalization and video anomaly understanding methods improve explainability but\nremain data-dependent and task-specific. We propose a unified reasoning\nframework that bridges the gap between temporal detection, spatial\nlocalization, and textual explanation. Our approach is built upon a chained\ntest-time reasoning process that sequentially connects these tasks, enabling\nholistic zero-shot anomaly analysis without any additional training.\nSpecifically, our approach leverages intra-task reasoning to refine temporal\ndetections and inter-task chaining for spatial and semantic understanding,\nyielding improved interpretability and generalization in a fully zero-shot\nmanner. Without any additional data or gradients, our method achieves\nstate-of-the-art zero-shot performance across multiple video anomaly detection,\nlocalization, and explanation benchmarks. The results demonstrate that careful\nprompt design with task-wise chaining can unlock the reasoning power of\nfoundation models, enabling practical, interpretable video anomaly analysis in\na fully zero-shot manner. Project Page:\nhttps://rathgrith.github.io/Unified_Frame_VAA/.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u8fde\u63a5\u65f6\u95f4\u68c0\u6d4b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u89e3\u91ca\u4efb\u52a1\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5168\u9762\u7684\u5f02\u5e38\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u7814\u7a76\u5927\u591a\u505c\u7559\u5728\u5e27\u7ea7\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u7a7a\u95f4\u548c\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u65e0\u6cd5\u89e3\u91ca\u5f02\u5e38\u539f\u56e0\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u4ecd\u4f9d\u8d56\u6570\u636e\u548c\u7279\u5b9a\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u94fe\u5f0f\u6d4b\u8bd5\u65f6\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4efb\u52a1\u5185\u63a8\u7406\u4f18\u5316\u65f6\u95f4\u68c0\u6d4b\uff0c\u4efb\u52a1\u95f4\u94fe\u5f0f\u8fde\u63a5\u5b9e\u73b0\u7a7a\u95f4\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u5728\u5b8c\u5168\u96f6\u6837\u672c\u65b9\u5f0f\u4e0b\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u68af\u5ea6\u66f4\u65b0\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4e0e\u4efb\u52a1\u94fe\u5f0f\u8fde\u63a5\u53ef\u4ee5\u91ca\u653e\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u5b9e\u7528\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u5f02\u5e38\u5206\u6790\u3002"}}
{"id": "2511.00981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00981", "abs": "https://arxiv.org/abs/2511.00981", "authors": ["Suzhong Fu", "Rui Sun", "Xuan Ding", "Jingqi Dong", "Yiming Yang", "Yao Zhu", "Min Chang Jordan Ren", "Delin Deng", "Angelica Aviles-Rivero", "Shuguang Cui", "Zhen Li"], "title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel", "comment": null, "summary": "Accurate vessel segmentation is critical for clinical applications such as\ndisease diagnosis and surgical planning, yet remains challenging due to thin,\nbranching structures and low texture contrast. While foundation models like the\nSegment Anything Model (SAM) have shown promise in generic segmentation, they\nperform sub-optimally on vascular structures. In this work, we present VesSAM,\na powerful and efficient framework tailored for 2D vessel segmentation. VesSAM\nintegrates (1) a convolutional adapter to enhance local texture features, (2) a\nmulti-prompt encoder that fuses anatomical prompts, including skeletons,\nbifurcation points, and segment midpoints, via hierarchical cross-attention,\nand (3) a lightweight mask decoder to reduce jagged artifacts. We also\nintroduce an automated pipeline to generate structured multi-prompt\nannotations, and curate a diverse benchmark dataset spanning 8 datasets across\n5 imaging modalities. Experimental results demonstrate that VesSAM consistently\noutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%\nIoU, and achieves competitive performance compared to fully fine-tuned methods,\nwith significantly fewer parameters. VesSAM also generalizes well to\nout-of-distribution (OoD) settings, outperforming all baselines in average OoD\nDice and IoU.", "AI": {"tldr": "VesSAM\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e2D\u8840\u7ba1\u5206\u5272\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5377\u79ef\u9002\u914d\u5668\u3001\u591a\u63d0\u793a\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u63a9\u7801\u89e3\u7801\u5668\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u7684\u8840\u7ba1\u5206\u5272\u5bf9\u4e8e\u75be\u75c5\u8bca\u65ad\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8840\u7ba1\u7ed3\u6784\u7ec6\u8584\u3001\u5206\u652f\u590d\u6742\u4e14\u7eb9\u7406\u5bf9\u6bd4\u5ea6\u4f4e\uff0c\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5982SAM\u5728\u8840\u7ba1\u5206\u5272\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "VesSAM\u6846\u67b6\u5305\u542b\uff1a(1)\u5377\u79ef\u9002\u914d\u5668\u589e\u5f3a\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff1b(2)\u591a\u63d0\u793a\u7f16\u7801\u5668\u901a\u8fc7\u5206\u5c42\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u89e3\u5256\u5b66\u63d0\u793a\uff08\u9aa8\u67b6\u3001\u5206\u53c9\u70b9\u3001\u6bb5\u4e2d\u70b9\uff09\uff1b(3)\u8f7b\u91cf\u7ea7\u63a9\u7801\u89e3\u7801\u5668\u51cf\u5c11\u952f\u9f7f\u4f2a\u5f71\u3002\u8fd8\u5f15\u5165\u4e86\u81ea\u52a8\u5316\u591a\u63d0\u793a\u6ce8\u91ca\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVesSAM\u57288\u4e2a\u6570\u636e\u96c6\u30015\u79cd\u6210\u50cf\u6a21\u6001\u4e0a\uff0c\u6bd4\u6700\u5148\u8fdb\u7684PEFT-based SAM\u53d8\u4f53\u5728Dice\u548cIoU\u4e0a\u5206\u522b\u63d0\u5347\u8d85\u8fc710%\u548c13%\uff0c\u4e0e\u5b8c\u5168\u5fae\u8c03\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u4f46\u53c2\u6570\u66f4\u5c11\uff0c\u5728\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VesSAM\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u9ad8\u6548\u7684\u8840\u7ba1\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u7ec4\u4ef6\u663e\u8457\u63d0\u5347\u4e86\u8840\u7ba1\u5206\u5272\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\u3002"}}
{"id": "2511.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01000", "abs": "https://arxiv.org/abs/2511.01000", "authors": ["Hassan Ugail", "Ismail Lujain Jaleel"], "title": "Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya", "comment": null, "summary": "Art authentication of Francisco Goya's works presents complex computational\nchallenges due to his heterogeneous stylistic evolution and extensive\nhistorical patterns of forgery. We introduce a novel multimodal machine\nlearning framework that applies identical feature extraction techniques to both\nvisual and X-ray radiographic images of Goya paintings. The unified feature\nextraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,\nLocal Binary Patterns, entropy measures, energy calculations, and colour\ndistribution analysis applied consistently across both imaging modalities. The\nextracted features from both visual and X-ray images are processed through an\noptimised One-Class Support Vector Machine with hyperparameter tuning. Using a\ndataset of 24 authenticated Goya paintings with corresponding X-ray images,\nsplit into an 80/20 train-test configuration with 10-fold cross-validation, the\nframework achieves 97.8% classification accuracy with a 0.022 false positive\nrate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy\nof our pipeline, achieving 92.3% authentication confidence through unified\nmultimodal feature analysis. Our results indicate substantial performance\nimprovement over single-modal approaches, establishing the effectiveness of\napplying identical computational methods to both visual and radiographic\nimagery in art authentication applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6208\u96c5\u753b\u4f5c\u8ba4\u8bc1\u7684\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7279\u5f81\u63d0\u53d6\u6280\u672f\u5904\u7406\u89c6\u89c9\u548cX\u5c04\u7ebf\u56fe\u50cf\uff0c\u4f7f\u7528\u4f18\u5316\u7684\u4e00\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5b9e\u73b097.8%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u6208\u96c5\u4f5c\u54c1\u7684\u827a\u672f\u8ba4\u8bc1\u9762\u4e34\u590d\u6742\u8ba1\u7b97\u6311\u6218\uff0c\u5305\u62ec\u5176\u5f02\u8d28\u98ce\u683c\u6f14\u53d8\u548c\u5e7f\u6cdb\u7684\u5386\u53f2\u4f2a\u9020\u6a21\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u8ba4\u8bc1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7279\u5f81\u63d0\u53d6\u7ba1\u9053\uff0c\u5305\u62ec\u7070\u5ea6\u5171\u751f\u77e9\u9635\u63cf\u8ff0\u7b26\u3001\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\u3001\u71b5\u6d4b\u91cf\u3001\u80fd\u91cf\u8ba1\u7b97\u548c\u989c\u8272\u5206\u5e03\u5206\u6790\uff0c\u5e94\u7528\u4e8e\u89c6\u89c9\u548cX\u5c04\u7ebf\u56fe\u50cf\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u4e00\u7c7b\u652f\u6301\u5411\u91cf\u673a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u572824\u5e45\u8ba4\u8bc1\u6208\u96c5\u753b\u4f5c\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752880/20\u8bad\u7ec3\u6d4b\u8bd5\u914d\u7f6e\u548c10\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u6846\u67b6\u8fbe\u523097.8%\u5206\u7c7b\u51c6\u786e\u7387\u548c0.022\u5047\u9633\u6027\u7387\u3002\u6848\u4f8b\u7814\u7a76\u663e\u793a\u5bf9\u300a\u5de8\u4eba\u300b\u7684\u8ba4\u8bc1\u7f6e\u4fe1\u5ea6\u8fbe92.3%\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u5728\u89c6\u89c9\u548c\u653e\u5c04\u5f71\u50cf\u4e0a\u5e94\u7528\u76f8\u540c\u8ba1\u7b97\u65b9\u6cd5\u5728\u827a\u672f\u8ba4\u8bc1\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.01013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01013", "abs": "https://arxiv.org/abs/2511.01013", "authors": ["Mohammad Amanour Rahman"], "title": "HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images", "comment": "This manuscript has been submitted to Informatics in Medicine\n  Unlocked", "summary": "B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,\noperator dependency, and indistinct boundaries. Existing deep learning suffers\nfrom single-task learning, architectural constraints (CNNs lack global context,\nTransformers local features), and black-box decision-making. These gaps hinder\nclinical adoption.\n  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous\nsegmentation and classification with intrinsic interpretability. Its\ndual-branch encoder integrates EfficientNet-B3 and Swin Transformer via\nmulti-scale hierarchical fusion blocks. An attention-gated decoder provides\nprecision and explainability. We introduce dual-pipeline interpretability: (1)\nintrinsic attention validation with quantitative IoU verification (mean: 0.86),\nand (2) Grad-CAM for classification reasoning.\n  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and\naccuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant\nRecall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling\nyields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant\nRecall, eliminating false negatives. Ablation studies confirm multi-scale\nfusion contributes +16.8% Dice and attention gates add +5.9%.\n  Crucially, we conduct the first cross-dataset generalization study for hybrid\nCNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),\nconfirming domain shift. However, progressive fine-tuning with only 10%\ntarget-domain data (68 images) recovers 92.5% performance. With 50% data, our\nmodel achieves 77.3% Dice, exceeding source-domain performance (76.1%) and\ndemonstrating true generalization.", "AI": {"tldr": "HyFormer-Net\u662f\u4e00\u79cd\u6df7\u5408CNN-Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u4e73\u817a\u764c\u8d85\u58f0\u56fe\u50cf\u7684\u540c\u6b65\u5206\u5272\u548c\u5206\u7c7b\uff0c\u5177\u6709\u5185\u5728\u53ef\u89e3\u91ca\u6027\uff0c\u5728BUSI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6570\u636e\u96c6\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3B\u578b\u8d85\u58f0\u4e73\u817a\u764c\u8bca\u65ad\u9762\u4e34\u7684\u6311\u6218\uff1a\u6591\u70b9\u566a\u58f0\u3001\u64cd\u4f5c\u8005\u4f9d\u8d56\u6027\u548c\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7684\u5355\u4efb\u52a1\u5b66\u4e60\u3001\u67b6\u6784\u9650\u5236\uff08CNN\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u3001Transformer\u7f3a\u4e4f\u5c40\u90e8\u7279\u5f81\uff09\u548c\u9ed1\u76d2\u51b3\u7b56\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faHyFormer-Net\u6df7\u5408\u67b6\u6784\uff0c\u5305\u542b\u53cc\u5206\u652f\u7f16\u7801\u5668\uff08\u96c6\u6210EfficientNet-B3\u548cSwin Transformer\uff09\u3001\u591a\u5c3a\u5ea6\u5206\u5c42\u878d\u5408\u6a21\u5757\u548c\u6ce8\u610f\u529b\u95e8\u63a7\u89e3\u7801\u5668\uff0c\u5e76\u5f15\u5165\u53cc\u7ba1\u9053\u53ef\u89e3\u91ca\u6027\u673a\u5236\uff08\u5185\u5728\u6ce8\u610f\u529b\u9a8c\u8bc1\u548cGrad-CAM\uff09\u3002", "result": "\u5728BUSI\u6570\u636e\u96c6\u4e0a\u8fbe\u5230Dice\u5206\u65700.761\u00b10.072\u548c\u51c6\u786e\u738793.2%\uff0c\u6076\u6027\u53ec\u56de\u738792.1\u00b12.2%\uff1b\u96c6\u6210\u6a21\u578b\u5b9e\u73b0Dice 90.2%\u3001\u51c6\u786e\u738799.5%\u548c100%\u6076\u6027\u53ec\u56de\u7387\uff1b\u4ea4\u53c9\u6570\u636e\u96c6\u7814\u7a76\u4e2d\uff0c\u4ec5\u752810%\u76ee\u6807\u57df\u6570\u636e\u5373\u53ef\u6062\u590d92.5%\u6027\u80fd\u3002", "conclusion": "HyFormer-Net\u5728\u4e73\u817a\u764c\u8d85\u58f0\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u548c\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u6709\u9650\u76ee\u6807\u57df\u6570\u636e\u4e0b\u5b9e\u73b0\u6709\u6548\u6cdb\u5316\u7684\u80fd\u529b\uff0c\u4e3a\u4e34\u5e8a\u91c7\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.01079", "categories": ["cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.01079", "abs": "https://arxiv.org/abs/2511.01079", "authors": ["Nikolay I. Kalmykov", "Razan Dibo", "Kaiyu Shen", "Xu Zhonghan", "Anh-Huy Phan", "Yipeng Liu", "Ivan Oseledets"], "title": "T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression", "comment": "Submitted to Information Systems. Code will be released upon journal\n  publication", "summary": "Neural image compression (NIC) has become the state-of-the-art for\nrate-distortion performance, yet its security vulnerabilities remain\nsignificantly less understood than those of classifiers. Existing adversarial\nattacks on NICs are often naive adaptations of pixel-space methods, overlooking\nthe unique, structured nature of the compression pipeline. In this work, we\npropose a more advanced class of vulnerabilities by introducing T-MLA, the\nfirst targeted multiscale log--exponential attack framework. Our approach\ncrafts adversarial perturbations in the wavelet domain by directly targeting\nthe quality of the attacked and reconstructed images. This allows for a\nprincipled, offline attack where perturbations are strategically confined to\nspecific wavelet subbands, maximizing distortion while ensuring perceptual\nstealth. Extensive evaluation across multiple state-of-the-art NIC\narchitectures on standard image compression benchmarks reveals a large drop in\nreconstruction quality while the perturbations remain visually imperceptible.\nOur findings reveal a critical security flaw at the core of generative and\ncontent delivery pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86T-MLA\u653b\u51fb\u6846\u67b6\uff0c\u9488\u5bf9\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7cfb\u7edf\u8fdb\u884c\u9488\u5bf9\u6027\u591a\u5c3a\u5ea6\u5bf9\u6570\u6307\u6570\u653b\u51fb\uff0c\u901a\u8fc7\u5728\u5c0f\u6ce2\u57df\u4e2d\u5236\u4f5c\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u663e\u8457\u964d\u4f4e\u91cd\u5efa\u56fe\u50cf\u8d28\u91cf\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7cfb\u7edf\u7684\u653b\u51fb\u65b9\u6cd5\u591a\u4e3a\u50cf\u7d20\u7a7a\u95f4\u65b9\u6cd5\u7684\u7b80\u5355\u6539\u7f16\uff0c\u5ffd\u89c6\u4e86\u538b\u7f29\u7ba1\u9053\u7684\u72ec\u7279\u7ed3\u6784\u5316\u7279\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u6f0f\u6d1e\u5229\u7528\u65b9\u6cd5\u3002", "method": "\u63d0\u51faT-MLA\u653b\u51fb\u6846\u67b6\uff0c\u5728\u5c0f\u6ce2\u57df\u4e2d\u5236\u4f5c\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u76f4\u63a5\u9488\u5bf9\u653b\u51fb\u540e\u548c\u91cd\u5efa\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5c06\u6270\u52a8\u7b56\u7565\u6027\u5730\u9650\u5236\u5728\u7279\u5b9a\u5c0f\u6ce2\u5b50\u5e26\u4e2d\uff0c\u6700\u5927\u5316\u5931\u771f\u540c\u65f6\u786e\u4fdd\u611f\u77e5\u9690\u853d\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u67b6\u6784\u548c\u6807\u51c6\u56fe\u50cf\u538b\u7f29\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u91cd\u5efa\u8d28\u91cf\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u6270\u52a8\u5728\u89c6\u89c9\u4e0a\u4ecd\u7136\u4e0d\u53ef\u5bdf\u89c9\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u751f\u6210\u548c\u5185\u5bb9\u4f20\u8f93\u7ba1\u9053\u6838\u5fc3\u5b58\u5728\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7cfb\u7edf\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\u3002"}}
{"id": "2511.01082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01082", "abs": "https://arxiv.org/abs/2511.01082", "authors": ["Narges Ghasemi", "Amir Ziashahabi", "Salman Avestimehr", "Cyrus Shahabi"], "title": "GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Image geolocalization, the task of determining an image's geographic origin,\nposes significant challenges, largely due to visual similarities across\ndisparate locations and the large search space. To address these issues, we\npropose a hierarchical sequence prediction approach inspired by how humans\nnarrow down locations from broad regions to specific addresses. Analogously,\nour model predicts geographic tokens hierarchically, first identifying a\ngeneral region and then sequentially refining predictions to increasingly\nprecise locations. Rather than relying on explicit semantic partitions, our\nmethod uses S2 cells, a nested, multiresolution global grid, and sequentially\npredicts finer-level cells conditioned on visual inputs and previous\npredictions. This procedure mirrors autoregressive text generation in large\nlanguage models. Much like in language modeling, final performance depends not\nonly on training but also on inference-time strategy. We investigate multiple\ntop-down traversal methods for autoregressive sampling, incorporating\ntechniques from test-time compute scaling used in language models.\nSpecifically, we integrate beam search and multi-sample inference while\nexploring various selection strategies to determine the final output. This\nenables the model to manage uncertainty by exploring multiple plausible paths\nthrough the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k\ndatasets against two distinct sets of baselines: those that operate without a\nMultimodal Large Language Model (MLLM) and those that leverage one. In the\nMLLM-free setting, our model surpasses other comparable baselines on nearly all\nmetrics, achieving state-of-the-art performance with accuracy gains of up to\n13.9%. When augmented with an MLLM, our model outperforms all baselines,\nsetting a new state-of-the-art across all metrics. The source code is available\nat https://github.com/NNargesNN/GeoToken.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u5e8f\u5217\u9884\u6d4b\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528S2\u7f51\u683c\u5355\u5143\u4ece\u7c97\u5230\u7ec6\u9010\u6b65\u7cbe\u786e\u5b9a\u4f4d\uff0c\u7ed3\u5408\u6ce2\u675f\u641c\u7d22\u548c\u591a\u6837\u672c\u63a8\u7406\u7b56\u7565\uff0c\u5728Im2GPS3k\u548cYFCC4k\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4e2d\u56e0\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u5927\u641c\u7d22\u7a7a\u95f4\u5e26\u6765\u7684\u6311\u6218\uff0c\u6a21\u4eff\u4eba\u7c7b\u4ece\u5927\u533a\u57df\u5230\u5177\u4f53\u5730\u5740\u7684\u5b9a\u4f4d\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528S2\u7f51\u683c\u5355\u5143\u6784\u5efa\u5206\u5c42\u7ed3\u6784\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u4f9d\u6b21\u9884\u6d4b\u66f4\u7cbe\u7ec6\u7684\u7f51\u683c\u5355\u5143\uff0c\u7ed3\u5408\u6ce2\u675f\u641c\u7d22\u548c\u591a\u6837\u672c\u63a8\u7406\u7b49\u8bed\u8a00\u6a21\u578b\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u65e0MLLM\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8fbe13.9%\uff1b\u7ed3\u5408MLLM\u65f6\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u5206\u5c42\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u95ee\u9898\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.01340", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01340", "abs": "https://arxiv.org/abs/2511.01340", "authors": ["Trishanu Das", "Abhilash Nandy", "Khush Bajaj", "Deepiha S"], "title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles", "comment": "7 pages, 5 figures, 4 tables", "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse\nbenchmark of $1,333$ English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and\n$20-30\\%$ using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b1333\u4e2a\u82f1\u8bedRebus\u8c1c\u9898\u7684\u5927\u578b\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86RebusDescProgICE\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u975e\u7ed3\u6784\u5316\u63cf\u8ff0\u548c\u57fa\u4e8e\u4ee3\u7801\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728Rebus\u8c1c\u9898\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "Rebus\u8c1c\u9898\u9700\u8981\u56fe\u50cf\u8bc6\u522b\u3001\u8ba4\u77e5\u6280\u80fd\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u591a\u6b65\u63a8\u7406\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u6587\u5b57\u6e38\u620f\u7b49\u591a\u79cd\u80fd\u529b\uff0c\u8fd9\u5bf9\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86RebusDescProgICE\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u63cf\u8ff0\u548c\u57fa\u4e8e\u4ee3\u7801\u7684\u7ed3\u6784\u5316\u63a8\u7406\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u4e0a\u4e0b\u6587\u793a\u4f8b\u9009\u62e9\u7b56\u7565\u3002", "result": "\u76f8\u6bd4\u601d\u7ef4\u94fe\u63a8\u7406\uff0c\u8be5\u6846\u67b6\u5728\u95ed\u6e90\u6a21\u578b\u4e0a\u63d0\u5347\u4e862.1-4.1%\u7684\u6027\u80fd\uff0c\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\u63d0\u5347\u4e8620-30%\u7684\u6027\u80fd\u3002", "conclusion": "RebusDescProgICE\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742Rebus\u8c1c\u9898\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u975e\u7ed3\u6784\u5316\u63cf\u8ff0\u548c\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.01618", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01618", "abs": "https://arxiv.org/abs/2511.01618", "authors": ["Xiaoyu Zhan", "Wenxuan Huang", "Hao Sun", "Xinyu Fu", "Changfeng Ma", "Shaosheng Cao", "Bohan Jia", "Shaohui Lin", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Viewpoint Learning\u4efb\u52a1\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7Viewpoint-100K\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57282D\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u590d\u67423D\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u80fd\u5426\u6709\u6548\u6355\u6349\u8be6\u7ec6\u7684\u7a7a\u95f4\u4fe1\u606f\u4ee5\u786e\u4fdd\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86Viewpoint Learning\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5bf9\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u56fe\u50cf\u548c\u95ee\u7b54\u5bf9\u7684Viewpoint-100K\u6570\u636e\u96c6\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6ce8\u5165\u57fa\u7840\u77e5\u8bc6\uff0c\u7136\u540e\u4f7f\u7528GRPO\u7b97\u6cd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u6df7\u5408\u51b7\u542f\u52a8\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6fc0\u6d3b\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53d1\u5c55\u57fa\u7840\u7a7a\u95f4\u6280\u80fd\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u81ea\u4e3b\u7cfb\u7edf\u548c3D\u573a\u666f\u7406\u89e3\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.01098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01098", "abs": "https://arxiv.org/abs/2511.01098", "authors": ["Veronica Marsico", "Antonio Quintero-Rincon", "Hadj Batatia"], "title": "Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images", "comment": "12 pages, 6 figures, 3 tables", "summary": "This study presents a novel method for diagnosing respiratory diseases using\nimage data. It combines Epanechnikov's non-parametric kernel density estimation\n(EKDE) with a bimodal logistic regression classifier in a\nstatistical-model-based learning scheme. EKDE's flexibility in modeling data\ndistributions without assuming specific shapes and its adaptability to pixel\nintensity variations make it valuable for extracting key features from medical\nimages. The method was tested on 13808 randomly selected chest X-rays from the\nCOVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of\n59.26%, and a specificity of 74.18%, demonstrating moderate performance in\ndetecting respiratory disease while showing room for improvement in\nsensitivity. While clinical expertise remains essential for further refining\nthe model, this study highlights the potential of EKDE-based approaches to\nenhance diagnostic accuracy and reliability in medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Epanechnikov\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u53cc\u5cf0\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u4e8e\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u547c\u5438\u7cfb\u7edf\u75be\u75c5\uff0c\u5728COVID-19\u80f8\u90e8X\u5149\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e2d\u7b49\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u5efa\u6a21\u533b\u5b66\u56fe\u50cf\u6570\u636e\u5206\u5e03\u800c\u4e0d\u9700\u8981\u5047\u8bbe\u7279\u5b9a\u5f62\u72b6\u7684\u8bca\u65ad\u65b9\u6cd5\uff0c\u63d0\u9ad8\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528Epanechnikov\u975e\u53c2\u6570\u6838\u5bc6\u5ea6\u4f30\u8ba1(EKDE)\u7ed3\u5408\u53cc\u5cf0\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u5668\uff0c\u6784\u5efa\u57fa\u4e8e\u7edf\u8ba1\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u6848\uff0c\u5229\u7528EKDE\u5bf9\u50cf\u7d20\u5f3a\u5ea6\u53d8\u5316\u7684\u9002\u5e94\u6027\u63d0\u53d6\u533b\u5b66\u56fe\u50cf\u5173\u952e\u7279\u5f81\u3002", "result": "\u572813808\u5f20\u968f\u673a\u9009\u62e9\u7684\u80f8\u90e8X\u5149\u56fe\u50cf\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u738770.14%\uff0c\u654f\u611f\u602759.26%\uff0c\u7279\u5f02\u602774.18%\uff0c\u5728\u68c0\u6d4b\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u65b9\u9762\u8868\u73b0\u51fa\u4e2d\u7b49\u6027\u80fd\uff0c\u4f46\u654f\u611f\u6027\u6709\u5f85\u63d0\u9ad8\u3002", "conclusion": "\u867d\u7136\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u4e8e\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u4ecd\u7136\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u9879\u7814\u7a76\u7a81\u51fa\u4e86\u57fa\u4e8eEKDE\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.01109", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01109", "abs": "https://arxiv.org/abs/2511.01109", "authors": ["Alexander Thorley", "Agis Chartsias", "Jordan Strom", "Jeremy Slivnick", "Dipak Kotecha", "Alberto Gomez", "Jinming Duan"], "title": "Anatomically Constrained Transformers for Echocardiogram Analysis", "comment": null, "summary": "Video transformers have recently demonstrated strong potential for\nechocardiogram (echo) analysis, leveraging self-supervised pre-training and\nflexible adaptation across diverse tasks. However, like other models operating\non videos, they are prone to learning spurious correlations from non-diagnostic\nregions such as image backgrounds. To overcome this limitation, we propose the\nVideo Anatomically Constrained Transformer (ViACT), a novel framework that\nintegrates anatomical priors directly into the transformer architecture. ViACT\nrepresents a deforming anatomical structure as a point set and encodes both its\nspatial geometry and corresponding image patches into transformer tokens.\nDuring pre-training, ViACT follows a masked autoencoding strategy that masks\nand reconstructs only anatomical patches, enforcing that representation\nlearning is focused on the anatomical region. The pre-trained model can then be\nfine-tuned for tasks localized to this region. In this work we focus on the\nmyocardium, demonstrating the framework on echo analysis tasks such as left\nventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)\ndetection. The anatomical constraint focuses transformer attention within the\nmyocardium, yielding interpretable attention maps aligned with regions of known\nCA pathology. Moreover, ViACT generalizes to myocardium point tracking without\nrequiring task-specific components such as correlation volumes used in\nspecialized tracking networks.", "AI": {"tldr": "ViACT\u662f\u4e00\u4e2a\u5c06\u89e3\u5256\u5148\u9a8c\u77e5\u8bc6\u76f4\u63a5\u96c6\u6210\u5230transformer\u67b6\u6784\u4e2d\u7684\u89c6\u9891\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u53d8\u5f62\u89e3\u5256\u7ed3\u6784\u4e3a\u70b9\u96c6\u5e76\u7f16\u7801\u5176\u7a7a\u95f4\u51e0\u4f55\u548c\u56fe\u50cf\u5757\uff0c\u5728\u9884\u8bad\u7ec3\u65f6\u4ec5\u91cd\u5efa\u89e3\u5256\u533a\u57df\uff0c\u4f7f\u8868\u793a\u5b66\u4e60\u4e13\u6ce8\u4e8e\u89e3\u5256\u533a\u57df\u3002", "motivation": "\u89c6\u9891transformer\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5bb9\u6613\u4ece\u975e\u8bca\u65ad\u533a\u57df\uff08\u5982\u56fe\u50cf\u80cc\u666f\uff09\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faViACT\u6846\u67b6\uff0c\u5c06\u89e3\u5256\u7ed3\u6784\u8868\u793a\u4e3a\u70b9\u96c6\uff0c\u7f16\u7801\u7a7a\u95f4\u51e0\u4f55\u548c\u56fe\u50cf\u5757\u5230transformer tokens\uff0c\u91c7\u7528\u63a9\u7801\u81ea\u7f16\u7801\u7b56\u7565\u4ec5\u91cd\u5efa\u89e3\u5256\u533a\u57df\uff0c\u9884\u8bad\u7ec3\u540e\u53ef\u7528\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\u3002", "result": "ViACT\u5728\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\u56de\u5f52\u548c\u5fc3\u810f\u6dc0\u7c89\u6837\u53d8\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6ce8\u610f\u529b\u56fe\u4e0e\u5df2\u77e5\u75c5\u7406\u533a\u57df\u5bf9\u9f50\uff0c\u4e14\u65e0\u9700\u7279\u5b9a\u7ec4\u4ef6\u5373\u53ef\u6cdb\u5316\u5230\u5fc3\u808c\u70b9\u8ddf\u8e2a\u4efb\u52a1\u3002", "conclusion": "ViACT\u901a\u8fc7\u89e3\u5256\u7ea6\u675f\u5c06transformer\u6ce8\u610f\u529b\u805a\u7126\u4e8e\u5fc3\u808c\u533a\u57df\uff0c\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u5e76\u5728\u591a\u4e2a\u8d85\u58f0\u5fc3\u52a8\u56fe\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01129", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01129", "abs": "https://arxiv.org/abs/2511.01129", "authors": ["Fabio Diniz Rossi"], "title": "Boosting performance of computer vision applications through embedded GPUs on the edge", "comment": "4 pages, 6 figures", "summary": "Computer vision applications, especially those using augmented reality\ntechnology, are becoming quite popular in mobile devices. However, this type of\napplication is known as presenting significant demands regarding resources. In\norder to enable its utilization in devices with more modest resources, edge\ncomputing can be used to offload certain high intensive tasks. Still, edge\ncomputing is usually composed of devices with limited capacity, which may\nimpact in users quality of experience when using computer vision applications.\nThis work proposes the use of embedded devices with graphics processing units\n(GPUs) to overcome such limitation. Experiments performed shown that GPUs can\nattain a performance gain when compared to using only CPUs, which guarantee a\nbetter experience to users using such kind of application.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4f7f\u7528\u5d4c\u5165\u5f0fGPU\u8bbe\u5907\u6765\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u6027\u80fd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1GPU\u76f8\u6bd4CPU\u80fd\u83b7\u5f97\u6027\u80fd\u589e\u76ca\uff0c\u4ece\u800c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u5bf9\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u901a\u5e38\u5bb9\u91cf\u6709\u9650\uff0c\u4f1a\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u3002", "method": "\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u4f7f\u7528\u5e26\u6709\u56fe\u5f62\u5904\u7406\u5355\u5143\uff08GPU\uff09\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u6765\u5378\u8f7d\u9ad8\u5f3a\u5ea6\u7684\u8ba1\u7b97\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528CPU\u76f8\u6bd4\uff0cGPU\u80fd\u591f\u83b7\u5f97\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u4f7f\u7528\u5d4c\u5165\u5f0fGPU\u8bbe\u5907\u53ef\u4ee5\u514b\u670d\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u7684\u6027\u80fd\u9650\u5236\uff0c\u4e3a\u7528\u6237\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u4f53\u9a8c\u3002"}}
{"id": "2511.01131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01131", "abs": "https://arxiv.org/abs/2511.01131", "authors": ["Md Nahiduzzaman", "Steven Korevaar", "Alireza Bab-Hadiashar", "Ruwan Tennakoon"], "title": "Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis", "comment": null, "summary": "Human-interpretable predictions are essential for deploying AI in medical\nimaging, yet most interpretable-by-design (IBD) frameworks require concept\nannotations for training data, which are costly and impractical to obtain in\nclinical contexts. Recent attempts to bypass annotation, such as zero-shot\nvision-language models or concept-generation frameworks, struggle to capture\ndomain-specific medical features, leading to poor reliability. In this paper,\nwe propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised\nframework that enables concept answer prediction without explicit supervision\nor reliance on language models. PCP leverages class-level concept priors as\nweak supervision and incorporates a refinement mechanism with KL divergence and\nentropy regularization to align predictions with clinical reasoning.\nExperiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves\nconcept-level F1-score by over 33% compared to zero-shot baselines, while\ndelivering competitive classification performance on four medical datasets\n(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept\nbottleneck models (CBMs) and V-IP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6982\u5ff5\u6807\u6ce8\u7684\u5f31\u76d1\u7763\u6982\u5ff5\u9884\u6d4b\u6846\u67b6PCP\uff0c\u5229\u7528\u7c7b\u522b\u7ea7\u6982\u5ff5\u5148\u9a8c\u4f5c\u4e3a\u5f31\u76d1\u7763\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u9884\u6d4b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfAI\u9700\u8981\u53ef\u89e3\u91ca\u9884\u6d4b\uff0c\u4f46\u73b0\u6709\u53ef\u89e3\u91ca\u8bbe\u8ba1\u6846\u67b6\u9700\u8981\u6602\u8d35\u7684\u6982\u5ff5\u6807\u6ce8\uff0c\u800c\u96f6\u5c04\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u533b\u5b66\u9886\u57df\u7279\u5f81\u3002", "method": "PCP\u6846\u67b6\u4f7f\u7528\u7c7b\u522b\u7ea7\u6982\u5ff5\u5148\u9a8c\u4f5c\u4e3a\u5f31\u76d1\u7763\uff0c\u7ed3\u5408KL\u6563\u5ea6\u548c\u71b5\u6b63\u5219\u5316\u7684\u7cbe\u70bc\u673a\u5236\u6765\u5bf9\u9f50\u4e34\u5e8a\u63a8\u7406\u3002", "result": "\u5728PH2\u548cWBCatt\u6570\u636e\u96c6\u4e0a\uff0cPCP\u76f8\u6bd4\u96f6\u5c04\u57fa\u7ebf\u5c06\u6982\u5ff5\u7ea7F1\u5206\u6570\u63d0\u5347\u8d85\u8fc733%\uff0c\u5728\u56db\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u7ade\u4e89\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "PCP\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u6216\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u65b9\u6848\uff0c\u5728\u533b\u5b66\u5f71\u50cf\u53ef\u89e3\u91ca\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.01163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01163", "abs": "https://arxiv.org/abs/2511.01163", "authors": ["Yongyuan Liang", "Wei Chow", "Feng Li", "Ziqiao Ma", "Xiyao Wang", "Jiageng Mao", "Jiuhai Chen", "Jiatao Gu", "Yue Wang", "Furong Huang"], "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation", "comment": "Project Page: https://roverbench.github.io/", "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.", "AI": {"tldr": "ROVER\u662f\u4e00\u4e2a\u9488\u5bf9\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b1312\u4e2a\u4efb\u52a1\u548c1876\u5f20\u56fe\u50cf\uff0c\u91cd\u70b9\u5173\u6ce8\u89c6\u89c9\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u76f8\u4e92\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c06\u591a\u6a21\u6001\u80fd\u529b\u5b64\u7acb\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6709\u6548\u6d4b\u8bd5\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8fd9\u662f\u5b9e\u73b0\u771f\u6b63\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u7684\u6838\u5fc3\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u8bbe\u7f6e\u7684\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\uff1a\u8bed\u8a00\u589e\u5f3a\u7684\u89c6\u89c9\u751f\u6210\u63a8\u7406\uff08\u7528\u8bed\u8a00\u63d0\u793a\u6307\u5bfc\u56fe\u50cf\u5408\u6210\uff09\u548c\u89c6\u89c9\u589e\u5f3a\u7684\u8bed\u8a00\u751f\u6210\u63a8\u7406\uff08\u751f\u6210\u4e2d\u95f4\u53ef\u89c6\u5316\u6765\u52a0\u5f3a\u63a8\u7406\u8fc7\u7a0b\uff09\u3002", "result": "\u5bf917\u4e2a\u7edf\u4e00\u6a21\u578b\u7684\u5b9e\u9a8c\u53d1\u73b0\uff1a\u8de8\u6a21\u6001\u63a8\u7406\u51b3\u5b9a\u89c6\u89c9\u751f\u6210\u8d28\u91cf\uff0c\u4ea4\u9519\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u4ea4\u9519\u6a21\u578b\uff1b\u6a21\u578b\u5728\u7269\u7406\u548c\u7b26\u53f7\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u5206\u79bb\uff0c\u80fd\u89e3\u91ca\u611f\u77e5\u6982\u5ff5\u4f46\u65e0\u6cd5\u4e3a\u7b26\u53f7\u4efb\u52a1\u6784\u5efa\u89c6\u89c9\u62bd\u8c61\u3002", "conclusion": "\u76f8\u4e92\u8de8\u6a21\u6001\u63a8\u7406\u662f\u5b9e\u73b0\u771f\u6b63\u5168\u6a21\u6001\u751f\u6210\u7684\u5173\u952e\u524d\u6cbf\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6b64\u65b9\u9762\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\u3002"}}
{"id": "2511.01169", "categories": ["cs.CV", "I.2.10; I.4.5"], "pdf": "https://arxiv.org/pdf/2511.01169", "abs": "https://arxiv.org/abs/2511.01169", "authors": ["Brian Nlong Zhao", "Jiajun Wu", "Shangzhe Wu"], "title": "Web-Scale Collection of Video Data for 4D Animal Reconstruction", "comment": "NeurIPS 2025 Datasets and Benchmarks", "summary": "Computer vision for animals holds great promise for wildlife research but\noften depends on large-scale data, while existing collection methods rely on\ncontrolled capture setups. Recent data-driven approaches show the potential of\nsingle-view, non-invasive analysis, yet current animal video datasets are\nlimited--offering as few as 2.4K 15-frame clips and lacking key processing for\nanimal-centric 3D/4D tasks. We introduce an automated pipeline that mines\nYouTube videos and processes them into object-centric clips, along with\nauxiliary annotations valuable for downstream tasks like pose estimation,\ntracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos\n(2M frames)--an order of magnitude more than prior works. To demonstrate its\nutility, we focus on the 4D quadruped animal reconstruction task. To support\nthis task, we present Animal-in-Motion (AiM), a benchmark of 230 manually\nfiltered sequences with 11K frames showcasing clean, diverse animal motions. We\nevaluate state-of-the-art model-based and model-free methods on\nAnimal-in-Motion, finding that 2D metrics favor the former despite unrealistic\n3D shapes, while the latter yields more natural reconstructions but scores\nlower--revealing a gap in current evaluation. To address this, we enhance a\nrecent model-free approach with sequence-level optimization, establishing the\nfirst 4D animal reconstruction baseline. Together, our pipeline, benchmark, and\nbaseline aim to advance large-scale, markerless 4D animal reconstruction and\nrelated tasks from in-the-wild videos. Code and datasets are available at\nhttps://github.com/briannlongzhao/Animal-in-Motion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\u4eceYouTube\u89c6\u9891\u4e2d\u63d0\u53d6\u52a8\u7269\u4e2d\u5fc3\u526a\u8f91\uff0c\u6784\u5efa\u4e86\u5305\u542b30K\u89c6\u9891\uff082M\u5e27\uff09\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u521b\u5efa\u4e86Animal-in-Motion\u57fa\u51c6\uff0c\u7528\u4e8e4D\u56db\u8db3\u52a8\u7269\u91cd\u5efa\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u52a8\u7269\u89c6\u9891\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff08\u4ec52.4K 15\u5e27\u526a\u8f91\uff09\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u7269\u4e2d\u5fc33D/4D\u4efb\u52a1\u7684\u5173\u952e\u5904\u7406\uff0c\u9700\u8981\u4ece\u91ce\u5916\u89c6\u9891\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u65e0\u6807\u8bb0\u76844D\u52a8\u7269\u91cd\u5efa\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u7a0b\u6316\u6398YouTube\u89c6\u9891\u5e76\u5904\u7406\u6210\u5bf9\u8c61\u4e2d\u5fc3\u526a\u8f91\uff0c\u9644\u5e26\u59ff\u6001\u4f30\u8ba1\u3001\u8ddf\u8e2a\u548c3D/4D\u91cd\u5efa\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6709\u4ef7\u503c\u8f85\u52a9\u6ce8\u91ca\u3002", "result": "\u6536\u96c6\u4e8630K\u89c6\u9891\uff082M\u5e27\uff09\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u591a\u4e00\u4e2a\u6570\u91cf\u7ea7\uff1b\u521b\u5efa\u4e86\u5305\u542b230\u4e2a\u624b\u52a8\u7b5b\u9009\u5e8f\u5217\u7684Animal-in-Motion\u57fa\u51c6\uff1b\u53d1\u73b0\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u57282D\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\u4f463D\u5f62\u72b6\u4e0d\u771f\u5b9e\uff0c\u800c\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\u4ea7\u751f\u66f4\u81ea\u7136\u7684\u91cd\u5efa\u4f46\u5f97\u5206\u8f83\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u7ba1\u9053\u3001\u57fa\u51c6\u548c\u57fa\u7ebf\uff0c\u65e8\u5728\u63a8\u8fdb\u4ece\u91ce\u5916\u89c6\u9891\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u65e0\u6807\u8bb0\u76844D\u52a8\u7269\u91cd\u5efa\u53ca\u76f8\u5173\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u5efa\u7acb\u4e86\u9996\u4e2a4D\u52a8\u7269\u91cd\u5efa\u57fa\u7ebf\u3002"}}
{"id": "2511.01175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01175", "abs": "https://arxiv.org/abs/2511.01175", "authors": ["Peng Du", "Hui Li", "Han Xu", "Paul Barom Jeon", "Dongwook Lee", "Daehyun Ji", "Ran Yang", "Feng Zhu"], "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution", "comment": null, "summary": "Discrete Wavelet Transform (DWT) has been widely explored to enhance the\nperformance of image superresolution (SR). Despite some DWT-based methods\nimproving SR by capturing fine-grained frequency signals, most existing\napproaches neglect the interrelations among multiscale frequency sub-bands,\nresulting in inconsistencies and unnatural artifacts in the reconstructed\nimages. To address this challenge, we propose a Diffusion Transformer model\nbased on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the\nsuperiority of diffusion models and transformers to capture the interrelations\namong multiscale frequency sub-bands, leading to a more consistence and\nrealistic SR image. Specifically, we use a Multi-level Discrete Wavelet\nTransform (MDWT) to decompose images into wavelet spectra. A pyramid\ntokenization method is proposed which embeds the spectra into a sequence of\ntokens for transformer model, facilitating to capture features from both\nspatial and frequency domain. A dual-decoder is designed elaborately to handle\nthe distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,\nwithout omitting their alignment in image generation. Extensive experiments on\nmultiple benchmark datasets demonstrate the effectiveness of our method, with\nhigh performance on both perception quality and fidelity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u50cf\u5c0f\u6ce2\u8c31\u7684\u6269\u6563Transformer\u6a21\u578bDTWSR\uff0c\u901a\u8fc7\u6355\u6349\u591a\u5c3a\u5ea6\u9891\u7387\u5b50\u5e26\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u6765\u89e3\u51b3\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDWT\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u591a\u5c3a\u5ea6\u9891\u7387\u5b50\u5e26\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u548c\u4e0d\u81ea\u7136\u4f2a\u5f71\u3002", "method": "\u4f7f\u7528\u591a\u7ea7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5206\u89e3\u56fe\u50cf\u4e3a\u5c0f\u6ce2\u8c31\uff0c\u63d0\u51fa\u91d1\u5b57\u5854\u6807\u8bb0\u5316\u65b9\u6cd5\u5c06\u8c31\u5d4c\u5165\u4e3atoken\u5e8f\u5217\uff0c\u8bbe\u8ba1\u53cc\u89e3\u7801\u5668\u5206\u522b\u5904\u7406\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\u7684\u5dee\u5f02\u5e76\u4fdd\u6301\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u3002", "conclusion": "DTWSR\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u5c3a\u5ea6\u9891\u7387\u5b50\u5e26\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u751f\u6210\u66f4\u4e00\u81f4\u548c\u771f\u5b9e\u7684\u8d85\u5206\u8fa8\u7387\u56fe\u50cf\u3002"}}
{"id": "2511.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01200", "abs": "https://arxiv.org/abs/2511.01200", "authors": ["Mengyuan Liu", "Sheng Yan", "Yong Wang", "Yingjie Li", "Gui-Bin Bian", "Hong Liu"], "title": "MoSa: Motion Generation with Scalable Autoregressive Modeling", "comment": null, "summary": "We introduce MoSa, a novel hierarchical motion generation framework for\ntext-driven 3D human motion generation that enhances the Vector\nQuantization-guided Generative Transformers (VQ-GT) paradigm through a\ncoarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale\nToken Preservation Strategy (MTPS) integrated into a hierarchical residual\nvector quantization variational autoencoder (RQ-VAE). MTPS employs\ninterpolation at each hierarchical quantization to effectively retain\ncoarse-to-fine multi-scale tokens. With this, the generative transformer\nsupports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,\nunlike traditional methods that predict only one token at each step.\nConsequently, MoSa requires only 10 inference steps, matching the number of\nRQ-VAE quantization layers. To address potential reconstruction degradation\nfrom frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive\nconvolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and\nincorporates attention mechanisms to better capture global dependencies.\nExtensive experiments show that MoSa achieves state-of-the-art generation\nquality and efficiency, outperforming prior methods in both fidelity and speed.\nOn the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)\nwhile reducing inference time by 27 percent. Moreover, MoSa generalizes well to\ndownstream tasks such as motion editing, requiring no additional fine-tuning.\nThe code is available at https://mosa-web.github.io/MoSa-web", "AI": {"tldr": "MoSa\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u9a71\u52a8\u76843D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u7684\u5206\u5c42\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6807\u8bb0\u4fdd\u7559\u7b56\u7565\u548c\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u62103D\u4eba\u4f53\u8fd0\u52a8\u65f6\u9700\u8981\u5927\u91cf\u63a8\u7406\u6b65\u9aa4\uff0c\u6548\u7387\u8f83\u4f4e\u3002MoSa\u65e8\u5728\u901a\u8fc7\u5206\u5c42\u91cf\u5316\u7b56\u7565\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u6807\u8bb0\u4fdd\u7559\u7b56\u7565(MTPS)\u548c\u5206\u5c42\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(RQ-VAE)\uff0c\u7ed3\u5408\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u5efa\u6a21(SAR)\u548c\u8f7b\u91cf\u7ea7\u5377\u79ef-\u6ce8\u610f\u529b\u6df7\u5408VQ-VAE(CAQ-VAE)\u3002", "result": "\u5728Motion-X\u6570\u636e\u96c6\u4e0aFID\u8fbe\u52300.06\uff0c\u76f8\u6bd4MoMask\u76840.20\u6709\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1127%\uff0c\u5728\u8fd0\u52a8\u7f16\u8f91\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "MoSa\u901a\u8fc7\u5206\u5c42\u8fd0\u52a8\u751f\u6210\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01210", "abs": "https://arxiv.org/abs/2511.01210", "authors": ["Heyu Guo", "Shanmu Wang", "Ruichun Ma", "Shiqi Jiang", "Yasaman Ghasempour", "Omid Abari", "Baining Guo", "Lili Qi"], "title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA", "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization for\naction prediction through large-scale vision-language pretraining. However,\nmost existing models rely solely on RGB cameras, limiting their perception and,\nconsequently, manipulation capabilities. We present OmniVLA, an omni-modality\nVLA model that integrates novel sensing modalities for physically-grounded\nspatial intelligence beyond RGB perception. The core of our approach is the\nsensor-masked image, a unified representation that overlays spatially grounded\nand physically meaningful masks onto the RGB images, derived from sensors\nincluding an infrared camera, a mmWave radar, and a microphone array. This\nimage-native unification keeps sensor input close to RGB statistics to\nfacilitate training, provides a uniform interface across sensor hardware, and\nenables data-efficient learning with lightweight per-sensor projectors. Built\non this, we present a multisensory vision-language-action model architecture\nand train the model based on an RGB-pretrained VLA backbone. We evaluate\nOmniVLA on challenging real-world tasks where sensor-modality perception is\nneeded to guide the manipulation. OmniVLA achieves an average task success rate\nof 84%, significantly outperforms both RGB-only and raw-sensor-input baseline\nmodels by 59% and 28% respectively, meanwhile showing higher learning\nefficiency and stronger generalization capability.", "AI": {"tldr": "OmniVLA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u7ea2\u5916\u76f8\u673a\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548c\u9ea6\u514b\u98ce\u9635\u5217\u7b49\u65b0\u578b\u4f20\u611f\u6a21\u6001\uff0c\u8d85\u8d8a\u4e86\u4ec5\u4f9d\u8d56RGB\u76f8\u673a\u7684\u611f\u77e5\u9650\u5236\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8684%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56RGB\u76f8\u673a\uff0c\u9650\u5236\u4e86\u611f\u77e5\u80fd\u529b\u548c\u64cd\u4f5c\u80fd\u529b\u3002\u9700\u8981\u6574\u5408\u66f4\u591a\u4f20\u611f\u6a21\u6001\u6765\u589e\u5f3a\u7269\u7406\u7a7a\u95f4\u667a\u80fd\u3002", "method": "\u63d0\u51fa\u4f20\u611f\u5668\u63a9\u7801\u56fe\u50cf\u7684\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u7a7a\u95f4\u57fa\u7840\u548c\u7269\u7406\u610f\u4e49\u7684\u63a9\u7801\u53e0\u52a0\u5230RGB\u56fe\u50cf\u4e0a\u3002\u57fa\u4e8eRGB\u9884\u8bad\u7ec3\u7684VLA\u9aa8\u5e72\u6784\u5efa\u591a\u611f\u5b98\u67b6\u6784\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4f20\u611f\u5668\u6295\u5f71\u5668\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u5728\u9700\u8981\u4f20\u611f\u5668\u6a21\u6001\u611f\u77e5\u7684\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cOmniVLA\u8fbe\u523084%\u7684\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\uff0c\u6bd4\u4ec5RGB\u6a21\u578b\u548c\u539f\u59cb\u4f20\u611f\u5668\u8f93\u5165\u57fa\u7ebf\u5206\u522b\u9ad8\u51fa59%\u548c28%\uff0c\u540c\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OmniVLA\u901a\u8fc7\u591a\u6a21\u6001\u4f20\u611f\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u611f\u77e5\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u4f20\u611f\u5668\u63a9\u7801\u56fe\u50cf\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7269\u7406\u57fa\u7840\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.01213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01213", "abs": "https://arxiv.org/abs/2511.01213", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Parijat Deshpande", "Venkataramana Runkana"], "title": "Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering", "comment": "10 pages, 11 figures, 6 tables", "summary": "The immense diversity in the culture and culinary of Indian cuisines calls\nattention to the major shortcoming of the existing Visual Question\nAnswering(VQA) systems which are inclined towards the foods from Western\nregion. Recent attempt towards building a VQA dataset for Indian food is a step\ntowards addressing this challenge. However, their approach towards VQA follows\na two-step process in which the answer is generated first, followed by the\nexplanation of the expected answer. In this work, we claim that food VQA\nrequires to follow a multi-step reasoning process to arrive at an accurate\nanswer, especially in the context of India food, which involves understanding\ncomplex culinary context and identifying relationships between various food\nitems. With this hypothesis we create reasoning chains upon the QA with minimal\nhuman intervention. We fine-tune smaller LLMs and VLMs with auto-validated\nreasoning chains and further train them using reinforcement learning with\nlarger data. With augmentation of reasoning chains, we observed accuracy\nimprovement of an average 10 percentage points on the baseline. We provide\ndetailed analysis in terms the effect of addition of reasoning chains for the\nIndian Food VQA task.\n  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge\nGraph.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5370\u5ea6\u98df\u7269\u7684\u591a\u6b65\u63a8\u7406\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u63a8\u7406\u94fe\u6765\u63d0\u5347VQA\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e8610\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709VQA\u7cfb\u7edf\u4e3b\u8981\u9762\u5411\u897f\u65b9\u98df\u7269\uff0c\u65e0\u6cd5\u5904\u7406\u5370\u5ea6\u98df\u7269\u7684\u6587\u5316\u591a\u6837\u6027\u548c\u590d\u6742\u70f9\u996a\u80cc\u666f\u3002\u73b0\u6709\u5370\u5ea6\u98df\u7269VQA\u6570\u636e\u96c6\u91c7\u7528\u4e24\u6b65\u6cd5\uff08\u5148\u751f\u6210\u7b54\u6848\u518d\u89e3\u91ca\uff09\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u624d\u80fd\u51c6\u786e\u7406\u89e3\u5370\u5ea6\u98df\u7269\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "1. \u521b\u5efa\u81ea\u52a8\u9a8c\u8bc1\u7684\u63a8\u7406\u94fe\uff0c\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\uff1b2. \u5bf9\u8f83\u5c0f\u7684LLM\u548cVLM\u8fdb\u884c\u5fae\u8c03\uff1b3. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff1b4. \u901a\u8fc7\u63a8\u7406\u94fe\u589e\u5f3a\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u63a8\u7406\u94fe\u589e\u5f3a\uff0c\u5728\u5370\u5ea6\u98df\u7269VQA\u4efb\u52a1\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e8610\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u7406\u94fe\u6dfb\u52a0\u6548\u679c\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u5bf9\u4e8e\u5904\u7406\u5370\u5ea6\u98df\u7269\u7684\u590d\u6742\u70f9\u996a\u80cc\u666f\u548c\u98df\u7269\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u63a8\u7406\u94fe\u589e\u5f3a\u80fd\u663e\u8457\u63d0\u5347VQA\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u6587\u5316\u591a\u6837\u6027\u98df\u7269\u7684\u89c6\u89c9\u95ee\u7b54\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01237", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01237", "abs": "https://arxiv.org/abs/2511.01237", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video", "comment": "Accepted at RAAI 2025", "summary": "Human gaze offers rich supervisory signals for understanding visual attention\nin complex visual environments. In this paper, we propose Eyes on Target, a\nnovel depth-aware and gaze-guided object detection framework designed for\negocentric videos. Our approach injects gaze-derived features into the\nattention mechanism of a Vision Transformer (ViT), effectively biasing spatial\nfeature selection toward human-attended regions. Unlike traditional object\ndetectors that treat all regions equally, our method emphasises\nviewer-prioritised areas to enhance object detection. We validate our method on\nan egocentric simulator dataset where human visual attention is critical for\ntask assessment, illustrating its potential in evaluating human performance in\nsimulation scenarios. We evaluate the effectiveness of our gaze-integrated\nmodel through extensive experiments and ablation studies, demonstrating\nconsistent gains in detection accuracy over gaze-agnostic baselines on both the\ncustom simulator dataset and public benchmarks, including Ego4D Ego-Motion and\nEgo-CH-Gaze datasets. To interpret model behaviour, we also introduce a\ngaze-aware attention head importance metric, revealing how gaze cues modulate\ntransformer attention dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEyes on Target\u7684\u6df1\u5ea6\u611f\u77e5\u548c\u6ce8\u89c6\u5f15\u5bfc\u7684\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6ce8\u89c6\u7279\u5f81\u6ce8\u5165Vision Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u504f\u5411\u4eba\u7c7b\u6ce8\u89c6\u533a\u57df\u6765\u589e\u5f3a\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u6ce8\u89c6\u4e3a\u7406\u89e3\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u4f46\u5728\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u5668\u4e2d\u6240\u6709\u533a\u57df\u88ab\u5e73\u7b49\u5bf9\u5f85\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6ce8\u89c6\u4fe1\u606f\u6765\u5f3a\u8c03\u89c2\u5bdf\u8005\u4f18\u5148\u5173\u6ce8\u7684\u533a\u57df\u3002", "method": "\u5728Vision Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u6ce8\u5165\u6ce8\u89c6\u884d\u751f\u7279\u5f81\uff0c\u4f7f\u7a7a\u95f4\u7279\u5f81\u9009\u62e9\u504f\u5411\u4eba\u7c7b\u6ce8\u89c6\u533a\u57df\uff0c\u5e76\u5f15\u5165\u4e86\u6ce8\u89c6\u611f\u77e5\u6ce8\u610f\u529b\u5934\u91cd\u8981\u6027\u6307\u6807\u6765\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u6a21\u62df\u5668\u6570\u636e\u96c6\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff08Ego4D Ego-Motion\u548cEgo-CH-Gaze\u6570\u636e\u96c6\uff09\u4e0a\uff0c\u76f8\u6bd4\u65e0\u89c6\u6ce8\u89c6\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u4fe1\u606f\u6765\u589e\u5f3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u89c6\u9891\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\uff0c\u5728\u6a21\u62df\u573a\u666f\u4e2d\u8bc4\u4f30\u4eba\u7c7b\u6027\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u52a8\u6001\u5206\u6790\u63ed\u793a\u4e86\u6ce8\u89c6\u7ebf\u7d22\u5982\u4f55\u8c03\u8282Transformer\u6ce8\u610f\u529b\u3002"}}
{"id": "2511.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01240", "abs": "https://arxiv.org/abs/2511.01240", "authors": ["Zhixuan Zhang", "Pingyu Wang", "Xingjian Zheng", "Linbo Qing", "Qi Liu"], "title": "Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability", "comment": "Accepted by Pattern Recognition in Nov 01,2025", "summary": "Transferable attacks generate adversarial examples on surrogate models to\nfool unknown victim models, posing real-world threats and growing research\ninterest. Despite focusing on flat losses for transferable adversarial\nexamples, recent studies still fall into suboptimal regions, especially the\nflat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce\na novel black-box gradient-based transferable attack from a perspective of\ndual-order information. Specifically, we feasibly propose Adversarial Flatness\n(AF) to the deceptive flatness problem and a theoretical assurance for\nadversarial transferability. Based on this, using an efficient approximation of\nour objective, we instantiate our attack as Adversarial Flatness Attack (AFA),\naddressing the altered gradient sign issue. Additionally, to further improve\nthe attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by\nenhancing the inner-loop sampling efficiency. The comprehensive results on\nImageNet-compatible dataset demonstrate superiority over six baselines,\ngenerating adversarial examples in flatter regions and boosting transferability\nacross model architectures. When tested on input transformation attacks or the\nBaidu Cloud API, our method outperforms baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u9636\u4fe1\u606f\u7684\u9ed1\u76d2\u68af\u5ea6\u53ef\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u5e73\u5766\u5ea6(AF)\u89e3\u51b3\u6b3a\u9a97\u6027\u5e73\u5766\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u5bf9\u6297\u5e73\u5766\u5ea6\u653b\u51fb(AFA)\u548c\u8499\u7279\u5361\u6d1b\u5bf9\u6297\u91c7\u6837(MCAS)\u6765\u63d0\u5347\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u53ef\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e73\u5766\u635f\u5931\uff0c\u4f46\u4ecd\u9677\u5165\u6b21\u4f18\u533a\u57df\uff0c\u7279\u522b\u662f\u5e73\u5766\u4f46\u5c16\u9510\u7684\u533a\u57df\uff08\u79f0\u4e3a\u6b3a\u9a97\u6027\u5e73\u5766\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u6297\u6837\u672c\u5728\u672a\u77e5\u53d7\u5bb3\u8005\u6a21\u578b\u4e0a\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "method": "1. \u5f15\u5165\u5bf9\u6297\u5e73\u5766\u5ea6(AF)\u89e3\u51b3\u6b3a\u9a97\u6027\u5e73\u5766\u95ee\u9898\uff1b2. \u63d0\u51fa\u5bf9\u6297\u5e73\u5766\u5ea6\u653b\u51fb(AFA)\uff0c\u901a\u8fc7\u76ee\u6807\u51fd\u6570\u7684\u9ad8\u6548\u8fd1\u4f3c\u89e3\u51b3\u68af\u5ea6\u7b26\u53f7\u6539\u53d8\u95ee\u9898\uff1b3. \u8bbe\u8ba1\u8499\u7279\u5361\u6d1b\u5bf9\u6297\u91c7\u6837(MCAS)\uff0c\u63d0\u5347\u5185\u5faa\u73af\u91c7\u6837\u6548\u7387\u3002", "result": "\u5728ImageNet\u517c\u5bb9\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u516d\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u4f4d\u4e8e\u66f4\u5e73\u5766\u533a\u57df\uff0c\u5e76\u63d0\u9ad8\u4e86\u8de8\u6a21\u578b\u67b6\u6784\u7684\u53ef\u8fc1\u79fb\u6027\u3002\u5728\u8f93\u5165\u53d8\u6362\u653b\u51fb\u548c\u767e\u5ea6\u4e91API\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u53cc\u9636\u4fe1\u606f\u7684\u53ef\u8fc1\u79fb\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6b3a\u9a97\u6027\u5e73\u5766\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5a01\u80c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.01250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01250", "abs": "https://arxiv.org/abs/2511.01250", "authors": ["YoungJae Cheong", "Jhonghyun An"], "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop", "comment": null, "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u51e0\u4f55\u611f\u77e5\u9002\u914d\u5668\uff0c\u901a\u8fc7\u65b9\u4f4d\u5bf9\u9f50\u548c\u6c34\u5e73\u5faa\u73af\u586b\u5145\u6765\u4fdd\u62a4LiDAR\u8fb9\u754c\u8fde\u7eed\u6027\uff0c\u4f7f\u7528\u5c40\u90e8KNN\u6536\u96c6\u51e0\u4f55\u7279\u5f81\uff0c\u5728\u8bad\u7ec3\u65f6\u9a71\u52a8\u533a\u57df\u611f\u77e5\u6b63\u5219\u5316\u4ee5\u7a33\u5b9a\u7ed3\u6784\u8106\u5f31\u533a\u57df\u7684\u9884\u6d4b\uff0c\u5728\u8de8\u5929\u6c14\u8bed\u4e49\u5206\u5272\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LiDAR\u8bed\u4e49\u5206\u5272\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u6298\u5c04\u3001\u6563\u5c04\u548c\u70b9\u4e22\u5931\u4f1a\u7834\u574f\u51e0\u4f55\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8fb9\u754c\u3001\u89d2\u843d\u548c\u7a00\u758f\u533a\u57df\u7684\u7ed3\u6784\u8106\u5f31\u6027\u3002", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u51e0\u4f55\u611f\u77e5\u9002\u914d\u5668\uff0c\u5305\u62ec\u65b9\u4f4d\u5bf9\u9f50\u548c\u6c34\u5e73\u5faa\u73af\u586b\u5145\u6765\u4fdd\u6301\u8fb9\u754c\u8fde\u7eed\u6027\uff0c\u4f7f\u7528\u5c40\u90e8\u7a97\u53e3KNN\u6536\u96c6\u9644\u8fd1\u70b9\u5e76\u8ba1\u7b97\u5c40\u90e8\u7edf\u8ba1\u7279\u5f81\uff0c\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\uff0c\u5728\u8bad\u7ec3\u65f6\u9a71\u52a8\u533a\u57df\u611f\u77e5\u6b63\u5219\u5316\u3002", "result": "\u5728SemanticKITTI\u8bad\u7ec3\u3001SemanticSTF\u8bc4\u4f30\u7684\u8de8\u5929\u6c14\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u6bd4\u6570\u636e\u589e\u5f3a\u57fa\u7ebf\u63d0\u5347mIoU 7.9\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u6bd4\u7c7b\u522b\u4e2d\u5fc3\u6b63\u5219\u5316\u57fa\u7ebf\u63d0\u53470.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u51e0\u4f55\u9a71\u52a8\u7684\u6b63\u5219\u5316\u662f\u5168\u5929\u6c14LiDAR\u5206\u5272\u7684\u5173\u952e\u65b9\u5411\uff0c\u8be5\u9002\u914d\u5668\u5373\u63d2\u5373\u7528\uff0c\u4ec5\u5728\u8bad\u7ec3\u65f6\u542f\u7528\uff0c\u63a8\u7406\u6210\u672c\u53ef\u5ffd\u7565\u3002"}}
{"id": "2511.01266", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01266", "abs": "https://arxiv.org/abs/2511.01266", "authors": ["Joonghyuk Shin", "Zhengqi Li", "Richard Zhang", "Jun-Yan Zhu", "Jaesik Park", "Eli Schechtman", "Xun Huang"], "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls", "comment": "Project webpage: https://joonghyuk.com/motionstream-web/", "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.", "AI": {"tldr": "MotionStream\u662f\u4e00\u4e2a\u5b9e\u65f6\u8fd0\u52a8\u6761\u4ef6\u89c6\u9891\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u53cc\u5411\u6559\u5e08\u6a21\u578b\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\u548c\u9ad8\u8fbe29 FPS\u7684\u6d41\u5f0f\u751f\u6210\uff0c\u652f\u6301\u65e0\u9650\u957f\u5ea6\u89c6\u9891\u7684\u5b9e\u65f6\u4ea4\u4e92\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8fd0\u52a8\u6761\u4ef6\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u9ad8\uff08\u5206\u949f\u7ea7\uff09\u548c\u975e\u56e0\u679c\u5904\u7406\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u81ea\u5f3a\u5236\u5206\u5e03\u5339\u914d\u84b8\u998f\u5c06\u53cc\u5411\u6559\u5e08\u6a21\u578b\u8f6c\u5316\u4e3a\u56e0\u679c\u5b66\u751f\u6a21\u578b\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u56e0\u679c\u6ce8\u610f\u529b\u7ed3\u5408\u6ce8\u610f\u529b\u6c47\u6280\u672f\uff0c\u5728\u8bad\u7ec3\u4e2d\u901a\u8fc7\u81ea\u5c55\u5f00\u548cKV\u7f13\u5b58\u6eda\u52a8\u6a21\u62df\u63a8\u7406\u65f6\u5916\u63a8\uff0c\u5b9e\u73b0\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6052\u5b9a\u901f\u5ea6\u751f\u6210\u3002", "result": "\u5728\u8fd0\u52a8\u8ddf\u968f\u548c\u89c6\u9891\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u5347\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u591f\u5b9e\u73b0\u65e0\u9650\u957f\u5ea6\u7684\u6d41\u5f0f\u751f\u6210\u3002", "conclusion": "MotionStream\u5b9e\u73b0\u4e86\u771f\u6b63\u4ea4\u4e92\u5f0f\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u4f53\u9a8c\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u7ed8\u5236\u8f68\u8ff9\u3001\u63a7\u5236\u76f8\u673a\u6216\u4f20\u8f93\u8fd0\u52a8\u6765\u5b9e\u65f6\u67e5\u770b\u7ed3\u679c\u3002"}}
{"id": "2511.01274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01274", "abs": "https://arxiv.org/abs/2511.01274", "authors": ["Tan Tang", "Yanhong Wu", "Junming Gao", "Yingcai Wu"], "title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers", "comment": null, "summary": "Ancient Chinese paintings are a valuable cultural heritage that is damaged by\nirreversible color degradation. Reviving color-degraded paintings is\nextraordinarily difficult due to the complex chemistry mechanism. Progress is\nfurther slowed by the lack of comprehensive, high-quality datasets, which\nhampers the creation of end-to-end digital restoration tools. To revive colors,\nwe propose PRevivor, a prior-guided color transformer that learns from recent\npaintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and\nSong Dynasty). To develop PRevivor, we decompose color restoration into two\nsequential sub-tasks: luminance enhancement and hue correction. For luminance\nenhancement, we employ two variational U-Nets and a multi-scale mapping module\nto translate faded luminance into restored counterparts. For hue correction, we\ndesign a dual-branch color query module guided by localized hue priors\nextracted from faded paintings. Specifically, one branch focuses attention on\nregions guided by masked priors, enforcing localized hue correction, whereas\nthe other branch remains unconstrained to maintain a global reasoning\ncapability. To evaluate PRevivor, we conduct extensive experiments against\nstate-of-the-art colorization methods. The results demonstrate superior\nperformance both quantitatively and qualitatively.", "AI": {"tldr": "PRevivor\u662f\u4e00\u4e2a\u57fa\u4e8e\u5148\u9a8c\u5f15\u5bfc\u7684\u989c\u8272\u8f6c\u6362\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u660e\u6e05\u65f6\u671f\u7ed8\u753b\u6765\u6062\u590d\u5510\u5b8b\u65f6\u671f\u53e4\u753b\u7684\u8272\u5f69\uff0c\u901a\u8fc7\u4eae\u5ea6\u589e\u5f3a\u548c\u8272\u8c03\u6821\u6b63\u4e24\u4e2a\u5b50\u4efb\u52a1\u5b9e\u73b0\u989c\u8272\u6062\u590d\u3002", "motivation": "\u4e2d\u56fd\u53e4\u4ee3\u7ed8\u753b\u662f\u5b9d\u8d35\u7684\u6587\u5316\u9057\u4ea7\uff0c\u4f46\u56e0\u4e0d\u53ef\u9006\u7684\u989c\u8272\u9000\u5316\u800c\u53d7\u635f\u3002\u7531\u4e8e\u590d\u6742\u7684\u5316\u5b66\u673a\u5236\u548c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u989c\u8272\u6062\u590d\u975e\u5e38\u56f0\u96be\uff0c\u963b\u788d\u4e86\u7aef\u5230\u7aef\u6570\u5b57\u4fee\u590d\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "\u5c06\u989c\u8272\u6062\u590d\u5206\u89e3\u4e3a\u4eae\u5ea6\u589e\u5f3a\u548c\u8272\u8c03\u6821\u6b63\u4e24\u4e2a\u987a\u5e8f\u5b50\u4efb\u52a1\u3002\u4eae\u5ea6\u589e\u5f3a\u4f7f\u7528\u4e24\u4e2a\u53d8\u5206U-Net\u548c\u591a\u5c3a\u5ea6\u6620\u5c04\u6a21\u5757\uff1b\u8272\u8c03\u6821\u6b63\u8bbe\u8ba1\u53cc\u5206\u652f\u989c\u8272\u67e5\u8be2\u6a21\u5757\uff0c\u901a\u8fc7\u4ece\u892a\u8272\u7ed8\u753b\u4e2d\u63d0\u53d6\u7684\u5c40\u90e8\u8272\u8c03\u5148\u9a8c\u8fdb\u884c\u5f15\u5bfc\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u7740\u8272\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PRevivor\u80fd\u591f\u6709\u6548\u6062\u590d\u53e4\u753b\u7684\u8272\u5f69\uff0c\u4e3a\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u5b57\u4fee\u590d\u5de5\u5177\u3002"}}
{"id": "2511.01284", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01284", "abs": "https://arxiv.org/abs/2511.01284", "authors": ["Karma Phuntsho", "Abdullah", "Kyungmi Lee", "Ickjai Lee", "Euijoon Ahn"], "title": "Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions", "comment": null, "summary": "Foundation models (FMs) have emerged as a transformative paradigm in medical\nimage analysis, offering the potential to provide generalizable, task-agnostic\nsolutions across a wide range of clinical tasks and imaging modalities. Their\ncapacity to learn transferable representations from large-scale data has the\npotential to address the limitations of conventional task-specific models.\nHowever, adaptation of FMs to real-world clinical practice remains constrained\nby key challenges, including domain shifts, limited availability of\nhigh-quality annotated data, substantial computational demands, and strict\nprivacy requirements. This review presents a comprehensive assessment of\nstrategies for adapting FMs to the specific demands of medical imaging. We\nexamine approaches such as supervised fine-tuning, domain-specific pretraining,\nparameter-efficient fine-tuning, self-supervised learning, hybrid methods, and\nmultimodal or cross-modal frameworks. For each, we evaluate reported\nperformance gains, clinical applicability, and limitations, while identifying\ntrade-offs and unresolved challenges that prior reviews have often overlooked.\nBeyond these established techniques, we also highlight emerging directions\naimed at addressing current gaps. These include continual learning to enable\ndynamic deployment, federated and privacy-preserving approaches to safeguard\nsensitive data, hybrid self-supervised learning to enhance data efficiency,\ndata-centric pipelines that combine synthetic generation with human-in-the-loop\nvalidation, and systematic benchmarking to assess robust generalization under\nreal-world clinical variability. By outlining these strategies and associated\nresearch gaps, this review provides a roadmap for developing adaptive,\ntrustworthy, and clinically integrated FMs capable of meeting the demands of\nreal-world medical imaging.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5168\u9762\u8bc4\u4f30\u4e86\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03\u3001\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u6301\u7eed\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u7b49\u65b0\u5174\u65b9\u5411\uff0c\u4e3a\u5f00\u53d1\u9002\u5e94\u6027\u5f3a\u3001\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8a\u96c6\u6210\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u9762\u4e34\u9886\u57df\u504f\u79fb\u3001\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\u548c\u9690\u79c1\u8981\u6c42\u4e25\u683c\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u9002\u5e94\u7b56\u7565\u3002", "method": "\u8bc4\u4f30\u4e86\u76d1\u7763\u5fae\u8c03\u3001\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u6df7\u5408\u65b9\u6cd5\u4ee5\u53ca\u591a\u6a21\u6001/\u8de8\u6a21\u6001\u6846\u67b6\u7b49\u591a\u79cd\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\u589e\u76ca\u3001\u4e34\u5e8a\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\u3002", "result": "\u8bc6\u522b\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6743\u8861\u548c\u672a\u89e3\u51b3\u6311\u6218\uff0c\u540c\u65f6\u7a81\u51fa\u4e86\u6301\u7eed\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u3001\u6df7\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u6570\u636e\u4e3a\u4e2d\u5fc3\u7ba1\u9053\u548c\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u7b49\u65b0\u5174\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u5dee\u8ddd\u3002", "conclusion": "\u901a\u8fc7\u6982\u8ff0\u8fd9\u4e9b\u7b56\u7565\u548c\u76f8\u5173\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u80fd\u591f\u6ee1\u8db3\u771f\u5b9e\u4e16\u754c\u533b\u5b66\u5f71\u50cf\u9700\u6c42\u7684\u9002\u5e94\u6027\u3001\u53ef\u4fe1\u8d56\u548c\u4e34\u5e8a\u96c6\u6210\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002"}}
{"id": "2511.01293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01293", "abs": "https://arxiv.org/abs/2511.01293", "authors": ["Yonggang Zhang", "Jun Nie", "Xinmei Tian", "Mingming Gong", "Kun Zhang", "Bo Han"], "title": "Detecting Generated Images by Fitting Natural Image Distributions", "comment": "25 pages, 9 figures, NeurIPS 2025 spotlight", "summary": "The increasing realism of generated images has raised significant concerns\nabout their potential misuse, necessitating robust detection methods. Current\napproaches mainly rely on training binary classifiers, which depend heavily on\nthe quantity and quality of available generated images. In this work, we\npropose a novel framework that exploits geometric differences between the data\nmanifolds of natural and generated images. To exploit this difference, we\nemploy a pair of functions engineered to yield consistent outputs for natural\nimages but divergent outputs for generated ones, leveraging the property that\ntheir gradients reside in mutually orthogonal subspaces. This design enables a\nsimple yet effective detection method: an image is identified as generated if a\ntransformation along its data manifold induces a significant change in the loss\nvalue of a self-supervised model pre-trained on natural images. Further more,\nto address diminishing manifold disparities in advanced generative models, we\nleverage normalizing flows to amplify detectable differences by extruding\ngenerated images away from the natural image manifold. Extensive experiments\ndemonstrate the efficacy of this method. Code is available at\nhttps://github.com/tmlr-group/ConV.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u6d41\u5f62\u51e0\u4f55\u5dee\u5f02\u7684\u56fe\u50cf\u751f\u6210\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u548c\u751f\u6210\u56fe\u50cf\u5728\u6d41\u5f62\u7ed3\u6784\u4e0a\u7684\u6b63\u4ea4\u6027\u5dee\u5f02\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6a21\u578b\u635f\u5931\u53d8\u5316\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u653e\u5927\u53ef\u68c0\u6d4b\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u751f\u6210\u56fe\u50cf\u771f\u5b9e\u611f\u7684\u63d0\u5347\uff0c\u5176\u6f5c\u5728\u6ee5\u7528\u98ce\u9669\u589e\u52a0\uff0c\u9700\u8981\u5f3a\u5927\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u5206\u7c7b\u5668\uff0c\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u6570\u91cf\u548c\u8d28\u91cf\u4f9d\u8d56\u6027\u5f3a\u3002", "method": "\u5229\u7528\u81ea\u7136\u56fe\u50cf\u548c\u751f\u6210\u56fe\u50cf\u6570\u636e\u6d41\u5f62\u7684\u51e0\u4f55\u5dee\u5f02\uff0c\u8bbe\u8ba1\u4e00\u5bf9\u51fd\u6570\u4f7f\u81ea\u7136\u56fe\u50cf\u8f93\u51fa\u4e00\u81f4\u800c\u751f\u6210\u56fe\u50cf\u8f93\u51fa\u53d1\u6563\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6a21\u578b\u5728\u6570\u636e\u6d41\u5f62\u53d8\u6362\u65f6\u7684\u635f\u5931\u53d8\u5316\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528\u5f52\u4e00\u5316\u6d41\u653e\u5927\u5dee\u5f02\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u5148\u8fdb\u751f\u6210\u6a21\u578b\u4e2d\u9010\u6e10\u51cf\u5c0f\u7684\u6d41\u5f62\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2511.01295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01295", "abs": "https://arxiv.org/abs/2511.01295", "authors": ["Feng Han", "Yibin Wang", "Chenglin Li", "Zheming Liang", "Dianyi Wang", "Yang Jiao", "Zhipeng Wei", "Chao Gong", "Cheng Jin", "Jingjing Chen", "Jiaqi Wang"], "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark", "comment": "Project page: https://maplebb.github.io/UniREditBench", "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.", "AI": {"tldr": "\u63d0\u51fa\u4e86UniREditBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u6027\u80fd\uff0c\u6db5\u76d6\u771f\u5b9e\u4e16\u754c\u548c\u6e38\u620f\u4e16\u754c\u573a\u666f\uff0c\u5305\u542b2700\u4e2a\u6837\u672c\u548c8\u4e2a\u4e3b\u8981\u7ef4\u5ea6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u9700\u8981\u9690\u5f0f\u63a8\u7406\u7684\u590d\u6742\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u5bf9\u8c61\u5c5e\u6027\u53d8\u6362\uff0c\u5ffd\u89c6\u4e86\u591a\u5bf9\u8c61\u4ea4\u4e92\u548c\u6e38\u620f\u4e16\u754c\u573a\u666f\uff0c\u4e14\u4ec5\u4f9d\u8d56\u6587\u672c\u53c2\u8003\u53ef\u80fd\u5bfc\u81f4\u8bef\u5224\u3002", "method": "\u6784\u5efaUniREditBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2700\u4e2a\u6837\u672c\u8986\u76d68\u4e2a\u4e3b\u8981\u7ef4\u5ea6\u548c18\u4e2a\u5b50\u7ef4\u5ea6\uff1b\u5f15\u5165\u591a\u6a21\u6001\u53cc\u53c2\u8003\u8bc4\u4f30\uff08\u6587\u672c\u548c\u771f\u5b9e\u56fe\u50cf\u53c2\u8003\uff09\uff1b\u8bbe\u8ba1\u81ea\u52a8\u5316\u591a\u573a\u666f\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u521b\u5efaUniREdit-Data-100K\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff1b\u5728Bagel\u6a21\u578b\u4e0a\u5fae\u8c03\u5f00\u53d1UniREdit-Bagel\u3002", "result": "UniREdit-Bagel\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff1b\u901a\u8fc7\u5bf9\u5f00\u6e90\u548c\u95ed\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5f31\u70b9\u3002", "conclusion": "UniREditBench\u4e3a\u57fa\u4e8e\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u591a\u6a21\u6001\u53cc\u53c2\u8003\u8bc4\u4f30\u63d0\u9ad8\u4e86\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u652f\u6301\u6a21\u578b\u8bad\u7ec3\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.01302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01302", "abs": "https://arxiv.org/abs/2511.01302", "authors": ["Nu-Fnag Xiao", "De-Xing Huang", "Le-Tian Wang", "Mei-Jiang Gui", "Qi Fu", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuangyi Wang", "Zeng-Guang Hou", "Ying-Wei Wang", "Xiao-Hu Zhou"], "title": "REASON: Probability map-guided dual-branch fusion framework for gastric content assessment", "comment": "Under Review. 12 pages, 10 figures, 6 tables", "summary": "Accurate assessment of gastric content from ultrasound is critical for\nstratifying aspiration risk at induction of general anesthesia. However,\ntraditional methods rely on manual tracing of gastric antra and empirical\nformulas, which face significant limitations in both efficiency and accuracy.\nTo address these challenges, a novel two-stage probability map-guided\ndual-branch fusion framework (REASON) for gastric content assessment is\nproposed. In stage 1, a segmentation model generates probability maps that\nsuppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch\nclassifier fuses information from two standard views, right lateral decubitus\n(RLD) and supine (SUP), to improve the discrimination of learned features.\nExperimental results on a self-collected dataset demonstrate that the proposed\nframework outperforms current state-of-the-art approaches by a significant\nmargin. This framework shows great promise for automated preoperative\naspiration risk assessment, offering a more robust, efficient, and accurate\nsolution for clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aREASON\u7684\u4e24\u9636\u6bb5\u6982\u7387\u56fe\u5f15\u5bfc\u53cc\u5206\u652f\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u8d85\u58f0\u81ea\u52a8\u8bc4\u4f30\u80c3\u5185\u5bb9\u7269\uff0c\u4ee5\u6539\u5584\u5168\u8eab\u9ebb\u9189\u8bf1\u5bfc\u65f6\u7684\u8bef\u5438\u98ce\u9669\u5206\u5c42\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u8ffd\u8e2a\u80c3\u7aa6\u548c\u4f7f\u7528\u7ecf\u9a8c\u516c\u5f0f\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u51c6\u786e\u7684\u80c3\u5185\u5bb9\u7269\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5206\u5272\u6a21\u578b\u751f\u6210\u6291\u5236\u4f2a\u5f71\u5e76\u7a81\u51fa\u80c3\u89e3\u5256\u7ed3\u6784\u7684\u6982\u7387\u56fe\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u53cc\u5206\u652f\u5206\u7c7b\u5668\u878d\u5408\u53f3\u4fa7\u5367\u4f4d\u548c\u4ef0\u5367\u4f4d\u4e24\u4e2a\u6807\u51c6\u89c6\u56fe\u7684\u4fe1\u606f\uff0c\u4ee5\u6539\u5584\u5b66\u4e60\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u672f\u524d\u8bef\u5438\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u9ad8\u6548\u548c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.01304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01304", "abs": "https://arxiv.org/abs/2511.01304", "authors": ["Chentao Li", "Behzad Bozorgtabar", "Yifang Ping", "Pan Huang", "Jing Qin"], "title": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation", "comment": "Our code is available at https://github.com/Prince-Lee-PathAI/PG-CIDL", "summary": "Multiple instance learning (MIL) has been widely used for representing\nwhole-slide pathology images. However, spatial, semantic, and decision\nentanglements among instances limit its representation and interpretability. To\naddress these challenges, we propose a latent factor grouping-boosted\ncluster-reasoning instance disentangled learning framework for whole-slide\nimage (WSI) interpretable representation in three phases. First, we introduce a\nnovel positive semi-definite latent factor grouping that maps instances into a\nlatent subspace, effectively mitigating spatial entanglement in MIL. To\nalleviate semantic entanglement, we employs instance probability counterfactual\ninference and optimization via cluster-reasoning instance disentangling.\nFinally, we employ a generalized linear weighted decision via instance effect\nre-weighting to address decision entanglement. Extensive experiments on\nmulticentre datasets demonstrate that our model outperforms all\nstate-of-the-art models. Moreover, it attains pathologist-aligned\ninterpretability through disentangled representations and a transparent\ndecision-making process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u5207\u7247\u75c5\u7406\u56fe\u50cf\u8868\u793a\u7684\u4e09\u9636\u6bb5\u89e3\u7f20\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u56e0\u5b50\u5206\u7ec4\u3001\u805a\u7c7b\u63a8\u7406\u5b9e\u4f8b\u89e3\u7f20\u548c\u5b9e\u4f8b\u6548\u5e94\u91cd\u52a0\u6743\u6765\u89e3\u51b3\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u51b3\u7b56\u7ea0\u7f20\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5b9e\u73b0\u75c5\u7406\u5b66\u5bb6\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u5b9e\u4f8b\u5b66\u4e60\u5728\u75c5\u7406\u56fe\u50cf\u8868\u793a\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5b9e\u4f8b\u95f4\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u51b3\u7b56\u7ea0\u7f20\u9650\u5236\u4e86\u5176\u8868\u793a\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u6b63\u534a\u5b9a\u6f5c\u5728\u56e0\u5b50\u5206\u7ec4\u6620\u5c04\u5b9e\u4f8b\u5230\u6f5c\u5728\u5b50\u7a7a\u95f4\u7f13\u89e3\u7a7a\u95f4\u7ea0\u7f20\uff1b2\uff09\u901a\u8fc7\u805a\u7c7b\u63a8\u7406\u5b9e\u4f8b\u89e3\u7f20\u4f7f\u7528\u5b9e\u4f8b\u6982\u7387\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u4f18\u5316\u7f13\u89e3\u8bed\u4e49\u7ea0\u7f20\uff1b3\uff09\u901a\u8fc7\u5b9e\u4f8b\u6548\u5e94\u91cd\u52a0\u6743\u7684\u5e7f\u4e49\u7ebf\u6027\u52a0\u6743\u51b3\u7b56\u89e3\u51b3\u51b3\u7b56\u7ea0\u7f20\u3002", "result": "\u5728\u591a\u4e2a\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u6240\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u89e3\u7f20\u8868\u793a\u548c\u900f\u660e\u51b3\u7b56\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u75c5\u7406\u5b66\u5bb6\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u7f20\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u56fe\u50cf\u8868\u793a\u4e2d\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u51b3\u7b56\u7ea0\u7f20\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.01307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01307", "abs": "https://arxiv.org/abs/2511.01307", "authors": ["Tae-Young Lee", "Juwon Seo", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models", "comment": "26 pages, 9 figures, 16 tables, NeurIPS 2025", "summary": "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86APDM\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fdd\u62a4\u76ee\u6807\u4ece\u56fe\u50cf\u8f6c\u79fb\u5230\u6269\u6563\u6a21\u578b\u672c\u8eab\u6765\u9632\u6b62\u7279\u5b9a\u4e3b\u4f53\u7684\u4e2a\u6027\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\u5728\u5b58\u5728\u5e72\u51c0\u56fe\u50cf\u6216\u7b80\u5355\u56fe\u50cf\u53d8\u6362\u65f6\u5931\u6548\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u7279\u5b9a\u4e3b\u4f53\uff08\u5982\u8eab\u4efd\u6216\u7269\u4f53\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u4e5f\u5e26\u6765\u4e86\u9690\u79c1\u98ce\u9669\uff0c\u53ef\u80fd\u88ab\u6076\u610f\u7528\u6237\u7528\u4e8e\u751f\u6210\u672a\u7ecf\u6388\u6743\u7684\u5185\u5bb9\u3002\u73b0\u6709\u57fa\u4e8e\u5bf9\u6297\u6270\u52a8\u7684\u65b9\u6cd5\u5b58\u5728\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff0c\u5728\u5c11\u91cf\u5e72\u51c0\u56fe\u50cf\u6216\u7b80\u5355\u56fe\u50cf\u53d8\u6362\u4e0b\u5c31\u4f1a\u5931\u6548\u3002", "method": "\u63d0\u51fa\u4e86Anti-Personalized Diffusion Models (APDM)\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u7406\u8bba\u5206\u6790\u8868\u660e\u73b0\u6709\u635f\u5931\u51fd\u6570\u65e0\u6cd5\u786e\u4fdd\u9c81\u68d2\u53cd\u4e2a\u6027\u5316\u6536\u655b\uff1b2\uff09\u5f15\u5165Direct Protective Optimization (DPO)\u635f\u5931\u51fd\u6570\uff0c\u5728\u4e0d\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5e72\u6270\u76ee\u6807\u6a21\u578b\u7684\u4e3b\u4f53\u4e2a\u6027\u5316\uff1b3\uff09\u63d0\u51fa\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565Learning to Protect (L2P)\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u4e2a\u6027\u5316\u548c\u4fdd\u62a4\u8def\u5f84\u6765\u6a21\u62df\u672a\u6765\u4e2a\u6027\u5316\u8f68\u8ff9\u5e76\u81ea\u9002\u5e94\u5f3a\u5316\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPDM\u6846\u67b6\u5728\u9632\u6b62\u672a\u7ecf\u6388\u6743\u4e2a\u6027\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "APDM\u901a\u8fc7\u5c06\u4fdd\u62a4\u76ee\u6807\u8f6c\u79fb\u5230\u6269\u6563\u6a21\u578b\u672c\u8eab\uff0c\u5e76\u91c7\u7528DPO\u635f\u5931\u51fd\u6570\u548cL2P\u4f18\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2a\u6027\u5316\u5e26\u6765\u7684\u9690\u79c1\u98ce\u9669\u95ee\u9898\uff0c\u4e3a\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u4fdd\u969c\u3002"}}
{"id": "2511.01317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01317", "abs": "https://arxiv.org/abs/2511.01317", "authors": ["Sampriti Soor", "Alik Pramanick", "Jothiprakash K", "Arijit Sur"], "title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model", "comment": "18 pages, 3 figures", "summary": "The rapid growth of deep learning has brought about powerful models that can\nhandle various tasks, like identifying images and understanding language.\nHowever, adversarial attacks, an unnoticed alteration, can deceive models,\nleading to inaccurate predictions. In this paper, a generative adversarial\nattack method is proposed that uses the CLIP model to create highly effective\nand visually imperceptible adversarial perturbations. The CLIP model's ability\nto align text and image representation helps incorporate natural language\nsemantics with a guided loss to generate effective adversarial examples that\nlook identical to the original inputs. This integration allows extensive scene\nmanipulation, creating perturbations in multi-object environments specifically\ndesigned to deceive multilabel classifiers. Our approach integrates the\nconcentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with\nthe dissimilar text embeddings similar to Generative Adversarial Multi-Object\nScene Attacks (GAMA), resulting in perturbations that both deceive\nclassification models and maintain high structural similarity to the original\nimages. The model was tested on various tasks across diverse black-box victim\nmodels. The experimental results show that our method performs competitively,\nachieving comparable or superior results to existing techniques, while\npreserving greater visual fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u751f\u6210\u5f0f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u6765\u521b\u5efa\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u4f46\u6709\u6548\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u5728\u591a\u5bf9\u8c61\u73af\u5883\u4e2d\u6b3a\u9a97\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u6709\u6548\u6b3a\u9a97\u5206\u7c7b\u5668\u53c8\u4fdd\u6301\u9ad8\u89c6\u89c9\u76f8\u4f3c\u6027\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u5c06SSAE\u7684\u96c6\u4e2d\u6270\u52a8\u7b56\u7565\u4e0eGAMA\u7684\u5dee\u5f02\u6587\u672c\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u5229\u7528CLIP\u6a21\u578b\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5bfc\u635f\u5931\u51fd\u6570\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5728\u5404\u79cd\u9ed1\u76d2\u53d7\u5bb3\u8005\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u8fbe\u5230\u6216\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u6b3a\u9a97\u5206\u7c7b\u6a21\u578b\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u56fe\u50cf\u7684\u9ad8\u5ea6\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u4e3a\u5bf9\u6297\u653b\u51fb\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01357", "abs": "https://arxiv.org/abs/2511.01357", "authors": ["Qiangguo Jin", "Xianyao Zheng", "Hui Cui", "Changming Sun", "Yuqi Fang", "Cong Cong", "Ran Su", "Leyi Wei", "Ping Xuan", "Junbo Wang"], "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering", "comment": "The paper has been accepted by the 33rd Pacific Conference on\n  Computer Graphics and Applications (Pacific Graphics 2025)", "summary": "Medical visual question answering (Med-VQA) is a crucial multimodal task in\nclinical decision support and telemedicine. Recent self-attention based methods\nstruggle to effectively handle cross-modal semantic alignments between vision\nand language. Moreover, classification-based methods rely on predefined answer\nsets. Treating this task as a simple classification problem may make it unable\nto adapt to the diversity of free-form answers and overlook the detailed\nsemantic information of free-form answers. In order to tackle these challenges,\nwe introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)\nframework that learns cross-modal feature representations from images and\ntexts. CMI-MTL comprises three key modules: fine-grained visual-text feature\nalignment (FVTA), cross-modal interleaved feature representation (CIFR), and\nfree-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most\nrelevant regions in image-text pairs through fine-grained visual-text feature\nalignment. CIFR captures cross-modal sequential interactions via cross-modal\ninterleaved feature representation. FFAE leverages auxiliary knowledge from\nopen-ended questions through free-form answer-enhanced multi-task learning,\nimproving the model's capability for open-ended Med-VQA. Experimental results\nshow that CMI-MTL outperforms the existing state-of-the-art methods on three\nMed-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more\ninterpretability experiments to prove the effectiveness. The code is publicly\navailable at https://github.com/BioMedIA-repo/CMI-MTL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCMI-MTL\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6a21\u5757\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u3001\u8de8\u6a21\u6001\u4ea4\u9519\u7279\u5f81\u8868\u793a\u548c\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u589e\u5f3a\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u4e14\u57fa\u4e8e\u5206\u7c7b\u7684\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b54\u6848\u96c6\uff0c\u65e0\u6cd5\u9002\u5e94\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u7684\u591a\u6837\u6027\uff0c\u5ffd\u7565\u4e86\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u7684\u8be6\u7ec6\u8bed\u4e49\u4fe1\u606f\u3002", "method": "CMI-MTL\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1aFVTA\uff08\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\uff09\u3001CIFR\uff08\u8de8\u6a21\u6001\u4ea4\u9519\u7279\u5f81\u8868\u793a\uff09\u548cFFAE\uff08\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u589e\u5f3a\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff09\uff0c\u5206\u522b\u5904\u7406\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u6700\u76f8\u5173\u533a\u57df\u63d0\u53d6\u3001\u8de8\u6a21\u6001\u5e8f\u5217\u4ea4\u4e92\u6355\u83b7\u548c\u5229\u7528\u5f00\u653e\u6027\u95ee\u9898\u8f85\u52a9\u77e5\u8bc6\u3002", "result": "\u5728\u4e09\u4e2aMed-VQA\u6570\u636e\u96c6\uff08VQA-RAD\u3001SLAKE\u548cOVQA\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCMI-MTL\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CMI-MTL\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5f00\u653e\u6027\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2511.01355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01355", "abs": "https://arxiv.org/abs/2511.01355", "authors": ["Linhao Huang"], "title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion", "comment": null, "summary": "Recent advancements in text-to-image diffusion models have significantly\nimproved the personalization and stylization of generated images. However,\nprevious studies have only assessed content similarity under a single style\nintensity. In our experiments, we observe that increasing style intensity leads\nto a significant loss of content features, resulting in a suboptimal\ncontent-style frontier. To address this, we propose a novel approach to expand\nthe content-style frontier by leveraging Content-Style Subspace Blending and a\nContent-Style Balance loss. Our method improves content similarity across\nvarying style intensities, significantly broadening the content-style frontier.\nExtensive experiments demonstrate that our approach outperforms existing\ntechniques in both qualitative and quantitative evaluations, achieving superior\ncontent-style trade-off with significantly lower Inverted Generational Distance\n(IGD) and Generational Distance (GD) scores compared to current methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5185\u5bb9-\u98ce\u683c\u5b50\u7a7a\u95f4\u6df7\u5408\u548c\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u635f\u5931\u6765\u6269\u5c55\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u5f3a\u5ea6\u589e\u52a0\u5bfc\u81f4\u5185\u5bb9\u7279\u5f81\u4e22\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5355\u4e00\u98ce\u683c\u5f3a\u5ea6\u4e0b\u8bc4\u4f30\u5185\u5bb9\u76f8\u4f3c\u6027\uff0c\u4f46\u5b9e\u9a8c\u53d1\u73b0\u589e\u52a0\u98ce\u683c\u5f3a\u5ea6\u4f1a\u5bfc\u81f4\u5185\u5bb9\u7279\u5f81\u663e\u8457\u4e22\u5931\uff0c\u5f62\u6210\u6b21\u4f18\u7684\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\u3002", "method": "\u91c7\u7528\u5185\u5bb9-\u98ce\u683c\u5b50\u7a7a\u95f4\u6df7\u5408\u6280\u672f\u548c\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u635f\u5931\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u98ce\u683c\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u5347\u5185\u5bb9\u76f8\u4f3c\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5185\u5bb9-\u98ce\u683c\u6743\u8861\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5012\u7f6e\u751f\u6210\u8ddd\u79bb\u548c\u751f\u6210\u8ddd\u79bb\u5f97\u5206\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6269\u5c55\u4e86\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\uff0c\u5728\u591a\u79cd\u98ce\u683c\u5f3a\u5ea6\u4e0b\u90fd\u80fd\u4fdd\u6301\u66f4\u597d\u7684\u5185\u5bb9\u76f8\u4f3c\u6027\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01399", "abs": "https://arxiv.org/abs/2511.01399", "authors": ["Ya Wen", "Yutong Qiao", "Chi Chiu Lam", "Ioannis Brilakis", "Sanghoon Lee", "Mun On Wong"], "title": "Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction", "comment": null, "summary": "Inventory management of firefighting assets is crucial for emergency\npreparedness, risk assessment, and on-site fire response. However, conventional\nmethods are inefficient due to limited capabilities in automated asset\nrecognition and reconstruction. To address the challenge, this research\nintroduces the Fire-ART dataset and develops a panoramic image-based\nreconstruction approach for semantic enrichment of firefighting assets into BIM\nmodels. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626\nimages and 6,627 instances, making it an extensive and publicly accessible\ndataset for asset recognition. In addition, the reconstruction approach\nintegrates modified cube-map conversion and radius-based spherical camera\nprojection to enhance recognition and localization accuracy. Through\nvalidations with two real-world case studies, the proposed approach achieves\nF1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,\nrespectively. The Fire-ART dataset and the reconstruction approach offer\nvaluable resources and robust technical solutions to enhance the accurate\ndigital management of fire safety equipment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86Fire-ART\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u5168\u666f\u56fe\u50cf\u7684\u6d88\u9632\u8d44\u4ea7\u91cd\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u6d88\u9632\u8d44\u4ea7\u8bed\u4e49\u4e30\u5bcc\u5230BIM\u6a21\u578b\u4e2d\uff0c\u63d0\u9ad8\u6d88\u9632\u8d44\u4ea7\u7ba1\u7406\u7684\u81ea\u52a8\u5316\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6d88\u9632\u8d44\u4ea7\u7ba1\u7406\u65b9\u6cd5\u5728\u81ea\u52a8\u8d44\u4ea7\u8bc6\u522b\u548c\u91cd\u5efa\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u5e94\u6025\u51c6\u5907\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u73b0\u573a\u706b\u707e\u54cd\u5e94\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b15\u79cd\u57fa\u672c\u8d44\u4ea7\u30012626\u5f20\u56fe\u50cf\u548c6627\u4e2a\u5b9e\u4f8b\u7684Fire-ART\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u96c6\u6210\u6539\u8fdb\u7684\u7acb\u65b9\u4f53\u8d34\u56fe\u8f6c\u6362\u548c\u57fa\u4e8e\u534a\u5f84\u7684\u7403\u5f62\u76f8\u673a\u6295\u5f71\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e8673%\u548c88%\u7684F1\u5206\u6570\uff0c\u4ee5\u53ca0.620\u7c73\u548c0.428\u7c73\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "conclusion": "Fire-ART\u6570\u636e\u96c6\u548c\u91cd\u5efa\u65b9\u6cd5\u4e3a\u6d88\u9632\u8bbe\u5907\u7cbe\u786e\u6570\u5b57\u5316\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u548c\u5f3a\u5927\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01427", "abs": "https://arxiv.org/abs/2511.01427", "authors": ["Yinchao Ma", "Yuyang Tang", "Wenfei Yang", "Tianzhu Zhang", "Xu Zhou", "Feng Wu"], "title": "UniSOT: A Unified Framework for Multi-Modality Single Object Tracking", "comment": "The paper has been accepted by TPAMI", "summary": "Single object tracking aims to localize target object with specific reference\nmodalities (bounding box, natural language or both) in a sequence of specific\nvideo modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different\nreference modalities enable various human-machine interactions, and different\nvideo modalities are demanded in complex scenarios to enhance tracking\nrobustness. Existing trackers are designed for single or several video\nmodalities with single or several reference modalities, which leads to separate\nmodel designs and limits practical applications. Practically, a unified tracker\nis needed to handle various requirements. To the best of our knowledge, there\nis still no tracker that can perform tracking with these above reference\nmodalities across these video modalities simultaneously. Thus, in this paper,\nwe present a unified tracker, UniSOT, for different combinations of three\nreference modalities and four video modalities with uniform parameters.\nExtensive experimental results on 18 visual tracking, vision-language tracking\nand RGB+X tracking benchmarks demonstrate that UniSOT shows superior\nperformance against modality-specific counterparts. Notably, UniSOT outperforms\nprevious counterparts by over 3.0\\% AUC on TNL2K across all three reference\nmodalities and outperforms Un-Track by over 2.0\\% main metric across all three\nRGB+X video modalities.", "AI": {"tldr": "UniSOT\u662f\u4e00\u4e2a\u7edf\u4e00\u8ddf\u8e2a\u5668\uff0c\u80fd\u591f\u5904\u7406\u4e09\u79cd\u53c2\u8003\u6a21\u6001\uff08\u8fb9\u754c\u6846\u3001\u81ea\u7136\u8bed\u8a00\u6216\u4e24\u8005\uff09\u548c\u56db\u79cd\u89c6\u9891\u6a21\u6001\uff08RGB\u3001RGB+\u6df1\u5ea6\u3001RGB+\u70ed\u6210\u50cf\u6216RGB+\u4e8b\u4ef6\uff09\u7684\u7ec4\u5408\uff0c\u4f7f\u7528\u7edf\u4e00\u53c2\u6570\u5b9e\u73b0\u8de8\u6a21\u6001\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u5668\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u6216\u5c11\u6570\u51e0\u79cd\u89c6\u9891\u6a21\u6001\u548c\u53c2\u8003\u6a21\u6001\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6a21\u578b\u5206\u79bb\u4e14\u9650\u5236\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u7edf\u4e00\u8ddf\u8e2a\u5668\u6765\u5904\u7406\u5404\u79cd\u9700\u6c42\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e0a\u8ff0\u6240\u6709\u53c2\u8003\u6a21\u6001\u548c\u89c6\u9891\u6a21\u6001\u7684\u8ddf\u8e2a\u5668\u3002", "method": "\u63d0\u51faUniSOT\u7edf\u4e00\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u53c2\u6570\u8bbe\u8ba1\u652f\u6301\u4e09\u79cd\u53c2\u8003\u6a21\u6001\u548c\u56db\u79cd\u89c6\u9891\u6a21\u6001\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u8ddf\u8e2a\u80fd\u529b\u3002", "result": "\u572818\u4e2a\u89c6\u89c9\u8ddf\u8e2a\u3001\u89c6\u89c9\u8bed\u8a00\u8ddf\u8e2a\u548cRGB+X\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniSOT\u8868\u73b0\u51fa\u4f18\u4e8e\u7279\u5b9a\u6a21\u6001\u5bf9\u5e94\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5728TNL2K\u4e0a\u6240\u6709\u4e09\u79cd\u53c2\u8003\u6a21\u6001\u7684AUC\u8d85\u8fc7\u5148\u524d\u65b9\u6cd53.0%\u4ee5\u4e0a\uff0c\u5728RGB+X\u89c6\u9891\u6a21\u6001\u4e0a\u4e3b\u8981\u6307\u6807\u8d85\u8fc7Un-Track 2.0%\u4ee5\u4e0a\u3002", "conclusion": "UniSOT\u8bc1\u660e\u4e86\u7edf\u4e00\u8ddf\u8e2a\u5668\u5728\u5904\u7406\u591a\u79cd\u53c2\u8003\u6a21\u6001\u548c\u89c6\u9891\u6a21\u6001\u7ec4\u5408\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01411", "abs": "https://arxiv.org/abs/2511.01411", "authors": ["Reza Karimzadeh", "Albert Alonso", "Frans Zdyb", "Julius B. Kirkegaard", "Bulat Ibragimov"], "title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "comment": null, "summary": "Faithful yet compact explanations for vision models remain a challenge, as\ncommonly used dense perturbation masks are often fragmented and overfitted,\nneeding careful post-processing. Here, we present a training-free explanation\nmethod that replaces dense masks with smooth tunable contours. A star-convex\nregion is parameterized by a truncated Fourier series and optimized under an\nextremal preserve/delete objective using the classifier gradients. The approach\nguarantees a single, simply connected mask, cuts the number of free parameters\nby orders of magnitude, and yields stable boundary updates without cleanup.\nRestricting solutions to low-dimensional, smooth contours makes the method\nrobust to adversarial masking artifacts. On ImageNet classifiers, it matches\nthe extremal fidelity of dense masks while producing compact, interpretable\nregions with improved run-to-run consistency. Explicit area control also\nenables importance contour maps, yielding a transparent fidelity-area profiles.\nFinally, we extend the approach to multi-contour and show how it can localize\nmultiple objects within the same framework. Across benchmarks, the method\nachieves higher relevance mass and lower complexity than gradient and\nperturbation based baselines, with especially strong gains on self-supervised\nDINO models where it improves relevance mass by over 15% and maintains positive\nfaithfulness correlations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7528\u5e73\u6ed1\u53ef\u8c03\u8f6e\u5ed3\u66ff\u4ee3\u5bc6\u96c6\u6270\u52a8\u63a9\u7801\uff0c\u901a\u8fc7\u661f\u51f8\u533a\u57df\u53c2\u6570\u5316\u548c\u5085\u91cc\u53f6\u7ea7\u6570\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u6781\u503c\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u751f\u6210\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u533a\u57df\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bc6\u96c6\u6270\u52a8\u63a9\u7801\u65b9\u6cd5\u5b58\u5728\u7684\u788e\u7247\u5316\u3001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9700\u8981\u590d\u6742\u540e\u5904\u7406\uff0c\u5bfb\u6c42\u66f4\u7d27\u51d1\u3001\u7a33\u5b9a\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\u53c2\u6570\u5316\u661f\u51f8\u533a\u57df\uff0c\u5728\u6781\u503c\u4fdd\u7559/\u5220\u9664\u76ee\u6807\u4e0b\u5229\u7528\u5206\u7c7b\u5668\u68af\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u4fdd\u8bc1\u5355\u4e00\u8fde\u901a\u63a9\u7801\uff0c\u5927\u5e45\u51cf\u5c11\u81ea\u7531\u53c2\u6570\u6570\u91cf\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u5668\u4e0a\u5339\u914d\u5bc6\u96c6\u63a9\u7801\u7684\u6781\u503c\u4fdd\u771f\u5ea6\uff0c\u4ea7\u751f\u7d27\u51d1\u53ef\u89e3\u91ca\u533a\u57df\uff0c\u63d0\u9ad8\u8fd0\u884c\u4e00\u81f4\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u68af\u5ea6\u548c\u6270\u52a8\u57fa\u7ebf\u83b7\u5f97\u66f4\u9ad8\u76f8\u5173\u8d28\u91cf\u548c\u66f4\u4f4e\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f4e\u7ef4\u5e73\u6ed1\u8f6e\u5ed3\u751f\u6210\u9c81\u68d2\u7684\u89e3\u91ca\uff0c\u7279\u522b\u5728\u81ea\u76d1\u7763DINO\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u5173\u6027\u8d28\u91cf\u63d0\u5347\u8d85\u8fc715%\uff0c\u4fdd\u6301\u6b63\u5fe0\u5b9e\u6027\u76f8\u5173\u6027\u3002"}}
{"id": "2511.01449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01449", "abs": "https://arxiv.org/abs/2511.01449", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Aayush Mishra", "Parijat Deshpande", "Beena Rai"], "title": "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction", "comment": "9 pages, 1 figure, 4 tables", "summary": "To effectively manage the wastage of perishable fruits, it is crucial to\naccurately predict their freshness or shelf life using non-invasive methods\nthat rely on visual data. In this regard, deep learning techniques can offer a\nviable solution. However, obtaining fine-grained fruit freshness labels from\nexperts is costly, leading to a scarcity of data. Closed proprietary Vision\nLanguage Models (VLMs), such as Gemini, have demonstrated strong performance in\nfruit freshness detection task in both zero-shot and few-shot settings.\nNonetheless, food retail organizations are unable to utilize these proprietary\nmodels due to concerns related to data privacy, while existing open-source VLMs\nyield sub-optimal performance for the task. Fine-tuning these open-source\nmodels with limited data fails to achieve the performance levels of proprietary\nmodels. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning\n(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes\nmeta-learning to address data sparsity and leverages label ordinality, thereby\nachieving state-of-the-art performance in the fruit freshness classification\ntask under both zero-shot and few-shot settings. Our method achieves an\nindustry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,\nOrdinal Regression", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5e8f\u6570\u5143\u5b66\u4e60\u7b97\u6cd5(MAOML)\uff0c\u7528\u4e8e\u8bad\u7ec3\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6c34\u679c\u65b0\u9c9c\u5ea6\u5206\u7c7b\u4efb\u52a1\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\uff0c\u800c\u4e13\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6570\u636e\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u4f7f\u7528\uff0c\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u6709\u6548\u5de5\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u5e8f\u6570\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u5229\u7528\u6807\u7b7e\u7684\u5e8f\u6570\u7279\u6027\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u6c34\u679c\u65b0\u9c9c\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8692.71%\u7684\u884c\u4e1a\u6807\u51c6\u51c6\u786e\u7387\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MAOML\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u8bad\u7ec3\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u63a5\u8fd1\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2511.01419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01419", "abs": "https://arxiv.org/abs/2511.01419", "authors": ["Yongqi Yang", "Huayang Huang", "Xu Peng", "Xiaobin Hu", "Donghao Luo", "Jiangning Zhang", "Chengjie Wang", "Yu Wu"], "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation", "comment": "Under double-blind review as a conference paper", "summary": "Recent hybrid video generation models combine autoregressive temporal\ndynamics with diffusion-based spatial denoising, but their sequential,\niterative nature leads to error accumulation and long inference times. In this\nwork, we propose a distillation-based framework for efficient causal video\ngeneration that enables high-quality synthesis with extremely limited denoising\nsteps. Our approach builds upon the Distribution Matching Distillation (DMD)\nframework and proposes a novel Adversarial Self-Distillation (ASD) strategy,\nwhich aligns the outputs of the student model's n-step denoising process with\nits (n+1)-step version at the distribution level. This design provides smoother\nsupervision by bridging small intra-student gaps and more informative guidance\nby combining teacher knowledge with locally consistent student behavior,\nsubstantially improving training stability and generation quality in extremely\nfew-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame\nEnhancement (FFE) strategy, which allocates more denoising steps to the initial\nframes to mitigate error propagation while applying larger skipping steps to\nlater frames. Extensive experiments on VBench demonstrate that our method\nsurpasses state-of-the-art approaches in both one-step and two-step video\ngeneration. Notably, our framework produces a single distilled model that\nflexibly supports multiple inference-step settings, eliminating the need for\nrepeated re-distillation and enabling efficient, high-quality video synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u9ad8\u6548\u56e0\u679c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u81ea\u84b8\u998f\u7b56\u7565\u548c\u9996\u5e27\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u6781\u5c11\u7684\u53bb\u566a\u6b65\u9aa4\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408\u89c6\u9891\u751f\u6210\u6a21\u578b\u7ed3\u5408\u81ea\u56de\u5f52\u65f6\u95f4\u52a8\u6001\u548c\u57fa\u4e8e\u6269\u6563\u7684\u7a7a\u95f4\u53bb\u566a\uff0c\u4f46\u5176\u987a\u5e8f\u8fed\u4ee3\u7279\u6027\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u957f\u63a8\u7406\u65f6\u95f4\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5206\u5e03\u5339\u914d\u84b8\u998f\u6846\u67b6\uff0c\u63d0\u51fa\u5bf9\u6297\u6027\u81ea\u84b8\u998f\u7b56\u7565\uff0c\u5c06\u5b66\u751f\u6a21\u578b\u7684n\u6b65\u53bb\u566a\u8f93\u51fa\u4e0e\u5176(n+1)\u6b65\u7248\u672c\u5728\u5206\u5e03\u7ea7\u522b\u5bf9\u9f50\uff1b\u540c\u65f6\u63d0\u51fa\u9996\u5e27\u589e\u5f3a\u7b56\u7565\uff0c\u4e3a\u9996\u5e27\u5206\u914d\u66f4\u591a\u53bb\u566a\u6b65\u9aa4\u4ee5\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728VBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u6b65\u548c\u4e24\u6b65\u89c6\u9891\u751f\u6210\u4e2d\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u652f\u6301\u591a\u79cd\u63a8\u7406\u6b65\u9aa4\u8bbe\u7f6e\u7684\u5355\u4e00\u84b8\u998f\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u84b8\u998f\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\uff0c\u6d88\u9664\u4e86\u91cd\u590d\u518d\u84b8\u998f\u7684\u9700\u6c42\uff0c\u4e3a\u6781\u5c11\u6570\u6b65\u9aa4\u573a\u666f\u4e0b\u7684\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u7a33\u5b9a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01450", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01450", "abs": "https://arxiv.org/abs/2511.01450", "authors": ["Jie Du", "Xinyu Gong", "Qingshan Tan", "Wen Li", "Yangming Cheng", "Weitao Wang", "Chenlu Zhan", "Suhui Wu", "Hao Zhang", "Jun Zhang"], "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation", "comment": null, "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\u7684\u65b9\u6cd5GT-Pair\uff0c\u4ee5\u53ca\u7ed3\u5408SFT\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u7684Reg-DPO\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684DPO\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u56fe\u50cf\u9886\u57df\u8303\u5f0f\uff0c\u4e14\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e0a\u5f00\u53d1\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u89c6\u9891\u4efb\u52a1\u7279\u6709\u7684\u6311\u6218\uff0c\u5982\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5185\u5b58\u6d88\u8017\u5927\u3002", "method": "1. \u63d0\u51faGT-Pair\u65b9\u6cd5\uff0c\u4f7f\u7528\u771f\u5b9e\u89c6\u9891\u4f5c\u4e3a\u6b63\u6837\u672c\uff0c\u6a21\u578b\u751f\u6210\u89c6\u9891\u4f5c\u4e3a\u8d1f\u6837\u672c\uff0c\u81ea\u52a8\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u5bf9\uff1b2. \u63d0\u51faReg-DPO\u65b9\u6cd5\uff0c\u5c06SFT\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u9879\u878d\u5165DPO\u76ee\u6807\uff0c\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u4fdd\u771f\u5ea6\uff1b3. \u7ed3\u5408FSDP\u6846\u67b6\u4e0e\u591a\u79cd\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u63d0\u5347\u8bad\u7ec3\u5bb9\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u7684I2V\u548cT2V\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u4f18\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u6784\u5efa\u504f\u597d\u5bf9\u3001\u5f15\u5165\u6b63\u5219\u5316\u9879\u548c\u4f18\u5316\u5185\u5b58\u4f7f\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.01458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01458", "abs": "https://arxiv.org/abs/2511.01458", "authors": ["Dennis Pierantozzi", "Luca Carlini", "Mauro Orazio Drago", "Chiara Lena", "Cesare Hassan", "Elena De Momi", "Danail Stoyanov", "Sophia Bano", "Mobarak I. Hoque"], "title": "When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA", "comment": null, "summary": "Safety and reliability are essential for deploying Visual Question Answering\n(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.\nMost surgical VQA research focuses on accuracy or linguistic quality while\noverlooking safety behaviors such as ambiguity awareness, referral to human\nexperts, or triggering a second opinion. Inspired by Automatic Failure\nDetection (AFD), we study uncertainty estimation as a key enabler of safer\ndecision making. We introduce Question Aligned Semantic Nearest Neighbor\nEntropy (QA-SNNE), a black box uncertainty estimator that incorporates question\nsemantics into prediction confidence. It measures semantic entropy by comparing\ngenerated answers with nearest neighbors in a medical text embedding space,\nconditioned on the question. We evaluate five models, including domain specific\nParameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large\nVision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models\ndegrade under mild paraphrasing, while LVLMs are more resilient. Across three\nLVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template\nsettings and enhances hallucination detection. The Area Under the ROC Curve\n(AUROC) increases by 15-38% for zero-shot models, with gains maintained under\nout-of-template stress. QA-SNNE offers a practical and interpretable step\ntoward AFD in surgical VQA by linking semantic uncertainty to question context.\nCombining LVLM backbones with question aligned uncertainty estimation can\nimprove safety and clinician trust. The code and model are available at\nhttps://github.com/DennisPierantozzi/QASNNE", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQA-SNNE\u7684\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u8bed\u4e49\u878d\u5165\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6765\u63d0\u9ad8\u624b\u672f\u89c6\u89c9\u95ee\u7b54(VQA)\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u533b\u5b66\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u751f\u6210\u7b54\u6848\u4e0e\u6700\u8fd1\u90bb\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u6d4b\u91cf\u8bed\u4e49\u71b5\u3002", "motivation": "\u624b\u672fVQA\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u9519\u8bef\u6216\u6a21\u7cca\u7684\u56de\u7b54\u53ef\u80fd\u5bf9\u60a3\u8005\u9020\u6210\u4f24\u5bb3\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u51c6\u786e\u6027\u6216\u8bed\u8a00\u8d28\u91cf\uff0c\u800c\u5ffd\u7565\u4e86\u5b89\u5168\u884c\u4e3a\uff0c\u5982\u6a21\u7cca\u610f\u8bc6\u3001\u8f6c\u8bca\u7ed9\u4eba\u7c7b\u4e13\u5bb6\u6216\u89e6\u53d1\u4e8c\u6b21\u610f\u89c1\u3002", "method": "\u5f15\u5165Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE)\uff0c\u8fd9\u662f\u4e00\u79cd\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5668\uff0c\u5c06\u95ee\u9898\u8bed\u4e49\u7eb3\u5165\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002\u5b83\u901a\u8fc7\u5728\u533b\u5b66\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u751f\u6210\u7b54\u6848\u4e0e\u6700\u8fd1\u90bb\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u6765\u6d4b\u91cf\u8bed\u4e49\u71b5\u3002", "result": "\u5728EndoVis18-VQA\u548cPitVQA\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e94\u4e2a\u6a21\u578b\u3002PEFT\u6a21\u578b\u5728\u8f7b\u5fae\u6539\u5199\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800cLVLMs\u66f4\u5177\u5f39\u6027\u3002QA-SNNE\u5728\u5927\u591a\u6570\u6a21\u677f\u5185\u8bbe\u7f6e\u4e2d\u63d0\u9ad8\u4e86AUROC\uff0c\u5e76\u589e\u5f3a\u4e86\u5e7b\u89c9\u68c0\u6d4b\u3002\u96f6\u6837\u672c\u6a21\u578b\u7684AUROC\u63d0\u9ad8\u4e8615-38%\uff0c\u5728\u6a21\u677f\u5916\u538b\u529b\u4e0b\u589e\u76ca\u4fdd\u6301\u3002", "conclusion": "QA-SNNE\u901a\u8fc7\u5c06\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u4e0e\u95ee\u9898\u4e0a\u4e0b\u6587\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u624b\u672fVQA\u4e2d\u7684\u81ea\u52a8\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u6b65\u9aa4\u3002\u5c06LVLM\u9aa8\u5e72\u4e0e\u95ee\u9898\u5bf9\u9f50\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u76f8\u7ed3\u5408\u53ef\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u4e34\u5e8a\u533b\u751f\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2511.01434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01434", "abs": "https://arxiv.org/abs/2511.01434", "authors": ["Seongkyu Choi", "Jhonghyun An"], "title": "Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation", "comment": null, "summary": "Off-road semantic segmentation suffers from thick, inconsistent boundaries,\nsparse supervision for rare classes, and pervasive label noise. Designs that\nfuse only at low resolution blur edges and propagate local errors, whereas\nmaintaining high-resolution pathways or repeating high-resolution fusions is\ncostly and fragile to noise. We introduce a resolutionaware token decoder that\nbalances global semantics, local consistency, and boundary fidelity under\nimperfect supervision. Most computation occurs at a low-resolution bottleneck;\na gated cross-attention injects fine-scale detail, and only a sparse,\nuncertainty-selected set of pixels is refined. The components are co-designed\nand tightly integrated: global self-attention with lightweight dilated\ndepthwise refinement restores local coherence; a gated cross-attention\nintegrates fine-scale features from a standard high-resolution encoder stream\nwithout amplifying noise; and a class-aware point refinement corrects residual\nambiguities with negligible overhead. During training, we add a boundary-band\nconsistency regularizer that encourages coherent predictions in a thin\nneighborhood around annotated edges, with no inference-time cost. Overall, the\nresults indicate competitive performance and improved stability across\ntransitions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u8fa8\u7387\u611f\u77e5\u7684token\u89e3\u7801\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u8d8a\u91ce\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8fb9\u754c\u6a21\u7cca\u3001\u7a00\u758f\u76d1\u7763\u548c\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u8bed\u4e49\u3001\u5c40\u90e8\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\u8bbe\u8ba1\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8d8a\u91ce\u8bed\u4e49\u5206\u5272\u9762\u4e34\u8fb9\u754c\u4e0d\u4e00\u81f4\u3001\u7a00\u6709\u7c7b\u522b\u7a00\u758f\u76d1\u7763\u548c\u666e\u904d\u6807\u7b7e\u566a\u58f0\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u878d\u5408\u65f6\u4f1a\u6a21\u7cca\u8fb9\u7f18\u5e76\u4f20\u64ad\u5c40\u90e8\u9519\u8bef\uff0c\u800c\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u8def\u5f84\u6216\u91cd\u590d\u9ad8\u5206\u8fa8\u7387\u878d\u5408\u5219\u6210\u672c\u9ad8\u4e14\u5bf9\u566a\u58f0\u654f\u611f\u3002", "method": "\u5f15\u5165\u5206\u8fa8\u7387\u611f\u77e5token\u89e3\u7801\u5668\uff0c\u4e3b\u8981\u8ba1\u7b97\u5728\u4f4e\u5206\u8fa8\u7387\u74f6\u9888\u8fdb\u884c\uff1b\u4f7f\u7528\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u7cbe\u7ec6\u5c3a\u5ea6\u7ec6\u8282\uff1b\u4ec5\u5bf9\u7a00\u758f\u3001\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u7684\u50cf\u7d20\u96c6\u8fdb\u884c\u7ec6\u5316\u3002\u7ec4\u4ef6\u5305\u62ec\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u4e0e\u8f7b\u91cf\u7ea7\u6269\u5f20\u6df1\u5ea6\u7ec6\u5316\u3001\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u7f16\u7801\u5668\u7279\u5f81\u3001\u7c7b\u522b\u611f\u77e5\u70b9\u7ec6\u5316\u3002\u8bad\u7ec3\u65f6\u6dfb\u52a0\u8fb9\u754c\u5e26\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5668\u3002", "result": "\u7ed3\u679c\u8868\u660e\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u548c\u8de8\u8fc7\u6e21\u7684\u6539\u8fdb\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8d8a\u91ce\u8bed\u4e49\u5206\u5272\u4e2d\u6709\u6548\u5e73\u8861\u4e86\u5168\u5c40\u8bed\u4e49\u3001\u5c40\u90e8\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u4fdd\u771f\u5ea6\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5206\u5272\u8d28\u91cf\u3002"}}
{"id": "2511.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01435", "abs": "https://arxiv.org/abs/2511.01435", "authors": ["SiWoo Kim", "JhongHyun An"], "title": "Contrast-Guided Cross-Modal Distillation for Thermal Object Detection", "comment": null, "summary": "Robust perception at night remains challenging for thermal-infrared\ndetection: low contrast and weak high-frequency cues lead to duplicate,\noverlapping boxes, missed small objects, and class confusion. Prior remedies\neither translate TIR to RGB and hope pixel fidelity transfers to detection --\nmaking performance fragile to color or structure artifacts -- or fuse RGB and\nTIR at test time, which requires extra sensors, precise calibration, and higher\nruntime cost. Both lines can help in favorable conditions, but do not directly\nshape the thermal representation used by the detector. We keep mono-modality\ninference and tackle the root causes during training. Specifically, we\nintroduce training-only objectives that sharpen instance-level decision\nboundaries by pulling together features of the same class and pushing apart\nthose of different classes -- suppressing duplicate and confusing detections --\nand that inject cross-modal semantic priors by aligning the student's\nmulti-level pyramid features with an RGB-trained teacher, thereby strengthening\ntexture-poor thermal features without visible input at test time. In\nexperiments, our method outperformed prior approaches and achieved\nstate-of-the-art performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u7528\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u70ed\u7ea2\u5916\u68c0\u6d4b\u4e2d\u7684\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u5f31\u9ad8\u9891\u7ebf\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u62c9\u8fd1\u540c\u7c7b\u7279\u5f81\u548c\u63a8\u8fdc\u4e0d\u540c\u7c7b\u7279\u5f81\u6765\u9510\u5316\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5229\u7528RGB\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u6ce8\u5165\u8de8\u6a21\u6001\u8bed\u4e49\u5148\u9a8c\uff0c\u5728\u4fdd\u6301\u5355\u6a21\u6001\u63a8\u7406\u7684\u540c\u65f6\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u70ed\u7ea2\u5916\u611f\u77e5\u5728\u591c\u95f4\u9762\u4e34\u6311\u6218\uff1a\u4f4e\u5bf9\u6bd4\u5ea6\u548c\u5f31\u9ad8\u9891\u7ebf\u7d22\u5bfc\u81f4\u91cd\u590d\u3001\u91cd\u53e0\u7684\u68c0\u6d4b\u6846\u3001\u5c0f\u76ee\u6807\u6f0f\u68c0\u548c\u7c7b\u522b\u6df7\u6dc6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5c06TIR\u8f6c\u6362\u4e3aRGB\uff08\u5bf9\u989c\u8272\u6216\u7ed3\u6784\u4f2a\u5f71\u654f\u611f\uff09\uff0c\u8981\u4e48\u5728\u6d4b\u8bd5\u65f6\u878d\u5408RGB\u548cTIR\uff08\u9700\u8981\u989d\u5916\u4f20\u611f\u5668\u548c\u7cbe\u786e\u6821\u51c6\uff09\uff0c\u90fd\u6ca1\u6709\u76f4\u63a5\u5851\u9020\u68c0\u6d4b\u5668\u4f7f\u7528\u7684\u70ed\u8868\u793a\u3002", "method": "\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u4e24\u4e2a\u76ee\u6807\uff1a1\uff09\u901a\u8fc7\u62c9\u8fd1\u540c\u7c7b\u7279\u5f81\u548c\u63a8\u8fdc\u4e0d\u540c\u7c7b\u7279\u5f81\u6765\u9510\u5316\u5b9e\u4f8b\u7ea7\u51b3\u7b56\u8fb9\u754c\uff0c\u6291\u5236\u91cd\u590d\u548c\u6df7\u6dc6\u68c0\u6d4b\uff1b2\uff09\u901a\u8fc7\u5c06\u5b66\u751f\u7684\u591a\u7ea7\u91d1\u5b57\u5854\u7279\u5f81\u4e0eRGB\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\u5bf9\u9f50\uff0c\u6ce8\u5165\u8de8\u6a21\u6001\u8bed\u4e49\u5148\u9a8c\uff0c\u589e\u5f3a\u7eb9\u7406\u8d2b\u4e4f\u7684\u70ed\u7279\u5f81\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u6a21\u6001\u63a8\u7406\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u8bad\u7ec3\u9636\u6bb5\u7684\u6539\u8fdb\u76f4\u63a5\u89e3\u51b3\u4e86\u70ed\u7ea2\u5916\u68c0\u6d4b\u7684\u6839\u672c\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u591c\u95f4\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2511.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01541", "abs": "https://arxiv.org/abs/2511.01541", "authors": ["Arthur Hubert", "Gamal Elghazaly", "Rapha\u00ebl Frank"], "title": "Driving scenario generation and evaluation using a structured layer representation and foundational models", "comment": null, "summary": "Rare and challenging driving scenarios are critical for autonomous vehicle\ndevelopment. Since they are difficult to encounter, simulating or generating\nthem using generative models is a popular approach. Following previous efforts\nto structure driving scenario representations in a layer model, we propose a\nstructured five-layer model to improve the evaluation and generation of rare\nscenarios. We use this model alongside large foundational models to generate\nnew driving scenarios using a data augmentation strategy. Unlike previous\nrepresentations, our structure introduces subclasses and characteristics for\nevery agent of the scenario, allowing us to compare them using an embedding\nspecific to our layer-model. We study and adapt two metrics to evaluate the\nrelevance of a synthetic dataset in the context of a structured representation:\nthe diversity score estimates how different the scenarios of a dataset are from\none another, while the originality score calculates how similar a synthetic\ndataset is from a real reference set. This paper showcases both metrics in\ndifferent generation setup, as well as a qualitative evaluation of synthetic\nvideos generated from structured scenario descriptions. The code and extended\nresults can be found at https://github.com/Valgiz/5LMSG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e94\u5c42\u7ed3\u6784\u5316\u6a21\u578b\u6765\u6539\u8fdb\u7f55\u89c1\u9a7e\u9a76\u573a\u666f\u7684\u8bc4\u4f30\u548c\u751f\u6210\uff0c\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7ed3\u5408\u5927\u578b\u57fa\u7840\u6a21\u578b\u751f\u6210\u65b0\u573a\u666f\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u6837\u6027\u548c\u539f\u521b\u6027\u6307\u6807\u6765\u8bc4\u4f30\u5408\u6210\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "motivation": "\u7f55\u89c1\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u9a7e\u9a76\u573a\u666f\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u96be\u4ee5\u9047\u5230\uff0c\u9700\u8981\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6765\u6a21\u62df\u6216\u751f\u6210\u8fd9\u4e9b\u573a\u666f\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u4e94\u5c42\u6a21\u578b\uff0c\u4e3a\u573a\u666f\u4e2d\u7684\u6bcf\u4e2a\u667a\u80fd\u4f53\u5f15\u5165\u5b50\u7c7b\u548c\u7279\u5f81\uff0c\u4f7f\u7528\u7279\u5b9a\u4e8e\u5c42\u6a21\u578b\u7684\u5d4c\u5165\u8fdb\u884c\u6bd4\u8f83\uff1b\u91c7\u7528\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7ed3\u5408\u5927\u578b\u57fa\u7840\u6a21\u578b\u751f\u6210\u65b0\u9a7e\u9a76\u573a\u666f\uff1b\u7814\u7a76\u5e76\u9002\u914d\u4e86\u591a\u6837\u6027\u8bc4\u5206\u548c\u539f\u521b\u6027\u8bc4\u5206\u4e24\u79cd\u6307\u6807\u3002", "result": "\u5728\u4e0d\u540c\u751f\u6210\u8bbe\u7f6e\u4e0b\u5c55\u793a\u4e86\u4e24\u79cd\u6307\u6807\u7684\u5e94\u7528\uff0c\u5e76\u5bf9\u4ece\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u751f\u6210\u7684\u5408\u6210\u89c6\u9891\u8fdb\u884c\u4e86\u5b9a\u6027\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u4e94\u5c42\u6a21\u578b\u548c\u8bc4\u4f30\u6307\u6807\u80fd\u591f\u6709\u6548\u6539\u8fdb\u7f55\u89c1\u9a7e\u9a76\u573a\u666f\u7684\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2511.01610", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01610", "abs": "https://arxiv.org/abs/2511.01610", "authors": ["Mahmut Selman Gokmen", "Cody Bumgardner"], "title": "DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning", "comment": null, "summary": "Vision Foundation Models (VFMs) have advanced representation learning through\nself-supervised methods. However, existing training pipelines are often\ninflexible, domain-specific, or computationally expensive, which limits their\nusability across different domains and resource settings. DINO-MX is a modular\nand extensible training framework that combines the core principles of DINO,\nDINOv2 and DINOv3 within a unified configuration-driven system. It supports a\nvariety of transformer-based architectures and is fully compatible with the\nHugging Face ecosystem. The framework includes multiple training strategies\nsuch as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,\nalong with support for distributed training through both Distributed Data\nParallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to\nwork with both natural and specialized data types, including single- and\nmulti-channel images. Experimental results on diverse datasets show that\nDINO-MX achieves competitive performance while significantly reducing\ncomputational costs. Additionally, it offers interpretability tools and a\nlabel-guided data augmentation method that improves attention-based\nlocalization without the need for extra detection or segmentation heads.\nDINO-MX provides a reproducible and scalable foundation for developing,\nadapting, and benchmarking self-supervised vision models across a range of\nresearch and real-world applications.", "AI": {"tldr": "DINO-MX\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u7edf\u4e00\u4e86DINO\u7cfb\u5217\u65b9\u6cd5\u7684\u6838\u5fc3\u539f\u7406\uff0c\u652f\u6301\u591a\u79cdTransformer\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u7ade\u4e89\u529b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u5b58\u5728\u4e0d\u7075\u6d3b\u3001\u9886\u57df\u7279\u5b9a\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u9886\u57df\u548c\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u53ef\u7528\u6027\u3002", "method": "\u91c7\u7528\u914d\u7f6e\u9a71\u52a8\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u7ed3\u5408DINO\u7cfb\u5217\u65b9\u6cd5\u539f\u7406\uff0c\u652f\u6301\u591a\u79cdTransformer\u67b6\u6784\uff0c\u63d0\u4f9bLoRA\u3001\u5c42\u51bb\u7ed3\u3001\u77e5\u8bc6\u84b8\u998f\u7b49\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u8bad\u7ec3\u652f\u6301\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDINO-MX\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7ade\u4e89\u529b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u548c\u6807\u7b7e\u5f15\u5bfc\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "conclusion": "DINO-MX\u4e3a\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.01510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01510", "abs": "https://arxiv.org/abs/2511.01510", "authors": ["Derong Kong", "Zhixiong Yang", "Shengxi Li", "Shuaifeng Zhi", "Li Liu", "Zhen Liu", "Jingyuan Xia"], "title": "Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement", "comment": "Accepted at NeurIPS 2025", "summary": "Low-light image enhancement (LLIE) faces persistent challenges in balancing\nreconstruction fidelity with cross-scenario generalization. While existing\nmethods predominantly focus on deterministic pixel-level mappings between\npaired low/normal-light images, they often neglect the continuous physical\nprocess of luminance transitions in real-world environments, leading to\nperformance drop when normal-light references are unavailable. Inspired by\nempirical analysis of natural luminance dynamics revealing power-law\ndistributed intensity transitions, this paper introduces Luminance-Aware\nStatistical Quantification (LASQ), a novel framework that reformulates LLIE as\na statistical sampling process over hierarchical luminance distributions. Our\nLASQ re-conceptualizes luminance transition as a power-law distribution in\nintensity coordinate space that can be approximated by stratified power\nfunctions, therefore, replacing deterministic mappings with probabilistic\nsampling over continuous luminance layers. A diffusion forward process is\ndesigned to autonomously discover optimal transition paths between luminance\nlayers, achieving unsupervised distribution emulation without normal-light\nreferences. In this way, it considerably improves the performance in practical\nsituations, enabling more adaptable and versatile light restoration. This\nframework is also readily applicable to cases with normal-light references,\nwhere it achieves superior performance on domain-specific datasets alongside\nbetter generalization-ability across non-reference datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u91cf\u5316\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u6846\u67b6LASQ\uff0c\u5c06\u4eae\u5ea6\u8f6c\u6362\u5efa\u6a21\u4e3a\u5e42\u5f8b\u5206\u5e03\uff0c\u901a\u8fc7\u5206\u5c42\u4eae\u5ea6\u5206\u5e03\u7684\u7edf\u8ba1\u91c7\u6837\u8fc7\u7a0b\u66ff\u4ee3\u786e\u5b9a\u6027\u6620\u5c04\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u6b63\u5e38\u5149\u7167\u53c2\u8003\u7684\u65e0\u76d1\u7763\u589e\u5f3a\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u50cf\u7d20\u7ea7\u6620\u5c04\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u4eae\u5ea6\u8f6c\u6362\u7684\u8fde\u7eed\u7269\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u5728\u7f3a\u4e4f\u6b63\u5e38\u5149\u7167\u53c2\u8003\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4eae\u5ea6\u611f\u77e5\u7edf\u8ba1\u91cf\u5316(LASQ)\u6846\u67b6\uff0c\u5c06\u4eae\u5ea6\u8f6c\u6362\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u5f3a\u5ea6\u5750\u6807\u7a7a\u95f4\u4e2d\u7684\u5e42\u5f8b\u5206\u5e03\uff0c\u901a\u8fc7\u5206\u5c42\u5e42\u51fd\u6570\u8fd1\u4f3c\uff0c\u8bbe\u8ba1\u6269\u6563\u524d\u5411\u8fc7\u7a0b\u81ea\u4e3b\u53d1\u73b0\u4eae\u5ea6\u5c42\u95f4\u7684\u6700\u4f18\u8f6c\u6362\u8def\u5f84\u3002", "result": "\u5728\u65e0\u6b63\u5e38\u5149\u7167\u53c2\u8003\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6709\u53c2\u8003\u60c5\u51b5\u4e0b\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LASQ\u6846\u67b6\u901a\u8fc7\u7edf\u8ba1\u91c7\u6837\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u4e2d\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0e\u8de8\u573a\u666f\u6cdb\u5316\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9002\u5e94\u548c\u901a\u7528\u7684\u5149\u7167\u6062\u590d\u3002"}}
{"id": "2511.01513", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.01513", "abs": "https://arxiv.org/abs/2511.01513", "authors": ["Andrei-Timotei Ardelean", "Tim Weyrich"], "title": "Example-Based Feature Painting on Textures", "comment": "\"\\c{opyright} 2025 Andrei-Timotei Ardelean, Tim Weyrich. This is the\n  author's version of the work. It is posted here for your personal use. Not\n  for redistribution. The definitive Version of Record was published in ACM\n  Trans. Graph., Vol. 44, No. 6, https://doi.org/10.1145/3763301", "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5b8c\u6574\u7684\u5de5\u4f5c\u6d41\u7a0b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u73b0\u5177\u6709\u72ec\u7279\u5c40\u90e8\u7279\u5f81\u7684\u7eb9\u7406\u7684\u53d7\u63a7\u521b\u4f5c\u548c\u7f16\u8f91\uff0c\u5305\u62ec\u6c61\u6e0d\u3001\u6495\u88c2\u3001\u5b54\u6d1e\u3001\u78e8\u635f\u3001\u53d8\u8272\u7b49\u8868\u9762\u6548\u679c\u3002", "motivation": "\u81ea\u7136\u754c\u4e2d\u666e\u904d\u5b58\u5728\u5404\u79cd\u6539\u53d8\u6750\u6599\u8868\u9762\u5916\u89c2\u7684\u6548\u679c\uff0c\u5c06\u8fd9\u4e9b\u53d8\u5316\u7eb3\u5165\u5408\u6210\u8fc7\u7a0b\u5bf9\u4e8e\u751f\u6210\u903c\u771f\u7eb9\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u672a\u6807\u8bb0\u793a\u4f8b\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6765\u8bc6\u522b\u5916\u89c2\u6539\u53d8\u7279\u5f81\uff0c\u7136\u540e\u81ea\u52a8\u5c06\u5404\u79cd\u7eb9\u7406\u7279\u5f81\u805a\u7c7b\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u7ec4\uff0c\u7528\u4e8e\u6307\u5bfc\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4ece\u5c11\u91cf\u56fe\u50cf\u96c6\u5408\u5230\u591a\u529f\u80fd\u751f\u6210\u6a21\u578b\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u4ea4\u4e92\u5f0f\u5730\u5728\u4efb\u610f\u5927\u5c0f\u7684\u7eb9\u7406\u4e0a\u521b\u5efa\u548c\u7ed8\u5236\u7279\u5f81\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4ece\u56fe\u50cf\u6536\u96c6\u5230\u751f\u6210\u6a21\u578b\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5176\u4e2d\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u548c\u65e0\u9650\u5e73\u7a33\u7eb9\u7406\u751f\u6210\u7684\u7b97\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u5728\u5176\u4ed6\u573a\u666f\u4e2d\u4e5f\u5e94\u6709\u7528\u3002"}}
{"id": "2511.01517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01517", "abs": "https://arxiv.org/abs/2511.01517", "authors": ["Serkan Ozturk", "Samet Hicsonmez", "Pinar Duygulu"], "title": "NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation", "comment": "Under review", "summary": "Current text conditioned image generation methods output realistic looking\nimages, but they fail to capture specific styles. Simply finetuning them on the\ntarget style datasets still struggles to grasp the style features. In this\nwork, we present a novel contrastive learning framework to improve the\nstylization capability of large text-to-image diffusion models. Motivated by\nthe astonishing advance in image generation models that makes synthetic data an\nintrinsic part of model training in various computer vision tasks, we exploit\nsynthetic image generation in our approach. Usually, the generated synthetic\ndata is dependent on the task, and most of the time it is used to enlarge the\navailable real training dataset. With NSYNC, alternatively, we focus on\ngenerating negative synthetic sets to be used in a novel contrastive training\nscheme along with real positive images. In our proposed training setup, we\nforward negative data along with positive data and obtain negative and positive\ngradients, respectively. We then refine the positive gradient by subtracting\nits projection onto the negative gradient to get the orthogonal component,\nbased on which the parameters are updated. This orthogonal component eliminates\nthe trivial attributes that are present in both positive and negative data and\ndirects the model towards capturing a more unique style. Experiments on various\nstyles of painters and illustrators show that our approach improves the\nperformance over the baseline methods both quantitatively and qualitatively.\nOur code is available at https://github.com/giddyyupp/NSYNC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6NSYNC\uff0c\u901a\u8fc7\u751f\u6210\u8d1f\u6837\u672c\u5408\u6210\u56fe\u50cf\u6765\u6539\u8fdb\u5927\u578b\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u5316\u80fd\u529b\uff0c\u4f7f\u7528\u6b63\u4ea4\u68af\u5ea6\u66f4\u65b0\u6765\u6d88\u9664\u6b63\u8d1f\u6837\u672c\u4e2d\u7684\u5171\u540c\u5c5e\u6027\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u72ec\u7279\u98ce\u683c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u7279\u5b9a\u98ce\u683c\u3002\u76f4\u63a5\u5728\u76ee\u6807\u98ce\u683c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4ecd\u96be\u4ee5\u638c\u63e1\u98ce\u683c\u7279\u5f81\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u5347\u98ce\u683c\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u5408\u6210\u56fe\u50cf\u751f\u6210\u6280\u672f\u521b\u5efa\u8d1f\u6837\u672c\u96c6\uff0c\u4e0e\u771f\u5b9e\u6b63\u6837\u672c\u4e00\u8d77\u8fdb\u884c\u5bf9\u6bd4\u8bad\u7ec3\u3002\u901a\u8fc7\u8ba1\u7b97\u6b63\u68af\u5ea6\u5728\u8d1f\u68af\u5ea6\u4e0a\u7684\u6295\u5f71\uff0c\u83b7\u5f97\u6b63\u4ea4\u5206\u91cf\u6765\u66f4\u65b0\u53c2\u6570\uff0c\u6d88\u9664\u6b63\u8d1f\u6837\u672c\u4e2d\u7684\u5171\u540c\u5c5e\u6027\uff0c\u4f7f\u6a21\u578b\u4e13\u6ce8\u4e8e\u5b66\u4e60\u72ec\u7279\u98ce\u683c\u7279\u5f81\u3002", "result": "\u5728\u591a\u79cd\u753b\u5bb6\u548c\u63d2\u753b\u5e08\u98ce\u683c\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u5316\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u3002", "conclusion": "NSYNC\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528\u8d1f\u6837\u672c\u5408\u6210\u6570\u636e\u5728\u98ce\u683c\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.01549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01549", "abs": "https://arxiv.org/abs/2511.01549", "authors": ["Mikhail Konov", "Lion J. Gleiter", "Khoa Co", "Monica Yabal", "Tingying Peng"], "title": "NOA: a versatile, extensible tool for AI-based organoid analysis", "comment": null, "summary": "AI tools can greatly enhance the analysis of organoid microscopy images, from\ndetection and segmentation to feature extraction and classification. However,\ntheir limited accessibility to biologists without programming experience\nremains a major barrier, resulting in labor-intensive and largely manual\nworkflows. Although a few AI models for organoid analysis have been developed,\nmost existing tools remain narrowly focused on specific tasks. In this work, we\nintroduce the Napari Organoid Analyzer (NOA), a general purpose graphical user\ninterface to simplify AI-based organoid analysis. NOA integrates modules for\ndetection, segmentation, tracking, feature extraction, custom feature\nannotation and ML-based feature prediction. It interfaces multiple\nstate-of-the-art algorithms and is implemented as an open-source napari plugin\nfor maximal flexibility and extensibility. We demonstrate the versatility of\nNOA through three case studies, involving the quantification of morphological\nchanges during organoid differentiation, assessment of phototoxicity effects,\nand prediction of organoid viability and differentiation state. Together, these\nexamples illustrate how NOA enables comprehensive, AI-driven organoid image\nanalysis within an accessible and extensible framework.", "AI": {"tldr": "NOA\u662f\u4e00\u4e2a\u57fa\u4e8eNapari\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\u5de5\u5177\uff0c\u65e8\u5728\u7b80\u5316\u57fa\u4e8eAI\u7684\u7c7b\u5668\u5b98\u5206\u6790\uff0c\u6574\u5408\u4e86\u68c0\u6d4b\u3001\u5206\u5272\u3001\u8ddf\u8e2a\u3001\u7279\u5f81\u63d0\u53d6\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7b49\u591a\u4e2a\u6a21\u5757\u3002", "motivation": "\u73b0\u6709AI\u5de5\u5177\u5bf9\u65e0\u7f16\u7a0b\u7ecf\u9a8c\u7684\u751f\u7269\u5b66\u5bb6\u6765\u8bf4\u96be\u4ee5\u4f7f\u7528\uff0c\u5bfc\u81f4\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\u4e14\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6613\u7528\u7684\u5206\u6790\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86Napari Organoid Analyzer (NOA)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u63d2\u4ef6\uff0c\u96c6\u6210\u4e86\u591a\u79cd\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u63d0\u4f9b\u68c0\u6d4b\u3001\u5206\u5272\u3001\u8ddf\u8e2a\u3001\u7279\u5f81\u63d0\u53d6\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86NOA\u7684\u591a\u529f\u80fd\u6027\uff1a\u91cf\u5316\u7c7b\u5668\u5b98\u5206\u5316\u8fc7\u7a0b\u4e2d\u7684\u5f62\u6001\u53d8\u5316\u3001\u8bc4\u4f30\u5149\u6bd2\u6027\u6548\u5e94\u3001\u9884\u6d4b\u7c7b\u5668\u5b98\u6d3b\u529b\u548c\u5206\u5316\u72b6\u6001\u3002", "conclusion": "NOA\u4e3a\u7c7b\u5668\u5b98\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u53ef\u8bbf\u95ee\u4e14\u53ef\u6269\u5c55\u7684AI\u9a71\u52a8\u6846\u67b6\u3002"}}
{"id": "2511.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01593", "abs": "https://arxiv.org/abs/2511.01593", "authors": ["Yizhu Chen", "Chen Ju", "Zhicheng Wang", "Shuai Xiao", "Xu Chen", "Jinsong Lan", "Xiaoyong Zhu", "Ying Chen"], "title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation", "comment": null, "summary": "The unification of understanding and generation within a single multi-modal\nlarge model (MLLM) remains one significant challenge, largely due to the\ndichotomy between continuous and discrete visual tokenizations. Continuous\ntokenizer (CT) achieves strong performance by bridging multiple\nindependently-trained understanding modules and generation modules, but suffers\nfrom complex multi-stage pipelines and substantial engineering overhead.\nConversely, discrete tokenizers (DT) offer a conceptually elegant idea by\nquantizing each image into a primitive, but inevitably leading to information\nloss and performance degradation. To resolve this tension, we question the\nbinary choice between CT and DT, inspired by the wave-particle duality of\nlight, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).\nWe treat visual data as a flexible composition of image primitives derived from\nquantized codebooks, with the crucial insight that the primitive number\nassigned to each visual sample is adaptively determined according to its\ncomplexity: simple instances use a few primitives, emulating discrete\ntokenization, while complex instances use many, approximating continuous\ntokenization. Two core components are designed: Diverse Quantitative\nPrimitives, which encourage primitives orthogonality to better populate\ninformation space, and Dynamic Primitive Allocator, which assesses sample\ncomplexity to determine the optimal set of primitives. Extensive experiments on\nreconstruction, retrieval and classification show that CDD-VT achieves superior\nperformance over to specialized CT and DT, effectively getting strong result\nwithin a concise and scalable MLLM.", "AI": {"tldr": "CDD-VT\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed-\u79bb\u6563\u53cc\u6001\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u56fe\u50cf\u57fa\u5143\u6570\u91cf\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u7684\u7edf\u4e00\u95ee\u9898\uff0c\u7b80\u5355\u5b9e\u4f8b\u4f7f\u7528\u5c11\u91cf\u57fa\u5143\uff08\u7c7b\u4f3c\u79bb\u6563\u6807\u8bb0\uff09\uff0c\u590d\u6742\u5b9e\u4f8b\u4f7f\u7528\u66f4\u591a\u57fa\u5143\uff08\u7c7b\u4f3c\u8fde\u7eed\u6807\u8bb0\uff09\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\u5316\u7684\u6311\u6218\uff0c\u514b\u670d\u8fde\u7eed\u6807\u8bb0\u5668\u590d\u6742\u591a\u9636\u6bb5\u6d41\u7a0b\u548c\u79bb\u6563\u6807\u8bb0\u5668\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u53d7\u5149\u7684\u6ce2\u7c92\u4e8c\u8c61\u6027\u542f\u53d1\u3002", "method": "\u8bbe\u8ba1\u8fde\u7eed-\u79bb\u6563\u53cc\u6001\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u5305\u542b\u591a\u6837\u5316\u91cf\u5316\u57fa\u5143\uff08\u4fc3\u8fdb\u57fa\u5143\u6b63\u4ea4\u6027\u4ee5\u586b\u5145\u4fe1\u606f\u7a7a\u95f4\uff09\u548c\u52a8\u6001\u57fa\u5143\u5206\u914d\u5668\uff08\u6839\u636e\u6837\u672c\u590d\u6742\u5ea6\u786e\u5b9a\u6700\u4f18\u57fa\u5143\u96c6\u5408\uff09\u3002", "result": "\u5728\u91cd\u5efa\u3001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCDD-VT\u5728\u6027\u80fd\u548c\u7b80\u6d01\u6027\u65b9\u9762\u4f18\u4e8e\u4e13\u95e8\u7684\u8fde\u7eed\u6807\u8bb0\u5668\u548c\u79bb\u6563\u6807\u8bb0\u5668\u3002", "conclusion": "CDD-VT\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u7b80\u6d01\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u53d6\u5f97\u4e86\u5f3a\u5927\u7ed3\u679c\u3002"}}
{"id": "2511.01600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01600", "abs": "https://arxiv.org/abs/2511.01600", "authors": ["Agnar Martin Bj\u00f8rnstad", "Elias Stenhede", "Arian Ranjbar"], "title": "Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography", "comment": null, "summary": "Accurate tumor size measurement is a cornerstone of evaluating cancer\ntreatment response. The most widely adopted standard for this purpose is the\nResponse Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on\nmeasuring the longest tumor diameter in a single plane. However, volumetric\nmeasurements have been shown to provide a more reliable assessment of treatment\neffect. Their clinical adoption has been limited, though, due to the\nlabor-intensive nature of manual volumetric annotation. In this paper, we\npresent Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed\nfor efficient volumetric tumor segmentation from CT scans annotated with RECIST\nannotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:\nPan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice\nSimilarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of\n63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an\naverage inference time of 14.4 s on CPU on the public validation dataset.", "AI": {"tldr": "Lite ENSAM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u5e26\u6709RECIST\u6807\u6ce8\u7684CT\u626b\u63cf\u4e2d\u9ad8\u6548\u8fdb\u884c\u80bf\u7624\u4f53\u79ef\u5206\u5272\uff0c\u5728MICCAI FLARE 2025\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u80bf\u7624\u4f53\u79ef\u6d4b\u91cf\u6bd4RECIST\u6807\u51c6\u66f4\u53ef\u9760\u8bc4\u4f30\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u624b\u52a8\u4f53\u79ef\u6807\u6ce8\u8017\u65f6\u8d39\u529b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eENSAM\u67b6\u6784\u7684\u8f7b\u91cf\u7ea7\u9002\u914d\uff0c\u4e13\u95e8\u7528\u4e8e\u4eceRECIST\u6807\u6ce8\u7684CT\u626b\u63cf\u4e2d\u8fdb\u884c\u80bf\u7624\u4f53\u79ef\u5206\u5272\u3002", "result": "\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u83b7\u5f97DSC 60.7%\u548cNSD 63.6%\uff0c\u5728\u516c\u5171\u9a8c\u8bc1\u96c6\u4e0a\u5e73\u5747RAM\u65f6\u95f450.6GB\uff0cCPU\u63a8\u7406\u65f6\u95f414.4\u79d2\u3002", "conclusion": "Lite ENSAM\u4e3a\u80bf\u7624\u4f53\u79ef\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4f53\u79ef\u6d4b\u91cf\u5728\u4e34\u5e8a\u4e2d\u7684\u91c7\u7528\u3002"}}
{"id": "2511.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01613", "abs": "https://arxiv.org/abs/2511.01613", "authors": ["Tom\u00e1\u0161 Krsi\u010dka", "Tibor Kub\u00edk"], "title": "Benchmark-Ready 3D Anatomical Shape Classification", "comment": "Shape in Medical Imaging, ShapeMI 2025, Held in Conjunction with\n  MICCAI 2025", "summary": "Progress in anatomical 3D shape classification is limited by the complexity\nof mesh data and the lack of standardized benchmarks, highlighting the need for\nrobust learning methods and reproducible evaluation. We introduce two key steps\ntoward clinically and benchmark-ready anatomical shape classification via\nself-supervised graph autoencoding. We propose Precomputed Structural Pooling\n(PSPooling), a non-learnable mesh pooling operator designed for efficient and\nstructure-preserving graph coarsening in 3D anatomical shape analysis.\nPSPooling precomputes node correspondence sets based on geometric proximity,\nenabling parallelizable and reversible pooling and unpooling operations with\nguaranteed support structure. This design avoids the sparsity and\nreconstruction issues of selection-based methods and the sequential overhead of\nedge contraction approaches, making it particularly suitable for\nhigh-resolution medical meshes. To demonstrate its effectiveness, we integrate\nPSPooling into a self-supervised graph autoencoder that learns anatomy-aware\nrepresentations from unlabeled surface meshes. We evaluate the downstream\nbenefits on MedShapeNet19, a new curated benchmark dataset we derive from\nMedShapeNet, consisting of 19 anatomical classes with standardized training,\nvalidation, and test splits. Experiments show that PSPooling significantly\nimproves reconstruction fidelity and classification accuracy in low-label\nregimes, establishing a strong baseline for medical 3D shape learning. We hope\nthat MedShapeNet19 will serve as a widely adopted benchmark for anatomical\nshape classification and further research in medical 3D shape analysis. Access\nthe complete codebase, model weights, and dataset information here:\nhttps://github.com/TomasKrsicka/MedShapeNet19-PSPooling.", "AI": {"tldr": "\u63d0\u51faPSPooling\u975e\u5b66\u4e60\u6027\u7f51\u683c\u6c60\u5316\u7b97\u5b50\u548cMedShapeNet19\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e3D\u89e3\u5256\u5f62\u72b6\u5206\u7c7b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u89e3\u5256\u611f\u77e5\u8868\u793a\uff0c\u5728\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u52563D\u5f62\u72b6\u5206\u7c7b\u53d7\u9650\u4e8e\u7f51\u683c\u6570\u636e\u590d\u6742\u6027\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u9700\u8981\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\u548c\u53ef\u590d\u73b0\u8bc4\u4f30\u3002", "method": "\u63d0\u51faPSPooling\u57fa\u4e8e\u51e0\u4f55\u90bb\u8fd1\u6027\u9884\u8ba1\u7b97\u8282\u70b9\u5bf9\u5e94\u96c6\uff0c\u5b9e\u73b0\u5e76\u884c\u5316\u53ef\u9006\u6c60\u5316\u548c\u53cd\u6c60\u5316\u64cd\u4f5c\uff1b\u6784\u5efa\u81ea\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u4ece\u65e0\u6807\u7b7e\u8868\u9762\u7f51\u683c\u5b66\u4e60\u89e3\u5256\u611f\u77e5\u8868\u793a\uff1b\u521b\u5efaMedShapeNet19\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "PSPooling\u663e\u8457\u6539\u5584\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e3a\u533b\u5b663D\u5f62\u72b6\u5b66\u4e60\u5efa\u7acb\u5f3a\u57fa\u7ebf\u3002", "conclusion": "PSPooling\u548cMedShapeNet19\u4e3a\u89e3\u5256\u5f62\u72b6\u5206\u7c7b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e0c\u671bMedShapeNet19\u6210\u4e3a\u533b\u5b663D\u5f62\u72b6\u5206\u6790\u7684\u5e7f\u6cdb\u91c7\u7528\u57fa\u51c6\u3002"}}
{"id": "2511.01617", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.01617", "abs": "https://arxiv.org/abs/2511.01617", "authors": ["Mohamed Eltahir", "Ali Habibullah", "Lama Ayash", "Tanveer Hussain", "Naeemullah Khan"], "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers", "comment": null, "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC", "AI": {"tldr": "ViC\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u5f02\u6784\u68c0\u7d22\u5668\u7684\u5019\u9009\u878d\u5408\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u5185\u5bb9\u8bc1\u636e\u548c\u68c0\u7d22\u5668\u5143\u6570\u636e\uff0c\u5728\u8de8\u6a21\u6001\u89c6\u9891\u68c0\u7d22\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u68c0\u7d22\u5668\u5728\u590d\u6742\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u89c6\u9891\uff09\u4e2d\u5019\u9009\u878d\u5408\u7684\u957f\u671f\u6311\u6218\uff0c\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6392\u540d\u6216\u5206\u6570\u4fe1\u53f7\u800c\u5ffd\u7565\u5019\u9009\u8868\u793a\u3002", "method": "\u63d0\u51faVote-in-Context (ViC)\u6846\u67b6\uff0c\u5c06\u5217\u8868\u91cd\u6392\u5e8f\u548c\u878d\u5408\u4f5c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u63a8\u7406\u4efb\u52a1\uff0c\u4f7f\u7528S-Grid\u7d27\u51d1\u5e8f\u5217\u5316\u5730\u56fe\u8868\u793a\u89c6\u9891\uff0c\u7ed3\u5408\u5185\u5bb9\u8bc1\u636e\u548c\u68c0\u7d22\u5668\u5143\u6570\u636e\u8fdb\u884c\u81ea\u9002\u5e94\u52a0\u6743\u3002", "result": "\u5728\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViC\u4f5c\u4e3a\u5355\u5217\u8868\u91cd\u6392\u5e8f\u5668\u663e\u8457\u63d0\u5347\u4e2a\u4f53\u68c0\u7d22\u5668\u7cbe\u5ea6\uff0c\u4f5c\u4e3a\u96c6\u6210\u878d\u5408\u5668\u6301\u7eed\u4f18\u4e8eCombSUM\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MSR-VTT\u548cVATEX\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u7d22\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0cRecall@1\u5f97\u5206\u5206\u522b\u8fbe\u523087.1%/89.0%\u548c99.6%\u3002", "conclusion": "ViC\u662f\u5c06\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u5f3a\u5927\u96f6\u6837\u672c\u91cd\u6392\u5e8f\u5668\u548c\u878d\u5408\u5668\u7684\u7b80\u5355\u3001\u53ef\u590d\u73b0\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01645", "abs": "https://arxiv.org/abs/2511.01645", "authors": ["Xiaogang Xu", "Ruihang Chu", "Jian Wang", "Kun Zhou", "Wenjie Shu", "Harry Yang", "Ser-Nam Lim", "Hao Chen", "Liang Lin"], "title": "Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward", "comment": null, "summary": "Reinforcement Learning (RL) has recently been incorporated into diffusion\nmodels, e.g., tasks such as text-to-image. However, directly applying existing\nRL methods to diffusion-based image restoration models is suboptimal, as the\nobjective of restoration fundamentally differs from that of pure generation: it\nplaces greater emphasis on fidelity. In this paper, we investigate how to\neffectively integrate RL into diffusion-based restoration models. First,\nthrough extensive experiments with various reward functions, we find that an\neffective reward can be derived from an Image Quality Assessment (IQA) model,\ninstead of intuitive ground-truth-based supervision, which has already been\noptimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,\nour strategy focuses on using RL for challenging samples that are significantly\ndistant from the ground truth, and our RL approach is innovatively implemented\nusing MLLM-based IQA models to align distributions with high-quality images\ninitially. As the samples approach the ground truth's distribution, RL is\nadaptively combined with SFT for more fine-grained alignment. This dynamic\nprocess is facilitated through an automatic weighting strategy that adjusts\nbased on the relative difficulty of the training samples. Our strategy is\nplug-and-play that can be seamlessly applied to diffusion-based restoration\nmodels, boosting its performance across various restoration tasks. Extensive\nexperiments across multiple benchmarks demonstrate the effectiveness of our\nproposed RL framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u6574\u5408\u5230\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u9488\u5bf9\u8fdc\u79bb\u771f\u5b9e\u503c\u7684\u56f0\u96be\u6837\u672c\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u4fee\u590d\u4efb\u52a1\u66f4\u5f3a\u8c03\u4fdd\u771f\u5ea6\u800c\u975e\u7eaf\u751f\u6210\u3002\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u5230\u6269\u6563\u4fee\u590d\u6a21\u578b\u4e2d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u7684\u5956\u52b1\u51fd\u6570\u66ff\u4ee3\u76f4\u89c2\u7684\u57fa\u4e8e\u771f\u5b9e\u503c\u7684\u76d1\u7763\uff1b\u9488\u5bf9\u8fdc\u79bb\u771f\u5b9e\u503c\u7684\u56f0\u96be\u6837\u672c\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff1b\u91c7\u7528MLLM-based IQA\u6a21\u578b\u8fdb\u884c\u5206\u5e03\u5bf9\u9f50\uff1b\u7ed3\u5408\u81ea\u9002\u5e94\u6743\u91cd\u7b56\u7565\u52a8\u6001\u8c03\u6574\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u7684\u7ec4\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\uff0c\u5728\u5404\u79cd\u4fee\u590d\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4fee\u590d\u6548\u679c\u3002"}}
{"id": "2511.01678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01678", "abs": "https://arxiv.org/abs/2511.01678", "authors": ["Ropeway Liu", "Hangjie Yuan", "Bo Dong", "Jiazheng Xing", "Jinwang Wang", "Rui Zhao", "Yan Xing", "Weihua Chen", "Fan Wang"], "title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback", "comment": "NeurIPS 2025", "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.", "AI": {"tldr": "UniLumos\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u50cf\u548c\u89c6\u9891\u91cd\u7167\u660e\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165RGB\u7a7a\u95f4\u7684\u51e0\u4f55\u53cd\u9988\u5230\u6d41\u5339\u914d\u4e3b\u5e72\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u7167\u660e\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8620\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u91cd\u7167\u660e\u4efb\u52a1\u4e2d\u7531\u4e8e\u5728\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u800c\u5bfc\u81f4\u7684\u7269\u7406\u4e0d\u6b63\u786e\u95ee\u9898\uff0c\u5982\u8fc7\u66dd\u9ad8\u5149\u3001\u9519\u4f4d\u9634\u5f71\u548c\u4e0d\u6b63\u786e\u906e\u6321\u3002", "method": "\u4f7f\u7528\u4ece\u8f93\u51fa\u4e2d\u63d0\u53d6\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u6765\u76d1\u7763\u6a21\u578b\uff0c\u5c06\u5149\u7167\u6548\u679c\u4e0e\u573a\u666f\u7ed3\u6784\u5bf9\u9f50\uff1b\u91c7\u7528\u8def\u5f84\u4e00\u81f4\u6027\u5b66\u4e60\u5b9e\u73b0\u5c11\u6b65\u8bad\u7ec3\uff1b\u8bbe\u8ba1\u4e86\u516d\u7ef4\u6807\u6ce8\u534f\u8bae\u7528\u4e8e\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "UniLumos\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u7167\u660e\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u91cd\u7167\u660e\u4e0a\u5747\u5b9e\u73b0\u4e8620\u500d\u52a0\u901f\u3002", "conclusion": "UniLumos\u901a\u8fc7\u51e0\u4f55\u53cd\u9988\u548c\u8def\u5f84\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91cd\u7167\u660e\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.01724", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01724", "abs": "https://arxiv.org/abs/2511.01724", "authors": ["Yi Zhang", "Zheng Wang", "Chen Zhen", "Wenjie Ruan", "Qing Guo", "Siddartha Khastgir", "Carsten Maple", "Xingyu Zhao"], "title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark", "comment": null, "summary": "Deep learning models are notoriously vulnerable to imperceptible\nperturbations. Most existing research centers on adversarial robustness (AR),\nwhich evaluates models under worst-case scenarios by examining the existence of\ndeterministic adversarial examples (AEs). In contrast, probabilistic robustness\n(PR) adopts a statistical perspective, measuring the probability that\npredictions remain correct under stochastic perturbations. While PR is widely\nregarded as a practical complement to AR, dedicated training methods for\nimproving PR are still relatively underexplored, albeit with emerging progress.\nAmong the few PR-targeted training methods, we identify three limitations: i\nnon-comparable evaluation protocols; ii limited comparisons to strong AT\nbaselines despite anecdotal PR gains from AT; and iii no unified framework to\ncompare the generalization of these methods. Thus, we introduce PRBench, the\nfirst benchmark dedicated to evaluating improvements in PR achieved by\ndifferent robustness training methods. PRBench empirically compares most common\nAT and PR-targeted training methods using a comprehensive set of metrics,\nincluding clean accuracy, PR and AR performance, training efficiency, and\ngeneralization error (GE). We also provide theoretical analysis on the GE of PR\nperformance across different training methods. Main findings revealed by\nPRBench include: AT methods are more versatile than PR-targeted training\nmethods in terms of improving both AR and PR performance across diverse\nhyperparameter settings, while PR-targeted training methods consistently yield\nlower GE and higher clean accuracy. A leaderboard comprising 222 trained models\nacross 7 datasets and 10 model architectures is publicly available at\nhttps://tmpspace.github.io/PRBenchLeaderboard/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PRBench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u4e0d\u540c\u9c81\u68d2\u6027\u8bad\u7ec3\u65b9\u6cd5\u5728\u6982\u7387\u9c81\u68d2\u6027\uff08PR\uff09\u65b9\u9762\u6539\u8fdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u8bad\u7ec3\uff08AT\uff09\u65b9\u6cd5\u5728\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6982\u7387\u9c81\u68d2\u6027\u65b9\u9762\u66f4\u5177\u901a\u7528\u6027\uff0c\u800cPR\u9488\u5bf9\u6027\u8bad\u7ec3\u65b9\u6cd5\u5728\u6cdb\u5316\u8bef\u5dee\u548c\u5e72\u51c0\u51c6\u786e\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u5fae\u5c0f\u6270\u52a8\u975e\u5e38\u8106\u5f31\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u9c81\u68d2\u6027\uff08AR\uff09\uff0c\u800c\u6982\u7387\u9c81\u68d2\u6027\uff08PR\uff09\u4ece\u7edf\u8ba1\u89d2\u5ea6\u8bc4\u4f30\u6a21\u578b\u5728\u968f\u673a\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u3002\u867d\u7136PR\u88ab\u89c6\u4e3aAR\u7684\u5b9e\u7528\u8865\u5145\uff0c\u4f46\u4e13\u95e8\u9488\u5bf9PR\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u76f8\u5bf9\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bc4\u4f30\u534f\u8bae\u4e0d\u53ef\u6bd4\u3001\u4e0e\u5f3aAT\u57fa\u7ebf\u6bd4\u8f83\u6709\u9650\u3001\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86PRBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5168\u9762\u7684\u6307\u6807\u96c6\uff08\u5305\u62ec\u5e72\u51c0\u51c6\u786e\u7387\u3001PR\u548cAR\u6027\u80fd\u3001\u8bad\u7ec3\u6548\u7387\u548c\u6cdb\u5316\u8bef\u5dee\uff09\u5bf9\u5e38\u89c1\u7684AT\u548cPR\u9488\u5bf9\u6027\u8bad\u7ec3\u65b9\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u5e76\u63d0\u4f9b\u4e86PR\u6027\u80fd\u6cdb\u5316\u8bef\u5dee\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1aAT\u65b9\u6cd5\u5728\u63d0\u5347AR\u548cPR\u6027\u80fd\u65b9\u9762\u66f4\u5177\u901a\u7528\u6027\uff0c\u800cPR\u9488\u5bf9\u6027\u8bad\u7ec3\u65b9\u6cd5\u59cb\u7ec8\u4ea7\u751f\u66f4\u4f4e\u7684\u6cdb\u5316\u8bef\u5dee\u548c\u66f4\u9ad8\u7684\u5e72\u51c0\u51c6\u786e\u7387\u3002\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b222\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u6392\u884c\u699c\uff0c\u6db5\u76d67\u4e2a\u6570\u636e\u96c6\u548c10\u79cd\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "PRBench\u4e3a\u8bc4\u4f30PR\u6539\u8fdb\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u672a\u6765PR\u8bad\u7ec3\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2511.01728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01728", "abs": "https://arxiv.org/abs/2511.01728", "authors": ["Tom Odem"], "title": "Toward Strategy Identification and Subtask Decomposition In Task Exploration", "comment": null, "summary": "This research builds on work in anticipatory human-machine interaction, a\nsubfield of human-machine interaction where machines can facilitate\nadvantageous interactions by anticipating a user's future state. The aim of\nthis research is to further a machine's understanding of user knowledge, skill,\nand behavior in pursuit of implicit coordination. A task explorer pipeline was\ndeveloped that uses clustering techniques, paired with factor analysis and\nstring edit distance, to automatically identify key global and local strategies\nthat are used to complete tasks. Global strategies identify generalized sets of\nactions used to complete tasks, while local strategies identify sequences that\nused those sets of actions in a similar composition. Additionally, meaningful\nsubtasks of various lengths are identified within the tasks. The task explorer\npipeline was able to automatically identify key strategies used to complete\ntasks and encode user runs with hierarchical subtask structures. In addition, a\nTask Explorer application was developed to easily review pipeline results. The\ntask explorer pipeline can be easily modified to any action-based time-series\ndata and the identified strategies and subtasks help to inform humans and\nmachines on user knowledge, skill, and behavior.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4efb\u52a1\u63a2\u7d22\u5668\u7ba1\u9053\uff0c\u4f7f\u7528\u805a\u7c7b\u6280\u672f\u3001\u56e0\u5b50\u5206\u6790\u548c\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\u81ea\u52a8\u8bc6\u522b\u5b8c\u6210\u4efb\u52a1\u7684\u5173\u952e\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\uff0c\u5e76\u8bc6\u522b\u4efb\u52a1\u4e2d\u6709\u610f\u4e49\u7684\u5b50\u4efb\u52a1\u3002", "motivation": "\u63a8\u8fdb\u673a\u5668\u5bf9\u7528\u6237\u77e5\u8bc6\u3001\u6280\u80fd\u548c\u884c\u4e3a\u7684\u7406\u89e3\uff0c\u4ee5\u5b9e\u73b0\u9690\u5f0f\u534f\u8c03\uff0c\u5728\u9884\u671f\u6027\u4eba\u673a\u4ea4\u4e92\u4e2d\u4fc3\u8fdb\u6709\u5229\u7684\u4e92\u52a8\u3002", "method": "\u5f00\u53d1\u4efb\u52a1\u63a2\u7d22\u5668\u7ba1\u9053\uff0c\u7ed3\u5408\u805a\u7c7b\u6280\u672f\u3001\u56e0\u5b50\u5206\u6790\u548c\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\uff0c\u81ea\u52a8\u8bc6\u522b\u5168\u5c40\u7b56\u7565\uff08\u5b8c\u6210\u4efb\u52a1\u7684\u52a8\u4f5c\u96c6\u5408\uff09\u548c\u5c40\u90e8\u7b56\u7565\uff08\u76f8\u4f3c\u52a8\u4f5c\u7ec4\u5408\u7684\u5e8f\u5217\uff09\uff0c\u5e76\u8bc6\u522b\u4e0d\u540c\u957f\u5ea6\u7684\u6709\u610f\u4e49\u7684\u5b50\u4efb\u52a1\u3002", "result": "\u4efb\u52a1\u63a2\u7d22\u5668\u7ba1\u9053\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u5b8c\u6210\u4efb\u52a1\u7684\u5173\u952e\u7b56\u7565\uff0c\u5e76\u7528\u5206\u5c42\u5b50\u4efb\u52a1\u7ed3\u6784\u7f16\u7801\u7528\u6237\u8fd0\u884c\u8f68\u8ff9\u3002\u5f00\u53d1\u4e86Task Explorer\u5e94\u7528\u7a0b\u5e8f\u6765\u8f7b\u677e\u67e5\u770b\u7ba1\u9053\u7ed3\u679c\u3002", "conclusion": "\u4efb\u52a1\u63a2\u7d22\u5668\u7ba1\u9053\u53ef\u8f7b\u677e\u4fee\u6539\u4ee5\u9002\u5e94\u4efb\u4f55\u57fa\u4e8e\u52a8\u4f5c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u8bc6\u522b\u51fa\u7684\u7b56\u7565\u548c\u5b50\u4efb\u52a1\u6709\u52a9\u4e8e\u4eba\u7c7b\u548c\u673a\u5668\u4e86\u89e3\u7528\u6237\u7684\u77e5\u8bc6\u3001\u6280\u80fd\u548c\u884c\u4e3a\u3002"}}
{"id": "2511.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01756", "abs": "https://arxiv.org/abs/2511.01756", "authors": ["Kai Zhai", "Ziyan Huang", "Qiang Nie", "Xiang Li", "Bo Ouyang"], "title": "HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain", "comment": null, "summary": "2D-to-3D human pose lifting is a fundamental challenge for 3D human pose\nestimation in monocular video, where graph convolutional networks (GCNs) and\nattention mechanisms have proven to be inherently suitable for encoding the\nspatial-temporal correlations of skeletal joints. However, depth ambiguity and\nerrors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous\nstudies have attempted to restrict jitters in the time domain, for instance, by\nconstraining the differences between adjacent frames while neglecting the\nglobal spatial-temporal correlations of skeletal joint motion. To tackle this\nproblem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid\nfeature aggregation and 3D trajectory consistency in the frequency domain.\nSpecifically, we propose a hop-hybrid graph attention (HGA) module and a\nTransformer encoder to model global joint spatial-temporal correlations. The\nHGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group\nto enlarge the receptive field and applies the attention mechanism to discover\nthe latent correlations of these groups globally. We then exploit global\ntemporal correlations by constraining trajectory consistency in the frequency\ndomain. To provide 3D information for depth inference across frames and\nmaintain coherence over time, a preliminary network is applied to estimate the\n3D pose. Extensive experiments were conducted on two standard benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed\nHGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional\naccuracy and temporal consistency.", "AI": {"tldr": "HGFreNet\u662f\u4e00\u4e2a\u65b0\u9896\u7684GraphFormer\u67b6\u6784\uff0c\u901a\u8fc7\u8df3\u6570\u6df7\u5408\u7279\u5f81\u805a\u5408\u548c\u9891\u57df3D\u8f68\u8ff9\u4e00\u81f4\u6027\u6765\u89e3\u51b32D\u52303D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u4e2d\u7684\u6df1\u5ea6\u6a21\u7cca\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u89e3\u51b32D\u52303D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u4e2d\u7684\u6df1\u5ea6\u6a21\u7cca\u548c2D\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u5bfc\u81f4\u76843D\u8f68\u8ff9\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u7ea6\u675f\u76f8\u90bb\u5e27\u5dee\u5f02\u800c\u5ffd\u7565\u4e86\u9aa8\u9abc\u5173\u8282\u8fd0\u52a8\u7684\u5168\u5c40\u65f6\u7a7a\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u8df3\u6570\u6df7\u5408\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u548cTransformer\u7f16\u7801\u5668\u5efa\u6a21\u5168\u5c40\u5173\u8282\u65f6\u7a7a\u76f8\u5173\u6027\uff0cHGA\u6a21\u5757\u5c06\u9aa8\u9abc\u5173\u8282\u7684k\u8df3\u90bb\u5c45\u5206\u7ec4\u4e3a\u6df7\u5408\u7ec4\u4ee5\u6269\u5927\u611f\u53d7\u91ce\uff0c\u5e76\u5728\u9891\u57df\u7ea6\u675f\u8f68\u8ff9\u4e00\u81f4\u6027\u6765\u5229\u7528\u5168\u5c40\u65f6\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGFreNet\u5728\u4f4d\u7f6e\u7cbe\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HGFreNet\u901a\u8fc7\u5168\u5c40\u65f6\u7a7a\u5efa\u6a21\u548c\u9891\u57df\u8f68\u8ff9\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u4e862D\u52303D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002"}}
{"id": "2511.01768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01768", "abs": "https://arxiv.org/abs/2511.01768", "authors": ["Zhe Liu", "Jinghua Hou", "Xiaoqing Ye", "Jingdong Wang", "Hengshuang Zhao", "Xiang Bai"], "title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs", "comment": null, "summary": "Although transformers have demonstrated remarkable capabilities across\nvarious domains, their quadratic attention mechanisms introduce significant\ncomputational overhead when processing long-sequence data. In this paper, we\npresent a unified autonomous driving model, UniLION, which efficiently handles\nlarge-scale LiDAR point clouds, high-resolution multi-view images, and even\ntemporal sequences based on the linear group RNN operator (i.e., performs\nlinear RNN for grouped features). Remarkably, UniLION serves as a single\nversatile architecture that can seamlessly support multiple specialized\nvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal\ntemporal fusion configurations) without requiring explicit temporal or\nmulti-modal fusion modules. Moreover, UniLION consistently delivers competitive\nand even state-of-the-art performance across a wide range of core tasks,\nincluding 3D perception (e.g., 3D object detection, 3D object tracking, 3D\noccupancy prediction, BEV map segmentation), prediction (e.g., motion\nprediction), and planning (e.g., end-to-end planning). This unified paradigm\nnaturally simplifies the design of multi-modal and multi-task autonomous\ndriving systems while maintaining superior performance. Ultimately, we hope\nUniLION offers a fresh perspective on the development of 3D foundation models\nin autonomous driving. Code is available at\nhttps://github.com/happinesslz/UniLION", "AI": {"tldr": "UniLION\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff0c\u4f7f\u7528\u7ebf\u6027\u7ec4RNN\u7b97\u5b50\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21LiDAR\u70b9\u4e91\u3001\u9ad8\u5206\u8fa8\u7387\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u65f6\u95f4\u5e8f\u5217\uff0c\u65e0\u9700\u663e\u5f0f\u7684\u65f6\u95f4\u6216\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u5728\u591a\u79cd\u6838\u5fc3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "Transformer\u7684\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u957f\u5e8f\u5217\u6570\u636e\u65f6\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u6765\u5904\u7406\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u57fa\u4e8e\u7ebf\u6027\u7ec4RNN\u7b97\u5b50\u6784\u5efa\u7edf\u4e00\u67b6\u6784\uff0c\u652f\u6301LiDAR-only\u3001\u65f6\u5e8fLiDAR\u3001\u591a\u6a21\u6001\u548c\u591a\u6a21\u6001\u65f6\u5e8f\u878d\u5408\u7b49\u591a\u79cd\u914d\u7f6e\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u878d\u5408\u6a21\u5757\u3002", "result": "\u57283D\u611f\u77e5\uff08\u7269\u4f53\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u5360\u7528\u9884\u6d4b\u3001BEV\u5730\u56fe\u5206\u5272\uff09\u3001\u9884\u6d4b\uff08\u8fd0\u52a8\u9884\u6d4b\uff09\u548c\u89c4\u5212\uff08\u7aef\u5230\u7aef\u89c4\u5212\uff09\u7b49\u6838\u5fc3\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u751a\u81f3\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UniLION\u4e3a\u81ea\u52a8\u9a7e\u9a763D\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u7b80\u5316\u4e86\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.01802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01802", "abs": "https://arxiv.org/abs/2511.01802", "authors": ["Tejas Sarnaik", "Manan Shah", "Ravi Hegde"], "title": "PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution", "comment": "Accepted in PReMI 2025", "summary": "Retrieval-Augmented Generation (RAG) has become a robust framework for\nenhancing Large Language Models (LLMs) with external knowledge. Recent advances\nin RAG have investigated graph based retrieval for intricate reasoning;\nhowever, the influence of prompt design on enhancing the retrieval and\nreasoning process is still considerably under-examined. In this paper, we\npresent a prompt-driven GraphRAG framework that underscores the significance of\nprompt formulation in facilitating entity extraction, fact selection, and\npassage reranking for multi-hop question answering. Our approach creates a\nsymbolic knowledge graph from text data by encoding entities and factual\nrelationships as structured facts triples. We use LLMs selectively during\nonline retrieval to perform semantic filtering and answer generation. We also\nuse entity-guided graph traversal through Personalized PageRank (PPR) to\nsupport efficient, scalable retrieval based on the knowledge graph we built.\nOur system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,\nwith F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,\nrespectively. These results show that prompt design is an important part of\nimproving retrieval accuracy and response quality. This research lays the\ngroundwork for more efficient and comprehensible multi-hop question-answering\nsystems, highlighting the importance of prompt-aware graph reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u9a71\u52a8\u7684GraphRAG\u6846\u67b6\uff0c\u5f3a\u8c03\u63d0\u793a\u8bbe\u8ba1\u5728\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5728HotpotQA\u548c2WikiMultiHopQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u5728\u590d\u6742\u63a8\u7406\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u63d0\u793a\u8bbe\u8ba1\u5bf9\u68c0\u7d22\u548c\u63a8\u7406\u8fc7\u7a0b\u7684\u5f71\u54cd\u4ecd\u88ab\u4e25\u91cd\u5ffd\u89c6\uff0c\u9700\u8981\u63a2\u7d22\u63d0\u793a\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "method": "\u6784\u5efa\u7b26\u53f7\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u5b9e\u4f53\u548c\u4e8b\u5b9e\u5173\u7cfb\uff0c\u4f7f\u7528LLM\u8fdb\u884c\u8bed\u4e49\u8fc7\u6ee4\u548c\u7b54\u6848\u751f\u6210\uff0c\u901a\u8fc7\u4e2a\u6027\u5316PageRank\u5b9e\u73b0\u57fa\u4e8e\u5b9e\u4f53\u7684\u56fe\u904d\u5386\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u68c0\u7d22\u3002", "result": "\u5728HotpotQA\u548c2WikiMultiHopQA\u4e0a\u5206\u522b\u8fbe\u523080.7%\u548c78.9%\u7684F1\u5206\u6570\uff0cRecall@5\u5206\u522b\u8fbe\u523097.1%\u548c98.1%\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u662f\u63d0\u9ad8\u68c0\u7d22\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u66f4\u9ad8\u6548\u548c\u53ef\u7406\u89e3\u7684\u591a\u8df3\u95ee\u7b54\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u51f8\u663e\u4e86\u63d0\u793a\u611f\u77e5\u56fe\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.01817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01817", "abs": "https://arxiv.org/abs/2511.01817", "authors": ["Sagi Eppel", "Alona Strugatski"], "title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art", "comment": null, "summary": "The ability to connect visual patterns with the processes that form them\nrepresents one of the deepest forms of visual understanding. Textures of clouds\nand waves, the growth of cities and forests, or the formation of materials and\nlandscapes are all examples of patterns emerging from underlying mechanisms. We\npresent the Scitextures dataset, a large-scale collection of textures and\nvisual patterns from all domains of science, tech, and art, along with the\nmodels and code that generate these images. Covering over 1,200 different\nmodels and 100,000 images of patterns and textures from physics, chemistry,\nbiology, sociology, technology, mathematics, and art, this dataset offers a way\nto explore the connection between the visual patterns that shape our world and\nthe mechanisms that produce them. Created by an agentic AI pipeline that\nautonomously collects and implements models in standardized form, we use\nSciTextures to evaluate the ability of leading AI models to link visual\npatterns to the models and code that generate them, and to identify different\npatterns that emerged from the same process. We also test AIs ability to infer\nand recreate the mechanisms behind visual patterns by providing a natural image\nof a real-world pattern and asking the AI to identify, model, and code the\nmechanism that formed the pattern, then run this code to generate a simulated\nimage that is compared to the real image. These benchmarks show that\nvision-language models (VLMs) can understand and simulate the physical system\nbeyond a visual pattern. The dataset and code are available at:\nhttps://zenodo.org/records/17485502", "AI": {"tldr": "Scitextures\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u79d1\u5b66\u7eb9\u7406\u548c\u89c6\u89c9\u6a21\u5f0f\u96c6\u5408\uff0c\u5305\u542b1200\u591a\u4e2a\u6a21\u578b\u548c10\u4e07\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u3001\u793e\u4f1a\u5b66\u3001\u6280\u672f\u3001\u6570\u5b66\u548c\u827a\u672f\u7b49\u9886\u57df\uff0c\u7528\u4e8e\u63a2\u7d22\u89c6\u89c9\u6a21\u5f0f\u4e0e\u751f\u6210\u673a\u5236\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u6a21\u5f0f\u4e0e\u5f62\u6210\u673a\u5236\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u7406\u89e3\u4e91\u5c42\u3001\u6ce2\u6d6a\u3001\u57ce\u5e02\u53d1\u5c55\u3001\u68ee\u6797\u751f\u957f\u7b49\u81ea\u7136\u548c\u4eba\u5de5\u6a21\u5f0f\u80cc\u540e\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u81ea\u4e3bAI\u7ba1\u9053\u6536\u96c6\u548c\u5b9e\u73b0\u6807\u51c6\u5316\u6a21\u578b\uff0c\u521b\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30AI\u6a21\u578b\u5728\u8fde\u63a5\u89c6\u89c9\u6a21\u5f0f\u4e0e\u751f\u6210\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6a21\u5f0f\u8bc6\u522b\u3001\u673a\u5236\u63a8\u65ad\u548c\u56fe\u50cf\u751f\u6210\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u6a21\u62df\u7269\u7406\u7cfb\u7edf\uff0c\u8d85\u8d8a\u5355\u7eaf\u7684\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\uff0c\u80fd\u591f\u63a8\u65ad\u5e76\u91cd\u65b0\u521b\u5efa\u5f62\u6210\u89c6\u89c9\u6a21\u5f0f\u7684\u673a\u5236\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86AI\u5728\u7406\u89e3\u89c6\u89c9\u6a21\u5f0f\u4e0e\u751f\u6210\u673a\u5236\u4e4b\u95f4\u8054\u7cfb\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2511.01833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01833", "abs": "https://arxiv.org/abs/2511.01833", "authors": ["Ming Li", "Jike Zhong", "Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Yuxiang Lai", "Wei Chen", "Konstantinos Psounis", "Kaipeng Zhang"], "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning", "comment": "Preprint", "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-\\textit{with}-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-\\textit{with}-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86TIR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u667a\u80fd\u56fe\u50cf\u5904\u7406\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e8622\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u572813\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u6355\u6349\u50cfOpenAI o3\u8fd9\u6837\u7684\u6a21\u578b\u5728\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u667a\u80fd\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b13\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u7684TIR-Bench\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u9700\u8981\u5728\u601d\u7ef4\u94fe\u4e2d\u8fdb\u884c\u65b0\u9896\u7684\u56fe\u50cf\u5904\u7406\u5de5\u5177\u4f7f\u7528\u3002", "result": "TIR-Bench\u5bf9\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u90fd\u5177\u6709\u6311\u6218\u6027\uff0c\u53ea\u6709\u5177\u5907\u771f\u6b63\u56fe\u50cf\u601d\u7ef4\u80fd\u529b\u7684\u6a21\u578b\u624d\u80fd\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u76f4\u63a5\u5fae\u8c03\u4e0e\u667a\u80fd\u5fae\u8c03\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u667a\u80fd\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002"}}
